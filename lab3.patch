From 39b167660d3e810564dad3a31f445bb2fc6b2874 Mon Sep 17 00:00:00 2001
From: Li Hongyu <543306408@qq.com>
Date: Sun, 14 May 2023 18:27:48 +0800
Subject: [PATCH] lab4 patch

---
 .gitignore                             |    3 +
 arch/arm64/Kconfig                     |    2 +
 drivers/Makefile                       |    2 +
 drivers/rros/Kconfig                   |    0
 drivers/rros/Makefile                  |    1 +
 drivers/rros/latmus.rs                 |  117 ++
 include/asm-generic/evl/irq.h          |   21 +
 include/asm-generic/evl/mm_info.h      |   28 +
 include/asm-generic/evl/netdevice.h    |   55 +
 include/asm-generic/evl/poll.h         |   34 +
 include/asm-generic/evl/thread_info.h  |   30 +
 include/dovetail/mm_info.h             |    1 +
 include/dovetail/netdevice.h           |    5 +-
 include/linux/netdevice.h              |    2 +-
 kernel/Kconfig.rros                    |   28 +
 kernel/dovetail.c                      |   25 +-
 kernel/rros/Kconfig                    |   12 +
 kernel/rros/arch/Makefile              |    0
 kernel/rros/arch/arm64/Makefile        |    0
 kernel/rros/arch/arm64/mod.rs          |    2 +
 kernel/rros/arch/arm64/syscall.rs      |   95 +
 kernel/rros/arch/mod.rs                |    2 +
 kernel/rros/clock.rs                   |  446 +++--
 kernel/rros/clock_test.rs              |   83 +-
 kernel/rros/cred.rs                    |    1 +
 kernel/rros/crossing.rs                |   98 +-
 kernel/rros/double_linked_list_test.rs |   27 +-
 kernel/rros/factory.rs                 |  847 +++++++--
 kernel/rros/fifo.rs                    |  261 +--
 kernel/rros/fifo_test.rs               |  191 +-
 kernel/rros/file.rs                    |  287 ++-
 kernel/rros/flags.rs                   |   56 +
 kernel/rros/idle.rs                    |   54 +-
 kernel/rros/init.rs                    |   92 +-
 kernel/rros/list.rs                    |   42 +-
 kernel/rros/list_test.rs               |   56 +-
 kernel/rros/lock.rs                    |   26 +-
 kernel/rros/memory.rs                  |    2 +-
 kernel/rros/memory_test.rs             |    0
 kernel/rros/monitor.rs                 |  215 ++-
 kernel/rros/mutex.rs                   | 1216 ++++++++++++
 kernel/rros/net.rs                     |   88 -
 kernel/rros/net/Makefile               |    0
 kernel/rros/net/constant.rs            |    2 +
 kernel/rros/net/device.rs              |  397 ++++
 kernel/rros/net/ethernet/Makefile      |    0
 kernel/rros/net/ethernet/input.rs      |  140 ++
 kernel/rros/net/ethernet/mod.rs        |    2 +
 kernel/rros/net/ethernet/output.rs     |   33 +
 kernel/rros/net/input.rs               |  141 ++
 kernel/rros/net/mod.rs                 |   80 +
 kernel/rros/net/output.rs              |  182 ++
 kernel/rros/net/packet.rs              |  578 ++++++
 kernel/rros/net/qdisc.rs               |   17 +
 kernel/rros/net/skb.rs                 |  614 ++++++
 kernel/rros/net/socket.rs              |  774 ++++++++
 kernel/rros/queue.rs                   |    1 -
 kernel/rros/rros/Kconfig               |    0
 kernel/rros/rros/Makefile              |    1 +
 kernel/rros/rros/latmus.rs             |  210 +++
 kernel/rros/rros/mod.rs                |    1 +
 kernel/rros/sched.rs                   | 1824 +++++++++++-------
 kernel/rros/sched_test.rs              |   39 +-
 kernel/rros/stat.rs                    |   96 +-
 kernel/rros/stat_test.rs               |   20 +-
 kernel/rros/syscall.rs                 |  472 +++++
 kernel/rros/test.rs                    |  146 +-
 kernel/rros/test_thread.rs             |   41 +
 kernel/rros/thread.rs                  | 2361 ++++++++++++++++++------
 kernel/rros/thread_test.rs             |  357 ++--
 kernel/rros/tick.rs                    |  336 ++--
 kernel/rros/timeout.rs                 |    6 +-
 kernel/rros/timer.rs                   |  318 ++--
 kernel/rros/timer_test.rs              |  109 +-
 kernel/rros/tp.rs                      |  701 +++++++
 kernel/rros/types.rs                   |  349 ++++
 kernel/rros/types_test.rs              |  392 ++++
 kernel/rros/uapi/Makefile              |    0
 kernel/rros/uapi/mod.rs                |    1 +
 kernel/rros/uapi/rros/Makefile         |    0
 kernel/rros/uapi/rros/mod.rs           |    2 +
 kernel/rros/uapi/rros/syscall.rs       |    5 +
 kernel/rros/uapi/rros/thread.rs        |    1 +
 kernel/rros/wait.rs                    |  293 ++-
 kernel/rros/weak.rs                    |    8 +-
 kernel/rros/work.rs                    |   84 +
 mm/slub.c                              |    4 +-
 rust/alloc/alloc_rros.rs               |  210 +++
 rust/alloc/lib.rs                      |    2 +-
 rust/alloc/rc.rs                       | 1226 +++++++++---
 rust/alloc/string.rs                   |  438 +++--
 rust/alloc/sync.rs                     | 1257 ++++++++++---
 rust/helpers.c                         |  463 ++++-
 rust/kernel/allocator.rs               |    9 +-
 rust/kernel/bindings_helper.h          |    7 +
 rust/kernel/bitmap.rs                  |   20 +-
 rust/kernel/chrdev.rs                  |   13 +-
 rust/kernel/class.rs                   |    4 +-
 rust/kernel/clockchips.rs              |  123 +-
 rust/kernel/completion.rs              |   13 +-
 rust/kernel/cpumask.rs                 |   38 +-
 rust/kernel/device.rs                  |    2 +-
 rust/kernel/double_linked_list.rs      |  366 ++--
 rust/kernel/double_linked_list2.rs     |    6 +-
 rust/kernel/double_linked_list_test.rs |    1 +
 rust/kernel/dovetail.rs                |    6 +-
 rust/kernel/error.rs                   |   18 +
 rust/kernel/file.rs                    |    6 +-
 rust/kernel/file_operations.rs         |   10 +-
 rust/kernel/fs.rs                      |   19 +-
 rust/kernel/irq_pipeline.rs            |    8 +-
 rust/kernel/irq_work.rs                |   24 +-
 rust/kernel/kthread.rs                 |    7 +-
 rust/kernel/ktime.rs                   |   25 +-
 rust/kernel/lib.rs                     |   23 +-
 rust/kernel/linked_list.rs             |   27 +-
 rust/kernel/memory_rros.rs             |    0
 rust/kernel/memory_rros_test.rs        |    0
 rust/kernel/net.rs                     |   13 +-
 rust/kernel/percpu_defs.rs             |   25 +-
 rust/kernel/raw_list.rs                |   49 +-
 rust/kernel/sync/mutex.rs              |    2 +-
 rust/kernel/sync/spinlock.rs           |   31 +-
 rust/kernel/sysfs.rs                   |    2 +-
 rust/kernel/task.rs                    |    6 +-
 rust/kernel/types.rs                   |   17 +-
 rust/kernel/uapi/mod.rs                |    1 +
 rust/kernel/uapi/time_types.rs         |   25 +
 rust/kernel/uidgid.rs                  |    6 +-
 rust/kernel/user_ptr.rs                |    5 +-
 rust/kernel/workqueue.rs               |    4 +-
 scripts/Makefile.build                 |    2 +-
 132 files changed, 16581 insertions(+), 3669 deletions(-)
 create mode 100644 drivers/rros/Kconfig
 create mode 100644 drivers/rros/Makefile
 create mode 100644 drivers/rros/latmus.rs
 create mode 100644 include/asm-generic/evl/irq.h
 create mode 100644 include/asm-generic/evl/mm_info.h
 create mode 100644 include/asm-generic/evl/netdevice.h
 create mode 100644 include/asm-generic/evl/poll.h
 create mode 100644 include/asm-generic/evl/thread_info.h
 create mode 100644 kernel/Kconfig.rros
 create mode 100644 kernel/rros/Kconfig
 create mode 100644 kernel/rros/arch/Makefile
 create mode 100644 kernel/rros/arch/arm64/Makefile
 create mode 100644 kernel/rros/arch/arm64/mod.rs
 create mode 100644 kernel/rros/arch/arm64/syscall.rs
 create mode 100644 kernel/rros/arch/mod.rs
 create mode 100644 kernel/rros/cred.rs
 create mode 100644 kernel/rros/flags.rs
 create mode 100644 kernel/rros/memory_test.rs
 create mode 100644 kernel/rros/mutex.rs
 delete mode 100644 kernel/rros/net.rs
 create mode 100644 kernel/rros/net/Makefile
 create mode 100644 kernel/rros/net/constant.rs
 create mode 100644 kernel/rros/net/device.rs
 create mode 100644 kernel/rros/net/ethernet/Makefile
 create mode 100644 kernel/rros/net/ethernet/input.rs
 create mode 100644 kernel/rros/net/ethernet/mod.rs
 create mode 100644 kernel/rros/net/ethernet/output.rs
 create mode 100644 kernel/rros/net/input.rs
 create mode 100644 kernel/rros/net/mod.rs
 create mode 100644 kernel/rros/net/output.rs
 create mode 100644 kernel/rros/net/packet.rs
 create mode 100644 kernel/rros/net/qdisc.rs
 create mode 100644 kernel/rros/net/skb.rs
 create mode 100644 kernel/rros/net/socket.rs
 create mode 100644 kernel/rros/rros/Kconfig
 create mode 100644 kernel/rros/rros/Makefile
 create mode 100644 kernel/rros/rros/latmus.rs
 create mode 100644 kernel/rros/rros/mod.rs
 create mode 100644 kernel/rros/syscall.rs
 create mode 100644 kernel/rros/test_thread.rs
 create mode 100644 kernel/rros/tp.rs
 create mode 100644 kernel/rros/types.rs
 create mode 100644 kernel/rros/types_test.rs
 create mode 100644 kernel/rros/uapi/Makefile
 create mode 100644 kernel/rros/uapi/mod.rs
 create mode 100644 kernel/rros/uapi/rros/Makefile
 create mode 100644 kernel/rros/uapi/rros/mod.rs
 create mode 100644 kernel/rros/uapi/rros/syscall.rs
 create mode 100644 kernel/rros/uapi/rros/thread.rs
 create mode 100644 kernel/rros/work.rs
 create mode 100644 rust/alloc/alloc_rros.rs
 create mode 100644 rust/kernel/memory_rros.rs
 create mode 100644 rust/kernel/memory_rros_test.rs
 create mode 100644 rust/kernel/uapi/mod.rs
 create mode 100644 rust/kernel/uapi/time_types.rs

diff --git a/.gitignore b/.gitignore
index aab78d7c3..3a4f92a5b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -50,6 +50,7 @@
 *.zst
 *.exp
 *.text
+*.py
 Module.symvers
 modules.order
 
@@ -170,3 +171,5 @@ sphinx_*/
 /rust-project.json
 
 compile.txt
+
+t1.txt
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 26a2f2bef..e5ed0c651 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -163,6 +163,7 @@ config ARM64
 	select HAVE_ARCH_TRACEHOOK
 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 	select HAVE_ARCH_VMAP_STACK
+	select HAVE_ARCH_RROS
 	select HAVE_ARM_SMCCC
 	select HAVE_ASM_MODVERSIONS
 	select HAVE_EBPF_JIT
@@ -1076,6 +1077,7 @@ config CC_HAVE_SHADOW_CALL_STACK
 	def_bool $(cc-option, -fsanitize=shadow-call-stack -ffixed-x18)
 
 source "kernel/Kconfig.dovetail"
+source "kernel/Kconfig.rros"
 
 config PARAVIRT
 	bool "Enable paravirtualization code"
diff --git a/drivers/Makefile b/drivers/Makefile
index 5a6d613e8..b1ec802a6 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -159,6 +159,8 @@ obj-$(CONFIG_REMOTEPROC)	+= remoteproc/
 obj-$(CONFIG_RPMSG)		+= rpmsg/
 obj-$(CONFIG_SOUNDWIRE)		+= soundwire/
 
+obj-y		+= rros/
+
 # Virtualization drivers
 obj-$(CONFIG_VIRT_DRIVERS)	+= virt/
 obj-$(CONFIG_HYPERV)		+= hv/
diff --git a/drivers/rros/Kconfig b/drivers/rros/Kconfig
new file mode 100644
index 000000000..e69de29bb
diff --git a/drivers/rros/Makefile b/drivers/rros/Makefile
new file mode 100644
index 000000000..0e5299896
--- /dev/null
+++ b/drivers/rros/Makefile
@@ -0,0 +1 @@
+# obj-y				+= latmus.o
\ No newline at end of file
diff --git a/drivers/rros/latmus.rs b/drivers/rros/latmus.rs
new file mode 100644
index 000000000..1a8d3e197
--- /dev/null
+++ b/drivers/rros/latmus.rs
@@ -0,0 +1,117 @@
+#![no_std]
+#![feature(allocator_api, global_asm)]
+use init::{
+    thread::{self, rros_sleep, KthreadRunner},
+};
+use rros::thread_test::KthreadRunner;
+
+static mut kthread_runner_1: KthreadRunner = KthreadRunner::new_empty();
+
+fn kthread_handler(ptr: *mut c_types::c_void) {
+    let k_runner = unsafe{&mut *(ptr as *mut KthreadRunner)};
+// void kthread_handler(void *arg)
+// {
+// 	struct kthread_runner *k_runner = arg;
+// 	ktime_t now;
+// 	int ret = 0;
+    // TODO: add the ret value
+    let kernel_lantency = [0; 1000];
+    let mut ret = 0;
+
+    loop {
+        // TODO: add the should stop function
+        // if thread::should_stop() {
+        //     break;
+        // }
+
+        // TODO: add the wait flag function
+        // ret = evl_wait_flag(&k_runner->barrier);
+        // if (ret)
+        //     break;
+
+        // TODO: change the runner period flag when change
+        ret = thread::rros_set_period(&clock::RROS_MONO_CLOCK,
+                k_runner.start_time,
+                k_runner.runner.period);
+        
+        // TODO: error handle
+        // if (ret)
+        //     break;
+
+        loop {
+            ret = timer::rros_wait_period(None);
+            if (ret && ret != -ETIMEDOUT) {
+                done_sampling(&k_runner.runner, ret);
+                rros_stop_kthread(&k_runner.kthread);
+                return;
+            }
+
+            now = timer::rros_read_clock(&clock::RROS_MONO_CLOCK);
+            if (k_runner->runner.add_sample(&k_runner->runner, now)) {
+                thread::rros_set_period(None, 0, 0);
+                break;
+            }
+        }
+    }
+// 	for (;;) {
+// 		if (evl_kthread_should_stop())
+// 			break;
+
+// 		ret = evl_wait_flag(&k_runner->barrier);
+// 		if (ret)
+// 			break;
+
+// 		ret = evl_set_period(&evl_mono_clock,
+// 				k_runner->start_time,
+// 				k_runner->runner.period);
+// 		if (ret)
+// 			break;
+
+// 		for (;;) {
+// 			ret = evl_wait_period(NULL);
+// 			if (ret && ret != -ETIMEDOUT)
+// 				goto out;
+
+// 			now = evl_read_clock(&evl_mono_clock);
+// 			if (k_runner->runner.add_sample(&k_runner->runner, now)) {
+// 				evl_set_period(NULL, 0, 0);
+// 				break;
+// 			}
+// 		}
+// 	}
+// out:
+// 	done_sampling(&k_runner->runner, ret);
+// 	evl_stop_kthread(&k_runner->kthread);
+// }
+}
+
+fn test_latmus() {
+    kthread_runner_1.init(Box::try_new(move || {
+        kthread_handler(&mut kthread_runner_1 as *mut KthreadRunner as *mut c_types::c_void);
+    }).unwrap());
+    kthread_runner_1.run(c_str!("latmus_thread"));
+}
+
+// TODO: move this to a file
+// struct Latmus;
+
+// impl KernelModule for Latmus {
+//     fn init() -> Result<Self> {
+//         // unsafe{Arc::try_new(SpinLock::new(rros_thread::new().unwrap())).unwrap()},
+//         unsafe{
+//             kthread_runner_1.init(Box::try_new(move || {
+//                 kthread_handler(&mut kthread_runner_1 as *mut KthreadRunner as *mut c_types::c_void);
+//             }).unwrap());
+//             kthread_runner_1.run(c_str!("latmus_thread"));
+//         }
+
+//         pr_info!("Hello world from latmus!\n");
+//         Ok(Rros)
+//     }
+// }
+
+// impl Drop for Rros {
+//     fn drop(&mut self) {
+//         pr_info!("Bye world from latmus!\n");
+//     }
+// }
diff --git a/include/asm-generic/evl/irq.h b/include/asm-generic/evl/irq.h
new file mode 100644
index 000000000..ea3d047c3
--- /dev/null
+++ b/include/asm-generic/evl/irq.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_IRQ_H
+#define _ASM_GENERIC_EVL_IRQ_H
+
+#include <evl/irq.h>
+
+static inline void irq_enter_pipeline(void)
+{
+#ifdef CONFIG_EVL
+	evl_enter_irq();
+#endif
+}
+
+static inline void irq_exit_pipeline(void)
+{
+#ifdef CONFIG_EVL
+	evl_exit_irq();
+#endif
+}
+
+#endif /* !_ASM_GENERIC_EVL_IRQ_H */
diff --git a/include/asm-generic/evl/mm_info.h b/include/asm-generic/evl/mm_info.h
new file mode 100644
index 000000000..092b3aab8
--- /dev/null
+++ b/include/asm-generic/evl/mm_info.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_MM_INFO_H
+#define _ASM_GENERIC_EVL_MM_INFO_H
+
+#ifdef CONFIG_EVL
+
+#include <linux/list.h>
+#include <evl/wait.h>
+
+#define EVL_MM_PTSYNC_BIT  0
+#define EVL_MM_ACTIVE_BIT  30
+#define EVL_MM_INIT_BIT    31
+
+struct evl_wait_queue;
+
+struct oob_mm_state {
+	unsigned long flags;	/* Guaranteed zero initially. */
+	struct list_head ptrace_sync;
+	struct evl_wait_queue ptsync_barrier;
+};
+
+#else
+
+struct oob_mm_state { };
+
+#endif	/* !CONFIG_EVL */
+
+#endif /* !_ASM_GENERIC_EVL_MM_INFO_H */
diff --git a/include/asm-generic/evl/netdevice.h b/include/asm-generic/evl/netdevice.h
new file mode 100644
index 000000000..700bea884
--- /dev/null
+++ b/include/asm-generic/evl/netdevice.h
@@ -0,0 +1,55 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_NETDEVICE_H
+#define _ASM_GENERIC_EVL_NETDEVICE_H
+
+// #ifdef CONFIG_EVL
+
+// #include <linux/list.h>
+// #include <evl/wait.h>
+// #include <evl/poll.h>
+// #include <evl/flag.h>
+// #include <evl/crossing.h>
+
+// struct evl_net_qdisc;
+// struct evl_kthread;
+
+// struct evl_net_skb_queue {
+// 	struct list_head queue;
+// 	hard_spinlock_t lock;
+// };
+
+// struct evl_netdev_state {
+// 	/* Buffer pool management */
+// 	struct list_head free_skb_pool;
+// 	size_t pool_free;
+// 	size_t pool_max;
+// 	size_t buf_size;
+// 	struct evl_wait_queue pool_wait;
+// 	struct evl_poll_head poll_head;
+// 	/* RX handling */
+// 	struct evl_kthread *rx_handler;
+// 	struct evl_flag rx_flag;
+// 	struct evl_net_skb_queue rx_queue;
+// 	/* TX handling */
+// 	struct evl_net_qdisc *qdisc;
+// 	struct evl_kthread *tx_handler;
+// 	struct evl_flag tx_flag;
+// 	/* Count of oob ports referring to this device. */
+// 	int refs;
+// };
+
+// struct oob_netdev_state {
+// 	struct evl_netdev_state *estate;
+// 	struct evl_crossing crossing;
+// 	struct list_head next;
+// };
+
+// #else
+
+struct oob_netdev_state {
+	void * wrapper;
+};
+
+// #endif	/* !CONFIG_EVL */
+
+#endif /* !_ASM_GENERIC_EVL_NETDEVICE_H */
diff --git a/include/asm-generic/evl/poll.h b/include/asm-generic/evl/poll.h
new file mode 100644
index 000000000..f7e15b75d
--- /dev/null
+++ b/include/asm-generic/evl/poll.h
@@ -0,0 +1,34 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_POLL_H
+#define _ASM_GENERIC_EVL_POLL_H
+
+#ifdef CONFIG_EVL
+
+#include <linux/list.h>
+
+/*
+ * Poll operation descriptor for f_op->oob_poll.  Can be attached
+ * concurrently to at most EVL_POLL_NR_CONNECTORS poll heads.
+ */
+#define EVL_POLL_NR_CONNECTORS  4
+
+struct evl_poll_head;
+
+struct oob_poll_wait {
+	struct evl_poll_connector {
+		struct evl_poll_head *head;
+		struct list_head next;
+		void (*unwatch)(struct evl_poll_head *head);
+		int events_received;
+		int index;
+	} connectors[EVL_POLL_NR_CONNECTORS];
+};
+
+#else
+
+struct oob_poll_wait {
+};
+
+#endif	/* !CONFIG_EVL */
+
+#endif /* !_ASM_GENERIC_EVL_POLL_H */
diff --git a/include/asm-generic/evl/thread_info.h b/include/asm-generic/evl/thread_info.h
new file mode 100644
index 000000000..2aa2a697a
--- /dev/null
+++ b/include/asm-generic/evl/thread_info.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_THREAD_INFO_H
+#define _ASM_GENERIC_EVL_THREAD_INFO_H
+
+#ifdef CONFIG_EVL
+
+struct evl_thread;
+struct evl_subscriber;
+
+struct oob_thread_state {
+	struct evl_thread *thread;
+	struct evl_subscriber *subscriber;
+	int preempt_count;
+};
+
+static inline
+void evl_init_thread_state(struct oob_thread_state *p)
+{
+	p->thread = NULL;
+	p->subscriber = NULL;
+	p->preempt_count = 0;
+}
+
+#else
+
+struct oob_thread_state { };
+
+#endif	/* !CONFIG_EVL */
+
+#endif /* !_ASM_GENERIC_EVL_THREAD_INFO_H */
diff --git a/include/dovetail/mm_info.h b/include/dovetail/mm_info.h
index 504bd1d87..79d987309 100644
--- a/include/dovetail/mm_info.h
+++ b/include/dovetail/mm_info.h
@@ -7,6 +7,7 @@
  */
 
 struct oob_mm_state {
+    unsigned int flags;
 };
 
 #endif /* !_DOVETAIL_MM_INFO_H */
diff --git a/include/dovetail/netdevice.h b/include/dovetail/netdevice.h
index 06e8205e2..583ae6585 100644
--- a/include/dovetail/netdevice.h
+++ b/include/dovetail/netdevice.h
@@ -6,8 +6,11 @@
  * Placeholder for per-device state information defined by the
  * out-of-band network stack.
  */
-
+#ifndef _ASM_GENERIC_EVL_NETDEVICE_H
 struct oob_netdev_state {
+    void * wrapper;
 };
 
+#endif /* !_ASM_GENERIC_EVL_NETDEVICE_H */
+
 #endif /* !_DOVETAIL_NETDEVICE_H */
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 8ec389795..014b09b40 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -4348,7 +4348,7 @@ static inline void netdev_disable_oob_port(struct net_device *dev)
 	dev->oob_context.flags &= ~IFF_OOB_PORT;
 }
 
-static inline bool netdev_is_oob_port(struct net_device *dev)
+static bool netdev_is_oob_port(struct net_device *dev)
 {
 	return !!(dev->oob_context.flags & IFF_OOB_PORT);
 }
diff --git a/kernel/Kconfig.rros b/kernel/Kconfig.rros
new file mode 100644
index 000000000..9a3a7f237
--- /dev/null
+++ b/kernel/Kconfig.rros
@@ -0,0 +1,28 @@
+# Rros real-time core
+config HAVE_ARCH_RROS
+	bool
+
+menuconfig Rros
+	bool "BUPT real-time core"
+	depends on HAVE_ARCH_RROS
+	select DOVETAIL
+	select DEVTMPFS
+	help
+
+	  The Rros core is a real-time component of the Linux kernel,
+	  which delivers very short and bounded response time to
+	  interrupt and task events.  Rros runs asynchronously to the
+	  common kernel services, on the high-priority, out-of-band
+	  stage managed by the Dovetail layer.
+
+if Rros
+
+source "kernel/rros/Kconfig"
+
+if WARN_CPUFREQ_GOVERNOR
+comment "WARNING! CPU_FREQ governors other than 'performance'"
+comment "or 'powersave' may significantly increase latency"
+comment "on this platform during the frequency transitions."
+endif
+
+endif
diff --git a/kernel/dovetail.c b/kernel/dovetail.c
index 0bcb0f705..c8fc7dfb9 100644
--- a/kernel/dovetail.c
+++ b/kernel/dovetail.c
@@ -65,15 +65,20 @@ void dovetail_stop_altsched(void)
 }
 EXPORT_SYMBOL_GPL(dovetail_stop_altsched);
 
-void __weak handle_oob_syscall(struct pt_regs *regs)
-{
-}
+extern void handle_oob_syscall(struct pt_regs *regs);
 
-int __weak handle_pipelined_syscall(struct irq_stage *stage,
-				    struct pt_regs *regs)
-{
-	return 0;
-}
+// void __weak handle_oob_syscall(struct pt_regs *regs)
+// {
+// }
+
+extern int handle_pipelined_syscall(struct irq_stage *stage,
+				    struct pt_regs *regs);
+
+// int __weak handle_pipelined_syscall(struct irq_stage *stage,
+// 				    struct pt_regs *regs)
+// {
+// 	return 0;
+// }
 
 void __weak handle_oob_mayday(struct pt_regs *regs)
 {
@@ -278,7 +283,7 @@ extern void rust_handle_inband_event(enum inband_event_type event, void *data);
 
 void __weak handle_inband_event(enum inband_event_type event, void *data)
 {
-	pr_info("rust_handle_inband_event in");
+	// pr_info("rust_handle_inband_event in");
 	rust_handle_inband_event(event,data);
 }
 
@@ -295,7 +300,7 @@ extern void rust_resume_oob_task(void *ptr);
 void __weak resume_oob_task(struct task_struct *p)
 {
 	void *thread = p->thread_info.oob_state.thread;
-	pr_info("the passed thread ptr is %px", thread);
+	// pr_info("the passed thread ptr is %px", thread);
 	rust_resume_oob_task(thread);
 }
 
diff --git a/kernel/rros/Kconfig b/kernel/rros/Kconfig
new file mode 100644
index 000000000..e080b3afd
--- /dev/null
+++ b/kernel/rros/Kconfig
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+config RROS_OOB_NET
+        bool "Out-of-band networking (EXPERIMENTAL)"
+	default n
+	select NET_OOB
+	select NET_SCHED
+	select NET_SCH_OOB
+	select INET
+	select VLAN_8021Q
+	help
+	This option enables preliminary networking support for Rros.
\ No newline at end of file
diff --git a/kernel/rros/arch/Makefile b/kernel/rros/arch/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/arch/arm64/Makefile b/kernel/rros/arch/arm64/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/arch/arm64/mod.rs b/kernel/rros/arch/arm64/mod.rs
new file mode 100644
index 000000000..9b7869bfc
--- /dev/null
+++ b/kernel/rros/arch/arm64/mod.rs
@@ -0,0 +1,2 @@
+#[macro_use]
+pub mod syscall;
diff --git a/kernel/rros/arch/arm64/syscall.rs b/kernel/rros/arch/arm64/syscall.rs
new file mode 100644
index 000000000..e653ac9e7
--- /dev/null
+++ b/kernel/rros/arch/arm64/syscall.rs
@@ -0,0 +1,95 @@
+use kernel::bindings;
+use kernel::prelude::*;
+
+#[macro_export]
+macro_rules! oob_retval {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[0])
+    };
+}
+
+#[macro_export]
+macro_rules! oob_arg1 {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[0])
+    };
+}
+
+#[macro_export]
+macro_rules! oob_arg2 {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[1])
+    };
+}
+
+#[macro_export]
+macro_rules! oob_arg3 {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[2])
+    };
+}
+
+#[macro_export]
+macro_rules! oob_arg4 {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[3])
+    };
+}
+
+#[macro_export]
+macro_rules! oob_arg5 {
+    ($ptr:expr) => {
+        ((*$ptr).__bindgen_anon_1.__bindgen_anon_1.regs[4])
+    };
+}
+
+#[macro_export]
+macro_rules! is_clock_gettime {
+    ($nr:expr) => {
+        (($nr) == bindings::__NR_clock_gettime as i32)
+    };
+}
+
+#[macro_export]
+#[cfg(not(__NR_clock_gettime64))]
+macro_rules! is_clock_gettime64 {
+    ($nr:expr) => {
+        false
+    };
+}
+
+#[macro_export]
+#[cfg(__NR_clock_gettime64)]
+macro_rules! is_clock_gettime64 {
+    ($nr:expr) => {
+        ((nr) == bindings::__NR_clock_gettime64)
+    };
+}
+
+pub fn is_oob_syscall(regs: *const bindings::pt_regs) -> bool {
+    (unsafe { (*regs).syscallno } & bindings::__OOB_SYSCALL_BIT as i32) != 0
+}
+
+pub fn oob_syscall_nr(regs: *const bindings::pt_regs) -> u32 {
+    pr_info!("the sys call number is {}", (*regs).syscallno as u32);
+    (unsafe { (*regs).syscallno as u32 } & !bindings::__OOB_SYSCALL_BIT as u32)
+}
+
+pub fn inband_syscall_nr(regs: *mut bindings::pt_regs, nr: *mut u32) -> bool {
+    unsafe {
+        *nr = oob_syscall_nr(regs);
+    }
+    !is_oob_syscall(regs)
+}
+
+pub fn set_oob_error(regs: *mut bindings::pt_regs, err: i32) {
+    unsafe {
+        oob_retval!(regs) = err as u64;
+    }
+}
+
+pub fn set_oob_retval(regs: *mut bindings::pt_regs, err: i32) {
+    unsafe {
+        oob_retval!(regs) = err as u64;
+    }
+}
diff --git a/kernel/rros/arch/mod.rs b/kernel/rros/arch/mod.rs
new file mode 100644
index 000000000..64a6d696e
--- /dev/null
+++ b/kernel/rros/arch/mod.rs
@@ -0,0 +1,2 @@
+#[macro_use]
+pub mod arm64;
diff --git a/kernel/rros/clock.rs b/kernel/rros/clock.rs
index c85547cff..27ed2b465 100644
--- a/kernel/rros/clock.rs
+++ b/kernel/rros/clock.rs
@@ -4,33 +4,33 @@
 
 #![allow(warnings, unused)]
 #![feature(stmt_expr_attributes)]
-use alloc::rc::Rc;
 use crate::factory;
 use crate::list::*;
+use crate::sched::__rros_timespec;
+use crate::sched::{rros_cpu_rq, this_rros_rq, RQ_TDEFER, RQ_TIMER, RQ_TPROXY};
+use crate::thread::T_ROOT;
 use crate::tick;
 use crate::timeout::RROS_INFINITE;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    tick::*,
-};
-use crate::sched::{
-    RQ_TIMER, RQ_TPROXY, RQ_TDEFER,
-    rros_cpu_rq, this_rros_rq, 
+    factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, tick::*, timer::*,
+    RROS_OOB_CPUS,
 };
-use crate::thread::T_ROOT;
+use alloc::rc::Rc;
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::clone::Clone;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::clone::Clone;
+use core::ops::Deref;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
+use factory::RROS_CLONE_PUBLIC;
 use kernel::{
-    bindings, c_types, cpumask, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*, 
+    bindings, c_types, cpumask, double_linked_list::*, file_operations::FileOperations, ktime::*,
+    percpu, prelude::*, premmpt, spinlock_init, str::CStr, sync::Lock, sync::SpinLock, sysfs,
+    timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use factory::RROS_CLONE_PUBLIC;
-use core::ops::DerefMut;
-use core::ops::Deref;
+use kernel::io_buffer::IoBufferWriter;
+use kernel::file::File;
 
 static mut CLOCKLIST_LOCK: SpinLock<i32> = unsafe { SpinLock::new(1) };
 
@@ -38,7 +38,7 @@ const CONFIG_RROS_LATENCY_USER: KtimeT = 0; //这里先定义为常量，后面
 const CONFIG_RROS_LATENCY_KERNEL: KtimeT = 0;
 const CONFIG_RROS_LATENCY_IRQ: KtimeT = 0;
 
-const CONFIG_RROS_NR_CLOCKS: u32 = 0; //未知
+const CONFIG_RROS_NR_CLOCKS: usize = 0; //未知
 
 #[derive(Default)]
 pub struct RustFileClock;
@@ -54,16 +54,8 @@ pub struct RrosClockGravity {
 }
 
 impl RrosClockGravity {
-    pub fn new(
-        irq: KtimeT,
-        kernel: KtimeT,
-        user: KtimeT,
-    ) -> Self {
-        RrosClockGravity {
-            irq,
-            kernel,
-            user,
-        }
+    pub fn new(irq: KtimeT, kernel: KtimeT, user: KtimeT) -> Self {
+        RrosClockGravity { irq, kernel, user }
     }
     pub fn get_irq(&self) -> KtimeT {
         self.irq
@@ -111,18 +103,18 @@ impl RrosClockOps {
         programremoteshot: Option<fn(&RrosClock, *mut RrosRq)>,
         setgravity: Option<fn(&mut RrosClock, RrosClockGravity)>,
         resetgravity: Option<fn(&mut RrosClock)>,
-        adjust: Option<fn(&mut RrosClock)>)
-        -> Self{
-            RrosClockOps {
-                read,
-                readcycles,
-                set,
-                programlocalshot,
-                programremoteshot,
-                setgravity,
-                resetgravity,
-                adjust,
-            }
+        adjust: Option<fn(&mut RrosClock)>,
+    ) -> Self {
+        RrosClockOps {
+            read,
+            readcycles,
+            set,
+            programlocalshot,
+            programremoteshot,
+            setgravity,
+            resetgravity,
+            adjust,
+        }
     }
 }
 
@@ -139,8 +131,8 @@ pub struct RrosClock {
     element: Option<Rc<RefCell<RrosElement>>>,
     dispose: Option<fn(&mut RrosClock)>,
     #[cfg(CONFIG_SMP)]
-    affinity: Option<cpumask::CpumaskT>,
-}//____cacheline_aligned
+    pub affinity: Option<cpumask::CpumaskT>,
+} //____cacheline_aligned
 
 //RrosClock主方法
 impl RrosClock {
@@ -156,9 +148,8 @@ impl RrosClock {
         next: *mut list_head,
         element: Option<Rc<RefCell<RrosElement>>>,
         dispose: Option<fn(&mut RrosClock)>,
-        #[cfg(CONFIG_SMP)]
-        affinity: Option<cpumask::CpumaskT>,
-    ) -> Self{
+        #[cfg(CONFIG_SMP)] affinity: Option<cpumask::CpumaskT>,
+    ) -> Self {
         RrosClock {
             resolution,
             gravity,
@@ -175,13 +166,15 @@ impl RrosClock {
             affinity,
         }
     }
-    pub fn read(&self) -> KtimeT {//错误处理
+    pub fn read(&self) -> KtimeT {
+        //错误处理
         if self.ops.read.is_some() {
             return self.ops.read.unwrap()(&self);
         }
         return 0;
     }
-    pub fn read_cycles(&self) -> u64 {//错误处理
+    pub fn read_cycles(&self) -> u64 {
+        //错误处理
         if self.ops.readcycles.is_some() {
             return self.ops.readcycles.unwrap()(&self);
         }
@@ -189,9 +182,9 @@ impl RrosClock {
     }
     pub fn set(&mut self, time: KtimeT) -> Result<usize> {
         if self.ops.set.is_some() {
-            self.ops.set.unwrap()(self,time);
-        }else{
-            return Err(kernel::Error::EFAULT);//阻止函数为null情况的执行
+            self.ops.set.unwrap()(self, time);
+        } else {
+            return Err(kernel::Error::EFAULT); //阻止函数为null情况的执行
         }
         Ok(0)
     }
@@ -207,7 +200,7 @@ impl RrosClock {
     }
     pub fn set_gravity(&mut self, gravity: RrosClockGravity) {
         if self.ops.setgravity.is_some() {
-            self.ops.setgravity.unwrap()(self,gravity);
+            self.ops.setgravity.unwrap()(self, gravity);
         }
     }
     pub fn reset_gravity(&mut self) {
@@ -220,10 +213,11 @@ impl RrosClock {
             self.ops.adjust.unwrap()(self);
         }
     }
-    pub fn get_timerdata_addr(&self) -> *mut RrosTimerbase {//错误处理
+    pub fn get_timerdata_addr(&self) -> *mut RrosTimerbase {
+        //错误处理
         return self.timerdata as *mut RrosTimerbase;
     }
-    
+
     pub fn get_gravity_irq(&self) -> KtimeT {
         self.gravity.get_irq()
     }
@@ -246,47 +240,57 @@ impl RrosClock {
 }
 
 //测试通过
-pub fn adjust_timer(clock: &RrosClock, timer: Arc<SpinLock<RrosTimer>>, tq: &mut List<Arc<SpinLock<RrosTimer>>>, delta: KtimeT) {
+pub fn adjust_timer(
+    clock: &RrosClock,
+    timer: Arc<SpinLock<RrosTimer>>,
+    tq: &mut List<Arc<SpinLock<RrosTimer>>>,
+    delta: KtimeT,
+) {
     let date = timer.lock().get_date();
-    timer.lock().set_date(ktime_sub(date,delta));
+    timer.lock().set_date(ktime_sub(date, delta));
     let is_periodic = timer.lock().is_periodic();
     if is_periodic == false {
-        rros_enqueue_timer(timer.clone(),tq);
+        rros_enqueue_timer(timer.clone(), tq);
         return;
     }
 
     let start_date = timer.lock().get_start_date();
     timer.lock().set_start_date(ktime_sub(start_date, delta));
 
-	let period = timer.lock().get_interval();
-	let diff = ktime_sub(clock.read(), rros_get_timer_expiry(timer.clone()));
+    let period = timer.lock().get_interval();
+    let diff = ktime_sub(clock.read(), rros_get_timer_expiry(timer.clone()));
 
-	if (diff >= period) {
-		let div = ktime_divns(diff, ktime_to_ns(period));
+    if (diff >= period) {
+        let div = ktime_divns(diff, ktime_to_ns(period));
         let periodic_ticks = timer.lock().get_periodic_ticks();
-		timer.lock().set_periodic_ticks((periodic_ticks as i64 + div) as u64);
-		
-	} else if (ktime_to_ns(delta) < 0
-		&& (timer.lock().get_status() & RROS_TIMER_FIRED != 0)
-		&& ktime_to_ns(ktime_add(diff, period)) <= 0) {
-		/*
-		 * Timer is periodic and NOT waiting for its first
-		 * shot, so we make it tick sooner than its original
-		 * date in order to avoid the case where by adjusting
-		 * time to a sooner date, real-time periodic timers do
-		 * not tick until the original date has passed.
-		 */
-		let div = ktime_divns(-diff, ktime_to_ns(period));
+        timer
+            .lock()
+            .set_periodic_ticks((periodic_ticks as i64 + div) as u64);
+    } else if (ktime_to_ns(delta) < 0
+        && (timer.lock().get_status() & RROS_TIMER_FIRED != 0)
+        && ktime_to_ns(ktime_add(diff, period)) <= 0)
+    {
+        /*
+         * Timer is periodic and NOT waiting for its first
+         * shot, so we make it tick sooner than its original
+         * date in order to avoid the case where by adjusting
+         * time to a sooner date, real-time periodic timers do
+         * not tick until the original date has passed.
+         */
+        let div = ktime_divns(-diff, ktime_to_ns(period));
         let periodic_ticks = timer.lock().get_periodic_ticks();
         let pexpect_ticks = timer.lock().get_pexpect_ticks();
-        timer.lock().set_periodic_ticks((periodic_ticks as i64 - div) as u64);
-        timer.lock().set_pexpect_ticks((pexpect_ticks as i64 - div) as u64);
-	}
+        timer
+            .lock()
+            .set_periodic_ticks((periodic_ticks as i64 - div) as u64);
+        timer
+            .lock()
+            .set_pexpect_ticks((pexpect_ticks as i64 - div) as u64);
+    }
     rros_update_timer_date(timer.clone());
-    rros_enqueue_timer(timer.clone(),tq);
+    rros_enqueue_timer(timer.clone(), tq);
 }
 
- 
 //简单测过
 //调整当前clock各个CPU tmb中List中的所有timer
 pub fn rros_adjust_timers(clock: &mut RrosClock, delta: KtimeT) {
@@ -295,8 +299,8 @@ pub fn rros_adjust_timers(clock: &mut RrosClock, delta: KtimeT) {
     //for_each_online_cpu(cpu) {
     let rq = rros_cpu_rq(cpu);
     let tmb = rros_percpu_timers(clock, cpu);
-    let tq = unsafe{&mut (*tmb).q};
-    
+    let tq = unsafe { &mut (*tmb).q };
+
     for i in 1..=tq.len() {
         let timer = tq.get_by_index(i).unwrap().value.clone();
         let get_clock = timer.lock().get_clock();
@@ -308,7 +312,7 @@ pub fn rros_adjust_timers(clock: &mut RrosClock, delta: KtimeT) {
 
     if rq != this_rros_rq() {
         rros_program_remote_tick(clock, rq);
-    }else {
+    } else {
         rros_program_local_tick(clock);
     }
     //}
@@ -318,7 +322,7 @@ pub fn rros_adjust_timers(clock: &mut RrosClock, delta: KtimeT) {
 pub fn rros_stop_timers(clock: &RrosClock) {
     let cpu = 0;
     let mut tmb = rros_percpu_timers(&clock, cpu);
-    let tq = unsafe{&mut (*tmb).q};
+    let tq = unsafe { &mut (*tmb).q };
     while tq.is_empty() == false {
         //raw_spin_lock_irqsave(&tmb->lock, flags);
         pr_info!("rros_stop_timers: 213");
@@ -328,26 +332,25 @@ pub fn rros_stop_timers(clock: &RrosClock) {
     }
 }
 
- /*
+/*
  void inband_clock_was_set(void)
 {
-	struct evl_clock *clock;
+    struct evl_clock *clock;
 
-	if (!evl_is_enabled())
-		return;
+    if (!evl_is_enabled())
+        return;
 
-	mutex_lock(&clocklist_lock);
+    mutex_lock(&clocklist_lock);
 
-	list_for_each_entry(clock, &clock_list, next) {
-		if (clock->ops.adjust)
-			clock->ops.adjust(clock);
-	}
+    list_for_each_entry(clock, &clock_list, next) {
+        if (clock->ops.adjust)
+            clock->ops.adjust(clock);
+    }
 
-	mutex_unlock(&clocklist_lock);
+    mutex_unlock(&clocklist_lock);
 }
  */
 
-
 //打印clock的初始化log
 fn rros_clock_log() {}
 
@@ -411,9 +414,8 @@ fn reset_coreclk_gravity(clock: &mut RrosClock) {
 }
 
 //两个全局变量MONO和REALTIME
-static RROS_MONO_CLOCK_NAME: &CStr = unsafe { 
-    CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_MONOTONIC_DEV\0".as_bytes())
-};
+static RROS_MONO_CLOCK_NAME: &CStr =
+    unsafe { CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_MONOTONIC_DEV\0".as_bytes()) };
 pub static mut RROS_MONO_CLOCK: RrosClock = RrosClock {
     name: RROS_MONO_CLOCK_NAME,
     resolution: 1,
@@ -446,19 +448,18 @@ pub static mut RROS_MONO_CLOCK: RrosClock = RrosClock {
     affinity: None,
 };
 
-static RROS_REALTIME_CLOCK_NAME: &CStr = unsafe {
-    CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_REALTIME_DEV\0".as_bytes())
-};
-pub static mut RROS_REALTIME_CLOCK: RrosClock = RrosClock{
+static RROS_REALTIME_CLOCK_NAME: &CStr =
+    unsafe { CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_REALTIME_DEV\0".as_bytes()) };
+pub static mut RROS_REALTIME_CLOCK: RrosClock = RrosClock {
     name: RROS_REALTIME_CLOCK_NAME,
     resolution: 1,
-    gravity: RrosClockGravity{
+    gravity: RrosClockGravity {
         irq: CONFIG_RROS_LATENCY_IRQ,
         kernel: CONFIG_RROS_LATENCY_KERNEL,
         user: CONFIG_RROS_LATENCY_USER,
     },
     flags: RROS_CLONE_PUBLIC,
-    ops: RrosClockOps{
+    ops: RrosClockOps {
         read: Some(read_realtime_clock),
         readcycles: Some(read_realtime_clock_cycles),
         set: None,
@@ -478,12 +479,12 @@ pub static mut RROS_REALTIME_CLOCK: RrosClock = RrosClock{
     affinity: None,
 };
 
-pub static mut CLOCK_LIST: List::<*mut RrosClock> = List::<*mut RrosClock> {
+pub static mut CLOCK_LIST: List<*mut RrosClock> = List::<*mut RrosClock> {
     head: Node::<*mut RrosClock> {
         next: None,
         prev: None,
         value: 0 as *mut RrosClock,
-    }
+    },
 };
 
 /*
@@ -495,63 +496,108 @@ struct rros_factory rros_clock_factory = {
     .dispose =	clock_factory_dispose,
 };
 */
-pub static mut RROS_CLOCK_FACTORY: SpinLock<factory::RrosFactory> = unsafe { SpinLock::new(factory::RrosFactory {
-    name: unsafe{CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_DEV\0".as_bytes())},
-    nrdev: CONFIG_RROS_NR_CLOCKS,
-    build: None,
-    dispose: Some(clock_factory_dispose),
-    attrs: None, //sysfs::attribute_group::new(),
-    flags: 2,
-    inside: Some(factory::RrosFactoryInside {
-        rrtype: None,
-        class: None,
-        cdev: None,
-        device: None,
-        sub_rdev: None,
-        kuid: None,
-        kgid: None,
-        minor_map: None,
-        index: None,
-        name_hash: None,
-        hash_lock: None,
-        register: None
-    }),
-})};
+pub static mut RROS_CLOCK_FACTORY: SpinLock<factory::RrosFactory> = unsafe {
+    SpinLock::new(factory::RrosFactory {
+        name: unsafe { CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_DEV\0".as_bytes()) },
+        nrdev: CONFIG_RROS_NR_CLOCKS,
+        build: None,
+        dispose: Some(clock_factory_dispose),
+        attrs: None, //sysfs::attribute_group::new(),
+        flags: 2,
+        inside: Some(factory::RrosFactoryInside {
+            rrtype: None,
+            class: None,
+            cdev: None,
+            device: None,
+            sub_rdev: None,
+            kuid: None,
+            kgid: None,
+            minor_map: None,
+            index: None,
+            name_hash: None,
+            hash_lock: None,
+            register: None,
+        }),
+    })
+};
+
+struct Clockops;
+
+impl FileOperations for Clockops {
+    kernel::declare_file_operations!(read);
+
+    fn read<T: IoBufferWriter>(
+        _this: &Self,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops of the rros clock factory.");
+        Ok(1)
+    }
+}
 
 pub fn clock_factory_dispose(ele: factory::RrosElement) {}
 
 /*
 void evl_core_tick(struct clock_event_device *dummy) /* hard irqs off */
 {
-	struct evl_rq *this_rq = this_evl_rq();
-	struct evl_timerbase *tmb;
-
-	if (EVL_WARN_ON_ONCE(CORE, !is_evl_cpu(evl_rq_cpu(this_rq))))
-		return;
-
-	tmb = evl_this_cpu_timers(&evl_mono_clock);
-	do_clock_tick(&evl_mono_clock, tmb);
-
-	/*
-	 * If an EVL thread was preempted by this clock event, any
-	 * transition to the in-band context will cause a pending
-	 * in-band tick to be propagated by evl_schedule() called from
-	 * evl_exit_irq(), so we may have to propagate the in-band
-	 * tick immediately only if the in-band context was preempted.
-	 */
-	if ((this_rq->local_flags & RQ_TPROXY) && (this_rq->curr->state & T_ROOT))
-		evl_notify_proxy_tick(this_rq);
+    struct evl_rq *this_rq = this_evl_rq();
+    struct evl_timerbase *tmb;
+
+    if (EVL_WARN_ON_ONCE(CORE, !is_evl_cpu(evl_rq_cpu(this_rq))))
+        return;
+
+    tmb = evl_this_cpu_timers(&evl_mono_clock);
+    do_clock_tick(&evl_mono_clock, tmb);
+
+    /*
+     * If an EVL thread was preempted by this clock event, any
+     * transition to the in-band context will cause a pending
+     * in-band tick to be propagated by evl_schedule() called from
+     * evl_exit_irq(), so we may have to propagate the in-band
+     * tick immediately only if the in-band context was preempted.
+     */
+    if ((this_rq->local_flags & RQ_TPROXY) && (this_rq->curr->state & T_ROOT))
+        evl_notify_proxy_tick(this_rq);
 }
 */
 
 fn timer_needs_enqueuing(timer: *mut RrosTimer) -> bool {
-    unsafe{return ((*timer).get_status() &
-		(RROS_TIMER_PERIODIC|RROS_TIMER_DEQUEUED|
-			RROS_TIMER_RUNNING|RROS_TIMER_KILLED))
-		== (RROS_TIMER_PERIODIC|RROS_TIMER_DEQUEUED|
-			RROS_TIMER_RUNNING);}
+    unsafe {
+        return ((*timer).get_status()
+            & (RROS_TIMER_PERIODIC
+                | RROS_TIMER_DEQUEUED
+                | RROS_TIMER_RUNNING
+                | RROS_TIMER_KILLED))
+            == (RROS_TIMER_PERIODIC | RROS_TIMER_DEQUEUED | RROS_TIMER_RUNNING);
+    }
 }
 
+/// # Handle the timer when the clock is ticking
+/// 
+/// # Requirements
+///     - You need to call the handler of each timer met the deadline
+///     - Remeber to dequeue the timer and set the `RROS_TIMER_FIRED` flag of the timer status when it is handled
+///     - Corner case: when this function is called you need to check whether the timer met the deadline
+///     - Corner case: when the `inband_timer` timer is dued, you need to set the `RQ_TPROXY` flag of the 
+///       `rq` and unset the `RQ_TPROXY` flag. The rros will handle the `inband_timer` in the later, so just
+///       call the `continue` to handle the next timer.
+///
+/// # Arguments
+///     - `arg`: not used
+///     
+/// # Return
+///     - 0 all the time
+///  
+/// # Tips: 
+///     - The `rros_dequeue_timer` and `get_handler` is used in the implementation.
+///     - If you want to know whether you are handling the `inband_timer`, you can compare the timer address with the `inband_timer` address.
+///     - The inband timer is used to handle the tick to the Linux kernel. It's only called when there is no task in the rros.
+///     - Don't call the lock funciton of the Arc<Spinlock<rros_timer>>, you need to use (*timer.locked_data().get()).foo
+///       to call the function of the rros_timer. If you figure out why we have this constraint, just contact us for the extra credit!
+///     - The first test case in the `test_do_clock_tick` is to test the length of the tmb queue. If you figure out why the length is 2,
+///       just contact us for the extra credit!
 //rq相关未测试，其余测试通过
 pub fn do_clock_tick(clock: &mut RrosClock, tmb: *mut RrosTimerbase) {
     let rq = this_rros_rq();
@@ -559,10 +605,12 @@ pub fn do_clock_tick(clock: &mut RrosClock, tmb: *mut RrosTimerbase) {
     // if hard_irqs_disabled() == false {
     //     hard_local_irq_disable();
     // }
-    let mut tq = unsafe{&mut (*tmb).q};
+    let mut tq = unsafe { &mut (*tmb).q };
     //unsafe{(*tmb).lock.lock();}
 
-    unsafe{(*rq).add_local_flags(RQ_TIMER);}
+    unsafe {
+        (*rq).add_local_flags(RQ_TIMER);
+    }
 
     let mut now = clock.read();
 
@@ -573,29 +621,12 @@ pub fn do_clock_tick(clock: &mut RrosClock, tmb: *mut RrosTimerbase) {
     //     }
     // }
 
-    unsafe{
+    unsafe {
         while tq.is_empty() == false {
             let mut timer = tq.get_head().unwrap().value.clone();
-            let date = (*timer.locked_data().get()).get_date();
-            if now < date{
-                break;
-            }
-            
-            rros_dequeue_timer(timer.clone(), tq);
-
-            rros_account_timer_fired(timer.clone());
-            (*timer.locked_data().get()).add_status(RROS_TIMER_FIRED);
-            let timer_addr = timer.locked_data().get();
-
-            let inband_timer_addr = (*rq).get_inband_timer().locked_data().get();
-            if (timer_addr == inband_timer_addr) {
-                    (*rq).add_local_flags(RQ_TPROXY) ;
-                    (*rq).change_local_flags(!RQ_TDEFER);
-            	continue;
-            }
-            let handler = (*timer.locked_data().get()).get_handler();
-            let c_ref = timer.locked_data().get();
-            handler(c_ref);
+            // TODO: your code here
+         
+            // end of your code
             now = clock.read();
             let var_timer_needs_enqueuing = timer_needs_enqueuing(timer.locked_data().get());
             if var_timer_needs_enqueuing == true {
@@ -605,7 +636,7 @@ pub fn do_clock_tick(clock: &mut RrosClock, tmb: *mut RrosTimerbase) {
                     rros_update_timer_date(timer.clone());
 
                     let date = (*timer.locked_data().get()).get_date();
-                    if date >= now{
+                    if date >= now {
                         break;
                     }
                 }
@@ -614,31 +645,32 @@ pub fn do_clock_tick(clock: &mut RrosClock, tmb: *mut RrosTimerbase) {
                     rros_enqueue_timer(timer.clone(), tq);
                 }
 
-                pr_info!("now is {}",now);
+                // pr_info!("now is {}", now);
                 // pr_info!("date is {}",timer.lock().get_date());
             }
         }
     }
-    unsafe{(*rq).change_local_flags(!RQ_TIMER)};
+    unsafe { (*rq).change_local_flags(!RQ_TIMER) };
 
-	rros_program_local_tick(clock as *mut RrosClock);
+    // TODO: When you pass the `` test, you can uncomment the following code
+    rros_program_local_tick(clock as *mut RrosClock);
 
-	//raw_spin_unlock(&tmb->lock);
-    
+    //raw_spin_unlock(&tmb->lock);
 }
 
 #[no_mangle]
-pub unsafe extern "C"  fn rros_core_tick(dummy: *mut bindings::clock_event_device) {
+pub unsafe extern "C" fn rros_core_tick(dummy: *mut bindings::clock_event_device) {
+    // pr_info!("in rros_core_tick");
     let this_rq = this_rros_rq();
     //	if (EVL_WARN_ON_ONCE(CORE, !is_evl_cpu(evl_rq_cpu(this_rq))))
     // pr_info!("in rros_core_tick");
-    unsafe{
+    unsafe {
         do_clock_tick(&mut RROS_MONO_CLOCK, rros_this_cpu_timers(&RROS_MONO_CLOCK));
 
         let rq_has_tproxy = ((*this_rq).local_flags & RQ_TPROXY != 0x0);
         let assd = (*(*this_rq).get_curr().locked_data().get()).state;
-        let curr_state_is_t_root = ( assd & (T_ROOT as u32) != 0x0); 
-                //这个if进不去有问题！！
+        let curr_state_is_t_root = (assd & (T_ROOT as u32) != 0x0);
+        //这个if进不去有问题！！
         // let a = ((*this_rq).local_flags & RQ_TPROXY != 0x0);
         // if rq_has_tproxy  {
         //     pr_info!("in rros_core_tick");
@@ -650,7 +682,7 @@ pub unsafe extern "C"  fn rros_core_tick(dummy: *mut bindings::clock_event_devic
         //     pr_info!("in rros_core_tick");
         //     pr_info!("in rros_core_tick");
         // }
-        // let b = ((*this_rq).get_curr().lock().deref_mut().state & (T_ROOT as u32) != 0x0); 
+        // let b = ((*this_rq).get_curr().lock().deref_mut().state & (T_ROOT as u32) != 0x0);
 
         // if curr_state_is_t_root  {
         //     pr_info!("in rros_core_tick");
@@ -668,7 +700,6 @@ pub unsafe extern "C"  fn rros_core_tick(dummy: *mut bindings::clock_event_devic
     }
 }
 
-
 //初始化时钟
 fn init_clock(clock: *mut RrosClock, master: *mut RrosClock) -> Result<usize> {
     // unsafe{
@@ -677,20 +708,20 @@ fn init_clock(clock: *mut RrosClock, master: *mut RrosClock) -> Result<usize> {
     //     }
     // }
     // unsafe{
-    //     factory::rros_init_element((*clock).element.as_ref().unwrap().clone(), 
+    //     factory::rros_init_element((*clock).element.as_ref().unwrap().clone(),
     //     &mut RROS_CLOCK_FACTORY, (*clock).flags & RROS_CLONE_PUBLIC);
     // }
-    unsafe{
+    unsafe {
         (*clock).master = master;
     }
     //rros_create_core_element_device()?;
 
-    unsafe{
+    unsafe {
         CLOCKLIST_LOCK.lock();
         CLOCK_LIST.add_head(clock);
         CLOCKLIST_LOCK.unlock();
     }
-    
+
     Ok(0)
 }
 
@@ -699,7 +730,7 @@ fn rros_init_slave_clock(clock: &mut RrosClock, master: &mut RrosClock) -> Resul
     premmpt::running_inband()?;
 
     //这里为什么会报错，timer就可以跑？为什么卧槽
-    // #[cfg(CONFIG_SMP)] 
+    // #[cfg(CONFIG_SMP)]
     // clock.affinity = master.affinity;
 
     clock.timerdata = master.get_timerdata_addr();
@@ -711,16 +742,20 @@ fn rros_init_slave_clock(clock: &mut RrosClock, master: &mut RrosClock) -> Resul
 //rros初始化时钟
 fn rros_init_clock(clock: &mut RrosClock, affinity: &cpumask::CpumaskT) -> Result<usize> {
     premmpt::running_inband()?;
-    let tmb =
-        percpu::alloc_per_cpu(size_of::<RrosTimerbase>() as usize, align_of::<RrosTimerbase>() as usize) as *mut RrosTimerbase; //8字节对齐
+    let tmb = percpu::alloc_per_cpu(
+        size_of::<RrosTimerbase>() as usize,
+        align_of::<RrosTimerbase>() as usize,
+    ) as *mut RrosTimerbase; //8字节对齐
     if tmb == 0 as *mut RrosTimerbase {
         return Err(kernel::Error::ENOMEM);
     }
     clock.timerdata = tmb;
 
     let mut tmb = rros_percpu_timers(clock, 0);
-    
-    unsafe {raw_spin_lock_init(&mut (*tmb).lock);}
+
+    unsafe {
+        raw_spin_lock_init(&mut (*tmb).lock);
+    }
 
     clock.offset = 0;
     let ret = init_clock(clock as *mut RrosClock, clock as *mut RrosClock);
@@ -748,19 +783,19 @@ pub fn rros_clock_init() -> Result<usize> {
     Ok(0)
 }
 
-pub fn rros_read_clock (clock: &RrosClock) -> KtimeT {
+pub fn rros_read_clock(clock: &RrosClock) -> KtimeT {
     let clock_add = clock as *const RrosClock;
-    let mono_add = unsafe{&RROS_MONO_CLOCK as *const RrosClock};
-	
+    let mono_add = unsafe { &RROS_MONO_CLOCK as *const RrosClock };
+
     if (clock_add == mono_add) {
         return rros_ktime_monotonic();
     }
 
-	clock.ops.read.unwrap()(&clock)
+    clock.ops.read.unwrap()(&clock)
 }
 
 fn rros_ktime_monotonic() -> KtimeT {
-	timekeeping::ktime_get_mono_fast_ns()
+    timekeeping::ktime_get_mono_fast_ns()
 }
 
 // static inline ktime_t evl_read_clock(struct evl_clock *clock)
@@ -776,3 +811,18 @@ fn rros_ktime_monotonic() -> KtimeT {
 
 // 	return clock->ops.read(clock);
 // }
+
+
+pub fn u_timespec_to_ktime(u_ts:__rros_timespec) -> KtimeT{
+    extern "C"{
+        fn rust_helper_timespec64_to_ktime(ts:bindings::timespec64) -> KtimeT;
+    }
+    let ts64 = bindings::timespec64{
+        tv_sec:u_ts.tv_sec as i64,
+        tv_nsec:u_ts.tv_nsec as i64,
+    };
+    
+    unsafe{
+        rust_helper_timespec64_to_ktime(ts64)
+    }
+}
\ No newline at end of file
diff --git a/kernel/rros/clock_test.rs b/kernel/rros/clock_test.rs
index 68bc1ec52..33a62e597 100644
--- a/kernel/rros/clock_test.rs
+++ b/kernel/rros/clock_test.rs
@@ -2,47 +2,76 @@
 //用于测试clock.rs里的函数正确&性
 use crate::factory;
 use crate::list::*;
+use crate::test_eq;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    sched::*, 
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, sched::*,
+    timer::*, RROS_OOB_CPUS, test::TestFailed
 };
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::Deref;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,
+    bindings, c_types, cpumask::CpumaskT, double_linked_list::*, file_operations::FileOperations,
+    ktime::*, percpu, percpu_defs, prelude::*, premmpt, spinlock_init, str::CStr, sync::Guard,
+    sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use core::ops::DerefMut;
-use core::ops::Deref;
-
 
 //测试通过
-pub fn test_do_clock_tick() -> Result<usize> {
-    pr_info!("~~~test_do_clock_tick begin~~~");
+pub fn test_do_clock_tick() -> crate::test::Result<()> {
+    // pr_info!("~~~test_do_clock_tick begin~~~");
     unsafe {
         let mut tmb = rros_percpu_timers(&RROS_MONO_CLOCK, 0);
         let mut a = SpinLock::new(RrosTimer::new(580000000));
         let pinned = unsafe { Pin::new_unchecked(&mut a) };
-        spinlock_init!(pinned, "zbw");
+        spinlock_init!(pinned, "timer1");
 
-        let mut xx = Arc::try_new(a)?;
+        let mut xx = Arc::try_new(a).unwrap();
         xx.lock().add_status(RROS_TIMER_DEQUEUED);
-        xx.lock().add_status(RROS_TIMER_PERIODIC);
+        // xx.lock().add_status(RROS_TIMER_PERIODIC);
         xx.lock().add_status(RROS_TIMER_RUNNING);
         xx.lock().set_clock(&mut RROS_MONO_CLOCK as *mut RrosClock);
         xx.lock().set_interval(1000);
 
+        let mut b = SpinLock::new(RrosTimer::new(580000000));
+        let pinned = unsafe { Pin::new_unchecked(&mut b) };
+        spinlock_init!(pinned, "timer2");
+
+        let mut yy = Arc::try_new(b).unwrap();
+        yy.lock().add_status(RROS_TIMER_DEQUEUED);
+        // yy.lock().add_status(RROS_TIMER_PERIODIC);
+        yy.lock().add_status(RROS_TIMER_RUNNING);
+        yy.lock().set_clock(&mut RROS_MONO_CLOCK as *mut RrosClock);
+        yy.lock().set_interval(1000);
+
         (*tmb).q.add_head(xx.clone());
+        // A helper test case. After you pass the `test_do_clock_tick`, use the below. Otherwise, use the above.
+        // test_eq!(1, (*tmb).q.len())?;
+        // test_eq!(2, (*tmb).q.len())?;
+        
 
-        pr_info!("before do_clock_tick");
-        do_clock_tick(&mut RROS_MONO_CLOCK, tmb);
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        (*tmb).q.add_head(yy.clone());
+        
+        // A helper test case. After you pass the `test_do_clock_tick`, use the below. Otherwise, use the above.
+        // test_eq!(2, (*tmb).q.len())?;
+        // test_eq!(3, (*tmb).q.len())?;
+
+        // pr_info!("len of tmb is {}", (*tmb).q.len());
+
+        // pr_info!("before do_clock_tick");
+        loop {
+            do_clock_tick(&mut RROS_MONO_CLOCK, tmb);
+            if (*tmb).q.len() == 0 {
+                break;
+            } 
+        }
+        test_eq!(0, (*tmb).q.len())?;
+        // pr_info!("len of tmb is {}", (*tmb).q.len());
     }
-    pr_info!("~~~test_do_clock_tick end~~~");
-    Ok(0)
+    // pr_info!("~~~test_do_clock_tick end~~~");
+    Ok(())
 }
 
 //测试通过
@@ -61,15 +90,14 @@ pub fn test_adjust_timer() -> Result<usize> {
         xx.lock().set_clock(&mut RROS_MONO_CLOCK as *mut RrosClock);
         xx.lock().set_interval(1000);
 
-       // (*tmb).q.add_head(xx.clone());
+        // (*tmb).q.add_head(xx.clone());
 
         pr_info!("before adjust_timer");
         adjust_timer(&RROS_MONO_CLOCK, xx.clone(), &mut (*tmb).q, 100);
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_adjust_timer end~~~");
     Ok(0)
-
 }
 
 //测试通过
@@ -106,13 +134,12 @@ pub fn test_rros_adjust_timers() -> Result<usize> {
         yy.lock().set_clock(&mut RROS_MONO_CLOCK as *mut RrosClock);
         yy.lock().set_interval(1000);
 
-
         (*tmb).q.add_head(xx.clone());
         (*tmb).q.add_head(yy.clone());
 
         pr_info!("before adjust_timer");
         rros_adjust_timers(&mut RROS_MONO_CLOCK, 100);
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_rros_adjust_timers end~~~");
     Ok(0)
@@ -146,15 +173,13 @@ pub fn test_rros_stop_timers() -> Result<usize> {
         yy.lock().set_interval(1000);
         yy.lock().set_base(tmb);
 
-
         (*tmb).q.add_head(xx.clone());
         (*tmb).q.add_head(yy.clone());
 
         pr_info!("before rros_adjust_timers");
         rros_stop_timers(&RROS_MONO_CLOCK);
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_rros_stop_timers end~~~");
     Ok(0)
-
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/cred.rs b/kernel/rros/cred.rs
new file mode 100644
index 000000000..8b1378917
--- /dev/null
+++ b/kernel/rros/cred.rs
@@ -0,0 +1 @@
+
diff --git a/kernel/rros/crossing.rs b/kernel/rros/crossing.rs
index a8dcd01e5..419210a9b 100644
--- a/kernel/rros/crossing.rs
+++ b/kernel/rros/crossing.rs
@@ -1,11 +1,107 @@
 use kernel::{
+    bindings,
     irq_work::IrqWork,
-    completion::Completion
+    completion::Completion, container_of, Result
 };
+
 use core::sync::atomic::{AtomicUsize, Ordering};
 
+
 pub struct RrosCrossing {
     oob_refs: AtomicUsize,
     oob_done: Completion,
     oob_work: IrqWork,
 }
+
+impl RrosCrossing{
+    pub fn new() -> Result<Self> {
+        Ok(RrosCrossing {
+            oob_refs: AtomicUsize::new(0),
+            oob_done: Completion::new(),
+            oob_work: IrqWork::new(),
+        })
+    }
+    
+    pub fn init(&mut self){
+        extern "C"{
+            fn rust_helper_atomic_set(v: *mut AtomicUsize, i: usize);
+        }
+        unsafe{rust_helper_atomic_set(&mut self.oob_refs as *mut AtomicUsize, 1)}
+        self.oob_done.init_completion();
+        self.oob_work.init_irq_work(rros_open_crossing);
+    }
+
+    #[inline]
+    pub fn down(&mut self){
+        extern "C"{
+            fn rust_helper_atomic_inc(v: *mut AtomicUsize);
+        }
+        unsafe{rust_helper_atomic_inc(&mut self.oob_refs as *mut AtomicUsize)}
+    }
+
+    #[inline]
+    pub fn up(&mut self){
+        extern "C"{
+            fn rust_helper_atomic_dec_return(v : *mut AtomicUsize) -> usize;
+        }
+        if unsafe{rust_helper_atomic_dec_return(&mut self.oob_refs as *mut AtomicUsize)} == 0{
+            self.oob_work.irq_work_queue();
+        }
+    }
+
+    #[inline]
+    pub fn pass(&mut self){
+        extern "C"{
+            fn rust_helper_atomic_dec_return(v : *mut AtomicUsize) -> usize;
+        }
+        if unsafe{rust_helper_atomic_dec_return(&mut self.oob_refs as *mut AtomicUsize)} > 0{
+            self.oob_done.wait_for_completion();
+        }
+    }
+}
+
+unsafe extern "C" fn rros_open_crossing(work: *mut bindings::irq_work) {
+    let mut c = kernel::container_of!(work, RrosCrossing, oob_work) as *mut RrosCrossing;
+    unsafe { (*c).oob_done.complete(); }
+}
+
+pub fn rros_init_crossing(crossing: &mut RrosCrossing) -> Result<usize> {
+    crossing.oob_refs.store(1, Ordering::Relaxed);
+    crossing.oob_done.init_completion();
+    crossing.oob_work.init_irq_work(rros_open_crossing);
+
+    Ok(0)
+}
+
+// pub fn rros_reinit_crossing(crossing: &mut RrosCrossing) -> Result<usize> {
+//     crossing.oob_refs.store(1, Ordering::Relaxed);
+//     crossing.oob_done.reinit_completion();
+
+//     Ok(0)
+// }
+
+pub fn rros_down_crossing(crossing: &mut RrosCrossing) -> Result<usize> {
+    crossing.oob_refs.fetch_add(1, Ordering::SeqCst);
+
+    Ok(0)
+}
+
+pub fn rros_up_crossing(crossing: &mut RrosCrossing) -> Result<usize> {
+    // CAUTION: the caller must guarantee that rros_down_crossing() cannot
+    // be invoked _after_ rros_pass_crossing() is entered for a given crossing.
+    crossing.oob_refs.fetch_sub(1, Ordering::SeqCst);
+    if crossing.oob_refs.load(Ordering::SeqCst) == 0 {
+        crossing.oob_work.irq_work_queue();
+    }
+
+    Ok(0)
+}
+
+pub fn rros_pass_crossing(crossing: &mut RrosCrossing) -> Result<usize> {
+    crossing.oob_refs.fetch_sub(1, Ordering::SeqCst);
+    if crossing.oob_refs.load(Ordering::SeqCst) > 0 {
+        crossing.oob_done.wait_for_completion();
+    }
+
+    Ok(0)
+}
\ No newline at end of file
diff --git a/kernel/rros/double_linked_list_test.rs b/kernel/rros/double_linked_list_test.rs
index 534988104..0679137fd 100644
--- a/kernel/rros/double_linked_list_test.rs
+++ b/kernel/rros/double_linked_list_test.rs
@@ -3,20 +3,20 @@
 use crate::factory;
 use crate::list::*;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    sched::*, 
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, sched::*,
+    timer::*, RROS_OOB_CPUS,
 };
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::Deref;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,
+    bindings, c_types, cpumask::CpumaskT, double_linked_list::*, file_operations::FileOperations,
+    ktime::*, percpu, percpu_defs, prelude::*, premmpt, spinlock_init, str::CStr, sync::Guard,
+    sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use core::ops::DerefMut;
-use core::ops::Deref;
 
 pub fn test_enqueue_by_index() -> Result<usize> {
     pr_info!("~~~test_double_linked_list begin~~~");
@@ -35,7 +35,6 @@ pub fn test_enqueue_by_index() -> Result<usize> {
         let pinned = unsafe { Pin::new_unchecked(&mut a) };
         spinlock_init!(pinned, "a");
 
-
         let mut xx = Arc::try_new(x)?;
         let mut yy = Arc::try_new(y)?;
         let mut zz = Arc::try_new(z)?;
@@ -46,14 +45,14 @@ pub fn test_enqueue_by_index() -> Result<usize> {
         (*tmb).q.add_head(zz.clone());
 
         pr_info!("before enqueue_by_index");
-        (*tmb).q.enqueue_by_index(2,aa);
-        pr_info!("len is {}",(*tmb).q.len());
+        (*tmb).q.enqueue_by_index(2, aa);
+        pr_info!("len is {}", (*tmb).q.len());
 
         for i in 1..=(*tmb).q.len() {
             let mut _x = (*tmb).q.get_by_index(i).unwrap().value.clone();
-            pr_info!("data of x is {}",_x.lock().get_date());
+            pr_info!("data of x is {}", _x.lock().get_date());
         }
     }
     pr_info!("~~~test_double_linked_list end~~~");
     Ok(0)
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/factory.rs b/kernel/rros/factory.rs
index 4e4d9d21c..f1c063373 100644
--- a/kernel/rros/factory.rs
+++ b/kernel/rros/factory.rs
@@ -1,41 +1,55 @@
-﻿use core::{cell::{RefCell, Ref}, mem, clone::Clone, result::Result::Ok,};
-
-use crate::{
-    clock, file::RrosFileBinding
+use core::{
+    cell::{Ref, RefCell},
+    clone::Clone,
+    mem,
+    result::Result::Ok, convert::TryInto, marker::PhantomData, default::Default,
 };
+
+use crate::{clock, thread, file::RrosFileBinding, Rros};
 use alloc::rc::Rc;
+use kernel::{io_buffer::IoBufferWriter};
+use kernel::file_operations::{FileOpener, IoctlCommand};
+use kernel::uidgid::{KgidT, KuidT};
 use kernel::{
+    prelude::*,
     bindings,
-    bitmap::{bitmap_zalloc, self},
-    c_str, c_types, chrdev, class::{self, DevT}, device,
+    bitmap::{self, bitmap_zalloc},
+    c_str, c_types, chrdev,
+    class::{self, DevT},
+    device,
     file::File,
-    fs::{self, Filename}, irq_work,
+    file_operations::FileOperations,
+    fs::{self, Filename},
+    irq_work,
     prelude::*,
     rbtree, spinlock_init,
     str::CStr,
-    sync::{SpinLock, Lock, Guard},
+    sync::{Guard, Lock, SpinLock},
     sysfs,
+    task::Task,
     types::{self, hash_init},
     uidgid, workqueue, ThisModule,
-    file_operations::FileOperations, task::Task,
 };
+use core::ptr;
 
 extern "C" {
     fn rust_helper_dev_name(dev: *mut bindings::device) -> *const c_types::c_char;
+    fn rust_helper_put_user(val: u32, ptr: *mut u32) -> c_types::c_int;
 }
 
 type fundle_t = u32;
 const EVL_NO_HANDLE: fundle_t = 0x00000000;
 
-pub const RROS_CLONE_PUBLIC: i32 =	(1 << 16);
-pub const RROS_CLONE_PRIVATE: i32 =	(0 << 16);
-const RROS_CLONE_OBSERVABLE: i32 =	(1 << 17);
-const RROS_CLONE_NONBLOCK: i32 =	(1 << 18);
-const RROS_CLONE_MASTER: i32 =	(1 << 19);
-const RROS_CLONE_INPUT: i32 =		(1 << 20);
-const RROS_CLONE_OUTPUT: i32 =	(1 << 21);
-const RROS_CLONE_COREDEV: i32 =	(1 << 31);
-const RROS_CLONE_MASK: i32 =		((-1 << 16) & !RROS_CLONE_COREDEV);
+pub const RROS_THREAD_CLONE_FLAGS: i32 = (RROS_CLONE_PUBLIC | RROS_CLONE_OBSERVABLE | RROS_CLONE_MASTER);
+pub const RROS_CLONE_PUBLIC: i32 = (1 << 16);
+pub const RROS_CLONE_PRIVATE: i32 = (0 << 16);
+pub const RROS_CLONE_OBSERVABLE: i32 = (1 << 17);
+const RROS_CLONE_NONBLOCK: i32 = (1 << 18);
+pub const RROS_CLONE_MASTER: i32 = (1 << 19);
+const RROS_CLONE_INPUT: i32 = (1 << 20);
+const RROS_CLONE_OUTPUT: i32 = (1 << 21);
+const RROS_CLONE_COREDEV: i32 = (1 << 31);
+const RROS_CLONE_MASK: i32 = ((-1 << 16) & !RROS_CLONE_COREDEV);
 
 const RROS_DEVHASH_BITS: i32 = 8;
 pub const NR_FACTORIES: usize = 8;
@@ -43,10 +57,10 @@ const NR_CLOCKNR: usize = 8;
 const RROS_NO_HANDLE: fundle_t = 0x00000000;
 const NAME_HASH_TABLE_SIZE: u32 = 1 << 8;
 
-const CONFIG_RROS: u32 = 0; // unknown
+const CONFIG_RROS: usize = 0; // unknown
 const RROS_MUTEX_FLCLAIM: fundle_t = 0x80000000;
 const RROS_MUTEX_FLCEIL: fundle_t = 0x40000000;
-const RROS_HANDLE_INDEX_MASK: fundle_t = RROS_MUTEX_FLCEIL|RROS_MUTEX_FLCLAIM;
+const RROS_HANDLE_INDEX_MASK: fundle_t = RROS_MUTEX_FLCEIL | RROS_MUTEX_FLCLAIM;
 
 pub struct RrosIndex {
     rbroot: rbtree::RBTree<u32, u32>, // Todo: modify the u32.
@@ -66,16 +80,68 @@ pub struct RrosFactoryInside {
     pub index: Option<RrosIndex>,
     pub name_hash: Option<[types::HlistHead; NAME_HASH_TABLE_SIZE as usize]>,
     pub hash_lock: Option<SpinLock<i32>>,
-    pub register: Option<Pin<Box<chrdev::Registration<{NR_FACTORIES}>>>>,
+    // FIXME: This const should not be limited to 256. But the rust compiler does not support it.
+    pub register: Option<Pin<Box<chrdev::Registration<{ 8 }>>>>,
+}
+
+trait rros_fops :FileOperations {
+    fn hello(&self, file: &File) -> i32 {
+        0
+    }
+    // fn release(&self, file: &File) -> Result {
+    //     Ok(0)
+    // }
+    // fn read(&self, file: &File, buf: &mut IoBufferWriter) -> Result {
+    //     Ok(0)
+    // }
+    // fn write(&self, file: &File, buf: &mut IoBufferWriter) -> Result {
+    //     Ok(0)
+    // }
+    // fn poll(&self, file: &File, wait: &mut bindings::poll_table_struct) -> Result {
+    //     Ok(0)
+    // }
+    // fn ioctl(&self, file: &File, cmd: u32, arg: u64) -> Result {
+    //     Ok(0)
+    // }
+    // fn mmap(&self, file: &File, vma: &mut bindings::vm_area_struct) -> Result {
+    //     Ok(0)
+    // }
+    // fn fasync(&self, file: &File, fd: i32, flag: i32) -> Result {
+    //     Ok(0)
+    // }
+    // fn lock(&self, file: &File, cmd: i32, lock: &mut bindings::flock) -> Result {
+    //     Ok(0)
+    // }
+    // fn compat_ioctl(&self, file: &File, cmd: u32, arg: u64) -> Result {
+    //     Ok(0)
+    // }
+    // fn flush(&self, file: &File) -> Result {
+    //     Ok(0)
+    // }
+    // fn fsync(&self, file: &File, datasync: i32) -> Result {
+    //     Ok(0)
+    // }
+    // fn fallocate(&self, file: &File, mode: i32, offset: i64, len: i64) -> Result {
+    //     Ok(0)
+    // }
+    // fn fadvise(&self, file: &File, offset: i64, len: i64, advice: i32) -> Result {
+    //     Ok(0)
+    // }
+    // fn sendpage(&self, file: &File, page: &mut bindings::page, offset: i32, size: i32, more: &mut i32) -> Result {
+    //     Ok(0)
+    // }
+    // fn splice_write(&self, file: &File, pipe: &mut bindings::pipe_inode_info, splice_desc: &mut bindings::splice_desc) -> Result {
+    //     Ok(0)
+    // }
 }
 
 pub struct RrosFactory {
     pub name: &'static CStr,
     // pub fops: RustFile, // This entry is attached to the cdev in the rfl. It can be omitted in the factory struct.
-    pub nrdev: u32,
+    pub nrdev: usize,
     pub build: Option<
         fn(
-            fac: *mut RrosFactory,
+            fac: &'static mut SpinLock<RrosFactory>,
             uname: &'static CStr,
             u_attrs: Option<*mut u8>,
             clone_flags: i32,
@@ -86,11 +152,12 @@ pub struct RrosFactory {
     pub attrs: Option<sysfs::AttributeGroup>, //此处暂时option了
     pub flags: i32,
     pub inside: Option<RrosFactoryInside>,
+    // pub fops: PhantomData<T>,
 }
 
 pub static mut RROS_FACTORY: SpinLock<RrosFactory> = unsafe {
     SpinLock::new(RrosFactory {
-        name: unsafe{CStr::from_bytes_with_nul_unchecked("RROS_DEV\0".as_bytes())},
+        name: unsafe { CStr::from_bytes_with_nul_unchecked("RROS_DEV\0".as_bytes()) },
         nrdev: CONFIG_RROS,
         build: None,
         dispose: None,
@@ -108,17 +175,33 @@ pub static mut RROS_FACTORY: SpinLock<RrosFactory> = unsafe {
             index: None,
             name_hash: None,
             hash_lock: None,
-            register: None
-        })
+            register: None,
+        }),
     })
 };
 
+struct Tmpops;
+
+impl FileOperations for Tmpops {
+    kernel::declare_file_operations!();
+
+    fn read<T: IoBufferWriter>(
+        _this: &Self,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops of the rros tmp factory.");
+        Ok(1)
+    }
+}
+
 pub struct RrosElement {
     pub rcu_head: types::RcuHead,
     pub factory: &'static mut SpinLock<RrosFactory>,
     pub cdev: Option<chrdev::Cdev>,
     pub dev: Option<device::Device>, // dev
-    pub devname : Option<fs::Filename>,
+    pub devname: Option<fs::Filename>,
     pub minor: u64,
     pub refs: i32,
     pub zombie: bool,
@@ -134,25 +217,23 @@ pub struct RrosElement {
 
 impl RrosElement {
     pub fn new() -> Result<Self> {
-        Ok(
-            Self {
-                rcu_head: types::RcuHead::new(),
-                factory: unsafe{&mut RROS_FACTORY},
-                cdev: None,
-                dev: None,
-                devname: None,
-                minor: 0,
-                refs: 0,
-                zombie: false,
-                ref_lock: unsafe {kernel::sync::SpinLock::<i32>::new(0)},
-                fundle: 0,
-                clone_flags: 0,
-                irq_work: irq_work::IrqWork::new(),
-                work: unsafe{workqueue::Work::new()},
-                hash: types::HlistNode::new(),
-                fpriv: RrosElementfpriv::new(),
-            }
-        )
+        Ok(Self {
+            rcu_head: types::RcuHead::new(),
+            factory: unsafe { &mut RROS_FACTORY },
+            cdev: None,
+            dev: None,
+            devname: None,
+            minor: 0,
+            refs: 0,
+            zombie: false,
+            ref_lock: unsafe { kernel::sync::SpinLock::<i32>::new(0) },
+            fundle: 0,
+            clone_flags: 0,
+            irq_work: irq_work::IrqWork::new(),
+            work: unsafe { workqueue::Work::new() },
+            hash: types::HlistNode::new(),
+            fpriv: RrosElementfpriv::new(),
+        })
     }
 }
 pub struct RrosElementfpriv {
@@ -162,10 +243,7 @@ pub struct RrosElementfpriv {
 
 impl RrosElementfpriv {
     fn new() -> Self {
-        Self {
-            filp: None,
-            efd: 0,
-        }
+        Self { filp: None, efd: 0 }
     }
 }
 
@@ -211,25 +289,32 @@ unsafe extern "C" fn factory_type_devnode() {
     // }
 }
 
-fn create_element_device(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock<RrosFactory>) -> Result<usize> {
+fn create_element_device(
+    e: Rc<RefCell<RrosElement>>,
+    fac: &'static mut SpinLock<RrosFactory>,
+) -> Result<usize> {
     let mut fac_lock = fac.lock();
 
-    let hlen: u64 = fs::hashlen_string(c_str!("RROS").as_char_ptr(), e.clone().borrow_mut().devname.as_mut().unwrap() as *mut Filename);
-    
+    let hlen: u64 = fs::hashlen_string(
+        c_str!("RROS").as_char_ptr(),
+        e.clone().borrow_mut().devname.as_mut().unwrap() as *mut Filename,
+    );
+
     let res = match fac_lock.inside {
         Some(ref mut inside) => {
             inside.hash_lock.as_ref().unwrap().lock();
 
-
             // hash_for_each_possible(fac->name_hash, n, hash, hlen)
             // if (!strcmp(n->devname->name, e->devname->name)) {
             //     mutex_unlock(&fac->hash_lock);
             //     goto fail_hash;
             // }
-    
+
             // hash_add(fac->name_hash, &e->hash, hlen);
 
-            unsafe{inside.hash_lock.as_ref().unwrap().unlock();}
+            unsafe {
+                inside.hash_lock.as_ref().unwrap().unlock();
+            }
 
             // let res = do_element_visibility();
 
@@ -273,7 +358,6 @@ fn create_element_device(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock
 
 //     fail:
 //     put_device(dev); /* ->release_sys_device() */
-
 //     return ERR_PTR(ret);
 // }
 
@@ -287,53 +371,62 @@ fn rros_element_is_public(e: Rc<RefCell<RrosElement>>) -> bool {
 fn rros_element_has_coredev(e: Rc<RefCell<RrosElement>>) -> bool {
     let e_clone = e.clone();
     let mut e_borrow = e_clone.borrow();
-    
-	(e_borrow.clone_flags & RROS_CLONE_COREDEV) == RROS_CLONE_COREDEV
-}
-
-fn do_element_visibility(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock<RrosFactory>, rdev: class::DevT) -> Result<usize>{
-    let e_clone = e.clone();
-    let mut e_mut = e_clone.borrow_mut();
-    
-    let core_dev_res = rros_element_has_coredev(e.clone());
-    let mm_res = Task::current().kernel();
-    if !core_dev_res && !mm_res {
-        e_mut.clone_flags |= RROS_CLONE_COREDEV;
-    }
 
-    let mut fac_lock = fac.lock();
-    let res = rros_element_is_public(e.clone());
-    if res == true {
-        let fac_res = match fac_lock.inside {
-            Some(ref mut inside) => {
-                fac_lock.inside.as_mut().unwrap().register.as_mut().unwrap().as_mut().register::<clock::RustFileClock>()?;//change this to fac->fops
-                0
-            }
-            None => 1,
-        };
-        match fac_res {
-            1=>return Err(kernel::Error::EINVAL),
-            _ =>return Ok(0),
-        }
-        // *rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
-        // cdev_init(&e->cdev, fac->fops);
-        // return cdev_add(&e->cdev, *rdev, 1);
-    }
+    (e_borrow.clone_flags & RROS_CLONE_COREDEV) == RROS_CLONE_COREDEV
+}
 
-    let res = rros_element_has_coredev(e.clone());
-    if res == true {
-        return Ok(0);
-    }
+// fn do_element_visibility(
+//     e: Rc<RefCell<RrosElement>>,
+//     fac: &'static mut SpinLock<RrosFactory>,
+//     rdev: class::DevT,
+// ) -> Result<usize> {
+//     let e_clone = e.clone();
+//     let mut e_mut = e_clone.borrow_mut();
+
+//     let core_dev_res = rros_element_has_coredev(e.clone());
+//     let mm_res = Task::current().kernel();
+//     if !core_dev_res && !mm_res {
+//         e_mut.clone_flags |= RROS_CLONE_COREDEV;
+//     }
 
+//     let mut fac_lock = fac.lock();
+//     let res = rros_element_is_public(e.clone());
+//     if res == true {
+//         let fac_res = match fac_lock.inside {
+//             Some(ref mut inside) => {
+//                 fac_lock
+//                     .inside
+//                     .as_mut()
+//                     .unwrap()
+//                     .register
+//                     .as_mut()
+//                     .unwrap()
+//                     .as_mut()
+//                     .register::<clock::RustFileClock>()?; //TODO: change this to fac->fops
+//                 0
+//             }
+//             None => 1,
+//         };
+//         match fac_res {
+//             1 => return Err(kernel::Error::EINVAL),
+//             _ => return Ok(0),
+//         }
+//         // *rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
+//         // cdev_init(&e->cdev, fac->fops);
+//         // return cdev_add(&e->cdev, *rdev, 1);
+//     }
 
+//     let res = rros_element_has_coredev(e.clone());
+//     if res == true {
+//         return Ok(0);
+//     }
 
-    Ok(0)
-}
+//     Ok(0)
+// }
 
 // fn bind_file_to_element(filp: File, e: Rc<RefCell<RrosElement>>>) -> Result<usize> {
 //     let fbind: RrosFileBinding;
 
-
 // }
 
 // static int bind_file_to_element(struct file *filp, struct evl_element *e)
@@ -361,78 +454,77 @@ fn do_element_visibility(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock
 //     struct evl_factory *fac,
 //     dev_t *rdev)
 // {
-    // struct file *filp;
-    // int ret, efd;
+// struct file *filp;
+// int ret, efd;
 
-    // if (EVL_WARN_ON(CORE, !evl_element_has_coredev(e) && !current->mm))
-    // e->clone_flags |= EVL_CLONE_COREDEV;
-
-    // /*
-    // * Unlike a private one, a publically visible element exports
-    // * a cdev in the /dev/evl hierarchy so that any process can
-    // * see it.  Both types are backed by a kernel device object so
-    // * that we can export their state to userland via /sysfs.
-    // */
+// if (EVL_WARN_ON(CORE, !evl_element_has_coredev(e) && !current->mm))
+// e->clone_flags |= EVL_CLONE_COREDEV;
 
-    // if (evl_element_is_public(e)) {
-    // *rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
-    // cdev_init(&e->cdev, fac->fops);
-    // return cdev_add(&e->cdev, *rdev, 1);
-    // }
+// /*
+// * Unlike a private one, a publically visible element exports
+// * a cdev in the /dev/evl hierarchy so that any process can
+// * see it.  Both types are backed by a kernel device object so
+// * that we can export their state to userland via /sysfs.
+// */
+// if (evl_element_is_public(e)) {
+// *rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
+// cdev_init(&e->cdev, fac->fops);
+// return cdev_add(&e->cdev, *rdev, 1);
+// }
 
-    // *rdev = MKDEV(0, e->minor);
+// *rdev = MKDEV(0, e->minor);
 
-    // if (evl_element_has_coredev(e))
-    // return 0;
+// if (evl_element_has_coredev(e))
+// return 0;
 
-    // /*
-    // * Create a private user element, passing the real fops so
-    // * that FMODE_CAN_READ/WRITE are set accordingly by the vfs.
-    // */
-    // filp = anon_inode_getfile(evl_element_name(e), fac->fops,
-    //     NULL, O_RDWR);
-    // if (IS_ERR(filp)) {
-    // ret = PTR_ERR(filp);
-    // return ret;
-    // }
+// /*
+// * Create a private user element, passing the real fops so
+// * that FMODE_CAN_READ/WRITE are set accordingly by the vfs.
+// */
+// filp = anon_inode_getfile(evl_element_name(e), fac->fops,
+//     NULL, O_RDWR);
+// if (IS_ERR(filp)) {
+// ret = PTR_ERR(filp);
+// return ret;
+// }
 
-    // /*
-    // * Now switch to dummy fops temporarily, until calling
-    // * evl_release_element() is safe for filp, meaning once
-    // * bind_file_to_element() has returned successfully.
-    // */
-    // replace_fops(filp, &dummy_fops);
+// /*
+// * Now switch to dummy fops temporarily, until calling
+// * evl_release_element() is safe for filp, meaning once
+// * bind_file_to_element() has returned successfully.
+// */
+// replace_fops(filp, &dummy_fops);
 
-    // /*
-    // * There will be no open() call for this new private element
-    // * since we have no associated cdev, bind it to the anon file
-    // * immediately.
-    // */
-    // ret = bind_file_to_element(filp, e);
-    // if (ret) {
-    // filp_close(filp, current->files);
-    // /*
-    // * evl_release_element() was not called: do a manual
-    // * disposal.
-    // */
-    // fac->dispose(e);
-    // return ret;
-    // }
+// /*
+// * There will be no open() call for this new private element
+// * since we have no associated cdev, bind it to the anon file
+// * immediately.
+// */
+// ret = bind_file_to_element(filp, e);
+// if (ret) {
+// filp_close(filp, current->files);
+// /*
+// * evl_release_element() was not called: do a manual
+// * disposal.
+// */
+// fac->dispose(e);
+// return ret;
+// }
 
-    // /* Back to the real fops for this element class. */
-    // replace_fops(filp, fac->fops);
+// /* Back to the real fops for this element class. */
+// replace_fops(filp, fac->fops);
 
-    // efd = get_unused_fd_flags(O_RDWR|O_CLOEXEC);
-    // if (efd < 0) {
-    // filp_close(filp, current->files);
-    // ret = efd;
-    // return ret;
-    // }
+// efd = get_unused_fd_flags(O_RDWR|O_CLOEXEC);
+// if (efd < 0) {
+// filp_close(filp, current->files);
+// ret = efd;
+// return ret;
+// }
 
-    // e->fpriv.filp = filp;
-    // e->fpriv.efd = efd;
+// e->fpriv.filp = filp;
+// e->fpriv.efd = efd;
 
-    // return 0;
+// return 0;
 // }
 
 // static int create_element_device(struct evl_element *e,
@@ -515,7 +607,11 @@ fn do_element_visibility(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock
 // return ret;
 // }
 
-fn rros_create_core_element_device(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock<RrosFactory>, name: &'static CStr) -> Result<usize> {
+fn rros_create_core_element_device(
+    e: Rc<RefCell<RrosElement>>,
+    fac: &'static mut SpinLock<RrosFactory>,
+    name: &'static CStr,
+) -> Result<usize> {
     let e_clone = e.clone();
     let mut e_mut = e_clone.borrow_mut();
 
@@ -560,8 +656,12 @@ fn rros_create_core_element_device(e: Rc<RefCell<RrosElement>>, fac: &'static mu
 // }
 
 // todo: The global variable should not use *mut to pass the value.
-pub fn rros_init_element(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock<RrosFactory>, clone_flags: i32) -> Result<usize> {
-    let mut minor=0;
+pub fn rros_init_element(
+    e: Rc<RefCell<RrosElement>>,
+    fac: &'static mut SpinLock<RrosFactory>,
+    clone_flags: i32,
+) -> Result<usize> {
+    let mut minor = 0;
     let mut fac_lock = fac.lock();
     let nrdev = fac_lock.nrdev;
 
@@ -569,23 +669,34 @@ pub fn rros_init_element(e: Rc<RefCell<RrosElement>>, fac: &'static mut SpinLock
         Some(ref mut inside) => {
             loop {
                 let minor_map;
-                if inside.minor_map.is_none(){
+                if inside.minor_map.is_none() {
                     return Err(kernel::Error::EINVAL);
                 }
                 minor_map = inside.minor_map.unwrap();
 
-                unsafe{minor = bitmap::find_first_zero_bit(minor_map as *mut u8 as *const c_types::c_ulong, nrdev as u64)}
+                unsafe {
+                    minor = bitmap::find_first_zero_bit(
+                        minor_map as *mut u8 as *const c_types::c_ulong,
+                        nrdev as u64,
+                    )
+                }
                 if minor >= nrdev as u64 {
                     pr_warn!("out of factory number");
                     return Err(kernel::Error::EINVAL);
                 }
-                unsafe{if !bitmap::test_and_set_bit(minor, minor_map as *mut c_types::c_ulong) { break; }}
+                unsafe {
+                    if !bitmap::test_and_set_bit(minor, minor_map as *mut c_types::c_ulong) {
+                        break;
+                    }
+                }
             }
             0
         }
         None => 1,
     };
-    unsafe{fac.unlock();}
+    unsafe {
+        fac.unlock();
+    }
     drop(fac_lock);
     let e_clone = e.clone();
     let mut e_mut = e_clone.borrow_mut();
@@ -620,7 +731,8 @@ fn rros_create_factory(
         Some(ref mut inside) => {
             let mut idevname = CStr::from_bytes_with_nul("clone\0".as_bytes())?;
 
-            if flags == 0 {//EVL_FACTORY_SINGLE
+            if flags == 0 {
+                //EVL_FACTORY_SINGLE
                 idevname = name;
                 inside.class = Some(rros_class.clone());
                 inside.minor_map = Some(0);
@@ -628,41 +740,56 @@ fn rros_create_factory(
                 // We use cdev_alloc to replace cdev_init. alloc_chrdev + cdev_alloc + cdev_add
                 chrdev_reg.as_mut().register::<clock::RustFileClock>()?;
                 // dev = create_sys_device(rdev, fac, NULL, idevname);
-            // }
-            } else if  flags == 1 {// EVL_FACTORY_CLONE&&
+                // }
+            } else if flags == 1 {
+                // EVL_FACTORY_CLONE&&
                 // create_element_class
-                inside.minor_map = Some(bitmap_zalloc(nrdev, bindings::GFP_KERNEL));
+                inside.minor_map = Some(bitmap_zalloc(nrdev.try_into().unwrap(), bindings::GFP_KERNEL));
                 if inside.minor_map == Some(0) {
                     return Err(kernel::Error::EINVAL);
                 }
-    
-                inside.class = Some(Arc::try_new(class::Class::new(this_module, name.as_char_ptr())?)?);
+
+                inside.class = Some(Arc::try_new(class::Class::new(
+                    this_module,
+                    name.as_char_ptr(),
+                )?)?);
                 inside.rrtype = Some(device::DeviceType::new().name(name.as_char_ptr()));
                 // fac.rrtype.devnode(Option::Some(factory_type_devnode));
                 // fac.kuid = GLOBAL_ROOT_UID;
                 // fac.kgid = GLOBAL_ROOT_GID;
-                let mut chrdev_reg: Pin<Box<chrdev::Registration<NR_CLOCKNR>>> =
-                    chrdev::Registration::new_pinned(c_str!("rust_clock"), 0, this_module)?;
+                // here we cannot get the number from the nrdev, because this requires const
+                let mut ele_chrdev_reg: Pin<Box<chrdev::Registration<{thread::CONFIG_RROS_NR_THREADS}>>> =
+                    chrdev::Registration::new_pinned(name, 0, this_module)?;
                 // alloc_chrdev is inside this function
-                chrdev_reg.as_mut().register::<clock::RustFileClock>()?;//alloc_chrdev + cdev_alloc + cdev_add
-                inside.register = Some(chrdev_reg);
+                // no need to call register here
+                // ele_chrdev_reg.as_mut().register::<fac.inside_data()>()?; //alloc_chrdev + cdev_alloc + cdev_add
+                inside.register = Some(ele_chrdev_reg);
                 // create_element_class end
-            
+
+                // FIXME: this should be variable. But the `register` needs a const value. We just hack for now. If we need more
+                // factory, we need to change the code here. One way here is to use index to find the struct.
                 // dev = create_sys_device(rdev, fac, NULL, idevname);
                 
-            } else { //clock_factory
+                // let factory_ops = fac.locked_data().into_inner();
+                chrdev_reg.as_mut().register::<CloneOps>()?;
+            } else {
+                //clock_factory
                 // create_element_class
-                inside.minor_map = Some(bitmap_zalloc(nrdev, bindings::GFP_KERNEL));
+                inside.minor_map = Some(bitmap_zalloc(nrdev.try_into().unwrap(), bindings::GFP_KERNEL));
                 if inside.minor_map == Some(0) {
                     return Err(kernel::Error::EINVAL);
                 }
-    
-                inside.class = Some(Arc::try_new(class::Class::new(this_module, name.as_char_ptr())?)?);
+
+                inside.class = Some(Arc::try_new(class::Class::new(
+                    this_module,
+                    name.as_char_ptr(),
+                )?)?);
                 inside.rrtype = Some(device::DeviceType::new().name(name.as_char_ptr()));
                 // fac.rrtype.devnode(Option::Some(factory_type_devnode));
                 // fac.kuid = GLOBAL_ROOT_UID;
                 // fac.kgid = GLOBAL_ROOT_GID;
-                let mut chrdev_reg: Pin<Box<chrdev::Registration<NR_CLOCKNR>>> =
+                // FIXME: this should be NR_CLOCKNR, but due to this number be consistent with the number of threads, we use the number of threads
+                let mut chrdev_reg: Pin<Box<chrdev::Registration<{thread::CONFIG_RROS_NR_THREADS}>>> =
                     chrdev::Registration::new_pinned(c_str!("rust_clock"), 0, this_module)?;
                 // alloc_chrdev is inside this function
                 // chrdev_reg.as_mut().register::<clock::RustFileClock>()?;//alloc_chrdev + cdev_alloc + cdev_add
@@ -695,10 +822,94 @@ fn rros_create_factory(
         None => 1,
     };
 
-    unsafe{fac.unlock()};
+    unsafe { fac.unlock() };
     match res {
-        1=>Err(kernel::Error::EINVAL),
-        _ =>Ok(0)
+        1 => Err(kernel::Error::EINVAL),
+        _ => Ok(0),
+    }
+}
+
+
+// TODO: adjust the order of use and funciton in the whole project
+
+// #[derive(Default)]
+pub struct CloneData{
+    pub ptr: *mut u8,
+}
+
+impl Default for CloneData{
+    fn default() -> Self {
+        CloneData{
+            ptr: 0 as *mut u8,
+        }
+    }
+}
+
+#[derive(Default)]
+pub struct CloneOps;
+
+impl FileOpener<u8> for CloneOps {
+    fn open(shared: &u8) -> Result<Self::Wrapper> {
+        let mut data = CloneData::default();
+        unsafe{
+            data.ptr = shared as *const u8 as *mut u8;
+            let a = KuidT((*(shared as *const u8 as *const bindings::inode)).i_uid);
+            let b = KgidT((*(shared as *const u8 as *const bindings::inode)).i_gid);
+            (*thread::RROS_TRHEAD_FACTORY.locked_data().get()).inside.as_mut().unwrap().kuid = Some(a);
+            (*thread::RROS_TRHEAD_FACTORY.locked_data().get()).inside.as_mut().unwrap().kgid = Some(b);
+        }
+        // bindings::stream_open();
+        Ok(Box::try_new(data)?)
+    }
+    // fn open<T: IoBufferWriter>(
+    //     _this: &Self,
+    //     _file: &File,
+    //     _data: &mut T,
+    //     _offset: u64,
+    // ) -> Result<usize> {
+    //     pr_info!("I'm the open ops from the clone ops.");
+
+    //     unsafe {
+    //         (*thread::RROS_TRHEAD_FACTORY.get_locked_data().get()).inside.as_ref().unwrap().kuid = i
+
+    //     };
+    //     Ok(1)
+    // }
+}
+
+// FIXME: all the ops is made for the thread factory. We need to change this later.
+impl FileOperations for CloneOps {
+    kernel::declare_file_operations!(read, ioctl);
+
+    type Wrapper = Box<CloneData>;
+
+    fn read<T: IoBufferWriter>(
+        _this: &CloneData,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops from the clone ops.");
+        Ok(1)
+    }
+    
+    fn release(
+        _this: Box<CloneData>,
+        _file: &File,
+    ) {
+        pr_info!("I'm the release ops from the clone ops.");
+        // FIXME: put the evl element
+    }
+
+    fn ioctl(
+        _this: &CloneData,
+        file: &File,
+        cmd: &mut IoctlCommand,
+    ) -> Result<i32> {
+        pr_info!("I'm the unlock_ioctl ops from the clone ops.");
+        // FIXME: use the IoctlCommand in the right way
+        ioctl_clone_device(file, cmd.cmd, cmd.arg);
+        Ok(0)
     }
 }
 
@@ -721,8 +932,13 @@ fn create_core_factories(
     Ok(0)
 }
 
-pub fn rros_early_init_factories(this_module: &'static ThisModule) -> Result<Pin<Box<chrdev::Registration<NR_FACTORIES>>>> {
-    let mut early_factories: [ &mut SpinLock<RrosFactory>; 1] = unsafe { [&mut clock::RROS_CLOCK_FACTORY] };
+pub fn rros_early_init_factories(
+    this_module: &'static ThisModule,
+) -> Result<Pin<Box<chrdev::Registration<NR_FACTORIES>>>> {
+    // TODO: move the number of factories to a variable
+    let mut early_factories: [&mut SpinLock<RrosFactory>; 2] =
+        unsafe { [&mut clock::RROS_CLOCK_FACTORY, &mut thread::RROS_TRHEAD_FACTORY
+                  ] };
     // static struct rros_factory *early_factories[] = {
     // 	&rros_clock_factory,
     // };
@@ -767,7 +983,7 @@ pub fn rros_early_init_factories(this_module: &'static ThisModule) -> Result<Pin
 // struct inode;
 
 // // fn rros_open_element(inode: Rc<RefCell<inode>>, flip: File) -> Result<usize>{
-    
+
 // // }
 
 // impl<T: Sync> FileOpenAdapter for Registration<T> {
@@ -785,16 +1001,269 @@ pub fn rros_early_init_factories(this_module: &'static ThisModule) -> Result<Pin
 #[derive(Default)]
 struct RRosRustFile;
 
+struct Ct {
+    pub count: i32,
+}
+
 impl FileOperations for RRosRustFile {
-    kernel::declare_file_operations!();
+    kernel::declare_file_operations!(read);
+
+    fn read<T: IoBufferWriter>(
+        _this: &Self,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops of the rros factory.");
+        Ok(1)
+    }
 }
 
-// impl FileOpener<Pin<Ref<SharedState>>> for RRosRustFile {
-//     fn open(shared: &Pin<Ref<SharedState>>) -> Result<Self::Wrapper> {
-//         Ok(shared.clone())
+// impl FileOpener<Pin<Ref<Ct>>> for RRosRustFile {
+//     fn open(shared: &Ref<Ct>) -> Result<Box<Self>> {
+//         Ok(Box::try_new(Self.clone())?)
 //     }
 // }
 
 pub fn rros_get_index(handle: fundle_t) -> fundle_t {
     handle & !RROS_HANDLE_INDEX_MASK
 }
+
+#[repr(C)]
+struct RrosCloneReq {
+    name: *const c_types::c_char,
+    attrs: *mut c_types::c_void,
+    clone_flags: c_types::c_uint,
+    eids: RrosElementIds,
+    efd: c_types::c_int,
+}
+
+#[repr(C)]
+#[derive(Clone, Copy)]
+struct RrosElementIds {
+    minor: c_types::c_uint,
+    fundle: fundle_t,
+    state_offset: c_types::c_uint,
+}
+
+impl Default for RrosElementIds {
+    fn default() -> Self {
+        Self {
+            minor: 0,
+            fundle: 0,
+            state_offset: 0,
+        }
+    }
+}
+
+impl RrosCloneReq {
+    fn new(name: *const c_types::c_char, attrs: *mut c_types::c_void) -> Self {
+        Self {
+            name,
+            attrs,
+            clone_flags: 0,
+            eids: RrosElementIds::default(),
+            efd: 0,
+        }
+    }
+
+    fn from_ptr(ptr: *mut RrosCloneReq) -> Self{
+        Self {
+            name: unsafe { (*ptr).name },
+            attrs: unsafe { (*ptr).attrs },
+            clone_flags: unsafe { (*ptr).clone_flags },
+            eids: unsafe { (*ptr).eids },
+            efd: unsafe { (*ptr).efd },
+        }
+    }
+}
+// struct evl_element_ids {
+// 	__u32 minor;
+// 	__u32 fundle;
+// 	__u32 state_offset;
+// };
+
+// struct evl_clone_req {
+// 	__u64 name_ptr;		/* (const char __user *name) */
+// 	__u64 attrs_ptr;	/* (void __user *attrs) */
+// 	__u32 clone_flags;
+// 	/* Output on success: */
+// 	struct evl_element_ids eids;
+// 	__u32 efd;
+// };
+
+fn rros_index_factory_element() {
+}
+// static inline void evl_index_factory_element(struct evl_element *e)
+// {
+// 	evl_index_element(&e->factory->index, e);
+// }
+
+use kernel::user_ptr::UserSlicePtr;
+use core::mem::size_of;
+
+extern "C" {
+    pub fn rust_helper_copy_from_user(
+        to: *mut c_types::c_void,
+        from: *const c_types::c_void,
+        n: c_types::c_ulong,
+    ) -> c_types::c_ulong;
+}
+
+pub fn ioctl_clone_device(file: &File, cmd: u32, arg: usize) -> Result<usize> {
+    // static long ioctl_clone_device(struct file *filp, unsigned int cmd,
+//     unsigned long arg)
+// {
+// struct evl_element *e = filp->private_data;
+// struct evl_clone_req req, __user *u_req;
+    let state_offset: u32 = u32::MAX;
+// __u32 val, state_offset = -1U;
+// const char __user *u_name;
+// struct evl_factory *fac;
+// void __user *u_attrs;
+// int ret;
+
+// if (cmd != EVL_IOC_CLONE)
+// return -ENOTTY;
+
+// if (!evl_is_running())
+// return -ENXIO;
+
+// if (e)
+// return -EBUSY;
+
+    // TODO: add the support of clone device cmd
+    // if (cmd != EVL_IOC_CLONE):
+        // return -ENOTTY;
+    
+    // TODO: add the evl running check
+    // if (!evl_is_running())
+    // return -ENXIO;
+
+    // TODO: add the support of private data check
+    // let e = filp->private_data;
+    // if (e)
+    //     return -EBUSY;    
+
+    // TODO: user parameters
+    pr_info!("size is {}", size_of::<RrosCloneReq>());
+    let mut real_req = RrosCloneReq::new(0 as *const c_types::c_char, 0 as *mut c_types::c_void);
+    let res = unsafe { rust_helper_copy_from_user(&mut real_req as *mut RrosCloneReq as *mut c_types::c_void, arg as *mut c_types::c_void, size_of::<RrosCloneReq>() as u64)};
+    if res != 0 {
+        pr_info!("copy from user failed");
+        return Err(Error::EFAULT);
+    }
+    // let u_req = unsafe{UserSlicePtr::new(arg as *mut c_types::c_void, size_of::<RrosCloneReq>())};
+    // let req = u_req.read_all()?;
+        
+    // let mut real_req: RrosCloneReq = unsafe{*ptr::slice_from_raw_parts_mut(core::mem::transmute(req.as_ptr()), req.len())};
+    // TODO: fix the unsafe code
+    // let mut real_req: RrosCloneReq = unsafe{core::mem::transmute_copy(&req.as_ptr())};
+    pr_info!("real_req {}", real_req.clone_flags);
+    pr_info!("real_req {}", real_req.efd);
+    pr_info!("real_req {}", real_req.eids.fundle);
+    pr_info!("real_req {}", real_req.eids.minor);
+    pr_info!("real_req {}", real_req.eids.state_offset);
+    pr_info!("real_req {:p}", real_req.name);
+    pr_info!("real_req {:p}", real_req.attrs);
+
+// u_req = (typeof(u_req))arg;
+// ret = copy_from_user(&req, u_req, sizeof(req));
+// if (ret)
+// return -EFAULT;
+
+    let u_name = real_req.name as *const u8;
+// u_name = evl_valptr64(req.name_ptr, const char);
+// if (u_name == NULL && req.clone_flags & EVL_CLONE_PUBLIC)
+// return -EINVAL;
+
+    let u_attrs = real_req.attrs as *mut u8;
+// u_attrs = evl_valptr64(req.attrs_ptr, void);
+// fac = container_of(filp->f_inode->i_cdev, struct evl_factory, cdev);
+    // FIXME: update the cdev logic to use container_of && update the uname
+    let e:*mut RrosElement = unsafe{(*thread::RROS_TRHEAD_FACTORY.locked_data().get()).build.unwrap()(&mut thread::RROS_TRHEAD_FACTORY, CStr::from_char_ptr(u_name as *const c_types::c_char), Some(u_attrs), 0, &state_offset)};
+    //real_req.clone_flags as i32
+
+    unsafe{pr_info!("4 uninit_thread: x ref is {}", Arc::strong_count(&thread::uthread.clone().unwrap()));}
+    unsafe{pr_info!("4.5 uninit_thread: x ref is {}", Arc::strong_count(&thread::uthread.clone().unwrap()));}
+// e = fac->build(fac, u_name, u_attrs, req.clone_flags, &state_offset);
+// if (IS_ERR(e))
+// return PTR_ERR(e);
+
+// /* This must be set before the device appears. */
+    // file.set_private_data(e as *mut c_types::c_void);
+// filp->private_data = e;
+// barrier();
+
+    // TODO: create the element device
+// ret = create_element_device(e, fac);
+// if (ret) {
+// /* release_clone_device() must skip cleanup. */
+// filp->private_data = NULL;
+// /*
+//  * If we failed to create a private element,
+//  * evl_release_element() did run via filp_close(), so
+//  * the disposal has taken place already.
+//  *
+//  * NOTE: this code should never directly handle core
+//  * devices, since we are running the user interface to
+//  * cloning a new element. Although a thread may be
+//  * associated with a coredev observable, the latter
+//  * does not export any direct interface to user.
+//  */
+// EVL_WARN_ON(CORE, evl_element_has_coredev(e));
+// /*
+//  * @e might be stale if it was private, test the
+//  * visibility flag from the request block instead.
+//  */
+// if (req.clone_flags & EVL_CLONE_PUBLIC)
+//     fac->dispose(e);
+// return ret;
+// }
+
+    let mut ret: i32 = 0;
+    unsafe{
+        // let val = (*e).minor;
+        // val = e->minor;
+        // ret |= rust_helper_put_user(val as u32, &mut real_req.eids.minor as *mut u32);
+        pr_info!("the ret is {}", ret);
+        // ret |= put_user(val, &u_req->eids.minor);
+        // let val = (*e).fundle;
+        // val = e->fundle;
+        // ret |= rust_helper_put_user(val, &mut real_req.eids.fundle as *mut u32);
+        // pr_info!("the ret is {}", ret);
+        // ret |= put_user(val, &u_req->eids.fundle);
+        // ret |= rust_helper_put_user(state_offset, &mut real_req.eids.state_offset as *mut u32);
+        // pr_info!("the ret is {}", ret);
+        // ret |= put_user(state_offset, &u_req->eids.state_offset);
+        // let val = (*e).fpriv.efd;
+        // val = e->fpriv.efd;
+        // pr_info!("the val is {}", val);
+        // ret |= rust_helper_put_user(val as u32, &mut real_req.efd as *mut i32 as *mut u32);
+        // pr_info!("the ret is {}", ret);
+        // ret |= put_user(val, &u_req->efd);
+    }
+    // pr_info!("the ret is {}", ret);
+    // if ret!=0{
+        // return Err(kernel::Error::EFAULT);
+    // }
+    unsafe{pr_info!("5 uninit_thread: x ref is {}", Arc::strong_count(&thread::uthread.clone().unwrap()));}
+    Ok(0)
+// return ret ? -EFAULT : 0;
+// }
+}
+
+fn rros_element_name(e: &RrosElement) -> *const c_types::c_char {
+    if e.devname.is_some() {
+        return e.devname.as_ref().unwrap().get_name();
+    }
+    0 as *const c_types::c_char
+} 
+// static inline const char *
+// evl_element_name(struct evl_element *e)
+// {
+// 	if (e->devname)
+// 		return e->devname->name;
+
+// 	return NULL;
+// }
\ No newline at end of file
diff --git a/kernel/rros/fifo.rs b/kernel/rros/fifo.rs
index 144d2587a..dd72c9fd4 100644
--- a/kernel/rros/fifo.rs
+++ b/kernel/rros/fifo.rs
@@ -1,14 +1,18 @@
-use crate::{
-    queue, sched, sched::rros_thread, thread::*,
-};
+use crate::{queue, sched, sched::rros_thread, thread::*};
 use alloc::rc::Rc;
 use core::cell::RefCell;
-use kernel::{c_types, prelude::*, Error,sync::{SpinLock,Lock, Guard},double_linked_list::Node,};
-use core::ptr::{null_mut, null};
+use core::ptr::{null, null_mut, NonNull};
+use kernel::{
+    c_types,
+    double_linked_list::Node,
+    prelude::*,
+    sync::{Guard, Lock, SpinLock},
+    Error,
+};
 
-pub static mut rros_sched_fifo:sched::rros_sched_class = sched::rros_sched_class{
-	sched_init: Some(rros_fifo_init),
-    sched_tick : Some(rros_fifo_tick),
+pub static mut rros_sched_fifo: sched::rros_sched_class = sched::rros_sched_class {
+    sched_init: Some(rros_fifo_init),
+    sched_tick: Some(rros_fifo_tick),
     sched_setparam: Some(rros_fifo_setparam),
     sched_getparam: Some(rros_fifo_getparam),
     sched_chkparam: Some(rros_fifo_chkparam),
@@ -29,7 +33,7 @@ pub static mut rros_sched_fifo:sched::rros_sched_class = sched::rros_sched_class
     sched_control: None,
     nthreads: 0,
     next: 0 as *mut sched::rros_sched_class,
-    flag:3,
+    flag: 3,
 };
 
 pub const RROS_FIFO_MIN_PRIO: i32 = 1;
@@ -72,221 +76,224 @@ fn rros_fifo_init(rq: *mut sched::rros_rq) -> Result<usize> {
     Ok(0)
 }
 
-fn rros_fifo_tick(rq:Option<*mut sched::rros_rq>) -> Result<usize>{
-    match rq{
+fn rros_fifo_tick(rq: Option<*mut sched::rros_rq>) -> Result<usize> {
+    match rq {
         Some(_) => (),
         None => return Err(kernel::Error::EINVAL),
     }
-	let rq_ptr = rq.unwrap();
-	let mut curr;
-	unsafe{
-        match (*rq_ptr).curr.clone(){
+    let rq_ptr = rq.unwrap();
+    let mut curr;
+    unsafe {
+        match (*rq_ptr).curr.clone() {
             Some(c) => curr = Some(c.clone()),
-            None=> {
+            None => {
                 pr_warn!("err");
                 return Err(kernel::Error::EINVAL);
             }
         }
     }
-	sched::rros_putback_thread(curr.unwrap());
+    sched::rros_putback_thread(curr.unwrap());
     Ok(0)
 }
 
 fn rros_fifo_setparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Rc<RefCell<sched::rros_sched_param>>>
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<usize> {
-    return __rros_set_fifo_schedparam(thread, p);
+    return __rros_set_fifo_schedparam(thread.clone(), p.clone());
 }
 
 fn rros_fifo_getparam(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: Rc<RefCell<sched::rros_sched_param>>,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
+    p: Option<Arc<SpinLock<sched::rros_sched_param>>>,
 ) {
     __rros_get_fifo_schedparam(thread.clone(), p.clone());
 }
 
 fn rros_fifo_chkparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
-    p: Option<Rc<RefCell<sched::rros_sched_param>>>,
+    thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<i32> {
     return __rros_chk_fifo_schedparam(thread.clone(), p.clone());
 }
 
-fn rros_fifo_trackprio(thread: Rc<RefCell<sched::rros_thread>>, p: *const sched::rros_sched_param) {
-    __rros_track_fifo_priority(thread.clone(), p);
+fn rros_fifo_trackprio(thread: Option<Arc<SpinLock<sched::rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>) {
+    __rros_track_fifo_priority(thread.clone(), p.clone());
 }
 
-fn rros_fifo_ceilprio(thread: Rc<RefCell<sched::rros_thread>>, prio: i32) {
+fn rros_fifo_ceilprio(thread: Arc<SpinLock<sched::rros_thread>>, prio: i32) {
     __rros_ceil_fifo_priority(thread.clone(), prio);
 }
 
 fn rros_fifo_show(
-    thread: Rc<RefCell<sched::rros_thread>>,
+    thread: *mut sched::rros_thread,
     buf: *mut c_types::c_char,
     count: sched::ssize_t,
 ) -> Result<usize> {
-    let thread_ptr = thread.borrow_mut();
-    if thread_ptr.state & T_RRB != 0 {
+    unsafe{
+    if (*thread).state & T_RRB != 0 {
         // return snprintf(buf, count, "%Ld\n",ktime_to_ns(thread->rrperiod));
         pr_warn!("rros_fifo_show error!!");
         return Err(kernel::Error::EPERM);
     }
+    }
     Ok(0)
 }
 
 fn __rros_set_fifo_schedparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Rc<RefCell<sched::rros_sched_param>>>
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<usize> {
     let thread_clone = thread.clone();
     let p_unwrap = p.unwrap();
-    let p_ptr = p_unwrap.borrow();
     let thread_unwrap = thread.unwrap();
     
-    let ret = sched::rros_set_effective_thread_priority(thread_clone, p_ptr.fifo.prio);
+    let prio = unsafe{(*p_unwrap.locked_data().get()).fifo.prio};
+    let ret = unsafe{sched::rros_set_effective_thread_priority(thread_clone, prio)};
     let mut state = thread_unwrap.lock().state;
     if state & T_BOOST == 0 {
         thread_unwrap.lock().state &= !T_WEAK;
     }
-    pr_info!("thread before calling {}", state);
+    // pr_info!("thread before calling {}", state);
     ret
 }
 
 fn __rros_get_fifo_schedparam(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: Rc<RefCell<sched::rros_sched_param>>,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
+    p: Option<Arc<SpinLock<sched::rros_sched_param>>>,
 ) {
-    let thread_ptr = thread.borrow_mut();
-    let mut p_ptr = p.borrow_mut();
-    p_ptr.fifo.prio = thread_ptr.cprio;
+    p.unwrap().lock().fifo.prio = thread.unwrap().lock().cprio;
 }
 
 //逻辑完整，未测试
 fn __rros_chk_fifo_schedparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
-    p: Option<Rc<RefCell<sched::rros_sched_param>>>,
+    thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<i32> {
     let thread_unwrap = thread.unwrap();
     let mut min = RROS_FIFO_MIN_PRIO;
     let mut max = RROS_FIFO_MAX_PRIO;
     let p_unwrap = p.unwrap();
-    let p_ptr = p_unwrap.borrow();
-
-    if thread_unwrap.lock().state & T_USER == 0x0 {
-        min = RROS_CORE_MIN_PRIO;
-        max = RROS_CORE_MAX_PRIO;
-    }
-
-    if p_ptr.fifo.prio < min || p_ptr.fifo.prio > max {
-        return Err(kernel::Error::EINVAL);
+    unsafe{
+        let state = (*thread_unwrap.locked_data().get()).state;
+        if state & T_USER == 0x0 {
+            min = RROS_CORE_MIN_PRIO;
+            max = RROS_CORE_MAX_PRIO;
+        }
+        let prio = (*p_unwrap.locked_data().get()).fifo.prio;
+        if prio < min || prio > max {
+            return Err(kernel::Error::EINVAL);
+        }
     }
     Ok(0)
 }
 
 fn __rros_track_fifo_priority(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: *const sched::rros_sched_param,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) {
-    let mut thread_ptr = thread.borrow_mut();
-    if p != null() {
-        thread_ptr.cprio = unsafe { (*p) }.fifo.prio;
+    let thread_unwrap = thread.unwrap();
+    if p.is_some() {
+        thread_unwrap.lock().cprio = p.unwrap().lock().fifo.prio;
     } else {
-        thread_ptr.cprio = thread_ptr.bprio;
-        thread_ptr.state &= !T_WEAK;
+        thread_unwrap.lock().cprio = thread_unwrap.lock().bprio;
+        thread_unwrap.lock().state &= !T_WEAK;
     }
 }
 
-fn __rros_ceil_fifo_priority(thread: Rc<RefCell<sched::rros_thread>>, prio: i32) {
-    let mut thread_ptr = thread.borrow_mut();
-    thread_ptr.cprio = prio;
+fn __rros_ceil_fifo_priority(thread: Arc<SpinLock<sched::rros_thread>>, prio: i32) {
+    unsafe{(*thread.locked_data().get()).cprio = prio};
 }
 
 //测试通过
-pub fn __rros_dequeue_fifo_thread(thread: Arc<SpinLock<sched::rros_thread>>) -> Result<usize>{
-    let mut rq_next = thread.lock().rq_next;
-    if rq_next == 0 as *mut Node<Arc<SpinLock<rros_thread>>> {
+pub fn __rros_dequeue_fifo_thread(thread: Arc<SpinLock<sched::rros_thread>>) -> Result<usize> {
+    let mut rq_next = thread.lock().rq_next.clone();
+    if rq_next.is_none(){
         return Err(kernel::Error::EINVAL);
     } else {
-        unsafe{
-            (*thread.lock().rq_next).remove();
+        unsafe {
+            thread.lock().rq_next.as_mut().unwrap().as_mut().remove();
             //这里是否要释放？
         }
-        rq_next = 0 as *mut Node<Arc<SpinLock<rros_thread>>>;
+        rq_next = None;
     }
-	Ok(0)
+    Ok(0)
 }
 
+/// # Enqueue the thread into the fifo rq
+/// 
+/// # Requirements
+///     - Enqueue the thread into the fifo rq by the priority of the thread. The higher priority, the front of the queue.
+///     - The If the priority of the thread is the same as the other threads in the queue, enqueue the new thread at the tail of old threads.
+///
+/// # Arguments
+///     - `thread`: the new thread to be enqueued
+///     
+/// # Return
+///     - Ok on success enqueuing the thread
+///     - Err on failure
+///  
+/// # Tips: 
+///     - The priority of the thread is set in the `cprio` item of the `rros_thread`.
+///     - It's easier to inverted search the queue to find the position to insert the new thread.
+///     - The `rq_next` item of the `rros_thread` is used to remove itself when dequeue. 
+///       Remeber to set it when you enqueue the thread. Read the `__rros_dequeue_fifo_thread` function for more details.
 //按优先级大小入队，注意这里要赋值rq_next---这个变量在出队的时候使用
 //测试通过
-pub fn __rros_enqueue_fifo_thread(thread: Arc<SpinLock<sched::rros_thread>>) -> Result<usize>{
-	let rq_ptr;
-	match thread.lock().rq.clone(){
-		Some(rq) => rq_ptr = rq,
-		None => return Err(kernel::Error::EINVAL),
-	}
-
-    let mut q = unsafe{(*rq_ptr).fifo.runnable.head.as_mut().unwrap()};
-    let new_cprio = thread.lock().cprio;
-    if q.is_empty() {
-        q.add_head(thread.clone());
-        unsafe{thread.lock().rq_next = q.head.prev.unwrap().as_ptr();}
-        pr_info!("addr: {:p}",thread.lock().rq_next);
-    } else {
-        let mut p = q.head.prev;
-        //倒序遍历
-        while true {
-            unsafe{
-                let pos_cprio = p.unwrap().as_ref().value.lock().cprio;
-                if p.unwrap().as_ptr() == &mut q.head as *mut Node<Arc<SpinLock<sched::rros_thread>>> || new_cprio <= pos_cprio {
-                    p.unwrap().as_mut().add(p.unwrap().as_ref().next.unwrap().as_ptr(), thread.clone());
-                    thread.lock().rq_next = p.unwrap().as_ref().next.unwrap().as_ptr();
-                    break;
-                } else {
-                    p = p.unwrap().as_ref().prev;
-                }
-            }
-            if p.unwrap().as_ptr() == q.head.prev.unwrap().as_ptr() {
-                break;
-            }
-        }
+pub fn __rros_enqueue_fifo_thread(thread: Arc<SpinLock<sched::rros_thread>>) -> Result<usize> {
+    let rq_ptr;
+    match thread.lock().rq.clone() {
+        Some(rq) => rq_ptr = rq,
+        None => return Err(kernel::Error::EINVAL),
     }
-	Ok(0)
+
+    // TODO: your code here
+   
+    // end of your code
+    Ok(0)
 }
 
 //enqueue_fifo_thread是new_cprio <= pos_cprio
 //requeue_fifo_thread是new_cprio < pos_cprio
 //默认测试通过
-pub fn __rros_requeue_fifo_thread(thread:Arc<SpinLock<sched::rros_thread>>) -> Result<usize>{
-	unsafe{
-    let rq_ptr;
-	match (*thread.locked_data().get()).rq.clone(){
-		Some(rq) => rq_ptr = rq,
-		None => return Err(kernel::Error::EINVAL),
-	}
-    let mut q = unsafe{(*rq_ptr).fifo.runnable.head.as_mut().unwrap()};
-    let new_cprio = (*thread.locked_data().get()).cprio;
-    if q.is_empty() {
-        q.add_head(thread.clone());
-        unsafe{(*thread.locked_data().get()).rq_next = q.head.prev.unwrap().as_ptr();}
-        pr_info!("addr: {:p}",(*thread.locked_data().get()).rq_next);
-    } else {
-        let mut p = q.head.prev;
-        //倒序遍历
-        while true {
-            unsafe{
-                let pos_cprio = (*(p.unwrap().as_ref().value).locked_data().get()).cprio;
-                if p.unwrap().as_ptr() == &mut q.head as *mut Node<Arc<SpinLock<sched::rros_thread>>> || new_cprio < pos_cprio {
-                    p.unwrap().as_mut().add(p.unwrap().as_ref().next.unwrap().as_ptr(), thread.clone());
-                    (*thread.locked_data().get()).rq_next = p.unwrap().as_ref().next.unwrap().as_ptr();
+pub fn __rros_requeue_fifo_thread(thread: Arc<SpinLock<sched::rros_thread>>) -> Result<usize> {
+    unsafe {
+        let rq_ptr;
+        match (*thread.locked_data().get()).rq.clone() {
+            Some(rq) => rq_ptr = rq,
+            None => return Err(kernel::Error::EINVAL),
+        }
+        let mut q = unsafe { (*rq_ptr).fifo.runnable.head.as_mut().unwrap() };
+        let new_cprio = (*thread.locked_data().get()).cprio;
+        if q.is_empty() {
+            q.add_head(thread.clone());
+            unsafe {
+                // (*thread.locked_data().get()).rq_next = Some(Node::new(q.head.prev.clone().unwrap().as_ref().value.clone()));
+                (*thread.locked_data().get()).rq_next = q.head.prev;
+            }
+            // pr_info!("addr: {:p}", (*thread.locked_data().get()).rq_next.clone().as_mut().unwrap());
+        } else {
+            let mut p = q.head.prev;
+            //倒序遍历
+            while true {
+                unsafe {
+                    let pos_cprio = (*(p.unwrap().as_ref().value).locked_data().get()).cprio;
+                    if p.unwrap().as_ptr()
+                        == &mut q.head as *mut Node<Arc<SpinLock<sched::rros_thread>>>
+                        || new_cprio < pos_cprio
+                    {
+                        p.unwrap()
+                            .as_mut()
+                            .add(p.unwrap().as_ref().next.unwrap().as_ptr(), thread.clone());
+                        // (*thread.locked_data().get()).rq_next =
+                        //     Some(Node::new(p.unwrap().as_ref().next.clone().unwrap().as_ref().value.clone()));
+                        (*thread.locked_data().get()).rq_next = p.unwrap().as_mut().next.clone();
+                            // Some(Node::new(p.unwrap().as_ref().next.clone().unwrap().as_ref().value.clone()));
+                        break;
+                    } else {
+                        p = p.unwrap().as_ref().prev;
+                    }
+                }
+                if p.unwrap().as_ptr() == q.head.prev.unwrap().as_ptr() {
                     break;
-                } else {
-                    p = p.unwrap().as_ref().prev;
                 }
             }
-            if p.unwrap().as_ptr() == q.head.prev.unwrap().as_ptr() {
-                break;
-            }
         }
-    }
-	Ok(0)
+        Ok(0)
     }
 }
diff --git a/kernel/rros/fifo_test.rs b/kernel/rros/fifo_test.rs
index 390565046..1d6d7f2d5 100644
--- a/kernel/rros/fifo_test.rs
+++ b/kernel/rros/fifo_test.rs
@@ -2,132 +2,237 @@
 //用于测试fifo.rs里的函数正确性
 use crate::factory;
 use crate::list::*;
+use crate::types;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*,
-    sched::*, fifo::*,thread::*,RROS_OOB_CPUS,
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, fifo::*, lock::*,
+    sched::*, thread::*, timer::*, RROS_OOB_CPUS, test::{self, TestFailed}, test_eq
 };
 use alloc::rc::Rc;
-use core::{cell::RefCell, clone::Clone};
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::DerefMut;
+use core::{cell::RefCell, clone::Clone};
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,c_str,
+    bindings, c_str, c_types, cpumask::CpumaskT, double_linked_list::*,
+    file_operations::FileOperations, ktime::*, percpu, percpu_defs, prelude::*, premmpt,
+    spinlock_init, str::CStr, sync::Guard, sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::ops::DerefMut;
 
 pub fn test_init_thread(thread: Arc<SpinLock<rros_thread>>, prio: i32) -> Result<usize> {
     let mut iattr = rros_init_thread_attr::new();
-    unsafe{
+    unsafe {
         iattr.affinity = &RROS_OOB_CPUS as *const CpumaskT;
         iattr.sched_class = Some(&rros_sched_fifo);
+        let sched_param =  unsafe{Arc::try_new(SpinLock::new(rros_sched_param::new()))?};
+        (*sched_param.locked_data().get()).fifo.prio = prio;
+        (*sched_param.locked_data().get()).idle.prio = prio;
+        (*sched_param.locked_data().get()).weak.prio = prio;
+        iattr.sched_param = Some(sched_param);
     }
-    let sched_param =  Rc::try_new(RefCell::new(rros_sched_param::new()))?;
-    sched_param.borrow_mut().fifo.prio = prio;
-    sched_param.borrow_mut().idle.prio = prio;
-    sched_param.borrow_mut().weak.prio = prio;
-    iattr.sched_param = Some(sched_param);
     rros_init_thread(& Some(thread), iattr, this_rros_rq(), c_str!("bw1"));
     Ok(0)
 }
 
 //测试通过
-pub fn test___rros_enqueue_fifo_thread() -> Result<usize> {
-    pr_info!("~~~test___rros_enqueue_fifo_thread begin~~~");
-    unsafe{
+pub fn test___rros_enqueue_fifo_thread() -> test::Result<()> {
+    // pr_info!("~~~test___rros_enqueue_fifo_thread begin~~~");
+    unsafe {
         let mut length;
 
         //创建thread1
         let mut t1 = SpinLock::new(rros_thread::new().unwrap());
         let pinned = Pin::new_unchecked(&mut t1);
         spinlock_init!(pinned, "create_thread1");
-        let mut thread1 = Arc::try_new(t1)?;
+        let mut thread1 = Arc::try_new(t1).unwrap();
 
         let mut r1 = SpinLock::new(RrosTimer::new(1));
-        let pinned_r1 =  Pin::new_unchecked(&mut r1);
+        let pinned_r1 = Pin::new_unchecked(&mut r1);
         spinlock_init!(pinned_r1, "rtimer_1");
         let mut p1 = SpinLock::new(RrosTimer::new(1));
-        let pinned_p =  Pin::new_unchecked(&mut p1);
+        let pinned_p = Pin::new_unchecked(&mut p1);
         spinlock_init!(pinned_p, "ptimer_1");
         thread1.lock().rtimer = Some(Arc::try_new(r1).unwrap());
         thread1.lock().ptimer = Some(Arc::try_new(p1).unwrap());
 
-        test_init_thread(thread1.clone(),22);
+        test_init_thread(thread1.clone(), 22);
 
         //创建thread2
         let mut t2 = SpinLock::new(rros_thread::new().unwrap());
         let pinned = Pin::new_unchecked(&mut t2);
         spinlock_init!(pinned, "create_thread1");
-        let mut thread2 = Arc::try_new(t2)?;
+        let mut thread2 = Arc::try_new(t2).unwrap();
 
         let mut r2 = SpinLock::new(RrosTimer::new(1));
-        let pinned_r2 =  Pin::new_unchecked(&mut r2);
+        let pinned_r2 = Pin::new_unchecked(&mut r2);
         spinlock_init!(pinned_r2, "rtimer_2");
         let mut p2 = SpinLock::new(RrosTimer::new(1));
-        let pinned_p =  Pin::new_unchecked(&mut p2);
+        let pinned_p = Pin::new_unchecked(&mut p2);
         spinlock_init!(pinned_p, "ptimer_2");
         thread2.lock().rtimer = Some(Arc::try_new(r2).unwrap());
         thread2.lock().ptimer = Some(Arc::try_new(p2).unwrap());
 
-        test_init_thread(thread2.clone(),33);
+        test_init_thread(thread2.clone(), 33);
 
         // //创建thread3
         let mut t3 = SpinLock::new(rros_thread::new().unwrap());
         let pinned = Pin::new_unchecked(&mut t3);
         spinlock_init!(pinned, "create_thread1");
-        let mut thread3 = Arc::try_new(t3)?;
-        
+        let mut thread3 = Arc::try_new(t3).unwrap();
 
         let mut r3 = SpinLock::new(RrosTimer::new(1));
-        let pinned_r3 =  Pin::new_unchecked(&mut r3);
+        let pinned_r3 = Pin::new_unchecked(&mut r3);
         spinlock_init!(pinned_r3, "rtimer_3");
         let mut p3 = SpinLock::new(RrosTimer::new(1));
-        let pinned_p =  Pin::new_unchecked(&mut p3);
+        let pinned_p = Pin::new_unchecked(&mut p3);
         spinlock_init!(pinned_p, "ptimer_3");
         thread3.lock().rtimer = Some(Arc::try_new(r3).unwrap());
         thread3.lock().ptimer = Some(Arc::try_new(p3).unwrap());
 
-        test_init_thread(thread3.clone(),44);
+        test_init_thread(thread3.clone(), 44);
 
         let rq_ptr1;
-        match thread1.lock().rq.clone(){
+        match thread1.lock().rq.clone() {
             Some(rq) => rq_ptr1 = rq,
-            None => return Err(kernel::Error::EINVAL),
+            None => panic!("rq_ptr1 is None"),
         }
 
         __rros_enqueue_fifo_thread(thread1.clone());
 
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        
+        test_eq!(length, 1)?;
 
         __rros_enqueue_fifo_thread(thread2.clone());
 
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        test_eq!(length, 2)?;
 
         __rros_enqueue_fifo_thread(thread3.clone());
-        
+
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
-        pr_info!("~~~test___rros_enqueue_fifo_thread end~~~");
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        test_eq!(length, 3)?;
+        // pr_info!("~~~test___rros_enqueue_fifo_thread end~~~");
 
         //__rros_dequeue_fifo_thread测试通过
-        pr_info!("~~~test___rros_dequeue_fifo_thread begin~~~");
+        // pr_info!("~~~test___rros_dequeue_fifo_thread begin~~~");
 
         __rros_dequeue_fifo_thread(thread1.clone());
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length1 is  {}", length);
+        // pr_info!("test___rros_enqueue_fifo_thread: length1 is  {}", length);
+        test_eq!(length, 2)?;
 
         __rros_dequeue_fifo_thread(thread2.clone());
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length2 is  {}", length);
+        // pr_info!("test___rros_enqueue_fifo_thread: length2 is  {}", length);
+        test_eq!(length, 1)?;
 
         __rros_dequeue_fifo_thread(thread3.clone());
         length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
-        pr_info!("test___rros_enqueue_fifo_thread: length3 is  {}", length);
+        // pr_info!("test___rros_enqueue_fifo_thread: length3 is  {}", length);
+        test_eq!(length, 0)?;
+
+        // pr_info!("~~~test___rros_dequeue_fifo_thread end~~~");
+    }
+    Ok(())
+}
+
+pub fn test___rros_enqueue_fifo_thread_without_dequeue() -> Result<usize> {
+    // pr_info!("~~~test___rros_enqueue_fifo_thread begin~~~");
+    unsafe {
+        let mut length;
+
+        //创建thread1
+        let mut t1 = SpinLock::new(rros_thread::new().unwrap());
+        let pinned = Pin::new_unchecked(&mut t1);
+        spinlock_init!(pinned, "create_thread1");
+        let mut thread1 = Arc::try_new(t1)?;
+
+        let mut r1 = SpinLock::new(RrosTimer::new(1));
+        let pinned_r1 = Pin::new_unchecked(&mut r1);
+        spinlock_init!(pinned_r1, "rtimer_1");
+        let mut p1 = SpinLock::new(RrosTimer::new(1));
+        let pinned_p = Pin::new_unchecked(&mut p1);
+        spinlock_init!(pinned_p, "ptimer_1");
+        thread1.lock().rtimer = Some(Arc::try_new(r1).unwrap());
+        thread1.lock().ptimer = Some(Arc::try_new(p1).unwrap());
+
+        test_init_thread(thread1.clone(), 22);
+
+        //创建thread2
+        let mut t2 = SpinLock::new(rros_thread::new().unwrap());
+        let pinned = Pin::new_unchecked(&mut t2);
+        spinlock_init!(pinned, "create_thread1");
+        let mut thread2 = Arc::try_new(t2)?;
+
+        let mut r2 = SpinLock::new(RrosTimer::new(1));
+        let pinned_r2 = Pin::new_unchecked(&mut r2);
+        spinlock_init!(pinned_r2, "rtimer_2");
+        let mut p2 = SpinLock::new(RrosTimer::new(1));
+        let pinned_p = Pin::new_unchecked(&mut p2);
+        spinlock_init!(pinned_p, "ptimer_2");
+        thread2.lock().rtimer = Some(Arc::try_new(r2).unwrap());
+        thread2.lock().ptimer = Some(Arc::try_new(p2).unwrap());
+
+        test_init_thread(thread2.clone(), 33);
+
+        // //创建thread3
+        let mut t3 = SpinLock::new(rros_thread::new().unwrap());
+        let pinned = Pin::new_unchecked(&mut t3);
+        spinlock_init!(pinned, "create_thread1");
+        let mut thread3 = Arc::try_new(t3)?;
+
+        let mut r3 = SpinLock::new(RrosTimer::new(1));
+        let pinned_r3 = Pin::new_unchecked(&mut r3);
+        spinlock_init!(pinned_r3, "rtimer_3");
+        let mut p3 = SpinLock::new(RrosTimer::new(1));
+        let pinned_p = Pin::new_unchecked(&mut p3);
+        spinlock_init!(pinned_p, "ptimer_3");
+        thread3.lock().rtimer = Some(Arc::try_new(r3).unwrap());
+        thread3.lock().ptimer = Some(Arc::try_new(p3).unwrap());
+
+        test_init_thread(thread3.clone(), 44);
+
+        let rq_ptr1;
+        match thread1.lock().rq.clone() {
+            Some(rq) => rq_ptr1 = rq,
+            None => return Err(kernel::Error::EINVAL),
+        }
+
+        __rros_enqueue_fifo_thread(thread1.clone());
+
+        length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+
+        __rros_enqueue_fifo_thread(thread2.clone());
+
+        length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
 
-        pr_info!("~~~test___rros_dequeue_fifo_thread end~~~");
+        __rros_enqueue_fifo_thread(thread3.clone());
+
+        length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length is  {}", length);
+        // pr_info!("~~~test___rros_enqueue_fifo_thread end~~~");
+
+        //__rros_dequeue_fifo_thread测试通过
+        // pr_info!("~~~test___rros_dequeue_fifo_thread begin~~~");
+
+        // __rros_dequeue_fifo_thread(thread1.clone());
+        // length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length1 is  {}", length);
+
+        // __rros_dequeue_fifo_thread(thread2.clone());
+        // length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length2 is  {}", length);
+
+        // __rros_dequeue_fifo_thread(thread3.clone());
+        // length = (*rq_ptr1).fifo.runnable.head.clone().unwrap().len();
+        // pr_info!("test___rros_enqueue_fifo_thread: length3 is  {}", length);
+
+        // pr_info!("~~~test___rros_dequeue_fifo_thread end~~~");
     }
     Ok(0)
 }
@@ -135,4 +240,4 @@ pub fn test___rros_enqueue_fifo_thread() -> Result<usize> {
 //测试通过，在上面
 pub fn test___rros_dequeue_fifo_thread() -> Result<usize> {
     Ok(0)
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/file.rs b/kernel/rros/file.rs
index 452006ac9..e76aba7fc 100644
--- a/kernel/rros/file.rs
+++ b/kernel/rros/file.rs
@@ -1,24 +1,289 @@
+use alloc::rc::Rc;
+
+use core::{
+    borrow::{ Borrow,BorrowMut },
+    cell::{ RefCell, Ref },
+    marker::PhantomData,
+    ptr,
+};
+
 use crate::{
+    crossing::{ RrosCrossing, rros_down_crossing, rros_init_crossing, rros_pass_crossing, rros_up_crossing },
     factory::RrosElement,
-    crossing::RrosCrossing,
+    list,
+    sched,
 };
 
 use kernel::{
-    file::File
-};
-
-use alloc::rc::Rc;
-use core::{
-    cell::{RefCell, Ref}
+    bindings,
+    c_str,
+    c_types,
+    double_linked_list::*,
+    file::File,
+    prelude::*,
+    rbtree,
+    str::CStr,
+    sync::{ SpinLock, Lock, Guard },
+    task::Task,
 };
 
 pub struct RrosFileBinding {
-    efile: Rc<RefCell<RrosFile>>,
-    evl_element:Rc<RefCell<RrosElement>>
+    pub rfile: Rc<RefCell<RrosFile>>,
+    pub rros_element:*mut RrosElement,
 }
 
 pub struct RrosFile {
-    filp: File,
-    crossing: RrosCrossing,
+    pub filp: *mut bindings::file,
+    pub crossing: RrosCrossing,
+}
+
+impl RrosFile {
+    pub fn rros_open_file(&mut self, filp: *mut bindings::file) -> Result<usize> {
+        self.filp = filp;
+    
+        unsafe { (*filp).oob_data = self as *const RrosFile as _;}
+        self.crossing.init();
+        Ok(0)
+    }
+
+    pub fn rros_release_file(&mut self) -> Result<usize> {
+        self.crossing.pass();
+        Ok(0)
+    }
+
+    pub fn flags(&self) -> u32 {
+        unsafe { (*self.filp).f_flags }
+    }
+    
+    pub fn from_ptr(ptr: *mut bindings::file) -> Result<Self> {
+        Ok(Self {
+            filp: ptr,
+            crossing: RrosCrossing::new()?,
+        })
+    }
+}
+
+pub struct RrosFd {
+    pub fd: u32,
+    pub rfilp: Option<RrosFile>,
+    pub files: *mut bindings::files_struct,
+    pub poll_nodes: list::list_head,
+}
+
+impl RrosFd {
+    pub fn new() -> Result<Self> {
+        Ok(RrosFd {
+            fd: 0,
+            rfilp: None,
+            files: 0 as *mut bindings::files_struct,
+            poll_nodes: list::list_head::default(),
+        })
+    }
+}
+
+
+// pub fn init_rbtree() -> *const rbtree::RBTree<u32, RrosFd> {
+//     let tree = rbtree::RBTree::new();
+//     unsafe { &tree as *const rbtree::RBTree<u32, RrosFd> }
+// }
+
+// pub static mut FD_TREE: SpinLock<rbtree::RBTree<u32, RrosFd>> = unsafe {
+//     SpinLock::new(rbtree::RBTree {
+//         root: bindings::rb_root { rb_node: 0 as *mut bindings::rb_node },
+//         _p: PhantomData
+//     })
+// };
+
+pub static mut FD_TREE: Option<Arc<SpinLock<rbtree::RBTree<u32, RrosFd>>>> = None;
+
+fn global_rbtree() -> Option<Arc<SpinLock<rbtree::RBTree<u32, RrosFd>>>> {
+    unsafe {
+        // *FD_TREE.borrow_mut() = Some(SpinLock::new(rbtree::RBTree::new()));
+        // FD_TREE.as_ref().unwrap()
+        Some(Arc::clone(FD_TREE.get_or_insert_with(|| {
+            Arc::try_new(SpinLock::new(rbtree::RBTree::new())).unwrap()
+        })))
+    }
+}
+
+// pub static mut FD_TREE: Option<Arc<SpinLock<rbtree::RBTree<u32, RrosFd>>>> = Some(init_rbtree().unwrap());
+
+/// Insert the given rfd to static rbtree FD_TREE.
+pub fn index_rfd(rfd: RrosFd, filp: *mut bindings::file) -> Result<usize> {
+    unsafe {
+        FD_TREE = global_rbtree();
+        let mut tree = FD_TREE.as_mut().unwrap();
+        let mut tree_lock = Arc::get_mut(&mut tree).unwrap().lock();
+        tree_lock.try_insert(rfd.fd, rfd)?;
+    }
+    Ok(0)
+}
+
+/// Search rfd in rbtree FD_TREE by fd.
+///
+/// Returns a reference to the rfd corresponding to the fd.
+pub fn lookup_rfd(fd: u32, files: *mut bindings::files_struct) -> Option<*mut RrosFd> {
+    unsafe {
+        FD_TREE = global_rbtree();
+        let mut tree = FD_TREE.as_mut().unwrap();
+        let mut tree_lock = Arc::get_mut(&mut tree).unwrap().lock();
+        // tree_lock.get_mut(&fd)
+        Some(tree_lock.get_mut(&fd)? as *mut RrosFd)
+    }
 }
 
+/// Removes the node with the given key from the rbtree.
+///
+/// It returns the value that was removed if rfd exists, or ['None'] otherwise.
+pub fn unindex_rfd(fd:u32, files: *mut bindings::files_struct) -> Option<RrosFd> {
+    unsafe {
+        FD_TREE = global_rbtree();
+        let mut tree = FD_TREE.as_mut().unwrap();
+        let mut tree_lock = Arc::get_mut(&mut tree).unwrap().lock();
+        Some(tree_lock.remove(&fd)?)
+    }
+}
+
+// in-band, caller may hold files->file_lock
+pub fn install_inband_fd(fd: u32, filp: *mut bindings::file, files: *mut bindings::files_struct) -> Result<usize> {
+    if ptr::null_mut() == unsafe { (*filp).oob_data } {
+        return Err(Error::EFAULT);
+    }
+
+    let mut rfd = RrosFd::new()?;
+    rfd.fd = fd;
+    rfd.files = files;
+    rfd.rfilp = Some(RrosFile::from_ptr(unsafe { (*filp).oob_data as *mut bindings::file })?);
+    index_rfd(rfd, filp);
+
+    Ok(0)
+}
+
+// fdt_lock held, irqs off. CAUTION: resched required on exit.
+// static void drop_watchpoints(struct evl_fd *efd)
+// {
+// 	if (!list_empty(&efd->poll_nodes))
+// 		evl_drop_watchpoints(&efd->poll_nodes);
+// }
+
+// in-band, caller holds files->file_lock
+pub fn uninstall_inband_fd(fd: u32, filp: *mut bindings::file, files: *mut bindings::files_struct) -> Result<usize> {
+    if ptr::null_mut() == unsafe { (*filp).oob_data } {
+        return Err(Error::EFAULT);
+    }
+
+    let rfd = unindex_rfd(fd, files);
+    match rfd {
+        Some(rfd) => (), // drop_watchpoints(rfd);
+        None => (),
+    }
+    unsafe { sched::rros_schedule(); }
+
+    Ok(0)
+}
+
+// in-band, caller holds files->file_lock
+pub fn replace_inband_fd(fd: u32, filp: *mut bindings::file, files: *mut bindings::files_struct) -> Result<usize> {
+    if ptr::null_mut() == unsafe { (*filp).oob_data } {
+        return Err(Error::EFAULT);
+    }
+
+    let rfd = lookup_rfd(fd, files);
+    match rfd {
+        Some(rfd) => {
+            // drop_watchpoints(rfd);
+            unsafe { (*rfd).rfilp = Some(RrosFile::from_ptr(unsafe { (*filp).oob_data as *mut bindings::file })?);
+            sched::rros_schedule(); }
+        },
+        None => {
+            install_inband_fd(fd, filp, files);
+        }
+    }
+
+    Ok(0)
+}
+
+pub fn rros_get_fileref(rfilp: &mut RrosFile) -> Result<usize> {
+    rros_down_crossing(&mut rfilp.crossing);
+    Ok(0)
+}
+
+pub fn rros_get_file(fd: u32) -> Option<Arc<RrosFile>> {
+    let rfd = lookup_rfd(fd, unsafe { (*Task::current_ptr()).files });
+
+    match rfd {
+        Some(rfd) => {
+            unsafe {
+                // rros_get_fileref((*rfd).rfilp);
+                // Some(Arc::try_new((*rfd).rfilp.unwrap()).unwrap())
+                rros_get_fileref((*rfd).rfilp.as_mut().unwrap());
+                Some(Arc::from_raw((*rfd).rfilp.as_ref().unwrap() as *const RrosFile))
+            }
+        },
+        None => None,
+    }
+}
+
+// pub fn rros_watch_fd(fd: u32, node: RrosPollNode) -> Option<Arc<RrosFile>> {
+//     let rfd = lookup_rfd(fd, unsafe { (*Task::current_ptr()).files });
+
+//     match rfd {
+//         Some(rfd) => {
+//             unsafe {
+//                 rros_get_fileref((*rfd).rfilp.as_mut().unwrap());
+//                 Some(Arc::from_raw((*rfd).rfilp.as_ref().unwrap() as *const RrosFile))
+//                 // (*rfd).poll_nodes.add(node.next);
+//             }
+//         },
+//         None => None,
+//     }
+// }
+
+// pub fn rros_ignore_fd(node: RrosPollNode) -> Result<usize> {
+//     // list_del(node.next);
+//     Ok(0)
+// }
+
+/// Rros_open_file - Open new file with oob capabilities
+///
+/// Called by chrdev with oob capabilities when a new @rfilp is opened.
+/// @rfilp is paired with the in-band file struct @filp
+pub fn rros_open_file(rfilp: &mut RrosFile, filp: *mut bindings::file) -> Result<usize> {
+    rfilp.filp = filp;
+
+    // mark filp as oob-capable.
+    unsafe { (*filp).oob_data = &(*rfilp) as *const RrosFile as _; }
+    rros_init_crossing(&mut rfilp.crossing);
+
+    Ok(0)
+}
+
+/// Rros_release_file - Drop an oob-capable file
+///
+/// Called by chrdev with oob capabilities when @rfilp is about to be released.
+/// Must be called from a fops->release() handler, and paired with a previous
+/// call to rros_open_file() from the fops->open() handler.
+pub fn rros_release_file(rfilp: &mut RrosFile) -> Result<usize> {
+    // Release the orginal reference on @rfilp. If oob references
+	// are still pending (e.g. some thread is still blocked in
+    // fops->oob_read()), we must wait for them to be dropped
+    // before allowing the in-band code to dismantle @efilp->filp.
+
+    // NOTE: In-band and out-of-band fds are working together in
+    // lockstep mode via dovetail_install/uninstall_fd() calls.
+    // Therefore, we can't livelock with evl_get_file() as @efilp
+    // was removed from the fd tree before fops->release() called us.
+
+    rros_pass_crossing(&mut rfilp.crossing);
+
+    Ok(0)
+}
+
+
+/// Rros_put_file - oob
+
+pub fn rros_put_file(rfilp: &mut RrosFile) -> Result<usize> {
+    rros_up_crossing(&mut rfilp.crossing);
+
+    Ok(0)
+}
\ No newline at end of file
diff --git a/kernel/rros/flags.rs b/kernel/rros/flags.rs
new file mode 100644
index 000000000..c15df9ddc
--- /dev/null
+++ b/kernel/rros/flags.rs
@@ -0,0 +1,56 @@
+use crate::{wait::{RrosWaitQueue, RROS_WAIT_PRIO}, sched::rros_schedule, clock::{RROS_MONO_CLOCK, RrosClock}, timeout::{rros_tmode, RROS_INFINITE}};
+use alloc::boxed::Box;
+use kernel::prelude::*;
+use kernel::bindings::{self, ktime_t};
+use core::{cell::Cell, ptr::NonNull};
+
+pub struct RrosFlag{
+    wait :  RrosWaitQueue,
+    raised : Cell<bool>
+}
+impl RrosFlag {
+    #[inline]
+    pub fn init(&mut self){
+        self.wait.init(unsafe{&mut RROS_MONO_CLOCK as *mut RrosClock}, RROS_WAIT_PRIO as i32);
+        self.raised = Cell::new(false);
+    }
+
+    #[inline]
+    pub fn destory(&mut self){
+        self.wait.destory();
+    }
+
+    // #[inline]
+    // pub fn wait_timeout(&mut self, timeout : bindings:ktime_t,) -> bool{
+    //     if self.raised == false{
+    //         self.wait.wait_timeout(timeout);
+    //     }
+    //     self.raised
+    // }
+    #[inline]
+    pub fn read(&self) -> bool{
+        if self.raised.get(){
+            self.raised.set(false);
+            return true;
+        }
+        false
+    }
+    #[inline]
+    pub fn wait(&mut self) -> i32{
+        // TODO:尝试绕开不可变借用的限制
+        let mut x = unsafe{NonNull::new_unchecked(&self.wait as *const _ as *mut RrosWaitQueue)}; 
+        unsafe{
+            x.as_mut().wait_timeout(RROS_INFINITE, rros_tmode::RROS_REL, ||self.read())
+        }
+    }
+
+    #[inline]
+    pub fn raise(&mut self){
+        let flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut self.wait.lock as *const _ as *mut bindings::raw_spinlock)};
+        self.raised.set(true);
+        self.wait.flush_locked(0);
+        unsafe{bindings::_raw_spin_unlock_irqrestore(&mut self.wait.lock as *const _ as *mut bindings::raw_spinlock, flags)};
+
+        unsafe{rros_schedule()};
+    }
+}
\ No newline at end of file
diff --git a/kernel/rros/idle.rs b/kernel/rros/idle.rs
index cdc97e7d1..03d35556a 100644
--- a/kernel/rros/idle.rs
+++ b/kernel/rros/idle.rs
@@ -1,12 +1,10 @@
 use crate::{
     sched, thread::*,
 };
-use alloc::rc::Rc;
-use core::cell::RefCell;
-use kernel::{init_static_sync, prelude::*, Error,sync::SpinLock};
+use kernel::{init_static_sync, prelude::*, Error,sync::{SpinLock, Lock, Guard}};
 use core::ptr::{null_mut, null};
 
-pub static mut rros_sched_idle:sched::rros_sched_class = sched::rros_sched_class{
+pub static mut rros_sched_idle: sched::rros_sched_class = sched::rros_sched_class {
     sched_pick: Some(rros_idle_pick),
     sched_setparam: Some(rros_idle_setparam),
     sched_getparam: Some(rros_idle_getparam),
@@ -29,7 +27,7 @@ pub static mut rros_sched_idle:sched::rros_sched_class = sched::rros_sched_class
     sched_control: None,
     nthreads: 0,
     next: 0 as *mut sched::rros_sched_class,
-    flag:1,
+    flag: 1,
 };
 
 pub const RROS_IDLE_PRIO: i32 = -1;
@@ -66,12 +64,12 @@ pub const RROS_IDLE_PRIO: i32 = -1;
 // }
 
 fn rros_idle_pick(rq: Option<*mut sched::rros_rq>) -> Result<Arc<SpinLock<sched::rros_thread>>> {
-    match rq{
+    match rq {
         Some(_) => (),
         None => return Err(kernel::Error::EINVAL),
     }
     let root_thread;
-    unsafe{
+    unsafe {
         match (*rq.unwrap()).root_thread.clone() {
             Some(t) => root_thread = t.clone(),
             None => return Err(kernel::Error::EINVAL),
@@ -81,59 +79,55 @@ fn rros_idle_pick(rq: Option<*mut sched::rros_rq>) -> Result<Arc<SpinLock<sched:
 }
 
 fn rros_idle_setparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Rc<RefCell<sched::rros_sched_param>>>
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<usize> {
-    return __rros_set_idle_schedparam(thread, p);
+    return __rros_set_idle_schedparam(thread.clone(), p.clone());
 }
 
 fn __rros_set_idle_schedparam(
-    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Rc<RefCell<sched::rros_sched_param>>>
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,p:Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) -> Result<usize> {
     let thread_clone = thread.clone();
     let thread_unwrap = thread_clone.unwrap();
     // let mut thread_lock = thread_unwrap.lock();
     let p_unwrap = p.unwrap();
-    let p_ptr = p_unwrap.borrow();
     thread_unwrap.lock().state &= !T_WEAK;
-    return sched::rros_set_effective_thread_priority(thread.clone(), p_ptr.idle.prio);
+    let prio = unsafe{(*p_unwrap.locked_data().get()).idle.prio};
+    unsafe{return sched::rros_set_effective_thread_priority(thread.clone(), prio)};
 }
 
 fn rros_idle_getparam(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: Rc<RefCell<sched::rros_sched_param>>,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
+    p: Option<Arc<SpinLock<sched::rros_sched_param>>>,
 ) {
-    __rros_get_idle_schedparam(thread, p);
+    __rros_get_idle_schedparam(thread.clone(), p.clone());
 }
 
 fn __rros_get_idle_schedparam(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: Rc<RefCell<sched::rros_sched_param>>,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>,
+    p: Option<Arc<SpinLock<sched::rros_sched_param>>>,
 ) {
-    let mut p_ptr = p.borrow_mut();
-    let thread_ptr = thread.borrow_mut();
-    p_ptr.idle.prio = thread_ptr.cprio;
+    p.unwrap().lock().idle.prio = thread.unwrap().lock().cprio;
 }
 
-fn rros_idle_trackprio(thread: Rc<RefCell<sched::rros_thread>>, p: *const sched::rros_sched_param) {
-    __rros_track_idle_priority(thread, p);
+fn rros_idle_trackprio(thread: Option<Arc<SpinLock<sched::rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>) {
+    __rros_track_idle_priority(thread.clone(), p.clone());
 }
 
 fn __rros_track_idle_priority(
-    thread: Rc<RefCell<sched::rros_thread>>,
-    p: *const sched::rros_sched_param,
+    thread: Option<Arc<SpinLock<sched::rros_thread>>>, p: Option<Arc<SpinLock<sched::rros_sched_param>>>
 ) {
-    let mut thread_ptr = thread.borrow_mut();
-    if p != null() {
+    if p.is_some(){
         pr_warn!("Inheriting a priority-less class makes no sense.");
     } else {
-        thread_ptr.cprio = RROS_IDLE_PRIO;
+        thread.unwrap().lock().cprio = RROS_IDLE_PRIO;
     }
 }
 
-fn rros_idle_ceilprio(thread: Rc<RefCell<sched::rros_thread>>, prio: i32) {
-    __rros_ceil_idle_priority(thread, prio);
+fn rros_idle_ceilprio(thread:Arc<SpinLock<sched::rros_thread>>, prio: i32) {
+    __rros_ceil_idle_priority(thread.clone(), prio);
 }
 
-fn __rros_ceil_idle_priority(thread: Rc<RefCell<sched::rros_thread>>, prio: i32) {
+fn __rros_ceil_idle_priority(thread: Arc<SpinLock<sched::rros_thread>>, prio: i32) {
     pr_warn!("RROS_WARN_ON_ONCE(CORE, 1)");
 }
diff --git a/kernel/rros/init.rs b/kernel/rros/init.rs
index baeeec936..e8238e6bf 100644
--- a/kernel/rros/init.rs
+++ b/kernel/rros/init.rs
@@ -1,26 +1,30 @@
 ﻿#![no_std]
 #![feature(allocator_api, global_asm)]
-use alloc::vec;
-use kernel::{cpumask, irqstage, prelude::*, str::CStr,
-     ThisModule, c_str, chrdev, file_operations::FileOperations,
-    kthread, c_types, dovetail, bindings, percpu};
+#![feature(const_fn_transmute, maybe_uninit_extra, new_uninit)]
+// use alloc::vec;
+use kernel::{
+    bindings, c_str, c_types, chrdev, cpumask, dovetail, file_operations::FileOperations, irqstage,
+    kthread, percpu, prelude::*, str::CStr, ThisModule,
+};
 
 use core::str;
 use core::sync::atomic::{AtomicU8, Ordering};
-use core::{mem::size_of, todo, mem::align_of};
+use core::{mem::align_of, mem::size_of, todo};
 
 mod idle;
 mod queue;
 mod sched;
 use sched::rros_init_sched;
 mod thread;
+mod rros;
 // mod weak;
 mod fifo;
+// mod tp;
 mod tick;
 use tick::rros_enable_tick;
 
-mod timeout;
 mod stat;
+mod timeout;
 
 mod clock;
 mod clock_test;
@@ -30,24 +34,40 @@ mod list;
 mod list_test;
 mod lock;
 mod memory;
+mod memory_test;
 mod monitor;
+// mod mutex;
+mod sched_test;
+mod syscall;
+mod thread_test;
 mod timer;
 mod timer_test;
-mod thread_test;
-mod sched_test;
-mod fifo_test;
-mod test;
+#[macro_use]
+mod arch;
 mod double_linked_list_test;
+mod fifo_test;
+#[macro_use]
+pub mod test;
+mod uapi;
 use memory::init_memory;
 mod factory;
 use factory::rros_early_init_factories;
-
 use crate::sched::this_rros_rq;
-mod file;
 mod crossing;
+mod file;
+mod flags;
+mod work;
+#[macro_use]
+pub mod types;
+mod types_test;
 mod wait;
 #[cfg(CONFIG_NET)]
 mod net;
+mod test_thread;
+
+// pub use net::netif_oob_switch_port;
+
+
 module! {
     type: Rros,
     name: b"rros",
@@ -73,7 +93,7 @@ module! {
     },
 }
 
-pub struct RrosMachineCpuData{}
+pub struct RrosMachineCpuData {}
 
 pub static mut RROS_MACHINE_CPUDATA: *mut RrosMachineCpuData = 0 as *mut RrosMachineCpuData;
 
@@ -84,9 +104,8 @@ enum RrosRunStates {
     RrosStateTeardown = 4,
     RrosStateWarmup = 5,
 }
-struct Rros
-{
-    factory: Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>,
+struct Rros {
+    factory: Pin<Box<chrdev::Registration<{ factory::NR_FACTORIES }>>>,
 }
 
 struct InitState {
@@ -132,7 +151,7 @@ fn set_rros_state(state: RrosRunStates) {
     RROS_RUNSTATE.store(state as u8, Ordering::Relaxed);
 }
 
-fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>> {
+fn init_core() -> Result<Pin<Box<chrdev::Registration<{ factory::NR_FACTORIES }>>>> {
     let res =
         irqstage::enable_oob_stage(CStr::from_bytes_with_nul("rros\0".as_bytes())?.as_char_ptr());
     pr_info!("hello");
@@ -144,7 +163,7 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
         }
     }
     pr_info!("hella");
-    // let res = init_memory(*sysheap_size_arg.read());
+    // let res = evl_init_memory();
     // match res {
     //     Ok(_o) => (),
     //     Err(_e) => {
@@ -162,7 +181,7 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
         }
     }
     pr_info!("haly");
-    
+
     let res = rros_clock_init();
     match res {
         Ok(_o) => (),
@@ -171,7 +190,7 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
             return Err(_e);
         }
     }
-    
+
     let res = rros_init_sched();
     match res {
         Ok(_o) => (),
@@ -199,7 +218,7 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
             pr_warn!("dovetail start wrong");
         }
     }
-    
+
     Ok(fac_reg)
 }
 
@@ -235,8 +254,13 @@ fn test_double_linked_list() {
 
 fn test_thread() {
     thread_test::test_thread_context_switch();
+    // thread_test::test_NetKthreadRunner();
 }
 
+// fn test_tp(){
+//     tp::test_tp();
+// }
+
 fn test_sched() {
     // sched_test::test_this_rros_rq_thread();
     // sched_test::test_cpu_smp();
@@ -260,11 +290,13 @@ impl KernelModule for Rros {
 
         let cpu_online_mask = unsafe { cpumask::read_cpu_online_mask() };
         //size_of 为0，align_of为4，alloc报错
-        // unsafe {RROS_MACHINE_CPUDATA = 
-        //     percpu::alloc_per_cpu(size_of::<RrosMachineCpuData>() as usize, 
+        // unsafe {RROS_MACHINE_CPUDATA =
+        //     percpu::alloc_per_cpu(size_of::<RrosMachineCpuData>() as usize,
         //                   align_of::<RrosMachineCpuData>() as usize) as *mut RrosMachineCpuData};
-        unsafe {RROS_MACHINE_CPUDATA = 
-            percpu::alloc_per_cpu(4 as usize, 4 as usize) as *mut RrosMachineCpuData};
+        unsafe {
+            RROS_MACHINE_CPUDATA =
+                percpu::alloc_per_cpu(4 as usize, 4 as usize) as *mut RrosMachineCpuData
+        };
         if str::from_utf8(oobcpus_arg.read())? != "" {
             let res = unsafe {
                 cpumask::cpulist_parse(
@@ -292,22 +324,22 @@ impl KernelModule for Rros {
 
         // test_timer();
         // test_double_linked_list();
-        
-        // test_clock();
-        // test_sched();
         // test_fifo();
-        test_thread();
+        // test_clock();
+        // test_thread();
         //test_double_linked_list();
+        test_thread::test_rros_thread();
         match res {
             Ok(_o) => {
                 pr_info!("Success boot the rros.");
                 fac_reg = _o;
-            },
+            }
             Err(_e) => {
                 pr_warn!("Boot failed!\n");
                 return Err(_e);
             }
         }
+        // test_lantency();
 
         // let mut evl_kthread1 = evl_kthread::new(fn1);
         // let mut evl_kthread2 = evl_kthread::new(fn2);
diff --git a/kernel/rros/list.rs b/kernel/rros/list.rs
index e5b4920f2..2802bdad2 100644
--- a/kernel/rros/list.rs
+++ b/kernel/rros/list.rs
@@ -1,9 +1,9 @@
-use kernel::{str::CStr, percpu_defs, prelude::*, sync::SpinLock, container_of, bindings};
+use kernel::{bindings, container_of, percpu_defs, prelude::*, str::CStr, sync::SpinLock};
 pub struct list_head {
     pub next: *mut list_head,
     pub prev: *mut list_head,
 }
-use core::ptr::{null_mut, null};
+use core::ptr::{null, null_mut};
 
 impl Default for list_head {
     fn default() -> Self {
@@ -15,20 +15,19 @@ impl Default for list_head {
 }
 
 impl list_head {
-
     //添加节点到self和next之间
     pub fn add(&mut self, new: *mut list_head) {
         if self.is_empty() {
             self.prev = new;
-            unsafe{
+            unsafe {
                 (*new).next = self as *mut list_head;
                 (*new).prev = self as *mut list_head;
-            } 
-        }else{
-            unsafe{
+            }
+        } else {
+            unsafe {
                 (*self.next).prev = new;
                 (*new).next = self.next;
-                (*new).prev = self as *mut list_head; 
+                (*new).prev = self as *mut list_head;
             }
         }
         self.next = new;
@@ -48,12 +47,12 @@ impl list_head {
     pub fn list_drop(&mut self) {
         if !self.is_empty() {
             if self.next == self.prev {
-                unsafe{
+                unsafe {
                     (*self.next).next = null_mut();
                     (*self.next).prev = null_mut();
                 }
-            }else{
-                unsafe{
+            } else {
+                unsafe {
                     (*self.next).prev = self.prev;
                     (*self.prev).next = self.next;
                 }
@@ -61,24 +60,3 @@ impl list_head {
         }
     }
 }
-
-#[macro_export]
-macro_rules! list_entry {
-    ($ptr:expr, $type:ty, $($f:tt)*) => {{
-        kernel::container_of!($ptr, $type, $($f)*)
-    }}
-}
-
-#[macro_export]
-macro_rules! list_first_entry {
-    ($ptr:expr, $type:ty, $($f:tt)*) => {{
-        list_entry!((*$ptr).next, $type, $($f)*)
-    }}
-}
-
-#[macro_export]
-macro_rules! list_last_entry {
-    ($ptr:expr, $type:ty, $($f:tt)*) => {{
-        list_entry!((*$ptr).prev, $type, $($f)*)
-    }}
-}
diff --git a/kernel/rros/list_test.rs b/kernel/rros/list_test.rs
index 0c0dd0cba..5d4ddd313 100644
--- a/kernel/rros/list_test.rs
+++ b/kernel/rros/list_test.rs
@@ -1,7 +1,7 @@
 //list.rs测试文件！
-use kernel::{prelude::*};
 use crate::list::*;
-
+use kernel::prelude::*;
+use crate::{list_first_entry,list_entry,list_last_entry};
 struct ListTest {
     num: i32,
     head: list_head,
@@ -9,22 +9,18 @@ struct ListTest {
 
 impl ListTest {
     fn new(num: i32, head: list_head) -> ListTest {
-        ListTest {
-            num,head
-        }
+        ListTest { num, head }
     }
 }
 
-fn print_info() {
-
-}
+fn print_info() {}
 
 fn traverse_list(head: &list_head) -> i32 {
     let mut count = 0;
-    if head as *const list_head == 0 as *const list_head{
-    }else if head.is_empty(){
+    if head as *const list_head == 0 as *const list_head {
+    } else if head.is_empty() {
         count = count + 1;
-    }else{
+    } else {
         count = count + 1;
         let mut p: *mut list_head = head.next;
         while p as *const list_head != head as *const list_head {
@@ -38,8 +34,6 @@ fn traverse_list(head: &list_head) -> i32 {
     return count;
 }
 
-
-
 fn test_list_method() {
     let mut head = list_head::default();
     let mut t1 = list_head::default();
@@ -50,61 +44,51 @@ fn test_list_method() {
     head.add(&mut t2 as *mut list_head);
     if traverse_list(&head) == 3 {
         pr_info!("test_list_add success");
-    }else{
+    } else {
         pr_info!("test_list_add failed");
     }
 
     //测试list_drop
-    unsafe{
+    unsafe {
         (*head.next).list_drop();
     }
     //head.next = &mut t2 as *mut list_head;
     if traverse_list(&head) == 2 {
         pr_info!("test_list_drop success");
-    }else{
+    } else {
         pr_info!("test_list_drop failed");
     }
 
     //测试last_is
     if head.last_is(&mut t1 as *mut list_head) {
         pr_info!("test_list_last_is success");
-    }else{
+    } else {
         pr_info!("test_list_last_is failed");
     }
 
-    unsafe{
+    unsafe {
         (*head.next).list_drop();
     }
 
     //测试empty
     if head.is_empty() {
         pr_info!("test_list_is_empty success");
-    }else{
+    } else {
         pr_info!("test_list_is_empty failed");
     }
 }
 
-
 pub fn test_entry() {
-    let mut t1 = ListTest::new(
-        111,
-        list_head::default(),
-    );
-    let mut t2 = ListTest::new(
-        222,
-        list_head::default(),
-    );
-    let mut t3 = ListTest::new(
-        333,
-        list_head::default(),
-    );
+    let mut t1 = ListTest::new(111, list_head::default());
+    let mut t2 = ListTest::new(222, list_head::default());
+    let mut t3 = ListTest::new(333, list_head::default());
     t1.head.add(&mut t2.head as *mut list_head);
     t1.head.add(&mut t3.head as *mut list_head);
     let _t1 = list_entry!(&mut t1.head as *mut list_head, ListTest, head);
     unsafe {
         if (*_t1).num == t1.num {
             pr_info!("test_list_entry success!");
-        }else{
+        } else {
             pr_info!("test_list_entry failed!");
         }
     }
@@ -112,7 +96,7 @@ pub fn test_entry() {
     unsafe {
         if (*_t2).num == t2.num {
             pr_info!("test_list_first_entry success!");
-        }else{
+        } else {
             pr_info!("test_list_first_entry failed!");
         }
     }
@@ -120,7 +104,7 @@ pub fn test_entry() {
     unsafe {
         if (*_t3).num == t3.num {
             pr_info!("test_list_last_entry success!");
-        }else{
+        } else {
             pr_info!("test_list_last_entry failed!");
         }
     }
@@ -129,4 +113,4 @@ pub fn test_entry() {
 pub fn test_list() {
     test_list_method();
     test_entry();
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/lock.rs b/kernel/rros/lock.rs
index 5ed97d6a7..fdd8d2dd0 100644
--- a/kernel/rros/lock.rs
+++ b/kernel/rros/lock.rs
@@ -1,4 +1,4 @@
-use kernel::{cpumask, prelude::*, spinlock_init, str::CStr, sync::SpinLock, c_types};
+use kernel::{c_types, cpumask, prelude::*, spinlock_init, str::CStr, sync::SpinLock};
 
 pub fn raw_spin_lock_init(lock: &mut SpinLock<i32>) {
     *lock = unsafe { SpinLock::new(1) };
@@ -11,17 +11,35 @@ extern "C" {
     fn rust_helper_hard_local_irq_restore(flags: c_types::c_ulong);
     fn rust_helper_preempt_enable();
     fn rust_helper_preempt_disable();
+    // fn rust_helper_raw_spin_lock_irqsave();
+    // fn rust_helper_raw_spin_unlock_irqrestore();
 }
 
+// TODO: modify this when we have the real smp support
 pub fn raw_spin_lock_irqsave() -> c_types::c_ulong {
-    let flags = unsafe{rust_helper_hard_local_irq_save()};
+    let flags = unsafe { rust_helper_hard_local_irq_save() };
     // unsafe{rust_helper_preempt_disable();}
     return flags;
 }
 
+// TODO: modify this when we have the real smp support
 pub fn raw_spin_unlock_irqrestore(flags: c_types::c_ulong) {
-    unsafe{
+    unsafe {
         rust_helper_hard_local_irq_restore(flags);
         // rust_helper_preempt_enable();
     }
-}
\ No newline at end of file
+}
+
+// pub fn right_raw_spin_lock_irqsave(lock: *mut spinlock_t, flags: *mut u32) {
+//     // let flags = unsafe { rust_helper_raw_local_irq_save() };
+//     unsafe { rust_helper_raw_local_irq_save() };
+//     // unsafe{rust_helper_preempt_disable();}
+//     // return flags;
+// }
+
+// pub fn right_raw_spin_unlock_irqrestore(lock: *mut spinlock_t, flags: *mut u32) {
+//     // let flags = unsafe { rust_helper_raw_local_irq_save() };
+//     unsafe { rust_helper_raw_spin_unlock_irqrestore() };
+//     // unsafe{rust_helper_preempt_disable();}
+//     // return flags;
+// }
\ No newline at end of file
diff --git a/kernel/rros/memory.rs b/kernel/rros/memory.rs
index 1acf3f9a9..ec32a487d 100644
--- a/kernel/rros/memory.rs
+++ b/kernel/rros/memory.rs
@@ -113,7 +113,7 @@ pub fn init_memory(sysheap_size_arg: u32) -> Result<usize> {
         Arc::try_new(unsafe { SpinLock::new(RrosHeap::new()?) })?;
     // let mut RrosHeapRangeManage: Arc<SpinLock<RrosHeapPgentry>> =
     //     Arc::try_new(unsafe{SpinLock::new(RrosHeapPgentry::new()?)})?;
-      
+
     let res = init_system_heap(rros_system_heap.clone(), sysheap_size_arg)?; //RrosHeapRangeManage.clone()
     let shared_res = init_shared_heap(rros_shared_heap.clone())?; // RrosHeapRangeManage.clone()
 
diff --git a/kernel/rros/memory_test.rs b/kernel/rros/memory_test.rs
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/monitor.rs b/kernel/rros/monitor.rs
index 553f507f5..149c45acb 100644
--- a/kernel/rros/monitor.rs
+++ b/kernel/rros/monitor.rs
@@ -8,26 +8,22 @@ use core::{
 };
 
 use crate::{
-    Box,
-    clock,
-    factory::{RrosElement, RrosFactory, rros_init_element},
-    factory,
+    clock, factory,
+    factory::{rros_init_element, RrosElement, RrosFactory},
     fifo::RROS_FIFO_MAX_PRIO,
     sched,
-    thread::atomic_set
+    thread::atomic_set,
+    Box, wait::RrosWaitQueue,
 };
 
 use kernel::{
-    bindings,
-    c_types,
-    Error,
-    spinlock_init,
-    prelude::*,
-    str::CStr,
-    sync::SpinLock,
-    user_ptr,
+    bindings, c_types, prelude::*, spinlock_init, str::CStr, sync::SpinLock, user_ptr, Error,
 };
 
+use kernel::file::File;
+use kernel::file_operations::FileOperations;
+use kernel::io_buffer::IoBufferWriter;
+
 pub struct RrosMonitorItem1 {
     pub mutex: SpinLock<i32>,
     pub events: sched::list_head,
@@ -37,15 +33,15 @@ pub struct RrosMonitorItem1 {
 impl RrosMonitorItem1 {
     fn new() -> Result<Self> {
         Ok(Self {
-            mutex: unsafe {SpinLock::new(0)},
+            mutex: unsafe { SpinLock::new(0) },
             events: sched::list_head::new(),
-            lock: unsafe {SpinLock::<i32>::new(0)},
+            lock: unsafe { SpinLock::<i32>::new(0) },
         })
     }
 }
 
 pub struct RrosMonitorItem2 {
-    pub wait_queue: sched::rros_wait_queue,
+    pub wait_queue: RrosWaitQueue,
     pub gate: Option<*mut u8>,
     pub poll_head: sched::rros_poll_head,
     pub next: sched::list_head,
@@ -55,7 +51,7 @@ pub struct RrosMonitorItem2 {
 impl RrosMonitorItem2 {
     fn new() -> Result<Self> {
         Ok(Self {
-            wait_queue: sched::rros_wait_queue::new(),
+            wait_queue: unsafe{core::mem::zeroed()},
             gate: None,
             poll_head: sched::rros_poll_head::new(),
             next: sched::list_head::new(),
@@ -99,13 +95,13 @@ impl RrosMonitor {
                 type_foo,
                 protocol,
                 item: RrosMonitorItem::Item2(subitem),
-            })
+            }),
         }
     }
 }
 
 // #[derive(Copy, Clone)]
-struct RrosMonitorStateItemGate {
+pub struct RrosMonitorStateItemGate {
     owner: AtomicUsize,
     ceiling: u32,
     recursive: u32,
@@ -113,7 +109,7 @@ struct RrosMonitorStateItemGate {
 }
 
 // #[derive(Copy, Clone)]
-struct RrosMonitorStateItemEvent {
+pub struct RrosMonitorStateItemEvent {
     value: AtomicUsize,
     pollrefs: AtomicUsize,
     gate_offset: u32,
@@ -135,10 +131,7 @@ pub struct RrosMonitorState {
 
 impl RrosMonitorState {
     pub fn new() -> Result<Self> {
-        Ok(Self {
-            flags: 0,
-            u: None,
-        })
+        Ok(Self { flags: 0, u: None })
     }
 }
 
@@ -160,90 +153,90 @@ impl RrosMonitorAttrs {
     }
 }
 
-pub const RROS_MONITOR_EVENT: u32 = 0;	/* Event monitor. */
-pub const RROS_EVENT_GATED: u32 = 0;	/* Gate protected. */
-pub const RROS_EVENT_COUNT: u32 = 1;	/* Semaphore. */
-pub const RROS_EVENT_MASK: u32 = 2;	/* Event (bit)mask. */
-pub const RROS_MONITOR_GATE: u32 = 1;	/* Gate monitor. */
-pub const RROS_GATE_PI: u32 = 0;	/* Gate with priority inheritance. */
-pub const RROS_GATE_PP: u32 = 1;	/* Gate with priority protection (ceiling). */
+pub const RROS_MONITOR_EVENT: u32 = 0; /* Event monitor. */
+pub const RROS_EVENT_GATED: u32 = 0; /* Gate protected. */
+pub const RROS_EVENT_COUNT: u32 = 1; /* Semaphore. */
+pub const RROS_EVENT_MASK: u32 = 2; /* Event (bit)mask. */
+pub const RROS_MONITOR_GATE: u32 = 1; /* Gate monitor. */
+pub const RROS_GATE_PI: u32 = 0; /* Gate with priority inheritance. */
+pub const RROS_GATE_PP: u32 = 1; /* Gate with priority protection (ceiling). */
 
 pub const RROS_MONITOR_NOGATE: u32 = 1;
 pub const CLOCK_MONOTONIC: u32 = 1;
 pub const CLOCK_REALTIME: u32 = 0;
 
-const CONFIG_RROS_MONITOR: u32 = 0; //未知
+const CONFIG_RROS_MONITOR: usize = 0; //未知
 
-pub fn monitor_factory_build(fac: *mut RrosFactory, uname: &'static CStr, u_attrs: Option<*mut u8>, clone_flags: i32, state_offp: &u32) -> Result<Rc<RefCell<RrosElement>>> {
-    if (clone_flags & !factory::RROS_CLONE_PUBLIC) != 0{
+pub fn monitor_factory_build(
+    fac: *mut RrosFactory,
+    uname: &'static CStr,
+    u_attrs: Option<*mut u8>,
+    clone_flags: i32,
+    state_offp: &u32,
+) -> Result<Rc<RefCell<RrosElement>>> {
+    if (clone_flags & !factory::RROS_CLONE_PUBLIC) != 0 {
         return Err(Error::EINVAL);
     }
 
     let mut attrs = RrosMonitorAttrs::new()?;
     let len = size_of::<RrosMonitorAttrs>();
     let ptr: *mut c_types::c_void = &mut attrs as *mut RrosMonitorAttrs as *mut c_types::c_void;
-    let u_attrs: *const c_types::c_void = &attrs as *const RrosMonitorAttrs as * const c_types::c_void;
-    let ret = unsafe {user_ptr::rust_helper_copy_from_user(ptr, u_attrs as _, len as _)};
+    let u_attrs: *const c_types::c_void =
+        &attrs as *const RrosMonitorAttrs as *const c_types::c_void;
+    let ret = unsafe { user_ptr::rust_helper_copy_from_user(ptr, u_attrs as _, len as _) };
     if ret != 0 {
         return Err(Error::EFAULT);
     }
 
     match attrs.type_foo {
-        RROS_MONITOR_GATE => {
-            match attrs.protocol {
-                RROS_GATE_PP => {
-                    if attrs.initval == 0 || attrs.initval > RROS_FIFO_MAX_PRIO as u32 {
-                        return Err(Error::EINVAL);
-                    }
-                },
-                RROS_GATE_PI => {
-                    if attrs.initval != 0 {
-                        return Err(Error::EINVAL);
-                    }
-                },
-                _ => return Err(Error::EINVAL)
+        RROS_MONITOR_GATE => match attrs.protocol {
+            RROS_GATE_PP => {
+                if attrs.initval == 0 || attrs.initval > RROS_FIFO_MAX_PRIO as u32 {
+                    return Err(Error::EINVAL);
+                }
             }
-        },
-        RROS_MONITOR_EVENT => {
-            match attrs.protocol {
-                RROS_EVENT_GATED|RROS_EVENT_COUNT|RROS_EVENT_MASK => (),
-                _ => return Err(Error::EINVAL),
+            RROS_GATE_PI => {
+                if attrs.initval != 0 {
+                    return Err(Error::EINVAL);
+                }
             }
-        }
+            _ => return Err(Error::EINVAL),
+        },
+        RROS_MONITOR_EVENT => match attrs.protocol {
+            RROS_EVENT_GATED | RROS_EVENT_COUNT | RROS_EVENT_MASK => (),
+            _ => return Err(Error::EINVAL),
+        },
         _ => return Err(Error::EINVAL),
     }
 
     let clock: Result<&mut clock::RrosClock> = {
         match attrs.clockfd {
-            CLOCK_MONOTONIC => unsafe {Ok(&mut clock::RROS_MONO_CLOCK)},
-            _ => unsafe {Ok(&mut clock::RROS_REALTIME_CLOCK)},
+            CLOCK_MONOTONIC => unsafe { Ok(&mut clock::RROS_MONO_CLOCK) },
+            _ => unsafe { Ok(&mut clock::RROS_REALTIME_CLOCK) },
         }
     };
 
-
     let element = Rc::try_new(RefCell::new(RrosElement::new()?))?;
-    let mut factory: &mut SpinLock<RrosFactory> = unsafe{&mut RROS_MONITOR_FACTORY};
+    let mut factory: &mut SpinLock<RrosFactory> = unsafe { &mut RROS_MONITOR_FACTORY };
     let ret = factory::rros_init_element(element.clone(), factory, clone_flags);
 
     let mut state = RrosMonitorState::new()?;
 
     match attrs.type_foo {
-        RROS_MONITOR_GATE => {
-            match attrs.protocol {
-                RROS_GATE_PP => {
-                    state.u = Some(RrosMonitorStateItem::Gate(RrosMonitorStateItemGate {
-                        owner: AtomicUsize::new(0),
-                        ceiling: attrs.initval,
-                        recursive: 0,
-                        nesting: 0,
-                    }));
-                },
-                RROS_GATE_PI => {
-                    ();
-                },
-                _ => ()
+        RROS_MONITOR_GATE => match attrs.protocol {
+            RROS_GATE_PP => {
+                state.u = Some(RrosMonitorStateItem::Gate(RrosMonitorStateItemGate {
+                    owner: AtomicUsize::new(0),
+                    ceiling: attrs.initval,
+                    recursive: 0,
+                    nesting: 0,
+                }));
             }
-        }
+            RROS_GATE_PI => {
+                ();
+            }
+            _ => (),
+        },
         RROS_MONITOR_EVENT => {
             state.u = Some(RrosMonitorStateItem::Event(RrosMonitorStateItemEvent {
                 value: AtomicUsize::new(usize::try_from(attrs.initval)?),
@@ -251,17 +244,17 @@ pub fn monitor_factory_build(fac: *mut RrosFactory, uname: &'static CStr, u_attr
                 gate_offset: RROS_MONITOR_NOGATE,
             }));
         }
-        _ => ()
+        _ => (),
     }
 
     // init monitor
     let mon = match state.u {
         Some(RrosMonitorStateItem::Gate(ref RrosMonitorStateItemGate)) => {
             let mut item = RrosMonitorItem1::new()?;
-            let pinned = unsafe {Pin::new_unchecked(&mut item.mutex)};
+            let pinned = unsafe { Pin::new_unchecked(&mut item.mutex) };
             spinlock_init!(pinned, "RrosMonitorItem1_lock");
 
-            let pinned = unsafe {Pin::new_unchecked(&mut item.lock)};
+            let pinned = unsafe { Pin::new_unchecked(&mut item.lock) };
             spinlock_init!(pinned, "value");
             RrosMonitor::new(
                 element,
@@ -270,7 +263,7 @@ pub fn monitor_factory_build(fac: *mut RrosFactory, uname: &'static CStr, u_attr
                 attrs.protocol as i32,
                 RrosMonitorItem::Item1(item),
             )?
-        },
+        }
         _ => {
             let item = RrosMonitorItem2::new()?;
             RrosMonitor::new(
@@ -289,27 +282,45 @@ pub fn monitor_factory_build(fac: *mut RrosFactory, uname: &'static CStr, u_attr
     return Ok(mon.element);
 }
 
-pub static mut RROS_MONITOR_FACTORY: SpinLock<factory::RrosFactory> = unsafe { SpinLock::new(factory::RrosFactory {
-    name: unsafe{CStr::from_bytes_with_nul_unchecked("RROS_MONITOR_DEV\0".as_bytes())},
-    nrdev: CONFIG_RROS_MONITOR,
-    build: None,
-    dispose: Some(monitor_factory_dispose),
-    attrs: None, //sysfs::attribute_group::new(),
-    flags: 2,
-    inside: Some(factory::RrosFactoryInside {
-        rrtype: None,
-        class: None,
-        cdev: None,
-        device: None,
-        sub_rdev: None,
-        kuid: None,
-        kgid: None,
-        minor_map: None,
-        index: None,
-        name_hash: None,
-        hash_lock: None,
-        register: None
-    }),
-})};
-
-pub fn monitor_factory_dispose(ele: factory::RrosElement) {}
\ No newline at end of file
+pub static mut RROS_MONITOR_FACTORY: SpinLock<factory::RrosFactory> = unsafe {
+    SpinLock::new(factory::RrosFactory {
+        name: unsafe { CStr::from_bytes_with_nul_unchecked("RROS_MONITOR_DEV\0".as_bytes()) },
+        nrdev: CONFIG_RROS_MONITOR,
+        build: None,
+        dispose: Some(monitor_factory_dispose),
+        attrs: None, //sysfs::attribute_group::new(),
+        flags: 2,
+        inside: Some(factory::RrosFactoryInside {
+            rrtype: None,
+            class: None,
+            cdev: None,
+            device: None,
+            sub_rdev: None,
+            kuid: None,
+            kgid: None,
+            minor_map: None,
+            index: None,
+            name_hash: None,
+            hash_lock: None,
+            register: None,
+        }),
+    })
+};
+
+pub fn monitor_factory_dispose(ele: factory::RrosElement) {}
+
+struct MonitorOps;
+
+impl FileOperations for MonitorOps {
+    kernel::declare_file_operations!(read);
+
+    fn read<T: IoBufferWriter>(
+        _this: &Self,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops of the rros monitor factory.");
+        Ok(1)
+    }
+}
\ No newline at end of file
diff --git a/kernel/rros/mutex.rs b/kernel/rros/mutex.rs
new file mode 100644
index 000000000..e86e32c64
--- /dev/null
+++ b/kernel/rros/mutex.rs
@@ -0,0 +1,1216 @@
+use kernel::{
+    bindings, c_types, prelude::*, str::CStr, c_str,double_linked_list::*,sync::{SpinLock, Lock, Guard},premmpt};
+use crate::{
+    sched::*,
+    clock::*,
+	fifo::*,
+	thread::*,
+	factory::*,
+	timeout,
+	lock,
+};
+
+pub const RROS_NO_HANDLE:u32 = 0x00000000;
+pub const RROS_MUTEX_PI: u32 = 1;
+pub const RROS_MUTEX_PP: u32 = 2;
+pub const RROS_MUTEX_CLAIMED: u32 = 4;
+pub const RROS_MUTEX_CEILING: u32 = 8;
+pub const RROS_MUTEX_FLCLAIM:u32 = 0x80000000;
+pub const RROS_MUTEX_FLCEIL:u32 = 0x40000000;
+pub const EDEADLK:i32 =	35;
+pub const EBUSY:i32 = 16;
+pub const EIDRM:i32 = 43;
+pub const ETIMEDOUT:i32 = 110;
+pub const EINTR:i32 = 4;
+
+type ktime_t = i64;
+type fundle_t = u32;
+
+static mut rros_nil:rros_value = rros_value{
+	val: 0,
+    lval: 0,
+    ptr: 0 as *mut c_types::c_void,
+};
+
+pub struct RrosMutex{
+    pub wprio: i32,
+	pub flags: i32,
+	pub owner: Option<Arc<SpinLock<rros_thread>>>,
+	pub clock: *mut RrosClock,
+	pub fastlock: *mut bindings::atomic_t,
+	pub ceiling_ref: u32,
+	pub lock: bindings::hard_spinlock_t,
+	pub wchan: rros_wait_channel,
+	pub next_booster: *mut Node<Arc<SpinLock<RrosMutex>>>,
+    pub next_tracker: *mut Node<Arc<SpinLock<RrosMutex>>>,
+}
+impl RrosMutex{
+	pub fn new()-> Self{
+		RrosMutex{
+			wprio: 0,
+			flags: 0,
+			owner: None,
+			clock: 0 as *mut RrosClock,
+			fastlock: &mut bindings::atomic_t { counter: 0 } as *mut bindings::atomic_t,
+			ceiling_ref: 0,
+			lock: bindings::hard_spinlock_t {
+                rlock: bindings::raw_spinlock {
+                    raw_lock: bindings::arch_spinlock_t {
+                        __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
+                            val: bindings::atomic_t { counter: 0 },
+                        },
+                    },
+                },
+                dep_map: bindings::phony_lockdep_map {
+                    wait_type_outer: 0,
+                    wait_type_inner: 0,
+                },
+            },
+			wchan: rros_wait_channel::new(),
+			next_booster: 0 as *mut Node<Arc<SpinLock<RrosMutex>>>,
+			next_tracker: 0 as *mut Node<Arc<SpinLock<RrosMutex>>>,
+		}
+	}
+}
+
+pub struct RrosKMutex{
+	pub mutex:*mut RrosMutex,
+	pub fastlock: *mut bindings::atomic_t,
+}
+impl RrosKMutex{
+	pub fn new() -> Self{
+		RrosKMutex { mutex: 0 as *mut RrosMutex, fastlock: &mut bindings::atomic_t { counter: 0 } as *mut bindings::atomic_t, }
+	}
+}
+
+extern "C"{
+    fn rust_helper_raw_spin_lock_init(lock: *mut bindings::hard_spinlock_t);
+	fn rust_helper_raw_spin_lock(lock: *mut bindings::hard_spinlock_t);
+	fn rust_helper_raw_spin_unlock(lock: *mut bindings::hard_spinlock_t);
+}
+
+pub fn raw_spin_lock_init(lock: *mut bindings::hard_spinlock_t){
+	unsafe{rust_helper_raw_spin_lock_init(lock)};
+}
+
+pub fn raw_spin_lock(lock: *mut bindings::hard_spinlock_t){
+	unsafe{rust_helper_raw_spin_lock(lock)};
+}
+
+pub fn raw_spin_unlock(lock: *mut bindings::hard_spinlock_t){
+	unsafe{rust_helper_raw_spin_unlock};
+}
+
+// #define for_each_evl_mutex_waiter(__pos, __mutex) \
+// 	list_for_each_entry(__pos, &(__mutex)->wchan.wait_list, wait_next)
+
+pub fn get_ceiling_value(mutex:*mut RrosMutex) -> u32{
+	let ceiling_ref = unsafe{(*mutex).ceiling_ref};
+	if ceiling_ref < 1{
+		return 1 as u32
+	}else if ceiling_ref <= RROS_FIFO_MAX_PRIO as u32{
+		return ceiling_ref
+	}else{
+		return RROS_FIFO_MAX_PRIO as u32
+	}
+}
+
+pub fn disable_inband_switch(curr:*mut rros_thread){	
+	unsafe{
+		if ((*curr).state & (T_WEAK|T_WOLI)) != 0{
+			atomic_inc(&mut (*curr).inband_disable_count as *mut bindings::atomic_t);
+		}
+	}
+}
+
+pub fn enable_inband_switch(curr:*mut rros_thread) -> bool{
+	unsafe{
+		if ((*curr).state & (T_WEAK|T_WOLI)) == 0{
+			return true;
+		}
+
+		if (atomic_dec_return(&mut (*curr).inband_disable_count as *mut bindings::atomic_t) >= 0){
+			return true;
+		}
+
+		atomic_set(&mut (*curr).inband_disable_count as *mut bindings::atomic_t, 0);
+		if (*curr).state & T_WOLI != 0{
+			rros_notify_thread(curr, RROS_HMDIAG_LKIMBALANCE as u32, &rros_nil);
+		}
+		return false;
+	}
+}
+
+pub fn raise_boost_flag(owner:Arc<SpinLock<rros_thread>>){
+	unsafe{
+		// assert_hard_lock(&owner->lock);
+		let lock = &mut (*(*owner.locked_data().get()).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(lock);
+		
+			let state = (*owner.locked_data().get()).state;
+			
+			if state & T_BOOST == 0 {
+				(*owner.locked_data().get()).bprio = (*owner.locked_data().get()).cprio;
+				(*owner.locked_data().get()).state |= T_BOOST;
+			}
+		raw_spin_unlock(lock);
+	}
+}
+
+pub fn inherit_thread_priority(owner:Arc<SpinLock<rros_thread>>, contender:Arc<SpinLock<rros_thread>>, originator:Arc<SpinLock<rros_thread>>) -> Result<i32>{
+	let ret:Result<i32>;
+
+	// assert_hard_lock(&owner->lock);
+	// assert_hard_lock(&contender->lock);
+
+	rros_track_thread_policy(owner.clone(), contender.clone());
+
+	let func;
+	unsafe{
+		let wchan = (*owner.locked_data().get()).wchan.clone().unwrap();
+		match (*wchan.locked_data().get()).reorder_wait{
+			Some(f) => func = f,
+			None => {
+				pr_warn!("inherit_thread_priority:reorder_wait function error");
+				return Err(kernel::Error::EINVAL);
+			}
+		}
+	}
+	unsafe{return func(owner.clone(),originator.clone())};
+
+}
+
+
+pub fn adjust_boost(owner:Arc<SpinLock<rros_thread>>,contender:Arc<SpinLock<rros_thread>>,
+	origin:*mut RrosMutex,originator:Arc<SpinLock<rros_thread>>) -> Result<i32>{
+	
+	unsafe{
+		let mut mutex = 0 as *mut RrosMutex;
+		let mut pprio:u32 = 0; 
+		let mut ret:Result<i32> = Ok(0);
+		// assert_hard_lock(&owner->lock);
+		// assert_hard_lock(&origin->lock);
+		let boosters = (*owner.locked_data().get()).boosters;
+		mutex = Arc::into_raw((*boosters).get_head().unwrap().value.clone()) as *mut SpinLock<RrosMutex> as *mut RrosMutex;
+		if mutex != origin{
+			raw_spin_lock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+		}
+		let wprio = (*owner.locked_data().get()).wprio;
+		if (*mutex).wprio == wprio {
+			if mutex != origin{
+				raw_spin_unlock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+			}
+			return Ok(0);
+		}
+
+		if (*mutex).flags & RROS_MUTEX_PP as i32 != 0 {
+			pprio = get_ceiling_value(mutex);
+			
+			rros_protect_thread_priority(owner.clone(), pprio as i32);
+			let wchan = (*owner.locked_data().get()).wchan.clone().unwrap();
+			let func;
+			match (*wchan.locked_data().get()).reorder_wait{
+				Some(f) => func = f,
+				None => {
+					pr_warn!("adjust_boost:reorder_wait function error");
+					return Err(kernel::Error::EINVAL);
+				}
+			}
+			ret = func(owner.clone(), originator.clone());
+			if mutex != origin{
+				raw_spin_unlock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+			}
+		} else {
+			if (*(*mutex).wchan.wait_list).is_empty() {
+				if mutex != origin{
+					raw_spin_unlock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+				}
+				return Ok(0);
+			}
+			let contender_ptr = Arc::into_raw(contender.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+			if contender_ptr == 0 as *mut rros_thread {
+				let contender = (*(*mutex).wchan.wait_list).get_head().unwrap().value.clone();
+				let lock = &mut (*contender.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+				raw_spin_lock(lock);
+				ret = inherit_thread_priority(owner.clone(), contender.clone(),originator.clone());
+				raw_spin_unlock(lock);
+			} else { 
+				ret = inherit_thread_priority(owner.clone(), contender.clone(),originator.clone());
+			}
+			if mutex != origin{
+				raw_spin_unlock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+			}
+		}
+		return ret;
+	}
+}
+
+pub fn ceil_owner_priority(mutex:*mut RrosMutex,originator:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+	unsafe{
+		let owner = (*mutex).owner.clone().unwrap().clone();
+		let wprio:i32;
+		// assert_hard_lock(&mutex->lock);
+		wprio = rros_calc_weighted_prio(&rros_sched_fifo,get_ceiling_value(mutex) as i32);
+		(*mutex).wprio = wprio;
+		let lock = &mut (*owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(lock);
+
+		let boosters = (*owner.locked_data().get()).boosters;
+		if (*boosters).is_empty(){
+			(*boosters).add_head((*(*mutex).next_booster).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*boosters).len()..=1{
+				let wprio_in_list =  (*(*(*boosters).get_by_index(i).unwrap()).value.clone().locked_data().get()).wprio;
+				if(*mutex).wprio <= wprio_in_list{
+					flag = 0;
+					(*boosters).enqueue_by_index(i,(*(*mutex).next_booster).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				(*boosters).add_head((*(*mutex).next_booster).value.clone());
+			}
+		}
+		
+		raise_boost_flag(owner.clone());
+		(*mutex).flags |= RROS_MUTEX_CEILING as i32;
+
+		let owner_wprio = (*owner.locked_data().get()).wprio;
+		if wprio > owner_wprio{
+			adjust_boost(owner.clone(), Arc::try_new(SpinLock::new(rros_thread::new()?))?, mutex, originator.clone());
+		}
+		raw_spin_unlock(lock);
+		Ok(0)
+	}
+}
+
+pub fn untrack_owner(mutex:*mut RrosMutex){
+	unsafe{
+		let prev = (*mutex).owner.clone();
+
+		// assert_hard_lock(&mutex->lock);
+		if prev.is_some() {
+			let flags = lock::raw_spin_lock_irqsave();
+			(*(*mutex).next_tracker).remove();
+			lock::raw_spin_unlock_irqrestore(flags);
+			// evl_put_element(&prev->element);
+			(*mutex).owner = None;
+		}
+	}
+}
+
+pub fn track_owner(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>){
+	unsafe{
+		let prev = (*mutex).owner.clone();
+		// assert_hard_lock(&mutex->lock);
+		// if (EVL_WARN_ON_ONCE(CORE, prev == owner))
+		// 	return;
+
+		let flags = lock::raw_spin_lock_irqsave();
+		if prev.is_some() {
+			(*(*mutex).next_tracker).remove();
+			// smp_wmb();
+			// evl_put_element(&prev->element);
+		}
+		(*(*owner.locked_data().get()).trackers).add_head((*((*mutex).next_tracker)).value.clone());
+		lock::raw_spin_unlock_irqrestore(flags);
+		(*mutex).owner = Some(owner.clone());
+	}
+}
+
+pub fn ref_and_track_owner(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>){
+	unsafe{
+		let ptr1 = Arc::into_raw((*mutex).owner.clone().unwrap()) as *mut SpinLock<rros_thread>;
+		let ptr2 = Arc::into_raw(owner.clone()) as *mut SpinLock<rros_thread>;
+		if ptr1 != ptr2 {
+			// evl_get_element(&owner->element);
+			track_owner(mutex, owner.clone());
+		}
+	}
+}
+
+pub fn fast_mutex_is_claimed(handle:u32) -> bool{
+	return handle & RROS_MUTEX_FLCLAIM != 0;
+}
+
+pub fn mutex_fast_claim(handle:u32) -> u32{
+	return handle | RROS_MUTEX_FLCLAIM;
+}
+
+pub fn mutex_fast_ceil(handle:u32) -> u32{
+	return handle | RROS_MUTEX_FLCEIL;
+}
+
+pub fn set_current_owner_locked(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>){
+	// assert_hard_lock(&mutex->lock);
+	ref_and_track_owner(mutex, owner.clone());
+	pr_info!("1111111111111111111111111111111111111111111");
+	unsafe{
+		if (*mutex).flags & RROS_MUTEX_PP as i32 != 0{
+			pr_info!("2222222222222222222222222222222222222");
+			ceil_owner_priority(mutex, owner.clone());
+			pr_info!("333333333333333333333333333333333333333");
+		}
+	}
+}
+
+pub fn set_current_owner(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+	pr_info!("00000000000000000000000000000000000000");
+	let flags = lock::raw_spin_lock_irqsave();
+	pr_info!("000000000000000.............555000000000000000000000.50.50.5");
+	set_current_owner_locked(mutex, owner.clone());
+	lock::raw_spin_unlock_irqrestore(flags);
+	pr_info!("99999999999999999999999999999999999999");
+	Ok(0)
+}
+
+pub fn get_owner_handle(mut ownerh:u32, mutex:*mut RrosMutex) -> u32{
+	unsafe{
+		if (*mutex).flags & RROS_MUTEX_PP as i32 != 0{
+			ownerh = mutex_fast_ceil(ownerh);
+		}
+		return ownerh;
+	}
+}
+
+pub fn clear_boost_locked(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>,flag:i32) -> Result<i32>{
+	unsafe{
+		// assert_hard_lock(&mutex->lock);
+		// assert_hard_lock(&owner->lock);
+		(*mutex).flags &= !flag;
+
+		(*(*mutex).next_booster).remove();
+		let boosters = (*owner.locked_data().get()).boosters;
+		if (*boosters).is_empty() {
+			let lock = &mut (*(*owner.locked_data().get()).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+			raw_spin_lock(lock);
+			(*owner.locked_data().get()).state &= !T_BOOST;
+			raw_spin_unlock(lock);
+			inherit_thread_priority(owner.clone(), owner.clone(), owner.clone());
+		} else{
+			adjust_boost(owner.clone(), Arc::try_new(SpinLock::new(rros_thread::new()?))?, mutex, owner.clone());
+		}
+		Ok(0)
+	}
+}
+
+pub fn clear_boost(mutex:*mut RrosMutex,owner:Arc<SpinLock<rros_thread>>,flag:i32) -> Result<usize>{
+	let lock = unsafe{&mut (*owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t};
+	raw_spin_lock(lock);
+	clear_boost_locked(mutex, owner.clone(), flag);
+	raw_spin_unlock(lock);
+	Ok(0)
+}
+
+pub fn detect_inband_owner(mutex:*mut RrosMutex,curr:*mut rros_thread){
+	unsafe{
+		let owner = (*mutex).owner.clone().unwrap();
+		let lock = &mut (*(*curr).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(lock);
+		let state = (*owner.locked_data().get()).state;
+		if (*curr).info & T_PIALERT !=0 {
+			(*curr).info &= !T_PIALERT;
+		} else if state & T_INBAND !=0 {
+			(*curr).info |= T_PIALERT;
+			raw_spin_unlock(lock);
+			rros_notify_thread(curr, RROS_HMDIAG_LKDEPEND as u32, &rros_nil);
+			return;
+		}
+
+		raw_spin_unlock(lock);
+	}
+}
+
+pub fn rros_detect_boost_drop(){
+	unsafe{
+		let curr = rros_current() as *mut rros_thread;
+		let mut waiter = 0 as *mut rros_thread;
+		let mutex = 0 as *mut RrosMutex;
+
+		let flags = lock::raw_spin_lock_irqsave();
+
+		let boosters = (*curr).boosters;
+		for i in 1..=(*boosters).len() {
+			let wait_list = (*(*boosters).get_by_index(i).unwrap().value.clone().locked_data().get()).wchan.wait_list;
+			// raw_spin_lock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+			for j in 1..= (*wait_list).len() {
+				let waiter_node = (*wait_list).get_by_index(j).unwrap().value.clone();
+				waiter = Arc::into_raw(waiter_node) as *mut SpinLock<rros_thread> as *mut rros_thread;
+				if (*waiter).state & T_WOLI == 0{
+					continue;
+				}
+				raw_spin_lock(&mut (*(*waiter).rq.unwrap()).lock as *mut bindings::hard_spinlock_t);
+				(*waiter).info |= T_PIALERT;
+				raw_spin_unlock(&mut (*(*waiter).rq.unwrap()).lock as *mut bindings::hard_spinlock_t);
+				rros_notify_thread(waiter, RROS_HMDIAG_LKDEPEND as u32, &rros_nil);
+			}
+			// raw_spin_unlock(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+		}
+
+		lock::raw_spin_unlock_irqrestore(flags);
+	}
+}
+
+pub fn __rros_init_mutex(mutex:*mut RrosMutex, clock:*mut RrosClock,
+		fastlock:*mut bindings::atomic_t, ceiling_ref:u32){
+	unsafe{
+		let mut Type:u32 = 0;
+		if ceiling_ref == 0{
+			Type = RROS_MUTEX_PI;
+		}else{
+			Type = RROS_MUTEX_PP;
+		}
+		if mutex == 0 as *mut RrosMutex{
+			pr_info!("__rros_init_mutex error!");
+			return;
+		}
+		(*mutex).fastlock = fastlock;
+		atomic_set(fastlock, RROS_NO_HANDLE as i32);
+		(*mutex).flags = (Type & !RROS_MUTEX_CLAIMED) as i32;
+		(*mutex).owner = None;
+		(*mutex).wprio = -1;
+		(*mutex).ceiling_ref = ceiling_ref;
+		(*mutex).clock = clock;
+		(*mutex).wchan.reorder_wait = Some(rros_reorder_mutex_wait);
+		(*mutex).wchan.follow_depend = Some(rros_follow_mutex_depend);
+		raw_spin_lock_init(&mut (*mutex).lock as *mut bindings::hard_spinlock_t);
+	}
+}
+
+
+pub fn flush_mutex_locked(mutex:*mut RrosMutex, reason:u32) ->Result<usize>{
+	let tmp = 0 as *mut rros_thread;
+	// assert_hard_lock(&mutex->lock);
+	unsafe{
+		let mut thread_node = Arc::try_new(SpinLock::new(rros_thread::new()?))?;
+		if (*(*mutex).wchan.wait_list).is_empty(){
+			// EVL_WARN_ON(CORE, mutex->flags & EVL_MUTEX_CLAIMED);
+		}else {
+			for i in 1..=(*(*mutex).wchan.wait_list).len(){
+				thread_node = (*(*(*mutex).wchan.wait_list).get_by_index(i).unwrap()).value.clone();
+				(*(*thread_node.locked_data().get()).wait_next).remove();
+				rros_wakeup_thread(thread_node.clone(), T_PEND, reason);
+			}
+			if (*mutex).flags & RROS_MUTEX_CLAIMED as i32 != 0{
+				clear_boost(mutex, (*mutex).owner.clone().unwrap(), RROS_MUTEX_CLAIMED as i32);
+			}
+		}
+	}
+	Ok(0)
+}
+
+pub fn rros_flush_mutex(mutex:*mut RrosMutex, reason:u32){
+
+	// trace_evl_mutex_flush(mutex);
+	let flags = lock::raw_spin_lock_irqsave();
+	flush_mutex_locked(mutex, reason);
+	lock::raw_spin_unlock_irqrestore(flags);
+}
+
+pub fn rros_destroy_mutex(mutex:*mut RrosMutex){
+
+	// trace_evl_mutex_destroy(mutex);
+	let flags = lock::raw_spin_lock_irqsave();
+	untrack_owner(mutex);
+	flush_mutex_locked(mutex, T_RMID);
+	lock::raw_spin_unlock_irqrestore(flags);
+}
+
+pub fn rros_trylock_mutex(mutex:*mut RrosMutex) -> Result<i32>{
+	let curr = unsafe{&mut *rros_current()};
+	let lockp = unsafe{(*mutex).fastlock};
+	let h:i32 = 0;
+
+	premmpt::running_inband()?;
+	// trace_evl_mutex_trylock(mutex);
+
+	// h = atomic_cmpxchg(lockp, RROS_NO_HANDLE as i32,
+	// 		get_owner_handle(fundle_of(curr), mutex) as i32);
+	// if h as i32 != RROS_NO_HANDLE{
+	// 	if rros_get_index(h) == fundle_of(curr){
+	// 		return -EDEADLK;
+	// 	}
+	// 	else{
+	// 		return -EBUSY;
+	// 	}
+	// }
+	
+	unsafe{set_current_owner(mutex, Arc::from_raw(curr as *const SpinLock<rros_thread>))};
+	disable_inband_switch(curr as *mut SpinLock<rros_thread> as *mut rros_thread);
+
+	return Ok(0);
+}
+
+pub fn wait_mutex_schedule(mutex:*mut RrosMutex) -> Result<i32>{
+	let curr = rros_current();
+	let flags:u32 = 0;
+	let mut ret:Result<i32> = Ok(0); 
+	let mut info:u32 = 0;
+
+	unsafe{rros_schedule()};
+
+	info = unsafe{(*(*rros_current()).locked_data().get()).info};
+	if info & T_RMID != 0{
+		return Err(kernel::Error::EIDRM);
+	}
+
+	if info & (T_TIMEO|T_BREAK) != 0 {
+		let flags = lock::raw_spin_lock_irqsave();
+		let wait_next = unsafe{(*(*curr).locked_data().get()).wait_next};
+		unsafe{(*wait_next).remove()};
+		if info & T_TIMEO != 0{
+			ret = Err(kernel::Error::ETIMEDOUT);
+		}
+		else if info & T_BREAK != 0{
+			ret = Err(kernel::Error::EINTR);
+		}
+		
+		lock::raw_spin_unlock_irqrestore(flags);
+	}
+	// } else if (IS_ENABLED(CONFIG_EVL_DEBUG_CORE)) {
+	// 	bool empty;
+	// 	// raw_spin_lock_irqsave(&mutex->lock, flags);
+	// 	empty = list_empty(&curr->wait_next);
+	// 	// raw_spin_unlock_irqrestore(&mutex->lock, flags);
+	// 	// EVL_WARN_ON_ONCE(CORE, !empty);
+	// }
+
+	return ret;
+}
+
+pub fn finish_mutex_wait(mutex:*mut RrosMutex){
+	unsafe{
+		let owner = (*mutex).owner.clone().unwrap();
+
+		// assert_hard_lock(&mutex->lock);
+
+		if (*mutex).flags & RROS_MUTEX_CLAIMED as i32 == 0{
+			return;
+		}
+
+		if (*(*mutex).wchan.wait_list).is_empty() {
+			clear_boost(mutex, owner.clone(), RROS_MUTEX_CLAIMED as i32);
+			return;
+		}
+
+		let contender = (*(*mutex).wchan.wait_list).get_head().unwrap().value.clone();
+		let owner_lock = &mut (*owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+		let contender_lock = &mut (*contender.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(owner_lock);
+		raw_spin_lock(contender_lock);
+		(*mutex).wprio = (*contender.locked_data().get()).wprio;
+		(*(*mutex).next_booster).remove();
+
+		let boosters = (*owner.locked_data().get()).boosters;
+		if (*boosters).is_empty(){
+			(*boosters).add_head((*(*mutex).next_booster).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*boosters).len()..=1{
+				let wprio_in_list = (*(*boosters).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+				if(*mutex).wprio <= wprio_in_list{
+					flag = 0;
+					(*boosters).enqueue_by_index(i,(*(*mutex).next_booster).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				(*boosters).add_head((*(*mutex).next_booster).value.clone());
+			}
+		}
+
+		adjust_boost(owner.clone(), contender.clone(), mutex, owner.clone());
+		raw_spin_unlock(contender_lock);
+		raw_spin_unlock(owner_lock);
+	}
+}
+
+pub fn check_lock_chain(owner:Arc<SpinLock<rros_thread>>,originator:Arc<SpinLock<rros_thread>>) -> Result<i32>{
+	unsafe{
+		let mut wchan = (*owner.locked_data().get()).wchan.clone();
+		// assert_hard_lock(&owner->lock);
+		// assert_hard_lock(&originator->lock);
+
+		if wchan.is_some(){
+			let func;
+			match (*wchan.clone().unwrap().locked_data().get()).follow_depend{
+				Some(f) => func = f,
+				None => {
+					pr_warn!("check_lock_chain:follow_depend function error");
+					return Err(kernel::Error::EINVAL);
+				}
+			}
+			return func(wchan.as_mut().unwrap().clone(), originator.clone());
+		}
+		Ok(0)
+	}
+}
+
+pub fn rros_lock_mutex_timeout(mutex:*mut RrosMutex, timeout:ktime_t,timeout_mode:timeout::rros_tmode) -> Result<i32>{
+	unsafe{
+	let curr = &mut *rros_current();
+	let owner= Arc::try_new(SpinLock::new(rros_thread::new()?))?;
+	let lockp = (*mutex).fastlock;
+	let mut currh:fundle_t = 0;
+	let mut h:fundle_t = 0;
+	let mut oldh:fundle_t = 0;
+	let flags:u32 = 0;
+	let mut ret:Result<i32>;
+	premmpt::running_inband()?;
+	// currh = fundle_of(curr);
+	// trace_evl_mutex_lock(mutex);
+	pr_info!("rros_lock_mutex_timeout rros_current address is {:p}",rros_current());
+	loop{
+	h = atomic_cmpxchg(lockp, RROS_NO_HANDLE as i32,get_owner_handle(currh, mutex) as i32) as fundle_t;
+	if h == RROS_NO_HANDLE{
+		let temp = Arc::from_raw(rros_current() as *const SpinLock<rros_thread>);
+		let test = temp.clone();
+		pr_info!("{:p}",test);
+		pr_info!("-1-1-1-1-1-1-1-1-1-1-1-1--1-1-11-1-1-1-1-1-1");
+		set_current_owner(mutex, temp.clone());
+
+		disable_inband_switch(curr as *mut SpinLock<rros_thread> as *mut rros_thread);
+
+		return Ok(0);
+	}
+	
+	if rros_get_index(h) == currh{
+		return Err(kernel::Error::EDEADLK);
+	}
+	
+	ret = Ok(0);
+	let mut test_no_owner = 0; // goto test_no_owner
+	let mut flags = lock::raw_spin_lock_irqsave();
+	let curr_lock = &mut (*curr.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+	raw_spin_lock(curr_lock);
+	if fast_mutex_is_claimed(h) == true {
+		oldh = atomic_read(lockp) as u32;
+		test_no_owner = 1;
+	}
+	
+	let mut redo = 0;
+	loop {
+		if test_no_owner == 0{
+			oldh = atomic_cmpxchg(lockp, h as i32, mutex_fast_claim(h) as i32) as u32;
+			if oldh == h{
+				break;
+			}
+		}
+		if oldh == RROS_NO_HANDLE {
+			raw_spin_unlock(curr_lock);
+			lock::raw_spin_unlock_irqrestore(flags);
+			redo = 1;
+			break;
+		}
+		h = oldh;
+		if fast_mutex_is_claimed(h) == true{
+			break;
+		}
+		test_no_owner = 0;
+	}
+	if redo == 1{
+		continue;
+	}
+	pr_info!("33333333333333333333333333333333");
+	// owner = evl_get_factory_element_by_fundle(&evl_thread_factory,evl_get_index(h),struct evl_thread);
+	let owner_ptr = Arc::into_raw(owner.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+	if owner_ptr == 0 as *mut rros_thread {
+		untrack_owner(mutex);
+		raw_spin_unlock(curr_lock);
+		lock::raw_spin_unlock_irqrestore(flags);
+		return Err(kernel::Error::EOWNERDEAD);
+	}
+	let ptr1 = Arc::into_raw((*mutex).owner.clone().unwrap()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+	let ptr2 = Arc::into_raw(owner.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+	if ptr1 != ptr2{
+		track_owner(mutex, owner.clone());
+	}
+	else{
+		// evl_put_element(&owner->element);
+	}
+	let owner_lock = &mut (*owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+	raw_spin_lock(owner_lock);
+	let state = (*curr.locked_data().get()).state;
+	if state & T_WOLI != 0{
+		detect_inband_owner(mutex, curr as *mut SpinLock<rros_thread> as *mut rros_thread);
+	}
+	let wprio =  (*curr.locked_data().get()).wprio;
+	let owner_wprio = (*owner.locked_data().get()).wprio;
+	if wprio > owner_wprio {
+		let info = (*owner.locked_data().get()).info;
+		let wwake = (*owner.locked_data().get()).wwake;
+		if info & T_WAKEN != 0 && wwake == &mut (*mutex).wchan as *mut rros_wait_channel {
+			let temp = Arc::from_raw(curr as *const SpinLock<rros_thread>);
+			set_current_owner_locked(mutex, temp.clone());
+			let owner_rq_lock = &mut (*(*owner.locked_data().get()).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+			raw_spin_lock(owner_rq_lock);
+			(*owner.locked_data().get()).info |= T_ROBBED;
+			raw_spin_unlock(owner_rq_lock);
+			raw_spin_unlock(owner_lock);
+			disable_inband_switch(curr as *mut SpinLock<rros_thread> as *mut rros_thread);
+			if (*(*mutex).wchan.wait_list).is_empty() == false{
+				currh = mutex_fast_claim(currh);
+			}
+			atomic_set(lockp, get_owner_handle(currh, mutex) as i32);
+			raw_spin_unlock(curr_lock);
+			lock::raw_spin_unlock_irqrestore(flags);
+			return ret;
+		}
+
+		if (*(*mutex).wchan.wait_list).is_empty(){
+			let wait_next = (*curr.locked_data().get()).wait_next;
+			(*(*mutex).wchan.wait_list).add_head((*wait_next).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*(*mutex).wchan.wait_list).len()..=1{
+				let curr_wprio = (*(*curr).locked_data().get()).wprio;
+				let wprio_in_list = (*(*(*mutex).wchan.wait_list).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+				if curr_wprio <= wprio_in_list{
+					flag = 0;
+					let wait_next = (*curr.locked_data().get()).wait_next;
+					(*(*mutex).wchan.wait_list).enqueue_by_index(i,(*wait_next).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				let wait_next = (*curr.locked_data().get()).wait_next;
+				(*(*mutex).wchan.wait_list).add_head((*wait_next).value.clone());
+			}
+		}
+
+		if (*mutex).flags & RROS_MUTEX_PI as i32 != 0 {
+			raise_boost_flag(owner.clone());
+			if (*mutex).flags & RROS_MUTEX_CLAIMED as i32 != 0{
+				(*(*mutex).next_booster).remove();
+			}
+			else{
+				(*mutex).flags |= RROS_MUTEX_CLAIMED as i32;
+			}
+			(*mutex).wprio = (*curr.locked_data().get()).wprio;
+
+			let boosters = (*owner.locked_data().get()).boosters;
+			if (*boosters).is_empty(){
+				(*boosters).add_head((*((*mutex).next_booster)).value.clone());
+			}else{
+				let mut flag = 1; // flag指示是否到头
+				for i in (*boosters).len()..=1{
+					let wprio_in_list = (*(*boosters).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+					if(*mutex).wprio <= wprio_in_list{
+						flag = 0;
+						(*boosters).enqueue_by_index(i,(*((*mutex).next_booster)).value.clone());
+						break;
+					}
+				}
+				if flag == 1{
+					(*boosters).add_head((*((*mutex).next_booster)).value.clone());
+				}
+			}
+			let temp = Arc::from_raw(curr as *const SpinLock<rros_thread>);
+			ret = inherit_thread_priority(owner.clone(), temp.clone(), temp.clone());
+		} else {
+			let temp = Arc::from_raw(curr as *const SpinLock<rros_thread>);
+			ret = check_lock_chain(owner.clone(), temp.clone());
+		}
+
+	} else {
+
+		if (*(*mutex).wchan.wait_list).is_empty(){
+			let wait_next = (*curr.locked_data().get()).wait_next;
+			(*(*mutex).wchan.wait_list).add_head((*wait_next).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*(*mutex).wchan.wait_list).len()..=1{
+				let curr_wprio = (*curr.locked_data().get()).wprio;
+				let wprio_in_list = (*(*(*mutex).wchan.wait_list).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+				if curr_wprio <= wprio_in_list{
+					flag = 0;
+					let wait_next = (*curr.locked_data().get()).wait_next;
+					(*(*mutex).wchan.wait_list).enqueue_by_index(i,(*wait_next).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				let wait_next = (*curr.locked_data().get()).wait_next;
+				(*(*mutex).wchan.wait_list).add_head((*wait_next).value.clone());
+			}
+		}
+		let temp = Arc::from_raw(curr as *const SpinLock<rros_thread>);
+		ret = check_lock_chain(owner.clone(), temp.clone());
+	}
+	raw_spin_unlock(owner_lock);
+	if ret != Ok(0) {
+		let curr_rq_lock = &mut (*(*curr.locked_data().get()).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(curr_rq_lock);
+		evl_sleep_on_locked(timeout, timeout_mode, &(*((*mutex).clock)), &mut (*mutex).wchan as *mut rros_wait_channel);
+		raw_spin_unlock(curr_rq_lock);
+		raw_spin_unlock(curr_lock);
+		lock::raw_spin_unlock_irqrestore(flags);
+		ret = wait_mutex_schedule(mutex);
+		flags = lock::raw_spin_lock_irqsave();
+	} else {
+		raw_spin_unlock(curr_lock);
+	}
+	
+	finish_mutex_wait(mutex);
+	raw_spin_lock(curr_lock);
+	(*curr.locked_data().get()).wwake = 0 as *mut rros_wait_channel;
+	let curr_rq_lock = &mut (*(*curr.locked_data().get()).rq.unwrap()).lock as *mut bindings::hard_spinlock_t;
+	raw_spin_lock(curr_rq_lock);
+	(*curr.locked_data().get()).info &= !T_WAKEN;
+	if ret != Ok(0) {
+		raw_spin_unlock(curr_rq_lock);
+		raw_spin_unlock(curr_lock);
+		lock::raw_spin_unlock_irqrestore(flags);
+		return ret;
+	}
+	let info = (*curr.locked_data().get()).info;
+	if info & T_ROBBED != 0 {
+		raw_spin_unlock(curr_rq_lock);
+		// if timeout_mode != timeout::rros_tmode::RROS_REL ||
+		// 	timeout == 0 ||
+		// 	rros_get_stopped_timer_delta((*curr).locked_data().get().rtimer) != 0 {
+		// 	// raw_spin_unlock(&curr->lock);
+		// 	// raw_spin_unlock_irqrestore(&mutex->lock, flags);
+		// 	continue;
+		// } // todo rros_get_stopped_timer_delta
+		raw_spin_unlock(curr_lock);
+		lock::raw_spin_unlock_irqrestore(flags);
+		return Err(kernel::Error::ETIMEDOUT);
+	}
+	raw_spin_unlock(curr_rq_lock);
+
+	disable_inband_switch(curr as *mut SpinLock<rros_thread> as *mut rros_thread);
+	if (*(*mutex).wchan.wait_list).is_empty() == false{
+		currh = mutex_fast_claim(currh);
+	}
+	atomic_set(lockp, get_owner_handle(currh, mutex) as i32);
+
+	raw_spin_unlock(curr_lock);
+	lock::raw_spin_unlock_irqrestore(flags);
+	return ret;
+	} // goto redo
+	} // unsafe
+}
+
+pub fn transfer_ownership(mutex:*mut RrosMutex,lastowner:Arc<SpinLock<rros_thread>>){
+	unsafe{
+		let lockp = (*mutex).fastlock;
+		let mut n_ownerh:fundle_t = 0;
+
+		// assert_hard_lock(&mutex->lock);
+
+		if (*(*mutex).wchan.wait_list).is_empty() {
+			untrack_owner(mutex);
+			atomic_set(lockp, RROS_NO_HANDLE as i32);
+			return;
+		}
+
+		let n_owner = (*(*mutex).wchan.wait_list).get_head().unwrap().value.clone();
+		let lock = &mut (*n_owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(lock);
+		(*n_owner.locked_data().get()).wwake = &mut (*mutex).wchan as *mut rros_wait_channel;
+		(*n_owner.locked_data().get()).wchan = None;
+		raw_spin_unlock(lock);
+		(*(*n_owner.locked_data().get()).wait_next).remove();
+		set_current_owner_locked(mutex, n_owner.clone());
+		rros_wakeup_thread(n_owner.clone(), T_PEND, T_WAKEN);
+
+		if (*mutex).flags & RROS_MUTEX_CLAIMED as i32 != 0{
+			clear_boost_locked(mutex, lastowner.clone(), RROS_MUTEX_CLAIMED as i32);
+		}
+		// n_ownerh = get_owner_handle(fundle_of(n_owner), mutex);
+		if (*(*mutex).wchan.wait_list).is_empty() == false{
+			n_ownerh = mutex_fast_claim(n_ownerh);
+		}
+
+		atomic_set(lockp, n_ownerh as i32);
+	}
+}
+
+pub fn __rros_unlock_mutex(mutex:*mut RrosMutex) -> Result<i32>{
+	let mut curr = unsafe{&mut *rros_current()};
+	let owner = unsafe{Arc::from_raw(curr as *const SpinLock<rros_thread>)};
+	let flags:u32 = 0;
+	let currh:fundle_t = 0;
+	let mut h:fundle_t = 0;
+	let mut lockp = 0 as *mut bindings::atomic_t;
+
+	// trace_evl_mutex_unlock(mutex);
+	
+	if enable_inband_switch(curr as *mut SpinLock<rros_thread> as *mut rros_thread) == false{
+		return Ok(0);
+	}
+
+	lockp = unsafe{(*mutex).fastlock};
+	// currh = fundle_of(curr);
+
+	let flags = lock::raw_spin_lock_irqsave();
+	let lock = unsafe{&mut (*curr.locked_data().get()).lock as *mut bindings::hard_spinlock_t};
+	raw_spin_lock(lock);
+
+	unsafe{if (*mutex).flags & RROS_MUTEX_CEILING as i32 != 0{
+		clear_boost_locked(mutex, owner.clone(), RROS_MUTEX_CEILING as i32);
+	}}
+	h = atomic_read(lockp) as u32;
+	h = atomic_cmpxchg(lockp, h as i32, RROS_NO_HANDLE as i32) as u32;
+	if (h & !RROS_MUTEX_FLCEIL) != currh {
+		transfer_ownership(mutex, owner.clone());
+	} else {
+		if h != currh{
+			atomic_set(lockp, RROS_NO_HANDLE as i32);
+		}
+		untrack_owner(mutex);
+	}
+	raw_spin_unlock(lock);
+	lock::raw_spin_unlock_irqrestore(flags);
+	Ok(0)
+}
+
+pub fn rros_unlock_mutex(mutex:*mut RrosMutex) -> Result<usize>{
+	unsafe{
+		let curr = &mut *rros_current();
+		// fundle_t currh = fundle_of(curr), h;
+
+		premmpt::running_inband()?;
+
+		// h = evl_get_index(atomic_read(mutex->fastlock));
+		// if (EVL_WARN_ON_ONCE(CORE, h != currh))
+		// 	return;
+
+		__rros_unlock_mutex(mutex);
+		rros_schedule();
+		Ok(0)
+	}
+}
+
+pub fn rros_drop_tracking_mutexes(curr:*mut rros_thread){
+	unsafe{
+		let mut mutex = 0 as *mut RrosMutex;
+		let flags:u32 = 0;
+		let mut h:fundle_t = 0;
+
+		let mut flags = lock::raw_spin_lock_irqsave();
+
+		while (*(*curr).trackers).is_empty() == false {
+			mutex = Arc::into_raw((*(*(*curr).trackers).get_head().unwrap()).value.clone()) as *mut SpinLock<RrosMutex> as *mut RrosMutex;
+			lock::raw_spin_unlock_irqrestore(flags);
+			h = rros_get_index(atomic_read((*mutex).fastlock) as fundle_t);
+			// if (h == fundle_of(curr)) {
+			// 	__rros_unlock_mutex(mutex);
+			// } else {
+			// 	// raw_spin_lock_irqsave(&mutex->lock, flags);
+			// 	if (*mutex).owner == curr{
+			// 		untrack_owner(mutex);
+			// 	}
+			// 	// raw_spin_unlock_irqrestore(&mutex->lock, flags);
+			// }
+			flags = lock::raw_spin_lock_irqsave();
+		}
+		lock::raw_spin_unlock_irqrestore(flags);
+	}
+}
+
+pub fn wchan_to_mutex(wchan:*mut rros_wait_channel) -> *mut RrosMutex{
+	return kernel::container_of!(wchan, RrosMutex, wchan) as *mut RrosMutex;
+}
+
+pub fn rros_reorder_mutex_wait(waiter: Arc<SpinLock<rros_thread>>, originator: Arc<SpinLock<rros_thread>>) -> Result<i32>{
+	unsafe{
+		let waiter_ptr = Arc::into_raw(waiter.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+		let originator_ptr = Arc::into_raw(originator.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+		let mutex = wchan_to_mutex(Arc::into_raw((*waiter_ptr).wchan.clone().unwrap()) as *mut SpinLock<rros_wait_channel> as *mut rros_wait_channel);
+		let owner = (*mutex).owner.clone().unwrap();
+		let owner_ptr = Arc::into_raw(owner.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+		// assert_hard_lock(&waiter->lock);
+		// assert_hard_lock(&originator->lock);
+
+		let mutex_lock = &mut (*mutex).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(mutex_lock);
+		if owner_ptr == originator_ptr {
+			raw_spin_unlock(mutex_lock);
+			return Err(kernel::Error::EDEADLK);
+		}
+
+		(*(*waiter_ptr).wait_next).remove();
+		if (*(*mutex).wchan.wait_list).is_empty(){
+			(*(*mutex).wchan.wait_list).add_head((*((*waiter_ptr).wait_next)).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*(*mutex).wchan.wait_list).len()..=1{
+				let wprio_in_list = (*(*(*mutex).wchan.wait_list).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+				if(*waiter_ptr).wprio <= wprio_in_list{
+					flag = 0;
+					(*(*mutex).wchan.wait_list).enqueue_by_index(i,(*((*waiter_ptr).wait_next)).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				(*(*mutex).wchan.wait_list).add_head((*((*waiter_ptr).wait_next)).value.clone());
+			}
+		}
+
+		if (*mutex).flags & RROS_MUTEX_PI as i32 == 0 {
+			raw_spin_unlock(mutex_lock);
+			return Ok(0);
+		}
+
+
+		(*mutex).wprio = (*waiter_ptr).wprio;
+		let owner_lock = &mut (*owner.locked_data().get()).lock as *mut bindings::hard_spinlock_t;
+		raw_spin_lock(owner_lock);
+
+		if (*mutex).flags & RROS_MUTEX_CLAIMED as i32 !=0 {
+			(*(*mutex).next_booster).remove();
+		} else {
+			(*mutex).flags |= RROS_MUTEX_CLAIMED as i32;
+			raise_boost_flag(owner.clone());
+		}
+
+		let boosters = (*owner.locked_data().get()).boosters;
+		if (*boosters).is_empty(){
+			(*boosters).add_head((*((*mutex).next_booster)).value.clone());
+		}else{
+			let mut flag = 1; // flag指示是否到头
+			for i in (*boosters).len()..=1{
+				let wprio_in_list = (*(*boosters).get_by_index(i).unwrap().value.clone().locked_data().get()).wprio;
+				if(*mutex).wprio <= wprio_in_list{
+					flag = 0;
+					(*boosters).enqueue_by_index(i,(*((*mutex).next_booster)).value.clone());
+					break;
+				}
+			}
+			if flag == 1{
+				(*boosters).add_head((*((*mutex).next_booster)).value.clone());
+			}
+		}
+
+		
+		raw_spin_unlock(owner_lock);
+		raw_spin_unlock(mutex_lock);
+		return adjust_boost(owner.clone(), waiter.clone(), mutex, originator.clone());
+	}
+}
+
+pub fn rros_follow_mutex_depend(wchan: Arc<SpinLock<rros_wait_channel>>, originator: Arc<SpinLock<rros_thread>>) -> Result<i32>{
+	let wchan = Arc::into_raw(wchan) as *mut SpinLock<rros_wait_channel> as *mut rros_wait_channel;
+	let originator_ref = Arc::into_raw(originator.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+	let mutex = wchan_to_mutex(wchan);
+	let mut waiter = 0 as *mut rros_thread;
+	let mut ret:Result<i32> = Ok(0);
+
+	// assert_hard_lock(&originator->lock);
+
+	let mutex_lock = unsafe{&mut (*mutex).lock as *mut bindings::hard_spinlock_t};
+	raw_spin_lock(mutex_lock);
+	unsafe{
+		let owner_ref = Arc::into_raw((*mutex).owner.clone().unwrap()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+		if owner_ref == originator_ref {
+			raw_spin_unlock(mutex_lock);
+			return Err(kernel::Error::EDEADLK);
+		}
+
+		for j in 1..= (*(*mutex).wchan.wait_list).len() {
+			let waiter_node = (*(*mutex).wchan.wait_list).get_by_index(j).unwrap().value.clone();
+			waiter = Arc::into_raw(waiter_node) as *mut SpinLock<rros_thread> as *mut rros_thread;
+
+			let waiter_lock = &mut (*waiter).lock as *mut bindings::hard_spinlock_t;
+			raw_spin_lock(waiter_lock);
+			let mut depend = (*waiter).wchan.clone();
+			if depend.is_some(){
+				let func;
+				match (*depend.clone().unwrap().locked_data().get()).follow_depend{
+					Some(f) => func = f,
+					None => {
+						pr_warn!("rros_follow_mutex_depend:follow_depend function error");
+						return Err(kernel::Error::EINVAL);
+					}
+				}
+				ret = func(depend.as_mut().unwrap().clone(), originator.clone());
+			}
+			raw_spin_unlock(waiter_lock);
+			if ret != Ok(0){
+				break;
+			}
+		}
+		raw_spin_unlock(mutex_lock);
+
+		return ret;
+	}
+}
+
+pub fn rros_commit_mutex_ceiling(mutex:*mut RrosMutex) -> Result<i32>{
+	unsafe{
+		let curr = &mut *rros_current();
+		let thread = unsafe{Arc::from_raw(curr as *const SpinLock<rros_thread>)};
+		let lockp = (*mutex).fastlock;
+		let flags:u32 = 0;
+		let mut oldh:i32 = 0; 
+		let mut h:i32 = 0;
+
+		let flags = lock::raw_spin_lock_irqsave();
+
+		// if (!rros_is_mutex_owner(lockp, fundle_of(curr)) ||(mutex->flags & EVL_MUTEX_CEILING))
+		// 	goto out;
+
+		ref_and_track_owner(mutex, thread.clone());
+		ceil_owner_priority(mutex, thread.clone());
+		
+		loop{
+			h = atomic_read(lockp);
+			oldh = atomic_cmpxchg(lockp, h, mutex_fast_ceil(h as u32) as i32);
+			if oldh == h{
+				break;
+			}
+		}
+	// out:
+		lock::raw_spin_unlock_irqrestore(flags);
+		Ok(0)
+	}
+}
+
+
+pub fn test_mutex() -> Result<i32>{
+	pr_info!("mutex test in~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+	pr_info!("test_mutex rros_current address is {:p}",rros_current());
+	let mut kmutex = RrosKMutex::new();
+	let mut kmutex = &mut kmutex as *mut RrosKMutex;
+	let mut mutex = RrosMutex::new();
+	unsafe{(*kmutex).mutex = &mut mutex as *mut RrosMutex};
+	rros_init_kmutex(kmutex);
+	pr_info!("init ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+	rros_lock_kmutex(kmutex);
+	pr_info!("lock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+	rros_unlock_kmutex(kmutex);
+	pr_info!("unlock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+	Ok(0)
+}
+
+pub fn rros_init_kmutex(kmutex:*mut RrosKMutex){
+	unsafe{
+		atomic_set((*kmutex).fastlock, 0);
+		rros_init_mutex_pi((*kmutex).mutex, &mut RROS_MONO_CLOCK as *mut RrosClock, (*kmutex).fastlock);
+	}
+}
+
+pub fn rros_init_mutex_pi(mutex:*mut RrosMutex,clock: *mut RrosClock,fastlock: *mut bindings::atomic_t){
+	__rros_init_mutex(mutex, clock, fastlock, 0);
+}
+
+pub fn rros_init_mutex_pp(mutex:*mut RrosMutex,clock: *mut RrosClock,fastlock: *mut bindings::atomic_t,ceiling:u32){
+	__rros_init_mutex(mutex, clock, fastlock, ceiling);
+}
+
+pub fn rros_lock_kmutex(kmutex:*mut RrosKMutex) -> Result<i32>{
+	unsafe{return rros_lock_mutex((*kmutex).mutex)};
+}
+
+pub fn rros_lock_mutex(mutex:*mut RrosMutex) -> Result<i32>{
+	return rros_lock_mutex_timeout(mutex, timeout::RROS_INFINITE, timeout::rros_tmode::RROS_REL);
+}
+
+pub fn rros_unlock_kmutex(kmutex:*mut RrosKMutex) -> Result<usize>{
+	unsafe{return rros_unlock_mutex((*kmutex).mutex)};
+}
+
+
diff --git a/kernel/rros/net.rs b/kernel/rros/net.rs
deleted file mode 100644
index ab583c04c..000000000
--- a/kernel/rros/net.rs
+++ /dev/null
@@ -1,88 +0,0 @@
-use crate::bindings::{sockaddr,
-    iovec,
-    hard_spinlock_t,
-    __poll_t,
-    oob_poll_wait,
-    atomic_t,
-    sock,
-    hlist_node,
-    socket};
-use crate::{sched::ssize_t,sched::__rros_timespec,
-    crossing::RrosCrossing,
-    wait::RrosWaitQueue,
-    file::RrosFile,
-    list::list_head};
-use alloc::rc::Rc;
-use core::cell::{RefCell, Ref};
-use core::sync::atomic::AtomicI32;
-use kernel::{
-    sync::SpinLock,
-    double_linked_list::List,
-    str::CStr,
-    net::Device,
-    net::Namespace as Net,
-    endian::be16
-};
-
-type pRrosSocket = Rc<RefCell<RrosSocket>>;
-type pSockaddr = Rc<RefCell<sockaddr>>;
-type pUserOobMsghdr = Rc<RefCell<UserOobMsghdr>>;
-type pIovec = Rc<RefCell<iovec>>;
-type pOobPollWait = Rc<RefCell<oob_poll_wait>>;
-type pNetDevice = Rc<RefCell<Device>>;
-
-pub struct UserOobMsghdr{
-    pub name_ptr : u64,
-    pub iov_ptr : u64,
-    pub ctl_ptr : u64,
-    pub namelen : u32,
-    pub iovlen : u32,
-    pub ctllen : u32,
-    pub count : i32,
-    pub flags : u32,
-    pub timeout : __rros_timespec,
-    pub timestamp : __rros_timespec,
-}
-
-pub struct RrosNetProto {
-    pub attach: Option<fn(pRrosSocket, &RrosNetProto, be16) -> i64>,
-    pub detach: Option<fn(pRrosSocket)>,
-    pub bind: Option<fn(pRrosSocket, pSockaddr,i32) -> i64>,
-    pub ioctl : Option<fn(pRrosSocket,u32,u64) -> i32>,
-    pub oob_send : Option<fn(pRrosSocket,pUserOobMsghdr,pIovec,usize) -> ssize_t>, 
-    pub oob_receive : Option<fn(pRrosSocket,pUserOobMsghdr,pIovec,usize) -> ssize_t>,
-    pub oob_poll : Option<fn(pRrosSocket,pOobPollWait)->__poll_t>,
-    pub get_netif : Option<fn(pRrosSocket)->pNetDevice>
-}
-
-pub struct RrosPollHead {
-	pub watchpoints : Rc<RefCell<list_head>>, 
-	pub lock : hard_spinlock_t
-}
-
-pub struct _binding{
-    pub real_ifindex : i32,
-    pub vlan_ifindex : i32,
-    pub vlan_id : u16,
-    pub proto_hash : u32
-}
-pub struct RrosSocket {
-    pub proto : RrosNetProto,
-    pub efile : Rc<RefCell<RrosFile>>,
-    pub net : Rc<RefCell<Net>>,
-    pub hash : Rc<RefCell<hlist_node>>,
-    pub input : Rc<RefCell<list_head>>,
-    pub input_wait: RrosWaitQueue,
-    pub poll_head : RrosPollHead,
-    pub next_sub : Rc<RefCell<list_head>>,
-    pub sk : Rc<RefCell<socket>>,
-    pub rmem_count : AtomicI32,
-    pub rmem_max :i32,
-    pub wmem_count : AtomicI32,
-    pub wmem_max :i32,
-    pub wmem_wait : RrosWaitQueue,
-    pub wmem_drain : RrosCrossing,
-    pub protocol : be16,
-    pub binding : _binding,
-    pub oob_lock : hard_spinlock_t,
-}
diff --git a/kernel/rros/net/Makefile b/kernel/rros/net/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/net/constant.rs b/kernel/rros/net/constant.rs
new file mode 100644
index 000000000..a31b31249
--- /dev/null
+++ b/kernel/rros/net/constant.rs
@@ -0,0 +1,2 @@
+
+const EVL_DOMAIN_HASH_BITS: u8 = 8;
diff --git a/kernel/rros/net/device.rs b/kernel/rros/net/device.rs
new file mode 100644
index 000000000..470a334b0
--- /dev/null
+++ b/kernel/rros/net/device.rs
@@ -0,0 +1,397 @@
+use core::cell::RefCell;
+use core::clone::Clone;
+use core::ffi::c_void;
+use core::mem::size_of;
+use core::ptr::NonNull;
+
+use kernel::linked_list::{List, GetLinks, Links};
+use kernel::sync::Lock;
+use kernel::{Result, vmalloc, c_str, spinlock_init};
+use kernel::{bindings, init_static_sync,sync::SpinLock};
+use kernel::prelude::*;
+use alloc::sync::Arc;
+
+use crate::crossing::RrosCrossing;
+use crate::flags::RrosFlag;
+use crate::net::input::rros_net_do_rx;
+use crate::sched::rros_thread;
+use crate::thread::{RrosKthread, rros_run_kthread, KthreadRunner};
+// use crate::uapi::rros::socket::RrosNetdevActivation;
+use crate::wait::RrosWaitQueue;
+
+use super::skb::{RrosSkbQueue};
+use super::socket::RrosNetdevActivation;
+
+const IFF_OOB_PORT : usize = 1 << 1;
+const IFF_OOB_CAPABLE : usize = 1 << 0;
+const RROS_DEFAULT_NETDEV_POOLSZ :usize = 128;
+const RROS_DEFAULT_NETDEV_BUFSZ : usize = 2048;
+const RROS_MAX_NETDEV_POOLSZ : usize = 16384;
+const RROS_MAX_NETDEV_BUFSZ : usize = 8192;
+const KTHREAD_RX_PRIO : usize = 1;
+const KTHREAD_TX_PRIO : usize = 1;
+
+/// orphan type for implementing `Sync` and `Send` for `List`
+struct ListThreadSafeWrapper(pub List<Box<NetDevice>>);
+unsafe impl Sync for ListThreadSafeWrapper{}
+unsafe impl Send for ListThreadSafeWrapper{}
+
+init_static_sync! {
+    static active_port_list : SpinLock<ListThreadSafeWrapper> = ListThreadSafeWrapper(List::new());
+}
+
+pub fn start_handler_thread(func : Box<dyn FnOnce()>,name:&'static kernel::str::CStr)-> Result<*mut KthreadRunner>{
+    // vmalloc::c_kzalloc()
+    // let runner = Box::try_new(KthreadRunner::new_empty());
+    
+    let mut runner = vmalloc::c_kzalloc(core::mem::size_of::<KthreadRunner>() as u64);
+    // if runner.is_err(){
+    //     return Err(bindings::ENOMEM);
+    // }
+    let mut runner = runner.unwrap() as *mut KthreadRunner;
+    unsafe{
+        (*runner).init(func);
+        (*runner).run(name, 34);
+    }
+
+    // if ret.is_err(){
+    //     unsafe{bindings::kfree(kt as *mut c_void)};
+    //     return unsafe{(-bindings::ENOMEM) as *const _ as *mut RrosKthread};
+    // }
+    return Ok(runner);
+}
+
+pub struct RrosNetdevState{
+    pub free_skb_pool : bindings::list_head,
+    pub pool_free : usize,
+    pub pool_max : usize,
+    pub buf_size : usize,
+    pub pool_wait : RrosWaitQueue,
+    // poll_head : EVLPollHead, // TODO:
+    pub rx_handler : *mut KthreadRunner,
+    pub rx_flag : RrosFlag,
+    pub rx_queue : RrosSkbQueue,
+    // pub qdisc : EVLNetQdisc,
+    // pub tx_handler : Rc<RefCell<rros_thread>>, // TODO:
+    // tx_flag : EVLFlag // TODO:
+    pub refs : i32
+}
+
+pub struct OOBNetdevState{
+    pub rstate : RrosNetdevState,
+    pub crossing : RrosCrossing, 
+    next : Links<NetDevice>
+}
+
+impl GetLinks for NetDevice{
+    type EntryType = NetDevice;
+    fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType>{
+        unsafe{
+            &(*(data.0.as_ref().oob_context.dev_state.wrapper as * const OOBNetdevState)).next
+        }
+    }
+}
+
+
+/// Wraps the pointer of kernel's `struct net_device` for socket buffer.
+/// We use the pointer instead of struct itself.
+pub struct NetDevice(pub(crate) NonNull<bindings::net_device>);
+
+impl NetDevice{
+    pub fn from_ptr(ptr:*mut bindings::net_device) -> Option<Self>{
+        if ptr.is_null(){
+            return None;
+        }
+        Some(Self(NonNull::new(ptr).unwrap()))
+    }
+
+    pub fn is_vlan_dev(&self) -> bool{
+        extern "C"{
+            fn rust_helper_is_vlan_dev(dev:*const bindings::net_device) -> bool;
+        }
+        unsafe{
+            rust_helper_is_vlan_dev(self.0.as_ptr())
+        }
+    }
+
+    #[inline]
+    pub fn dev_state_mut(&mut self) -> NonNull<OOBNetdevState>{
+        let wrapper = unsafe{(self.0.as_mut().oob_context.dev_state.wrapper) as *mut OOBNetdevState};
+        if wrapper.is_null(){
+            let state = vmalloc::c_kzalloc(size_of::<OOBNetdevState>() as u64).unwrap();
+            unsafe{self.0.as_mut()}.oob_context.dev_state.wrapper = state;
+            return NonNull::new(state as *mut OOBNetdevState).unwrap();
+        }else{
+            NonNull::new(wrapper).unwrap()
+        }
+    }
+
+    #[inline]
+    pub fn is_oob_port(&self) -> bool{
+        unsafe{self.0.as_ref()}.oob_context.flags & IFF_OOB_PORT as i32 != 0
+    }
+    #[inline]
+    fn set_oob_port(&mut self){
+        unsafe{
+            self.0.as_mut().oob_context.flags |= IFF_OOB_PORT as i32;
+        }
+    }
+
+    #[inline]
+    fn unset_oob_port(&mut self){
+        unsafe{
+            self.0.as_mut().oob_context.flags &= !IFF_OOB_PORT as i32;
+        }
+    }
+    #[inline]
+    pub fn is_oob_capable(&self) -> bool{
+        unsafe{self.0.as_ref()}.oob_context.flags & IFF_OOB_CAPABLE as i32 != 0
+    }
+
+    #[inline]
+    fn enable_oob_diversion(&mut self){
+        extern  "C"{
+            fn rust_helper_set_bit(state : u32,p:*mut u64);
+        }
+        unsafe{
+            rust_helper_set_bit(bindings::netdev_state_t___LINK_STATE_OOB as u32,&mut self.0.as_mut().state as *mut u64);
+        }
+    }
+
+    #[inline]
+    fn disable_oob_diversion(&mut self){
+        extern  "C"{
+            fn rust_helper_clear_bit(state : u32,p:*mut u64);
+        }
+        unsafe{
+            rust_helper_clear_bit(bindings::netdev_state_t___LINK_STATE_OOB as u32,&mut self.0.as_mut().state as *mut u64);
+        }
+    }
+
+    pub fn get_dev(&mut self){
+        let mut state = self.dev_state_mut();
+        unsafe{state.as_mut()}.crossing.down() // atomic increase
+    }
+
+    pub fn put_dev(&mut self){
+        let mut state = self.dev_state_mut();
+        unsafe{state.as_mut()}.crossing.up() // atomic decrease
+    }
+
+    pub fn vlan_dev_real_dev(&self) -> Self{
+        extern "C"{
+			fn rust_helper_vlan_dev_real_dev(dev:*const bindings::net_device) -> *const bindings::net_device;
+		}
+        let dev = unsafe{rust_helper_vlan_dev_real_dev(self.0.as_ptr() as *const _)};
+        Self(NonNull::new(dev as *mut _).unwrap())
+    }
+
+    pub fn get_net(&self) -> *const bindings::net{
+        extern "C"{
+            fn rust_helper_dev_net(dev:*const bindings::net_device) -> *const bindings::net;
+        }
+        unsafe{
+            rust_helper_dev_net(self.0.as_ptr() as *const bindings::net_device)
+        }
+    }
+
+    pub fn ifindex(&self) -> i32{
+        unsafe{self.0.as_ref()}.ifindex
+    }
+    fn enable_oob_port(&mut self,mut act: RrosNetdevActivation) -> i32{
+        
+        if !self.is_vlan_dev(){
+            return -(bindings::EINVAL as i32);
+        }
+        if self.is_oob_port(){
+            return 0;
+        }
+
+        let mut real_dev = self.vlan_dev_real_dev();
+        let mut nds = real_dev.dev_state_mut(); // create state
+        let est = &mut unsafe{nds.as_mut()}.rstate;
+
+        let mut vnds = self.dev_state_mut();
+        unsafe{vnds.as_mut()}.crossing.init();
+
+        if (est.refs == 0){
+            if (act.poolsz == 0){
+                act.poolsz =  RROS_DEFAULT_NETDEV_POOLSZ as u64;
+            }
+
+            if (act.bufsz == 0){
+                act.bufsz = RROS_DEFAULT_NETDEV_BUFSZ as u64;
+            }
+
+            // let mtu = READ_ONCE(real_dev->mtu);// TODO: READ_ONCE
+            let mtu = unsafe{real_dev.0.as_ref()}.mtu as u64;
+            if (act.bufsz < mtu){
+                act.bufsz = mtu;
+            }
+
+            est.pool_free = 0;
+            est.pool_max = act.poolsz as usize;
+            est.buf_size = act.bufsz as usize;
+            // est.qdisc = //TODO:
+
+            let pinned = unsafe { Pin::new_unchecked(&mut est.rx_queue) };
+            spinlock_init!(pinned,"RrosSkbQueue");
+            unsafe{(*est.rx_queue.locked_data().get()).init()};
+            est.rx_flag.init();
+            
+            let arg1 = NetDevice(real_dev.0.clone());
+            let arg2 = KTHREAD_RX_PRIO;
+            let func = Box::try_new(move||{
+                let dev = arg1;
+                rros_net_do_rx(dev);
+            }).unwrap();
+            est.rx_handler = start_handler_thread(func, c_str!("rros oob net rx handler")).unwrap();
+
+            // 只有有带外能力的才需要
+            if real_dev.is_oob_capable(){
+                
+                // evl_init_flag(&est->tx_flag);
+                // kt = start_handler_thread(real_dev, evl_net_do_tx,
+                //             KTHREAD_TX_PRIO, "tx");
+                // if (IS_ERR(kt))
+                //     goto fail_start_tx;
+        
+                // est->tx_handler = kt;
+            }
+            unsafe{nds.as_mut()}.crossing.init();
+            real_dev.enable_oob_diversion();
+        }
+        est.refs += 1;
+        self.set_oob_port();
+
+        let mut flags = 0;
+        
+        active_port_list.irq_lock_noguard(&mut flags);
+        unsafe{
+            (*active_port_list.locked_data().get()).0.push_back(Box::try_new(NetDevice(self.0.clone())).unwrap());
+        }
+        active_port_list.irq_unlock_noguard(flags);
+        pr_crit!("enable oob port success");
+
+        return 0;
+    // TODO: 优雅地处理一下异常
+    //     evl_stop_kthread(est->rx_handler);
+    //     evl_destroy_flag(&est->tx_flag);
+    // fail_start_rx:
+    //     evl_net_dev_purge_pool(real_dev);
+    //     evl_destroy_flag(&est->rx_flag);
+    // fail_build_pool:
+    //     evl_net_free_qdisc(est->qdisc);
+    //     kfree(est);
+    //     nds->estate = NULL;
+    }
+    
+    fn disable_oob_port(&mut self){
+        if !self.is_vlan_dev(){
+            return;
+        }
+        if !self.is_oob_port(){
+            return;
+        }
+        let vnds = unsafe{self.dev_state_mut().as_mut()};
+        // let flags = active_port_list.irqsave_lock();
+        // unsafe{
+        //     (*active_port_list.locked_data().get()).remove(index)
+        // }
+        // unsafe{
+        //     ()
+        // }
+        // unsafe{(*active_port_list.locked_data().get()).remove(vnds)};
+        // active_port_list.irq_unlock_noguard(flags);
+        vnds.crossing.pass();
+
+        self.unset_oob_port();
+
+        let mut real_dev = self.vlan_dev_real_dev();
+        let est = &mut unsafe{real_dev.dev_state_mut().as_mut()}.rstate;
+        if est.refs <= 0{
+            return ;
+        }
+        est.refs -= 1;
+        if est.refs > 0{
+            return;
+        }
+
+    	// evl_signal_poll_events(&est->poll_head, POLLERR);
+    	// evl_flush_wait(&est->pool_wait, T_RMID); //TODO:
+    	// evl_schedule();
+
+        self.disable_oob_diversion();
+        // evl_stop_kthread(est->rx_handler);
+        // if (est->tx_handler)
+        //     evl_stop_kthread(est->tx_handler); // TODO:
+
+
+    	// evl_net_free_qdisc(est->qdisc);
+
+
+    }
+    fn switch_oob_port(&mut self,act: Option<RrosNetdevActivation>) -> i32{
+        if let Some(act) = act{
+            self.enable_oob_port(act)
+        }else{
+            self.disable_oob_port();
+            return 0;
+        }
+    }
+
+    pub fn net_get_dev_by_index(net:*mut bindings::net,ifindex:i32) -> Option<Self>{
+        assert!(ifindex != 0);
+        let mut flags = 0;
+        active_port_list.irq_lock_noguard(&mut flags);
+
+        let mut list = unsafe{&mut (*active_port_list.locked_data().get()).0};
+        let cursor = list.cursor_front();
+        while let Some(item) = cursor.current_mut(){
+            if (core::ptr::eq(item.get_net(),net) && item.ifindex() == ifindex){
+                unsafe{item.dev_state_mut().as_mut().crossing.down()};
+                let ret = NetDevice(item.0.clone());
+                active_port_list.irq_unlock_noguard(flags);
+                return Some(ret);
+            }
+        }
+
+        active_port_list.irq_unlock_noguard(flags);
+        return None
+    }
+}
+
+
+#[no_mangle]
+pub fn netif_oob_switch_port(dev : *mut bindings::net_device,enabled : bool) -> i32{
+    let mut dev = NetDevice(NonNull::new(dev).unwrap());
+    if !dev.is_vlan_dev(){
+        return -(bindings::ENXIO as i32);
+    }
+    if enabled{
+        let act = RrosNetdevActivation{
+            poolsz: 0,
+            bufsz: 0,
+        };
+        return dev.switch_oob_port(Some(act));
+    }else{
+        dev.switch_oob_port(None);
+        return 0;
+    }
+}
+
+
+/// netdevice notifier
+fn rros_netdev_event(ev_block : *mut bindings::notifier_block,event : u64,ptr:*mut c_void) ->i32{
+    extern "C"{
+        fn rust_helper_netdev_notifier_info_to_dev(ptr:*mut c_void) -> *mut bindings::net_device;
+    }
+    
+    let dev = unsafe{
+        rust_helper_netdev_notifier_info_to_dev(ptr)
+    };
+    if event == bindings::netdev_cmd_NETDEV_GOING_DOWN as u64{
+        unimplemented!();
+    }
+    bindings::NOTIFY_DONE as i32
+}
diff --git a/kernel/rros/net/ethernet/Makefile b/kernel/rros/net/ethernet/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/net/ethernet/input.rs b/kernel/rros/net/ethernet/input.rs
new file mode 100644
index 000000000..36c38cde4
--- /dev/null
+++ b/kernel/rros/net/ethernet/input.rs
@@ -0,0 +1,140 @@
+use core::ops::Deref;
+
+use crate::{DECLARE_BITMAP, uapi::rros, net::{packet::rros_net_packet_deliver, skb::RrosSkBuff, input::rros_net_receive}};
+use kernel::{bindings, endian::be16, c_types::c_void,prelude::*};
+
+fn pick(mut skb: RrosSkBuff) -> bool{
+    rros_net_receive(skb, net_ether_ingress);
+    true
+}
+
+fn untag(mut skb: RrosSkBuff,ehdr:&mut bindings::vlan_ethhdr,mac_hdr:*mut u8) -> bool{
+    extern "C"{
+        fn rust_helper__vlan_hwaccel_put_tag(skb: *mut bindings::sk_buff,vlan_proto:be16,vlan_tci:u16) -> i32;
+		fn rust_helper_ntohs(x:be16) -> u16;
+    }
+    skb.protocol = ehdr.h_vlan_encapsulated_proto;
+    unsafe{rust_helper__vlan_hwaccel_put_tag(skb.0.as_ptr(),be16::from(ehdr.h_vlan_proto),rust_helper_ntohs(be16::from(ehdr.h_vlan_TCI)))};
+    unsafe{
+        bindings::skb_pull(skb.0.as_ptr(), bindings::VLAN_HLEN as u32);
+    }
+    let mac_len = unsafe{skb.deref().data.offset_from(mac_hdr) as usize};
+    if (mac_len > (bindings::VLAN_HLEN as usize + bindings::ETH_TLEN as usize) as usize){
+        unsafe{
+            bindings::memmove(mac_hdr.offset(bindings::VLAN_HLEN as isize) as *mut c_void, mac_hdr as *mut c_void, (mac_len - bindings::VLAN_HLEN as usize - bindings::ETH_TLEN as usize) as bindings::u_long);
+        }
+    }
+    skb.mac_header += bindings::VLAN_HLEN as u16;
+    return pick(skb);
+}
+
+pub fn rros_net_ether_accept(mut skb: RrosSkBuff) -> bool{
+    extern "C"{
+        fn rust_helper_test_bit(nr:usize,addr:*const usize) -> bool; // TODO: 这里addr应该是volatile的
+        fn rust_helper_skb_mac_header(skb:*mut bindings::sk_buff)->*mut u8;
+        fn rust_helper_eth_type_vlan(ethertype:be16)->bool;
+		fn rust_helper_htons(x:u16) -> be16;
+		fn rust_helper_ntohs(x:be16) -> u16;
+        fn rust_helper__vlan_hwaccel_get_tag(skb: *mut bindings::sk_buff,vlan_tci:*mut u16) -> i32;
+    }
+    let mut vlan_tci : u16 = 0;
+    let (tag,test_bit) = unsafe{
+         let tag = rust_helper__vlan_hwaccel_get_tag(skb.0.as_ptr(), &vlan_tci as *const _ as *mut u16) != 0;
+         let test_bit = rust_helper_test_bit((vlan_tci & VLAN_VID_MASK as u16) as usize, vlan_map.as_ptr());
+         (tag,test_bit)
+    };
+    if tag && test_bit{
+        return pick(skb);
+    }
+    if (skb.vlan_present() == 0 && unsafe{rust_helper_eth_type_vlan(be16::from(skb.protocol))}){
+        let mac_hdr = unsafe{skb.head.offset(skb.mac_header as isize) as *mut u8};
+        let ehdr =  mac_hdr as *mut bindings::vlan_ethhdr;
+        if unsafe{(*ehdr).h_vlan_encapsulated_proto} == unsafe{u16::from(rust_helper_htons(bindings::ETH_P_IP as u16))}{
+            let vlan_tci = unsafe{rust_helper_ntohs(be16::from((*ehdr).h_vlan_TCI))};
+            if unsafe{rust_helper_test_bit((vlan_tci & VLAN_VID_MASK as u16) as usize,vlan_map.as_ptr())}{
+                return untag(skb,unsafe{&mut *ehdr},mac_hdr);
+            }
+
+        }
+    }
+    return false;
+}
+
+fn net_ether_ingress(mut skb: RrosSkBuff){
+    if !rros_net_packet_deliver(&mut skb){
+        return
+    }
+    
+    // match 
+    // TODO: skb->protocol
+    // switch (ntohs(skb->protocol)) {
+    //     case ETH_P_IP:
+    //         break;		/* Try UDP.. */
+    //     }
+    panic!();
+    skb.free();
+}
+
+const VLAN_N_VID : usize = 4096;
+const VLAN_VID_MASK : u32 = 0x0fff;
+DECLARE_BITMAP!(vlan_map,VLAN_N_VID);
+
+#[no_mangle]
+pub fn rros_net_store_vlans(buf: *const u8, len:usize)->i32{
+    extern "C"{
+        fn rust_helper_test_bit(nr: i32, addr: *const u64) -> bool;
+        fn rust_helper_bitmap_copy(dst:*mut u64,src:*const u64,nbit:u32);
+    }
+    let new_map = unsafe{
+        bindings::bitmap_zalloc(VLAN_N_VID as u32, bindings::GFP_KERNEL)
+    };
+    let mut ret = unsafe{
+        bindings::bitmap_parselist(buf as *const i8, new_map, VLAN_N_VID as i32)
+    };
+    
+    if ret == 0 && unsafe{rust_helper_test_bit(0,new_map)} || unsafe{rust_helper_test_bit(VLAN_VID_MASK as i32,new_map)}{
+        ret = -(bindings::EINVAL as i32);
+    }
+    if ret != 0{
+        unsafe{
+            bindings::bitmap_free(new_map as *const u64);
+        }
+        return ret;
+    }
+    unsafe{
+        rust_helper_bitmap_copy(vlan_map.as_mut_ptr() as *const _ as *mut u64,new_map,VLAN_N_VID as u32);
+        bindings::bitmap_free(new_map as *const u64);
+    }
+    return len as i32;
+}
+
+// ssize_t evl_net_show_vlans(char *buf, size_t len)
+// {
+// 	return scnprintf(buf, len, "%*pbl\n", VLAN_N_VID, vlan_map);
+// }
+
+pub fn rros_show_vlans(){
+    extern "C"{
+        fn rust_helper_test_bit(nr: i32, addr: *const usize) -> bool;
+    }
+    let mut counter = 0;
+    let mut buffer = [0; 10];
+    let mut overflow = false;
+    for i in 0..4096{
+        if unsafe{rust_helper_test_bit(i,vlan_map.as_ptr())}{
+            buffer[counter] = i;
+            counter += 1;
+            if counter == 10{
+                overflow = true;
+                break;;
+            }
+        }
+    }
+    if overflow{
+        pr_info!("oob net port: {:?} ...(more)",buffer);
+    }else{
+        pr_info!("oob net port: {:?}",buffer);
+    }
+}
+
+// fn evl_net_show_vlans() // 
\ No newline at end of file
diff --git a/kernel/rros/net/ethernet/mod.rs b/kernel/rros/net/ethernet/mod.rs
new file mode 100644
index 000000000..186884ebd
--- /dev/null
+++ b/kernel/rros/net/ethernet/mod.rs
@@ -0,0 +1,2 @@
+pub mod input;
+pub mod output;
\ No newline at end of file
diff --git a/kernel/rros/net/ethernet/output.rs b/kernel/rros/net/ethernet/output.rs
new file mode 100644
index 000000000..4bf0589d0
--- /dev/null
+++ b/kernel/rros/net/ethernet/output.rs
@@ -0,0 +1,33 @@
+use core::ops::Deref;
+
+use kernel::endian::be16;
+use kernel::bindings;
+
+use crate::net::device::NetDevice;
+use crate::net::output::rros_net_transmit;
+use crate::net::skb::RrosSkBuff;
+
+
+pub fn rros_net_ether_transmit(dev:&mut NetDevice,skb:&mut RrosSkBuff) -> i32 {
+    extern "C"{
+        fn rust_helper_vlan_dev_vlan_proto(dev: *mut bindings::net_device) -> be16;
+        fn rust_helper_vlan_dev_vlan_id(dev: *mut bindings::net_device) -> u16;
+        fn rust_helper_vlan_dev_get_egress_qos_mask(dev: *mut bindings::net_device, mask:u32)->u16;
+        fn rust_helper__vlan_hwaccel_put_tag(skb: *mut bindings::sk_buff, vlan_proto:be16, vlan_tci:u16);
+    }
+    if !dev.is_vlan_dev(){
+        return -(bindings::EINVAL as i32);
+    }
+    let vlan_proto = unsafe{rust_helper_vlan_dev_vlan_proto(dev.0.as_ptr())};
+    let mut vlan_tci = unsafe{rust_helper_vlan_dev_vlan_id(dev.0.as_ptr())};
+    vlan_tci |= unsafe{rust_helper_vlan_dev_get_egress_qos_mask(dev.0.as_ptr(), skb.priority)};
+    unsafe{
+        rust_helper__vlan_hwaccel_put_tag(skb.0.as_ptr(), vlan_proto, vlan_tci);
+    }
+    let ret = rros_net_transmit(skb);
+    if ret.is_ok(){
+        return 0;
+    }else{
+        return ret.err().unwrap().to_kernel_errno();
+    }
+}
\ No newline at end of file
diff --git a/kernel/rros/net/input.rs b/kernel/rros/net/input.rs
new file mode 100644
index 000000000..54875bce3
--- /dev/null
+++ b/kernel/rros/net/input.rs
@@ -0,0 +1,141 @@
+use core::ops::Deref;
+use core::ptr::NonNull;
+
+use kernel::sync::Lock;
+use kernel::{c_types::c_void, vmalloc, sync::SpinLock};
+
+use kernel::{bindings, spinlock_init};
+
+use crate::RROS_NET_CB;
+use crate::net::ethernet::input::rros_net_ether_accept;
+use crate::net::skb::RrosSkBuff;
+use crate::sched::rros_schedule;
+
+use super::device::NetDevice;
+
+// pub struct EVLNetHandler{
+//     ingress : fn(skb : *mut bindings::sk_buff),
+// }
+
+
+pub struct RrosNetRxqueue{
+    pub hkey : u32,
+    pub hash : bindings::hlist_node,
+    pub subscribers : bindings::list_head,
+    pub lock : SpinLock<()>,
+    pub next : bindings::list_head,
+}
+
+impl RrosNetRxqueue{
+    // TODO: 使用Rc<Refcell<>>来替代
+    pub fn new(hkey : u32) -> Option<NonNull<Self>>{
+        extern "C"{
+            fn rust_helper_INIT_LIST_HEAD(list: *mut bindings::list_head);
+        }
+        let ptr = vmalloc::c_kzalloc(core::mem::size_of::<RrosNetRxqueue>() as u64);
+        if ptr.is_none(){
+            return None
+        }
+        let ptr = unsafe{&mut *(ptr.unwrap() as *const _ as *mut RrosNetRxqueue)};
+        ptr.hkey = hkey;
+        unsafe{rust_helper_INIT_LIST_HEAD(&mut ptr.subscribers)};
+        let pinned = unsafe{core::pin::Pin::new_unchecked(&mut ptr.lock)};
+        spinlock_init!(pinned, "RrosNetRxqueue");
+        unsafe{NonNull::new(ptr)}
+    }
+
+    pub fn free(&mut self){
+    	// EVL_WARN_ON(NET, !list_empty(&rxq->subscribers));
+        vmalloc::c_kzfree(&self as *const _ as *const c_void);
+    }
+}
+
+pub fn rros_net_do_rx(mut dev:NetDevice){ 
+    extern "C"{
+        fn rust_helper_list_del(list: *mut bindings::list_head);
+    }
+    let mut est = unsafe{dev.dev_state_mut().as_mut()};
+    let mut list = bindings::list_head{
+        next : core::ptr::null_mut(),
+        prev : core::ptr::null_mut(),
+    };
+    init_list_head!(&mut list);
+    // while !evl_kthread_should_stop(){
+    loop{ // TODO:
+        let ret = est.rstate.rx_flag.wait();
+        if ret != 0{
+            break;
+        }
+        let mut flags = 0;
+        est.rstate.rx_queue.irq_lock_noguard(&mut flags);
+        if !unsafe{(*est.rstate.rx_queue.locked_data().get()).move_queue(&mut list)}{
+            est.rstate.rx_queue.irq_unlock_noguard(flags);
+            continue;
+        }
+        est.rstate.rx_queue.irq_unlock_noguard(flags);
+        list_for_each_entry_safe!(skb,next,&mut list,bindings::sk_buff,{
+            let mut rskb = RrosSkBuff::from_raw_ptr(skb);
+            list_del!(rskb.list_mut());
+            (rskb.net_cb_mut().handler)(rskb);
+        },__bindgen_anon_1.list);
+        unsafe{rros_schedule()};
+    }
+}
+
+// NETIF 
+
+#[no_mangle]
+pub fn netif_oob_run(dev:*mut bindings::net_device){
+    let mut dev = unsafe{NetDevice(NonNull::new_unchecked(dev))};
+    unsafe{&mut dev.dev_state_mut().as_mut()}.rstate.rx_flag.raise();
+}
+
+#[no_mangle]
+fn netif_oob_deliver(skb: *mut bindings::sk_buff)->bool{
+    extern "C" {
+        fn rust_helper_eth_type_vlan(eth_type :bindings::__be16)->bool; 
+    }
+    let skb = RrosSkBuff::from_raw_ptr(skb);
+    let protocol = skb.protocol;
+    match protocol as u32{
+        bindings::ETH_P_IP => {
+            rros_net_ether_accept(skb)
+            // evl_net_ether_accept(skb) // TODO:
+        },
+        _ => {
+            /*
+		    * For those adapters without hw-accelerated VLAN
+		    * capabilities, check the ethertype directly.
+		    */
+            if unsafe{rust_helper_eth_type_vlan(protocol)}{
+                rros_net_ether_accept(skb)
+            }else{
+                false
+            }
+        }
+    }
+}
+
+
+pub fn rros_net_receive(mut skb:RrosSkBuff,handler:fn (skb:RrosSkBuff)){
+    extern "C" {
+        fn rust_helper_skb_list_del_init(skb:*mut bindings::sk_buff)->bool; 
+        fn rust_helper_running_inband() -> bool;
+    }
+    if !unsafe{skb.__bindgen_anon_1.__bindgen_anon_1.next.is_null()}{
+        unsafe{rust_helper_skb_list_del_init(skb.0.as_ptr())};
+    }
+    unsafe{skb.rros_control_cb().as_mut().handler = handler};
+
+    let mut rst = unsafe{skb.dev().unwrap().dev_state_mut().as_mut()};
+    let mut flags = 0;
+    rst.rstate.rx_queue.irq_lock_noguard(&mut flags);
+    unsafe{(*rst.rstate.rx_queue.locked_data().get()).add(&mut skb)};
+    rst.rstate.rx_queue.irq_unlock_noguard(flags);
+
+    if unsafe{!rust_helper_running_inband()}{
+        rst.rstate.rx_flag.raise();
+    }
+}
+
+
diff --git a/kernel/rros/net/mod.rs b/kernel/rros/net/mod.rs
new file mode 100644
index 000000000..9c9808e45
--- /dev/null
+++ b/kernel/rros/net/mod.rs
@@ -0,0 +1,80 @@
+use core::result::Result::Ok;
+
+use kernel::{Result, bindings::sock_register};
+mod socket;
+mod packet;
+mod skb;
+mod input;
+mod output;
+mod ethernet;
+mod constant;
+// mod qdisc;
+mod device;
+
+pub use device::netif_oob_switch_port;
+
+use self::{skb::rros_net_init_pools, ethernet::input::{rros_net_store_vlans, rros_show_vlans}};
+use kernel::bindings;
+
+const EVL_DOMAIN_HASH_BITS: u8 = 8;
+
+#[macro_export]
+macro_rules! sk_prot{
+	($x:ident) => {
+		unsafe{(*$x).__sk_common.skc_prot}
+    };
+}
+
+#[macro_export]
+macro_rules! RROS_NET_CB{
+    ($skb:ident) => {
+        unsafe{
+            &(*$skb).cb[0] as *const _ as *mut skb::RrosNetCb
+        }
+    };
+}
+
+pub fn set_42_as_oob_port(){
+    let mut msg: [u8; 6] = [0; 6];
+    msg.copy_from_slice("42\0\0\0\0".as_bytes());
+    rros_net_store_vlans(&msg as *const _ as *const u8, 3);
+}
+
+pub fn init()->Result<()>{
+    rros_net_init_pools()?;
+    set_42_as_oob_port();
+    rros_show_vlans();
+	// evl_net_init_tx();
+
+	// evl_net_init_qdisc();
+    // init_rros_af_oob_proto();
+    // let ret = unsafe{bindings::proto_register(&mut rros_af_oob_proto, 0)};
+    // if ret !=0{
+    //     panic!()
+    // }
+    // unsafe{
+    //     sock_register(&rros_family_ops);
+    // }
+    Ok(())
+}
+// bitmap
+// #[macro_export]
+// macro_rules! DECLARE_BITMAP {
+//     ($name,$bits) => {
+
+//         static mut $name: [usize;rust_helper_BITS_TO_LONGS($bits)] = [0;rust_helper_BITS_TO_LONGS($bits)];
+//     };
+// }
+
+
+
+/**
+ * VLAN
+ */
+#[macro_export]
+macro_rules! skb_vlan_tag_present {
+    ($skb:ident) => {
+        (*$skb)._bitfield_3.0[0]
+    };
+}
+
diff --git a/kernel/rros/net/output.rs b/kernel/rros/net/output.rs
new file mode 100644
index 000000000..afb32a81e
--- /dev/null
+++ b/kernel/rros/net/output.rs
@@ -0,0 +1,182 @@
+use core::{ffi::c_void, mem::align_of, ops::Deref};
+
+use kernel::{Result,bindings::{self, _raw_spin_lock_irq}, irq_work::IrqWork, percpu::alloc_per_cpu, sync::{SpinLock, Lock}, init_static_sync, Error};
+use crate::{list_next_entry, list_entry_is_head, net::{skb::RrosSkbQueueInner, socket::uncharge_socke_wmem}};
+
+use super::{device::{OOBNetdevState}, skb::RrosSkBuff};
+use kernel::percpu;
+
+// NOTE:initialize in evl_net_init_tx
+// TODO: 这里的实现没用DEFINE_PER_CPU，因为Rust还没有支持静态定义的percpu变量
+init_static_sync! {
+    static oob_tx_relay : SpinLock<RrosSkbQueueInner> = RrosSkbQueueInner::default();
+}
+static mut oob_xmit_work : IrqWork = unsafe{core::mem::transmute::<[u8; core::mem::size_of::<IrqWork>()], IrqWork>( [0; core::mem::size_of::<IrqWork>()])};
+
+
+// fn oob_start_xmit(dev: *mut bindings::net_device, skb: *mut bindings::sk_buff) -> i32 {
+//     unsafe{
+//         (*(*dev).netdev_ops).ndo_start_xmit.unwrap()(skb, dev)
+//     }
+// }
+
+// fn do_tx(qdisc: *mut EVLNetQdisc,dev:*mut bindings::net_device,skb:*mut bindings::sk_buff) {
+//     evl_unchange_socket_wmem(skb);  //TODO:
+//     let result = oob_start_xmit(dev, skb);
+//     match result{
+//         bindings::netdev_tx_NETDEV_TX_OK => {},
+//         _ => {// busy, or whatever
+//             unsafe{
+//                 (*qdisc).pack_dropped += 1;
+//             }
+//             evl_net_free_skb(skb); //TODO:
+//         }
+//     }
+// }
+
+// fn evl_net_do_tx(arg: *mut c_void){
+//     extern "C"{
+//         fn rust_helper_list_del_init(list: *mut bindings::list_head);
+//     }
+//     let list = bindings::list_head::default();
+//     init_as_list_head!(list);
+//     let dev = unsafe{
+//         arg as *mut bindings::net_device
+//     };
+//     let est = unsafe{
+//         (*dev). // TODO: estate
+//     }
+//     while !evl_kthread_should_stop(){
+        
+//         let ret = evl_wait_flag(unsafe{&(*est).flag});
+//         if ret{
+//             break;
+//         }
+//         let qdisc : *mut EVLNetQdisc = unsafe{
+//             (*est).qdisc
+//         };
+//         loop{
+//             let skb = unsafe{
+//                 (*(*qdisc).oob_ops).dequeue(qdisc)
+//             };
+//             if skb.is_null(){
+//                 break;
+//             }
+//             do_tx(qdisc,dev,skb);
+//         }
+//         let inband_q = unsafe{
+//             &(*qdisc).inband_q
+//         };
+//         if inband_q.move_queue(&list){
+//             // TODO: use macro instead
+//             let mut skb = list_first_entry!(&list,bindings::sk_buff,__bindgen_anon_1.list);
+//             let mut n = list_next_entry!(pos,bindings::sk_buff,__bindgen_anon_1.list);
+//             while !list_entry_is_head!(pos,&list,__bindgen_anon_1.list){
+//                 unsafe{
+//                     rust_helper_list_del_init(&(*skb).__bindgen_anon_1.list);
+//                 }
+//                 do_tx(qdisc, dev, skb);
+//                 // process next skb
+//                 pos = n;
+//                 n = list_next_entry!(n,bindings::sk_buff,__bindgen_anon_1.list);
+//             }
+//         }
+//     }
+// }
+
+// inband
+#[no_mangle]
+fn skb_inband_xmit_backlog(){
+    extern "C"{
+        fn rust_helper_this_cpu_ptr(ptr: *mut c_void) -> *mut c_void;
+    }
+    let mut list = bindings::list_head::default();
+    init_list_head!(&mut list);
+    let mut flags = 0;
+    oob_tx_relay.irq_lock_noguard(&mut flags); // TODO: 是否需要lock
+
+    if unsafe{(*oob_tx_relay.locked_data().get()).move_queue(&mut list)}{
+        list_for_each_entry_safe!(skb,n,&mut list,bindings::sk_buff,{
+            let mut ref_skb = RrosSkBuff::from_raw_ptr(skb);
+            uncharge_socke_wmem(&mut ref_skb);
+            unsafe{bindings::dev_queue_xmit(skb)};
+        },__bindgen_anon_1.list);
+    }
+    oob_tx_relay.irq_unlock_noguard(flags);
+}
+
+
+
+// fn xmit_oob(dev : *mut bindings::net_device, skb : *mut bindings::sk_buff) -> i32{
+//     // TODO: est,skb_inband_xmit_backlog,evl_raise_flag
+//     let est = unsafe{
+//         let ptr  = (*dev).oob_context.dev_state.wrapper as *mut OOBNetdevState;
+//         (*ptr).estate
+//     };
+//     let ret = evl_net_sched_packet(dev,skb);
+//     if ret{
+//         ret
+//     }
+//     evl_raise_flag(&est.tx_flag);
+//     return 0;
+// }
+
+pub fn rros_net_transmit(mut skb : &mut RrosSkBuff) -> Result<()>{
+    extern "C"{
+    }
+    let dev = skb.dev();
+    if dev.is_none(){
+        return Err(Error::EINVAL);
+    }
+    let dev = dev.unwrap();
+    if dev.is_vlan_dev(){
+        return Err(Error::EINVAL);
+    }
+
+    if unsafe{skb.deref().__bindgen_anon_2.sk.is_null()}{ // sk_buff->sk
+        return Err(Error::EINVAL);
+    }
+
+    if !skb.is_oob(){
+        return Err(Error::EINVAL);
+    }
+    // if dev.is_oob_capable(){
+    //     return xmit_oob()
+    // }
+
+    if kernel::premmpt::running_inband().is_ok(){
+        uncharge_socke_wmem(&mut skb);
+        unsafe{bindings::dev_queue_xmit(skb.0.as_ptr())};
+    }
+    
+    let mut flags = 0;
+    oob_tx_relay.irq_lock_noguard(&mut flags);
+    unsafe{(*oob_tx_relay.locked_data().get()).add(skb)};
+    oob_tx_relay.irq_unlock_noguard(flags);
+    unsafe{
+        bindings::irq_work_queue(oob_xmit_work.get_ptr());
+    }
+    Ok(())
+}
+
+// fn netif_xmit_oob(skb : *mut bindings::sk_buff) -> i32{
+//     if xmit_oob(unsafe{(*skb).__bindgen_anon_1.__bindgen_anon_1.__bindgen_anon_1.dev}, skb){
+//         bindings::NET_XMIT_DROP
+//     }else{
+//         bindings::NET_XMIT_SUCCESS
+//     }
+// }
+
+unsafe extern "C" fn xmit_inband(work : *mut bindings::irq_work){
+    unsafe{
+        bindings::__raise_softirq_irqoff(bindings::NET_TX_SOFTIRQ);
+    }
+}
+
+fn rros_init_tx(){
+    unsafe{
+        // oob_tx_relay = alloc_per_cpu(size_of::<EVLNetSkbQueue<*mut bindings::sk_buff>>(),align_of::<EVLNetSkbQueue<*mut bindings::sk_buff>>());
+        oob_xmit_work.init_irq_work(xmit_inband).unwrap();
+    }
+}
+// }
\ No newline at end of file
diff --git a/kernel/rros/net/packet.rs b/kernel/rros/net/packet.rs
new file mode 100644
index 000000000..6f5354451
--- /dev/null
+++ b/kernel/rros/net/packet.rs
@@ -0,0 +1,578 @@
+use core::convert::TryInto;
+use core::default::Default;
+use core::mem::{size_of, transmute};
+use core::ops::{Deref, DerefMut};
+use core::u16;
+use core::ptr::NonNull;
+use super::input::{RrosNetRxqueue};
+use super::skb::RrosSkBuff;
+use super::socket::{RrosSocket,RrosNetProto, UserOobMsghdr};
+use kernel::endian::be16;
+use kernel::ktime::KtimeT;
+use kernel::sync::Lock;
+use kernel::{double_linked_list, sync::{SpinLock}, c_types, init_static_sync};
+use kernel::{bindings, Result, Error};
+use kernel::types::{HlistHead,HlistNode};
+use crate::clock::u_timespec_to_ktime;
+use crate::net::device::NetDevice;
+use crate::net::ethernet::output::rros_net_ether_transmit;
+use crate::net::socket::{rros_export_iov, rros_import_iov, uncharge_socke_wmem};
+use crate::sched::{rros_disable_preempt, rros_enable_preempt, __rros_timespec};
+use crate::timeout::{RROS_INFINITE, rros_tmode, RROS_NONBLOCK};
+use crate::types::Hashtable;
+
+
+// protocol hash table
+init_static_sync! {
+	static protocol_hashtable: kernel::sync::SpinLock<Hashtable::<8>> = Hashtable::<8>::new();
+}
+
+fn get_protol_hash(protocol : be16) -> u32{
+	extern  "C" {
+		fn rust_helper_jhash(key: *const c_types::c_void, length:u32, initval:u32) -> u32;
+	}
+	let hsrc : u32 = u16::from(protocol) as u32;
+	unsafe{rust_helper_jhash(&hsrc as *const _ as *const c_types::c_void, 1, 0)}
+}
+
+fn find_rxqueue(hkey:u32) -> Option<NonNull<RrosNetRxqueue>>{
+	let head = unsafe{(*protocol_hashtable.locked_data().get()).head(hkey)};
+	hash_for_each_possible!(rxq,head,RrosNetRxqueue,hash,{
+		if unsafe{(*rxq).hkey} ==  hkey{
+			return NonNull::new(rxq as *mut RrosNetRxqueue);
+		}
+	});
+	None
+}
+fn find_packet_proto(prtocol: be16) ->Option<&'static dyn RrosNetProto>{
+	extern "C"{
+		fn rust_helper_htons(x:u16) -> be16;
+	}
+	let prtocol : u32 = u16::from(prtocol) as u32;
+	match prtocol{
+		bindings::ETH_P_ALL | bindings::ETH_P_IP => Some(&ethernet_net_proto),
+		_ => None,
+	} 
+}
+
+
+pub struct EthernetRrosNetProto;
+pub static ethernet_net_proto : EthernetRrosNetProto = EthernetRrosNetProto;
+impl RrosNetProto for EthernetRrosNetProto{
+	fn attach(&self,sock:&mut RrosSocket,protocol: be16) -> i32{
+		extern "C"{
+            fn rust_helper_list_add(new: *mut kernel::bindings::list_head, head: *mut kernel::bindings::list_head);
+        }
+		let hkey = get_protol_hash(protocol);
+		let rxq = RrosNetRxqueue::new(hkey);
+		if rxq.is_none(){
+			return -(bindings::ENOMEM as i32);
+		}
+		let mut rxq = rxq.unwrap();
+		let mut flags = 0;
+		let mut redundant_rxq = false;
+		protocol_hashtable.irq_lock_noguard(&mut flags);
+		sock.proto = Some(&ethernet_net_proto);
+		sock.binding.proto_hash = hkey;
+		sock.protocol = protocol;
+
+		let _rxq = find_rxqueue(hkey);
+		if let Some(queue) = _rxq{
+			redundant_rxq = true;
+			let queue = unsafe{&mut *queue.as_ptr()};
+			//evl_spin_lock
+			rros_disable_preempt();
+			queue.lock.lock_noguard();
+			unsafe{rust_helper_list_add(&mut sock.next_sub, &mut queue.subscribers)}
+			// evl_spin_unlock
+			unsafe{queue.lock.unlock()};
+			rros_enable_preempt();
+			// drop q_guard here
+		} else {
+			let queue = unsafe{&mut *rxq.as_ptr()};
+			unsafe{(*protocol_hashtable.locked_data().get()).add(&mut queue.hash,hkey)};
+			unsafe{rust_helper_list_add(&mut sock.next_sub, &mut queue.subscribers)}
+		}
+		protocol_hashtable.irq_unlock_noguard(flags);
+
+		if (redundant_rxq){
+			unsafe{rxq.as_mut().free()};
+		}
+
+		0
+	}
+    fn detach(&self,sock:&mut RrosSocket){
+		extern "C"{
+			fn rust_helper_list_empty(head:*const kernel::bindings::list_head) -> bool;
+		}
+		if unsafe{rust_helper_list_empty(&sock.next_sub)}{
+			return;
+		}
+		let mut tmp = bindings::list_head::default();
+		init_list_head!(&mut tmp);
+
+		let mut flags = 0;
+		protocol_hashtable.irq_lock_noguard(&mut flags);
+
+		let rxq = unsafe{find_rxqueue(sock.binding.proto_hash).unwrap().as_mut()};
+
+		list_del_init!(&mut sock.next_sub);
+		if unsafe{rust_helper_list_empty(&rxq.subscribers)}{
+			unsafe{(*protocol_hashtable.locked_data().get()).del(&mut rxq.hash)};
+			list_add!(&mut rxq.next,&mut tmp);
+		}	
+
+		protocol_hashtable.irq_unlock_noguard(flags);
+
+		list_for_each_entry_safe!(rxq,next,&mut tmp,RrosNetRxqueue,{
+			unsafe{(*rxq).free()};
+		},next);
+
+	}
+    fn bind(&self,sock:&mut RrosSocket, addr:&bindings::sockaddr, len:i32) -> i32{
+		extern "C"{
+			fn rust_helper_vlan_dev_vlan_id(dev:*const bindings::net_device) -> u16;
+			fn rust_helper_vlan_dev_real_dev(dev:*const bindings::net_device) -> *const bindings::net_device;
+		}
+		if len != core::mem::size_of::<bindings::sockaddr_ll>() as i32{
+			return -(bindings::EINVAL as i32);
+		}
+		let sll = unsafe{&mut *(addr as *const _ as *mut bindings::sockaddr_ll)};
+		if sll.sll_family != bindings::AF_PACKET as u16{
+			return -(bindings::EINVAL as i32);
+		}
+		let proto = find_packet_proto(be16::from(sll.sll_protocol));
+		if proto.is_none(){
+			return -(bindings::EINVAL as i32);
+		}
+		let proto = proto.unwrap();
+		let new_ifindex = sll.sll_ifindex;
+
+		sock.lock.lock_noguard();
+		let old_ifindex = sock.binding.vlan_ifindex;
+		let mut vlan_id =0;
+		let mut real_ifindex = 0;
+		let mut dev :Option<NetDevice> = None;
+		if (new_ifindex != old_ifindex){
+			// switch to new device
+			if (new_ifindex!=0){
+				if let Some(dev)= NetDevice::net_get_dev_by_index(sock.net,new_ifindex){
+					vlan_id = unsafe{rust_helper_vlan_dev_vlan_id(dev.0.as_ptr() as *const _)};
+					real_ifindex = unsafe{(*rust_helper_vlan_dev_real_dev(dev.0.as_ptr() as *const _)).ifindex};
+				}else{
+					return -(bindings::EINVAL as i32); 
+				}
+			}else{
+				vlan_id = 0;
+				real_ifindex = 0;
+			}
+		}
+		sll.sll_ifindex = real_ifindex;
+
+		if (u16::from(sock.protocol) != sll.sll_protocol){
+			self.detach(sock);
+			let ret = self.attach(sock, be16::from(sll.sll_protocol));
+			if ret != 0{
+				unsafe{sock.oob_lock.unlock()};
+				if dev.is_some(){
+					let mut dev =dev.unwrap();
+					dev.put_dev();
+				}
+				return ret;
+			}
+
+		}
+		let mut flags = 0;
+		sock.oob_lock.irq_lock_noguard(&mut flags);
+		if (new_ifindex != old_ifindex){
+			sock.binding.real_ifindex = real_ifindex;
+			sock.binding.vlan_id = vlan_id;
+			sock.binding.vlan_ifindex = new_ifindex;
+		}
+		sock.oob_lock.irq_unlock_noguard(flags);
+		unsafe{sock.oob_lock.unlock()};
+		if dev.is_some(){
+			let mut dev =dev.unwrap();
+			dev.put_dev();
+		}
+		return 0;
+
+	}
+    fn oob_send(&self, sock:&mut RrosSocket,msghdr:*mut UserOobMsghdr, iov:&mut bindings::iovec, iovlen:usize) -> isize{
+		extern "C"{
+			fn rust_helper_raw_get_user(result:*mut u32,addr :*const u32) -> isize;
+			fn rust_helper_raw_copy_from_user(dst:*mut u8,src:*const u8,size:usize) -> usize;
+			fn rust_helper_skb_tailroom(skb:*const bindings::sk_buff) -> i32;
+			fn rust_helper_dev_validate_header(dev:*const bindings::net_device,header:*const u8,len:i32) -> i32;
+			fn rust_helper_dev_parse_header_protocol(skb:*const bindings::sk_buff) -> be16;
+			fn rust_helper_skb_set_network_header(skb:*mut bindings::sk_buff,offset:i32);
+			fn rust_helper_htons(data:u16) -> be16;
+		}
+		let mut msg_flags = 0;
+		let msghdr = unsafe{&mut *msghdr};
+		let ret = unsafe{
+			rust_helper_raw_get_user(&mut msg_flags,&msghdr.flags)
+		};
+		if ret !=0 {
+			return -(bindings::EFAULT as isize);
+		}
+		if msg_flags & !bindings::MSG_DONTWAIT != 0{
+			return -(bindings::EINVAL as isize);
+		}
+		if sock.efile.flags() & bindings::O_NONBLOCK != 0{
+			msg_flags != bindings::MSG_DONTWAIT;
+		}
+		let mut uts : __rros_timespec = __rros_timespec{
+			tv_sec:0,
+			tv_nsec:0,
+		};
+		let ret = unsafe{rust_helper_raw_copy_from_user(&mut uts as *const _ as *mut u8, &msghdr.timeout as *const _ as *const u8,core::mem::size_of::<__rros_timespec>())};
+		if ret != 0{
+			return -(bindings::EFAULT as isize);
+		}
+		let timeout = if msg_flags & bindings::MSG_DONTWAIT != 0{
+			 RROS_NONBLOCK
+		}else{
+			u_timespec_to_ktime(uts)
+		};
+		let tmode = if timeout !=0{
+			rros_tmode::RROS_ABS
+		}else{
+			rros_tmode::RROS_REL
+		};
+
+		let dev = find_xmit_device(sock,msghdr);
+		if dev.is_err(){
+			return  -(dev.err().unwrap().to_kernel_errno() as isize);
+		}
+		let mut dev = dev.unwrap();
+		let real_dev = dev.vlan_dev_real_dev();
+		let skb =  RrosSkBuff::dev_alloc_skb(&mut dev, timeout, tmode);
+		if skb.is_none(){
+			// put_dev
+			panic!();
+			// return -(bindings::ENOMEM as isize);
+		}
+		let mut skb = skb.unwrap();
+		skb.reset_mac_header();
+		skb.protocol = u16::from(sock.protocol);
+		skb.set_dev(dev.0.as_ptr());
+		skb.priority = unsafe{(*sock.sk).sk_priority};
+		let skb_tailroom = unsafe{rust_helper_skb_tailroom(skb.0.as_ptr())} as usize;
+		let mut rem : usize = 0;
+		let count = rros_import_iov(iov, iovlen, skb.deref_mut().data, skb_tailroom as u64, &mut rem);
+		if rem !=0 || count as u32 > (unsafe{dev.0.as_ref().mtu + dev.0.as_ref().hard_header_len as u32} + bindings::VLAN_HLEN) as u32{
+			skb.free();
+			dev.put_dev();
+			return -(bindings::EMSGSIZE as isize);			
+			
+		}else if unsafe{rust_helper_dev_validate_header(dev.0.as_ptr(),skb.deref().data,count)} != 0{
+			skb.free();
+			dev.put_dev();
+			return -(bindings::EINVAL as isize);
+		}
+
+		skb.put(count as u32);
+
+		let skb_protocol = unsafe{skb.0.as_ref().protocol};
+		if skb_protocol == 0 || skb_protocol == u16::from(unsafe{rust_helper_htons(bindings::ETH_P_ALL as u16)}){
+			unsafe{
+				skb.0.as_mut().protocol = u16::from(rust_helper_dev_parse_header_protocol(skb.0.as_ptr()));
+			}			
+		}
+		unsafe{rust_helper_skb_set_network_header(skb.0.as_ptr(),real_dev.0.as_ref().hard_header_len as i32)};
+
+		sock.charge_socket_wmem_timeout(&mut skb, timeout, tmode);//TODO: check ret
+
+		let ret = rros_net_ether_transmit(&mut dev, &mut skb);
+		if ret != 0{
+			uncharge_socke_wmem(&mut skb);
+			skb.free();
+			dev.put_dev();
+			return ret as isize;
+		}
+		let ret = count as isize;
+		dev.put_dev();
+		ret
+	}
+    fn oob_receive(&self, sock:&mut RrosSocket,msg:*mut UserOobMsghdr, iov:&bindings::iovec, iovlen:usize) -> isize{
+		// 用户态
+		extern "C"{
+			fn rust_helper_raw_spin_lock_irqsave(lock:*mut bindings::raw_spinlock_t)->u64;
+			fn rust_helper_raw_spin_unlock_irqrestore(lock:*mut bindings::raw_spinlock_t,flags:u64);
+			fn rust_helper_raw_get_user(result:*mut u32,addr :*const u32) -> isize;
+			fn rust_helper_raw_copy_from_user(dst:*mut u8,src:*const u8,size:usize) -> usize;
+			fn rust_helper_list_empty(list:*const bindings::list_head) -> bool;
+			fn rust_helper_skb_mac_header(skb:*const bindings::sk_buff) -> *const u8;
+		}
+		let mut timeout:KtimeT = RROS_INFINITE;
+		let mut tmode = rros_tmode::RROS_REL;
+		let mut msg_flags = 0;
+		let mut uts = __rros_timespec::new();
+		let mut ret = 0;
+		if !msg.is_null(){
+			let ret = unsafe{
+				rust_helper_raw_get_user(&mut msg_flags,&(*msg).flags)
+			};
+			if ret !=0 {
+				return -(bindings::EFAULT as isize);
+			}
+			if msg_flags & !bindings::MSG_DONTWAIT != 0{
+				return -(bindings::EINVAL as isize);
+			}
+			
+			if unsafe{
+				rust_helper_raw_copy_from_user(&mut uts as *const _ as *mut u8,&(*msg).timeout as *const _ as *const u8,core::mem::size_of::<__rros_timespec>())
+			}!=0{
+				return -(bindings::EFAULT as isize);
+			}
+			timeout = u_timespec_to_ktime(uts);
+			if timeout !=0 {
+				tmode = rros_tmode::RROS_ABS;
+			}else{
+				tmode = rros_tmode::RROS_REL;
+			}
+		}else{
+			// do nothing
+		}
+		if (sock.efile.flags() as u32) & bindings::O_NONBLOCK != 0{
+			msg_flags |= bindings::MSG_DONTWAIT;
+		}
+
+		loop{
+			let flags = unsafe{
+				bindings::_raw_spin_lock_irqsave(&mut sock.input_wait as *const _ as *mut bindings::raw_spinlock_t)
+			};
+			
+			if !unsafe{rust_helper_list_empty(&mut sock.input)}{
+				let skb = list_get_entry!(&mut sock.input,bindings::sk_buff,__bindgen_anon_1.list);
+				let mut skb = RrosSkBuff::from_raw_ptr(skb);
+				unsafe{
+					bindings::_raw_spin_unlock_irqrestore(&mut sock.input_wait as *const _ as *mut bindings::raw_spinlock_t,flags);
+				};
+				unsafe{
+					let len = skb.data.offset_from(rust_helper_skb_mac_header(skb.0.as_ptr())); //skb->data - skb_mac_header(skb))
+					bindings::skb_push(skb.0.as_ptr(), len as u32);
+				}
+				let ret = copy_packet_to_user(msg, iov, iovlen, &mut skb);
+				sock.uncharge_socke_rmem(&skb);
+				skb.free();
+			}
+
+			if msg_flags & bindings::MSG_DONTWAIT != 0{
+				unsafe{
+					bindings::_raw_spin_unlock_irqrestore(&mut sock.input_wait as *const _ as *mut bindings::raw_spinlock_t,flags);
+				};
+				return -(bindings::EWOULDBLOCK as isize);
+			}
+
+			sock.input_wait.locked_add(timeout, tmode);
+			unsafe{
+				bindings::_raw_spin_unlock_irqrestore(&mut sock.input_wait as *const _ as *mut bindings::raw_spinlock_t,flags);
+			};
+			ret = sock.input_wait.wait_schedule();
+			if ret !=0{
+				break;
+			}
+		}
+		return ret as isize;
+	}
+
+    fn get_netif(&self,sock:&mut RrosSocket) -> Option<NetDevice>{
+		NetDevice::net_get_dev_by_index(sock.net, sock.binding.vlan_ifindex)
+	}
+}
+
+
+fn copy_packet_to_user(msg:*mut UserOobMsghdr,iov:&bindings::iovec,iovlen:usize,skb:&mut RrosSkBuff) -> isize{
+	extern "C"{
+		fn rust_helper_raw_get_user(result:*mut u32,addr :*const u32) -> isize;
+		fn rust_helper_raw_get_user_64(result:*mut u64,addr :*const u64) -> isize;
+
+		fn rust_helper_dev_parse_header(skb:*mut bindings::sk_buff,haddr:*mut u8) -> i32;
+		fn rust_helper_raw_copy_to_user(dst:*mut u8,src:*const u8,size:usize) -> usize;
+		fn rust_helper_raw_put_user(x:u32,ptr:*mut u32) -> i32;
+	}
+	let mut name_ptr:u64 = 0;
+	let mut namelen:u32 = 0;
+	let mut u_addr : *mut bindings::sockaddr_ll = core::ptr::null_mut();
+	let mut msg_flags = 0;
+	if unsafe {rust_helper_raw_get_user_64(&mut name_ptr,&(*msg).name_ptr) } !=0{
+		return -(bindings::EFAULT as isize);
+	}
+
+	if unsafe{rust_helper_raw_get_user(&mut namelen,&(*msg).namelen)} != 0{
+		return -(bindings::EFAULT as isize);
+	}
+	let mut addr = bindings::sockaddr_ll::default();
+	if name_ptr ==0{
+		if namelen !=0{
+			return -(bindings::EINVAL as isize);
+		}
+	}else{
+		if namelen < core::mem::size_of::<bindings::sockaddr_ll>() as u32{
+			return -(bindings::EINVAL as isize);
+		}
+		addr.sll_family = bindings::AF_PACKET as u16;
+		addr.sll_protocol = skb.protocol;
+		let dev = skb.dev().unwrap();
+		addr.sll_ifindex = dev.ifindex();
+		addr.sll_hatype = unsafe{dev.0.as_ref().type_};
+		addr.sll_pkttype = unsafe{skb.0.as_ref().pkt_type()};
+		addr.sll_halen = unsafe{
+			rust_helper_dev_parse_header(skb.0.as_ptr(),&mut addr.sll_addr as *const _ as *mut u8)
+		}.try_into().unwrap();
+		unsafe{
+			u_addr = transmute(name_ptr)
+		}
+		if unsafe{rust_helper_raw_copy_to_user(u_addr as *const _ as *mut u8,&addr as *const _ as *const u8,core::mem::size_of::<bindings::sockaddr_ll>())}!=0{
+			return - (bindings::EFAULT as isize);
+		}
+	}
+	let count = rros_export_iov(iov, iovlen, skb.data as *const _ as *mut u8, skb.len as usize);
+
+	if (count as u32) < skb.len{
+		msg_flags |= bindings::MSG_TRUNC;
+	}
+	if unsafe{rust_helper_raw_put_user(msg_flags,&mut (*msg).flags as *mut u32)} != 0{
+		return -(bindings::EFAULT as isize);
+	}
+
+	return count as isize;
+}
+
+fn __packet_deliver(rxq:&mut RrosNetRxqueue,skb:&mut RrosSkBuff,prtocol: be16) -> bool{
+	extern "C"{
+		fn rust_helper_skb_vlan_tag_get_id(skb:*mut bindings::sk_buff)->u16;
+		fn rust_helper_htons(x:u16) -> be16;
+	}
+	rros_disable_preempt();
+	rxq.lock.lock_noguard();
+	let dev =skb.dev().unwrap();
+	let mut delivered = false;
+
+	// list_for_each_entry，这里有continue和break，因此直接写了
+	let mut rsk = list_first_entry!(&mut rxq.subscribers,RrosSocket,next_sub);
+	while !list_entry_is_head!(rsk,&mut rxq.subscribers,next_sub){
+		let ref_rsk = unsafe{&mut *rsk};
+		let ifindex = ref_rsk.binding.real_ifindex; // TODO: read_once
+		if ifindex!=0{
+			if ifindex != dev.ifindex(){
+				rsk = list_next_entry!(rsk,RrosSocket,next_sub);
+				continue;
+			}
+			let vlan_id = ref_rsk.binding.vlan_id;
+			if unsafe{rust_helper_skb_vlan_tag_get_id(skb.0.as_ptr())} != vlan_id{
+				continue;
+			}
+		}
+		if !ref_rsk.charge_socket_rmem(skb){
+			continue;
+		}
+
+		let mut qskb = if u16::from(prtocol) == unsafe{u16::from(rust_helper_htons(bindings::ETH_P_ALL as u16))}{
+			if let Some(clone) = skb.net_clone_skb(){
+				clone
+			}else{
+				unimplemented!();
+			}
+		}else{
+			RrosSkBuff::from_raw_ptr(skb.0.as_ptr()) // DANGEROUS
+		};
+		unsafe{
+			bindings::_raw_spin_lock(&mut ref_rsk.input_wait.lock as *const _ as *mut bindings::raw_spinlock_t);
+		}
+		list_add_tail!(qskb.list_mut(),&mut ref_rsk.input);
+		if ref_rsk.input_wait.is_active(){
+			ref_rsk.input_wait.wake_up_head();
+		}
+		unsafe{
+			bindings::_raw_spin_unlock(&mut ref_rsk.input_wait.lock  as *const _ as *mut bindings::raw_spinlock_t);
+		}
+
+		// evl_signal_poll_events(&esk->poll_head,	POLLIN|POLLRDNORM);
+		delivered = true;
+
+		if u16::from(prtocol) != u16::from(unsafe{rust_helper_htons(bindings::ETH_P_ALL as u16)}) {
+			break;
+		}
+
+		rsk = list_next_entry!(rsk,RrosSocket,next_sub);
+	}
+	unsafe{rxq.lock.unlock()};
+	
+	delivered
+}
+
+fn packet_deliver(skb:&mut RrosSkBuff,protocol : be16) -> bool{
+	let hkey = get_protol_hash(protocol);
+
+	let mut flags = 0;
+	let mut ret = false;
+	protocol_hashtable.irq_lock_noguard(&mut flags);
+
+	if let Some(mut rxq) = find_rxqueue(hkey){
+		ret = __packet_deliver(unsafe{rxq.as_mut()},skb,protocol);
+	}
+
+	protocol_hashtable.irq_unlock_noguard(flags);
+	ret
+}
+
+pub fn rros_net_packet_deliver(skb:&mut RrosSkBuff) -> bool{
+	// evl_net_packet_deliver
+	extern "C"{
+		fn rust_helper_htons(x:u16) -> be16;
+	}
+	packet_deliver(skb, unsafe{rust_helper_htons(bindings::ETH_P_ALL as u16)});
+
+	return packet_deliver(skb, be16::from(skb.protocol));
+}
+// deliver
+
+fn find_xmit_device(rsk:&mut RrosSocket,msghdr:&mut UserOobMsghdr)->Result<NetDevice>{
+	extern "C"{
+		fn rust_helper_raw_get_user(result:*mut u32,addr :*const u32) -> isize;
+		fn rust_helper_raw_get_user_64(result:*mut u64,addr :*const u64) -> isize;
+		fn rust_helper_raw_copy_from_user(dst:*mut u8,src:*const u8,size:usize) -> usize;
+	}
+	let mut name_ptr : u64 = 0;
+	let mut namelen : u32 = 0;
+	let ret = unsafe{rust_helper_raw_get_user_64(&mut name_ptr, &mut (*msghdr).name_ptr as *mut u64)};
+	if ret != 0{
+		return Err(Error::EFAULT);
+	}
+
+	let ret = unsafe{rust_helper_raw_get_user(&mut namelen, &mut (*msghdr).namelen as *mut u32)};
+	if ret != 0{
+		return Err(Error::EFAULT);
+	}
+
+	let dev = if  name_ptr == 0 {
+		if namelen !=0 {
+			return Err(Error::EINVAL);
+		}
+		let proto = rsk.proto.unwrap();
+		proto.get_netif(rsk)
+	}else{
+		if namelen < core::mem::size_of::<bindings::sockaddr_ll>() as u32{
+			return Err(Error::EINVAL);
+		}
+		
+		let u_addr : *mut bindings::sockaddr_ll =  unsafe{transmute(name_ptr)};
+		let mut addr : bindings::sockaddr_ll = bindings::sockaddr_ll::default();
+		let ret = unsafe{rust_helper_raw_copy_from_user(&mut addr as *const _ as *mut u8,u_addr as *const _ as *const u8,core::mem::size_of::<bindings::sockaddr_ll>())};
+		if ret != 0{
+			return Err(Error::EFAULT);
+		}
+		
+		if addr.sll_family != bindings::AF_PACKET as u16 && addr.sll_family != bindings::AF_UNSPEC as u16{
+			return Err(Error::EINVAL);
+		}
+		NetDevice::net_get_dev_by_index(rsk.net, addr.sll_ifindex)
+	};
+	if dev.is_none(){
+		return Err(Error::EFAULT); // TODO： ENXIO
+	}
+	Ok(dev.unwrap())
+}
\ No newline at end of file
diff --git a/kernel/rros/net/qdisc.rs b/kernel/rros/net/qdisc.rs
new file mode 100644
index 000000000..273450b57
--- /dev/null
+++ b/kernel/rros/net/qdisc.rs
@@ -0,0 +1,17 @@
+use kernel::bindings;
+
+use super::skb::EVLNetSkbQueue;
+pub struct EVLNetQdisc{
+    pub oob_ops : *const EVLNetQdiscOps,
+    pub inband_q : EVLNetSkbQueue<*mut bindings::sk_buff>,
+    pub pack_dropped : usize
+}
+pub struct EVLNetQdiscOps{
+    // name: &'static u8,
+    pub priv_size : usize,
+    pub init : fn(qdisc: *mut EVLNetQdisc) -> i32,
+    pub destory : fn(qdisc: *mut EVLNetQdisc),
+    pub enqueue : fn(qdisc: *mut EVLNetQdisc, skb: *mut bindings::sk_buff) -> i32,
+    pub dequeue : fn(qdisc: *mut EVLNetQdisc) -> *mut bindings::sk_buff,
+    pub next : bindings::list_head,
+}
\ No newline at end of file
diff --git a/kernel/rros/net/skb.rs b/kernel/rros/net/skb.rs
new file mode 100644
index 000000000..cc6989861
--- /dev/null
+++ b/kernel/rros/net/skb.rs
@@ -0,0 +1,614 @@
+use core::cell::UnsafeCell;
+use core::default::Default;
+use core::mem::transmute;
+use core::ops::{Deref, DerefMut};
+use core::option::Option::None;
+use core::pin::Pin;
+use core::ptr::NonNull;
+use core::result::Result::Ok;
+use core::sync::atomic::{Ordering, AtomicBool};
+use kernel::sync::Lock;
+use kernel::{bindings, init_static_sync, spinlock_init, Result};
+use kernel::{sync::SpinLock};
+use kernel::ktime::KtimeT;
+use crate::clock::RROS_MONO_CLOCK;
+use crate::sched::rros_schedule;
+use crate::timeout::{rros_tmode, RROS_NONBLOCK};
+use crate::wait::RROS_WAIT_PRIO;
+use crate::work::RrosWork;
+use super::device::NetDevice;
+use super::socket::RrosSocket;
+// use super::{socket::RrosSocket, input::EVLNetHandler};
+struct CloneControl{
+    pub(crate) queue : bindings::list_head,
+    pub(crate) count : i32,
+}
+unsafe impl Sync for CloneControl{}
+unsafe impl Send for CloneControl{}
+
+struct RecyclingWork{
+    pub(crate) work : RrosWork,
+    pub(crate) count : i32,
+    pub(crate) queue : bindings::list_head,
+}
+
+unsafe impl Sync for RecyclingWork{}
+unsafe impl Send for RecyclingWork{}
+
+init_static_sync! {
+    static clone_queue: SpinLock<CloneControl> = CloneControl{
+        queue: bindings::list_head{
+            next: core::ptr::null_mut() as *mut bindings::list_head,
+            prev: core::ptr::null_mut() as *mut bindings::list_head,
+        },
+        count: 0,
+    };
+    static recycler_work : SpinLock<RecyclingWork> = RecyclingWork{
+        work : RrosWork::new(),
+        count : 0,
+        queue : bindings::list_head{
+            next: core::ptr::null_mut() as *mut bindings::list_head,
+            prev: core::ptr::null_mut() as *mut bindings::list_head,
+        },
+    };
+}
+
+const SKB_RECYCLING_THRESHOLD : usize = 64;
+const NET_CLONES : usize = 1024;
+pub struct RrosNetCb{
+    pub handler : fn (skb:RrosSkBuff),
+    pub origin : *mut bindings::sk_buff,
+    pub dma_addr : bindings::dma_addr_t,
+    pub tracker : *mut RrosSocket,
+}
+
+fn maybe_kick_recycler(){
+    unsafe{
+        if (*recycler_work.locked_data().get()).count > SKB_RECYCLING_THRESHOLD as i32{
+            (*recycler_work.locked_data().get()).work.call_inband();
+        }
+    }
+}
+
+
+/// Wraps the pointer of kernel's `struct sk_buff`.
+pub struct RrosSkBuff(pub(crate) NonNull<bindings::sk_buff>);
+
+impl Deref for RrosSkBuff{
+    type Target = bindings::sk_buff;
+    fn deref(&self) -> &Self::Target {
+        unsafe{self.0.as_ref()}
+    }
+}
+
+impl DerefMut for RrosSkBuff{
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        unsafe{self.0.as_mut()}
+    }
+}
+
+impl RrosSkBuff{
+    const OOB_BIT_OFFSET : usize = 12;
+    const OOB_CLONE_BIT_OFFSET : usize = 13;
+    const OOB_CLONED_BIT_OFFSET : usize = 14;
+    // evl_net_dev_alloc_skb
+    // fn new(dev:*mut bindings::net_device,timeout:)
+    pub fn from_raw_ptr(ptr:*mut bindings::sk_buff) -> Self{
+        unsafe{Self(NonNull::new_unchecked(ptr))}
+    }
+    #[inline]
+    pub fn net_cb_mut(&mut self) -> &mut RrosNetCb{
+        unsafe{
+            transmute(&mut self.0.as_mut().cb[0])
+        }
+    }
+    pub fn net_clone_skb(&mut self) -> Option<Self>{
+        extern "C"{
+            fn rust_helper_list_empty(list:*const bindings::list_head)->bool;
+            fn rust_helper_skb_morph_oob_skb(n:*mut bindings::sk_buff,skb:*mut bindings::sk_buff);
+        }
+        let mut flags =0;
+        clone_queue.irq_lock_noguard(&mut flags);
+        let clone_data_control = unsafe{&mut *clone_queue.locked_data().get()};
+        let clone : *mut bindings::sk_buff = if unsafe{rust_helper_list_empty(&clone_data_control.queue)}{
+            clone_data_control.count -=1;
+            list_get_entry!(&mut clone_data_control.queue,bindings::sk_buff,__bindgen_anon_1.list)
+        }else{
+            panic!("No more skb to clone");
+        };
+        clone_queue.irq_unlock_noguard(flags);
+
+        unsafe{
+            rust_helper_skb_morph_oob_skb(clone,self.0.as_ptr());
+        }
+        let mut clone = RrosSkBuff::from_raw_ptr(clone);
+        let origin = self.net_cb_mut().origin;
+        if origin == core::ptr::null_mut(){
+            clone.net_cb_mut().origin = self.0.as_ptr();
+        }else{
+            clone.net_cb_mut().origin = origin;
+        }
+        return Some(clone);
+    }
+    pub fn alloc_one_skb(dev : &mut NetDevice)-> Option<Self>{
+        extern "C"{
+            fn rust_helper_netdev_is_oob_capable(dev:*mut bindings::net_device)->bool;
+            fn rust_helper_netdev_alloc_oob_skb(dev:*mut bindings::net_device,dma_addr:*mut bindings::dma_addr_t)->*mut bindings::sk_buff;
+        }
+
+        if !unsafe {
+            rust_helper_netdev_is_oob_capable(dev.0.as_mut())
+        }{
+            let est = unsafe{dev.dev_state_mut().as_mut()};
+            let skb = unsafe{
+                bindings::__netdev_alloc_skb(dev.0.as_mut(), est.rstate.buf_size as u32, bindings::GFP_KERNEL)
+            };
+            return Some(Self::from_raw_ptr(skb))
+        }
+        let mut dma_addr = unsafe {
+            bindings::dma_addr_t::default()
+        };
+        let mut skb = unsafe{
+            rust_helper_netdev_alloc_oob_skb(dev.0.as_ptr() ,&mut dma_addr)
+        };
+        if !skb.is_null(){
+            let mut skb = Self::from_raw_ptr(skb);
+            unsafe{skb.rros_control_cb().as_mut().dma_addr = dma_addr;}
+            return Some(skb);
+        }else {
+            return None;
+        }
+
+    }
+    pub fn dev_alloc_skb(dev:&mut NetDevice,timeout:KtimeT,tmode:rros_tmode)->Option<Self>{
+        // evl_net_dev_alloc_skb
+        extern "C"{
+            fn rust_helper_list_empty(list:*const bindings::list_head)->bool;
+        }
+        if !dev.is_vlan_dev(){
+            return None;
+        }
+        let rst = unsafe{dev.dev_state_mut().as_mut()};
+        let mut ret_skb = None;
+        let mut flags = 0;
+        loop{
+            flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut rst.rstate.pool_wait.lock as *const _ as *mut bindings::raw_spinlock_t)};
+            
+            if !unsafe{rust_helper_list_empty(&mut rst.rstate.free_skb_pool)}{
+                let skb = list_get_entry!(&mut rst.rstate.free_skb_pool,bindings::sk_buff,__bindgen_anon_1.list);
+                rst.rstate.pool_free-=1;
+                ret_skb = Some(Self::from_raw_ptr(skb));
+            }
+
+            if timeout == RROS_NONBLOCK{
+                break;
+            }
+            rst.rstate.pool_wait.locked_add(timeout, tmode);
+            unsafe{bindings::_raw_spin_unlock_irqrestore(&mut rst.rstate.pool_wait.lock as *const _ as *mut bindings::raw_spinlock_t,flags)};
+            let ret = rst.rstate.pool_wait.wait_schedule();
+            if ret !=0{
+                break;
+            }
+        }
+        unsafe{bindings::_raw_spin_unlock_irqrestore(&mut rst.rstate.pool_wait.lock as *const _ as *mut bindings::raw_spinlock_t,flags)};
+        return ret_skb;
+    }
+    #[inline]
+    pub fn dev(&self) -> Option<NetDevice>{
+        unsafe{
+            NetDevice::from_ptr(self.0.as_ref().__bindgen_anon_1.__bindgen_anon_1.__bindgen_anon_1.dev)
+        }
+    }
+
+    #[inline]
+    pub fn set_dev(&mut self,dev:*mut bindings::net_device){
+        unsafe{
+            self.0.as_mut().__bindgen_anon_1.__bindgen_anon_1.__bindgen_anon_1.dev = dev;
+        }
+    }
+    #[inline]
+    pub fn is_oob(&self) -> bool{
+        unsafe{
+            self.0.as_ref().oob() != 0
+        }
+    }
+    #[inline]
+    pub fn set_oob_bit(&mut self, val:bool){
+        unsafe{
+            if val{
+                self.0.as_mut().set_oob(1);
+            }else{
+                self.0.as_mut().set_oob(0);
+            }
+        }
+    }
+    #[inline]
+    pub fn has_oob_clone(&self) -> bool{
+        unsafe{
+            self.0.as_ref().oob_clone() != 0
+        }
+    }
+
+    #[inline]
+    pub fn set_oob_clone_bit(&mut self, val:bool){
+        unsafe{
+            if val{
+                self.0.as_mut().set_oob_clone(1);
+            }else{
+                self.0.as_mut().set_oob_clone(0);
+            }
+        }
+    }
+
+
+    #[inline]
+    pub fn list_mut(&mut self) -> *mut bindings::list_head{
+        unsafe{
+            &mut self.0.as_mut().__bindgen_anon_1.list as *mut bindings::list_head
+        }
+    }
+    #[inline]
+    pub fn rros_control_cb(&mut self) -> NonNull<RrosNetCb>{
+        unsafe{
+            NonNull::new(self.0.as_mut().cb[0] as *const RrosNetCb as *mut RrosNetCb).unwrap()
+        }
+    }
+
+    #[inline]
+    pub fn oob_recycle(&mut self) -> bool{
+        if !self.is_oob() || self.dev().is_none(){
+            return false;
+        }
+        self.free();
+        return true;
+    }
+    
+    #[inline]
+    pub fn put(&mut self,len:u32){
+        unsafe{
+            bindings::skb_put(self.0.as_ptr(), len);
+        }
+    }
+    
+    pub fn free(&mut self){
+        // evl_net_free_skb
+        extern "C"{
+           fn rust_helper_hard_irqs_disabled() -> bool;
+        }
+        unsafe{rust_helper_hard_irqs_disabled()};
+        self.free_skb();
+
+        unsafe{rros_schedule();}
+        maybe_kick_recycler();
+    }
+    
+    fn free_skb_inband(&mut self){
+        extern  "C"{
+            fn rust_helper_list_add(new: *mut bindings::list_head, head: *mut bindings::list_head);
+        }
+        let mut flags = 0;
+        recycler_work.irq_lock_noguard(&mut flags);
+        unsafe{
+            rust_helper_list_add(self.list_mut(), &mut (*recycler_work.locked_data().get()).queue);
+        }
+        unsafe{(*recycler_work.locked_data().get()).count += 1};
+        recycler_work.irq_unlock_noguard(flags);
+
+    }
+
+    fn free_clone(&mut self){
+        // free_skb_clone
+        extern "C"{
+            fn rust_helper_list_add(new: *mut bindings::list_head, head: *mut bindings::list_head);
+        }
+        let mut flags = 0;
+        clone_queue.irq_lock_noguard(&mut flags);
+        unsafe{rust_helper_list_add(self.list_mut() as *mut bindings::list_head, &mut (*clone_queue.locked_data().get()).queue)};
+        unsafe{(*clone_queue.locked_data().get()).count+=1};
+        clone_queue.irq_unlock_noguard(flags);
+    }
+
+    fn free_to_dev(&mut self){
+        // free_skb_to_dev
+        extern  "C"{
+            fn rust_helper_raw_spin_lock_irqsave(lock: *mut bindings::hard_spinlock_t, flags: *mut u32);
+            fn rust_helper_raw_spin_unlock_irqrestore(lock: *mut bindings::hard_spinlock_t, flags: u32);
+        }
+        let mut dev = self.dev().expect("free_to_dev: dev is none");
+        let rst = unsafe{dev.dev_state_mut().as_mut()};
+        let mut flags = 0;
+        unsafe{
+            rust_helper_raw_spin_lock_irqsave(&mut rst.rstate.pool_wait.lock,&mut flags);
+        }        
+        list_add!(self.list_mut(),&mut rst.rstate.free_skb_pool);
+        rst.rstate.pool_free += 1;
+        if rst.rstate.pool_wait.is_active(){
+            rst.rstate.pool_wait.wake_up(core::ptr::null_mut(), 0);
+        }
+        unsafe{
+            rust_helper_raw_spin_unlock_irqrestore(&mut rst.rstate.pool_wait.lock, flags);
+        }
+    	// // evl_signal_poll_events(&est->poll_head,	POLLOUT|POLLWRNORM); // evl poll
+
+    }
+    fn free_skb(&mut self){
+        // 对应 free_skb
+        extern  "C"{
+            fn skb_release_oob_skb(skb:*mut bindings::sk_buff,dref :*mut i32) -> bool;
+            fn netdev_reset_oob_skb(dev:*mut bindings::net_device,skb:*mut bindings::sk_buff);
+        }
+        let dev = self.dev();
+        if dev.is_none(){
+            return;
+        }
+        let mut dev = dev.unwrap();
+        if dev.is_vlan_dev(){
+            return
+        }
+        /* TODO:
+        * We might receive requests to free regular skbs, or
+        * associated to devices for which diversion was just turned
+        * off: pass them on to the in-band stack if so.
+        * FIXME: should we receive that?
+	    */
+        // if (unlikely(!netif_oob_diversion(skb->dev)))
+        // skb->oob = false;
+        if (!self.is_oob()){
+            if (!self.has_oob_clone()){
+                self.free_skb_inband();
+            }
+            return;
+        }
+
+        let mut dref = 0;
+
+        if !unsafe{skb_release_oob_skb(self.0.as_ptr(),&mut dref)}{
+            return;
+        }
+        // if !skb_release_oob_skb // TODO:
+        if (self.has_oob_clone()){
+            let origin = unsafe{self.rros_control_cb().as_mut().origin};
+            let mut origin = RrosSkBuff::from_raw_ptr(origin);
+            origin.free_clone();
+            if (!origin.is_oob()){
+                assert!(dref == 1);
+                if dref == 1{
+                    origin.free_skb_inband();
+                }else{
+    				// EVL_WARN_ON(NET, dref < 1);
+                }
+                return
+            }
+            if (dref!=0){
+                unsafe{netdev_reset_oob_skb(dev.0.as_mut(),origin.0.as_mut())};
+                origin.free_to_dev();
+            }
+            return
+        }
+        if (dref!=0){
+            unsafe{netdev_reset_oob_skb(dev.0.as_mut(),self.0.as_mut())};
+            self.free_to_dev();
+        }
+    }
+
+    #[inline]
+    pub fn reset_mac_header(&mut self){
+        extern "C"{
+            fn rust_helper_skb_reset_mac_header(skb: *mut bindings::sk_buff);
+        }
+        unsafe{rust_helper_skb_reset_mac_header(self.0.as_mut())};
+    }
+}
+
+
+pub fn free_skb_list(list : *mut bindings::list_head){
+    // evl_net_free_skb_list
+    extern "C"{
+        fn rust_helper_hard_irqs_disabled() -> bool;
+        fn rust_helper_list_empty(list: *const bindings::list_head) -> bool;
+     }
+     unsafe{rust_helper_hard_irqs_disabled()};
+     let list_is_empty = unsafe{
+        rust_helper_list_empty(list)
+     };
+     if list_is_empty{
+        return;
+     }
+     list_for_each_entry_safe!(skb,next,list,bindings::sk_buff,{
+        let mut sk_buff = RrosSkBuff::from_raw_ptr(skb);
+        sk_buff.free();
+     },__bindgen_anon_1.list);
+
+     unsafe{rros_schedule();}
+     maybe_kick_recycler();
+}
+
+
+
+// pub struct EVLNetSkbQueue{
+//     pub queue : bindings::list_head,
+//     pub lock : SpinLock<()>, //不直接封装在上面是因为不够灵活，以后可以重新封装下
+//     // phantom : core::marker::PhantomData<RrosSkBuff>,
+// }
+
+
+pub struct RrosSkbQueueInner{
+    is_init : AtomicBool,
+    list : bindings::list_head
+}
+
+unsafe impl Send for RrosSkbQueueInner{}
+unsafe impl Sync for RrosSkbQueueInner{}
+
+pub type RrosSkbQueue = SpinLock<RrosSkbQueueInner>;
+
+impl RrosSkbQueueInner{
+    #[inline]
+    pub const fn default() -> Self{
+        Self{
+            is_init: AtomicBool::new(false),
+            list: bindings::list_head{
+            next : core::ptr::null_mut(),
+            prev : core::ptr::null_mut(),
+        }}
+    }
+    #[inline]
+    pub fn init(&mut self){
+        init_list_head!(&mut self.list);
+        self.is_init.store(true, Ordering::Relaxed);
+    }
+
+    #[inline]
+    pub fn is_empty(&self) -> bool{
+        extern  "C"{
+            fn rust_helper_list_empty(list: *const bindings::list_head) -> bool;
+        }
+        unsafe{
+            rust_helper_list_empty(&self.list as *const bindings::list_head)
+        }
+    }
+
+    #[inline]
+    pub fn destory(&mut self){
+        // evl_net_destroy_skb_queue
+        free_skb_list(&mut self.list as *mut bindings::list_head);
+    }
+    pub fn get(&mut self) -> Option<RrosSkBuff>{
+        // evl_net_get_skb_queue
+        extern  "C"{
+            fn rust_helper_list_empty(list: *const bindings::list_head) -> bool;
+        }
+
+        let list_is_empty = unsafe{
+            rust_helper_list_empty(&self.list as *const bindings::list_head)
+        };
+        let skb  =if !list_is_empty{
+            Some(RrosSkBuff::from_raw_ptr(list_get_entry!(&mut self.list as *mut bindings::list_head,bindings::sk_buff,__bindgen_anon_1.list)))
+        }else{
+            None
+        };
+        return skb;
+    }
+    pub fn move_queue(&mut self,list:*mut bindings::list_head)->bool{
+        // evl_net_move_skb_queue
+        extern "C"{
+            fn rust_helper_list_empty(list:*mut bindings::list_head)->bool;
+            fn rust_helper_list_splice_init(list:*mut bindings::list_head,head:*mut bindings::list_head);
+        }
+        let v = self.is_init.load(Ordering::Relaxed);
+        if !v{
+            self.init();
+        }
+        // move the node on self.queue to list
+        let ret = unsafe{
+            rust_helper_list_splice_init(&mut self.list,list);
+            !rust_helper_list_empty(list)
+        };
+        ret
+        // drop(guard);
+    }
+
+    pub fn add(&mut self, skb: &mut RrosSkBuff){
+        // evl_net_add_skb_queue
+        extern "C"{
+            fn rust_helper_list_add_tail(new: *mut bindings::list_head, head: *mut bindings::list_head);
+        }
+        unsafe{
+            rust_helper_list_add_tail(skb.list_mut(),&mut self.list);
+        }
+    }
+}
+
+
+
+pub fn rros_net_dev_build_pool(dev:&mut NetDevice) ->i32{
+    extern "C"{
+        fn rust_helper_is_vlan_dev(dev:*mut bindings::net_device) -> bool;
+        fn rust_helper_netif_oob_diversion(dev:*const bindings::net_device) -> bool;
+        fn rust_helper_list_add(new: *mut bindings::list_head, head: *mut bindings::list_head);
+
+    }
+    if unsafe{rust_helper_is_vlan_dev(dev.0.as_ptr())}{
+        return -(bindings::EINVAL as i32);
+    }
+    if unsafe{rust_helper_netif_oob_diversion(dev.0.as_ptr())}{
+        return -(bindings::EBUSY as i32);
+    }
+
+    let est = unsafe{dev.dev_state_mut().as_mut()};
+    init_list_head!(&mut est.rstate.free_skb_pool);
+
+
+    for n in 0..est.rstate.pool_max{
+        let skb = RrosSkBuff::alloc_one_skb(dev);
+        if let Some(mut skb) = skb{
+            unsafe{
+                rust_helper_list_add(skb.list_mut(),&mut est.rstate.free_skb_pool);
+            }
+        }else{
+            unimplemented!();
+			// TODO:evl_net_dev_purge_pool(dev);
+            return -(bindings::ENOMEM as i32);
+
+        }
+    }
+
+    est.rstate.pool_free = est.rstate.pool_max;
+    unsafe{
+        est.rstate.pool_wait.init(&mut RROS_MONO_CLOCK, RROS_WAIT_PRIO as i32);
+    }
+	// evl_init_poll_head(&est->poll_head);
+    0
+}
+
+fn skb_recycler(work :&mut RrosWork)->i32{
+    extern "C"{
+        fn rust_helper_list_splice_init(list:*mut bindings::list_head,head:*mut bindings::list_head);
+        fn rust_helper_skb_list_del_init(skb:*mut bindings::sk_buff)->bool; 
+        fn rust_helper_dev_kfree_skb(skb:*mut bindings::sk_buff);
+    }
+    let mut list = bindings::list_head::default();
+    init_list_head!(&mut list);
+
+    let mut flags = 0;
+    recycler_work.irq_lock_noguard(&mut flags);
+    unsafe{
+        rust_helper_list_splice_init(&mut (*recycler_work.locked_data().get()).queue, &mut list);
+    }
+    list_for_each_entry_safe!(skb,next,&mut list,bindings::sk_buff,{
+        unsafe{
+            rust_helper_skb_list_del_init(skb);
+            rust_helper_dev_kfree_skb(skb);
+        }
+    },__bindgen_anon_1.list);
+    recycler_work.irq_unlock_noguard(flags);
+    0
+}
+
+/// 初始化函数
+pub fn rros_net_init_pools() -> Result<()>{
+    extern "C"{
+        fn rust_helper_list_add(new: *mut bindings::list_head, head: *mut bindings::list_head);
+        fn rust_helper_INIT_LIST_HEAD(list: *mut bindings::list_head);
+    }
+    unsafe{rust_helper_INIT_LIST_HEAD(&mut (*(*clone_queue.locked_data()).get()).queue)};
+    unsafe{(*(*clone_queue.locked_data()).get()).count = NET_CLONES as i32};
+    // clone_queue.
+    for n in 0..NET_CLONES{
+        let clone =unsafe{
+            bindings::skb_alloc_oob_head(bindings::GFP_KERNEL)
+        };
+        if clone.is_null(){
+            unimplemented!()
+            // failed
+        }
+        unsafe{
+            rust_helper_list_add(RrosSkBuff::from_raw_ptr(clone).list_mut(),&mut (*(*clone_queue.locked_data()).get()).queue);
+        }
+    }
+
+    unsafe{(&mut *recycler_work.locked_data().get()).work.init(skb_recycler)};
+
+    Ok(())
+    // TODO: failed
+}
\ No newline at end of file
diff --git a/kernel/rros/net/socket.rs b/kernel/rros/net/socket.rs
new file mode 100644
index 000000000..962f4cb94
--- /dev/null
+++ b/kernel/rros/net/socket.rs
@@ -0,0 +1,774 @@
+/*
+ * EVL sockets are (almost) regular sockets, extended with out-of-band
+ * capabilities. In theory, this would allow us to provide out-of-band
+ * services on top of any common protocol already handled by the
+ * in-band network stack. EVL-specific protocols belong to the generic
+ * PF_OOB family, which we use as a protocol mutiplexor.
+ */
+use crate::bindings::{sockaddr,
+    iovec,
+    __poll_t,
+    oob_poll_wait,
+    atomic_t,
+    sock,
+    hlist_node,
+    socket};
+use crate::clock::RROS_MONO_CLOCK;
+use crate::lock::raw_spin_lock_init;
+use crate::timeout::rros_tmode;
+use crate::{bindings,c_types};
+use crate::{sched::ssize_t,sched::__rros_timespec,
+    crossing::RrosCrossing,
+    wait::RrosWaitQueue,
+    file::RrosFile,
+    list::list_head,
+    initialize_lock_hashtable};
+use alloc::rc::Rc;
+use kernel::ktime::KtimeT;
+use kernel::sync::SpinLock;
+use kernel::{ThisModule, mutex_init, spinlock_init, container_of};
+use core::cell::{RefCell, Ref,UnsafeCell};
+use core::default::Default;
+use core::mem::transmute;
+use core::pin::Pin;
+use core::ptr::NonNull;
+use core::sync::atomic::{AtomicI32, Ordering};
+use core::ptr;
+use kernel::{
+    sync::Mutex,
+    double_linked_list::List,
+    str::CStr,
+    net::Device,
+    net::Namespace as Net,
+    endian::be16,
+	file::File,
+    init_static_sync,
+    vmalloc::c_kzalloc
+};
+
+use super::device::NetDevice;
+use super::packet::ethernet_net_proto;
+use super::skb::{free_skb_list, RrosSkBuff};
+
+macro_rules! sk_family{
+	($x:ident) => {
+		unsafe{(*$x).__sk_common.skc_family}
+	};
+}
+
+pub struct RrosNetdevActivation{
+    pub poolsz : u64,
+    pub bufsz : u64,
+}
+
+pub struct UserOobMsghdr{
+    pub name_ptr : u64,
+    pub iov_ptr : u64,
+    pub ctl_ptr : u64,
+    pub namelen : u32,
+    pub iovlen : u32,
+    pub ctllen : u32,
+    pub count : i32,
+    pub flags : u32,
+    pub timeout : __rros_timespec,
+    pub timestamp : __rros_timespec,
+}
+
+pub trait RrosNetProto {
+    fn attach(&self,sock:&mut RrosSocket, protocol: be16) -> i32;
+    fn detach(&self,sock:&mut RrosSocket); 
+    fn bind(&self, sock:&mut RrosSocket,addr:&sockaddr, len:i32) -> i32;
+    // fn ioctl(&mut self, cmd:u32, arg:u64) -> i32;
+    fn oob_send(&self,sock:&mut RrosSocket, msg:*mut UserOobMsghdr, iov:&mut iovec, len:usize) -> isize;
+    fn oob_receive(&self,sock:&mut RrosSocket,msg:*mut UserOobMsghdr, iov:&iovec, len:usize) -> isize;
+    // fn oob_poll(&mut self, wait:&oob_poll_wait) -> __poll_t;
+    fn get_netif(&self,sock:&mut RrosSocket) -> Option<NetDevice>;
+}
+pub struct RrosPollHead {
+	pub watchpoints : list_head, 
+	pub lock : bindings::raw_spinlock_t
+}
+pub struct _binding{
+    pub real_ifindex : i32,
+    pub vlan_ifindex : i32,
+    pub vlan_id : u16,
+    pub proto_hash : u32
+}
+pub struct RrosSocket{
+    pub proto : Option<&'static dyn RrosNetProto>,
+    pub efile : RrosFile,
+    pub lock : Mutex<()>,
+    pub net : *mut bindings::net,
+    pub hash : kernel::bindings::hlist_node,
+    pub input : kernel::bindings::list_head,
+    pub input_wait: RrosWaitQueue,
+    pub poll_head : RrosPollHead,
+    pub next_sub :  kernel::bindings::list_head,
+    pub sk : *mut kernel::bindings::sock,
+    pub rmem_count : AtomicI32,
+    pub rmem_max :i32,
+    pub wmem_count : AtomicI32,
+    pub wmem_max :i32,
+    pub wmem_wait : RrosWaitQueue,
+    pub wmem_drain : RrosCrossing,
+    pub protocol : be16,
+    pub binding : _binding,
+    pub oob_lock : SpinLock<()> 
+}
+
+const EVL_SOCKIOC_RECVMSG : u32 = 1079045636;
+const EVL_SOCKIOC_SENDMSG : u32 = 3226529285;
+
+impl RrosSocket{
+    pub fn from_socket(sock : *mut bindings::socket) ->NonNull<Self>{
+        // evk_sk
+        unsafe{
+            NonNull::new((*(*sock).sk).oob_data as *const _ as *mut RrosSocket).unwrap()
+        }
+    }
+    #[inline]
+    pub fn from_file(filp:*mut bindings::file) -> Option<NonNull<Self>>{
+        unsafe{
+            if !(*filp).oob_data.is_null(){
+                let ptr = container_of!((*filp).oob_data,RrosSocket,efile) as *mut RrosSocket;
+                Some(NonNull::new_unchecked(ptr))
+            }else{
+                None
+            }
+        }
+    }
+
+    #[inline]
+    pub fn charge_socket_rmem(&mut self,skb:&RrosSkBuff) -> bool{
+        
+        if self.rmem_count.load(Ordering::Relaxed) >= self.rmem_max{
+            return false;
+        }
+        self.rmem_count.fetch_add(skb.truesize as i32, Ordering::Relaxed);
+        true
+    }
+    #[inline]
+    pub fn uncharge_socke_rmem(&mut self,skb:&RrosSkBuff){
+        self.rmem_count.fetch_sub(skb.truesize as i32, Ordering::Relaxed);
+    }
+    #[inline]
+    pub fn charge_socket_wmem(&mut self,skb:&mut RrosSkBuff)->bool{
+        if self.wmem_count.load(Ordering::Relaxed) >= self.wmem_max{
+            return false;
+        }
+
+        self.wmem_count.fetch_add(skb.truesize as i32, Ordering::Relaxed);
+        skb.net_cb_mut().tracker = self;
+        self.wmem_drain.down();
+        true
+    }
+
+    pub fn charge_socket_wmem_timeout(&mut self,skb:&mut RrosSkBuff,timeout:KtimeT,tmode:rros_tmode) ->i32{
+        // evl_charge_socket_wmem
+        skb.net_cb_mut().tracker = ptr::null_mut();
+        if self.wmem_max == 0{
+            return 0;
+        }
+        return 0;
+        // return self.wmem_wait.wait_timeout(timeout, time_mode, self.charge_socket_wmem(skb));
+    }
+    
+    pub fn send_or_recv(&mut self,msghdr:*mut UserOobMsghdr,cmd:i32)->i32{
+        extern "C"{
+			fn rust_helper_raw_get_user(result:*mut u32,addr :*const u32) -> isize;
+    		fn rust_helper_raw_get_user_64(result:*mut u64,addr :*const u64) -> isize;
+
+    		fn rust_helper_raw_put_user(x:u32,ptr:*mut u32) -> i32;
+        }
+        let mut fast_iov : [bindings::iovec;8] = [bindings::iovec::default();8];
+        let mut iov_ptr : u64 = 0;
+        let mut iovlen : u32 = 0;
+        if unsafe{rust_helper_raw_get_user_64(&mut iov_ptr,&(*msghdr).iov_ptr)} != 0{
+            return - (bindings::EFAULT as i32);
+        }
+
+        if unsafe{rust_helper_raw_get_user(&mut iovlen, &(*msghdr).iovlen)} !=0 {
+            return - (bindings::EFAULT as i32);
+        }
+	    let u_iov :*mut bindings::iovec= unsafe{
+            transmute(iov_ptr)
+        };
+        let iov = load_iov(u_iov,iovlen as usize,fast_iov.as_mut_ptr());
+
+        let proto = self.proto.unwrap();
+        let ref_iov = unsafe{&mut (*iov)};
+        let count = if cmd == (EVL_SOCKIOC_SENDMSG as i32){
+            proto.oob_send(self, msghdr, ref_iov, iovlen as usize)
+        }else{
+            proto.oob_receive(self, msghdr, ref_iov, iovlen as usize)
+        };
+
+        if core::ptr::eq(iov,fast_iov.as_mut_ptr()){
+            // free iov
+            // TODO:
+        }
+        if unsafe{rust_helper_raw_put_user(count as u32,&mut (*msghdr).count as *const _ as *mut u32)}!=0{
+            return - (bindings::EFAULT as i32);
+        }
+        return 0;
+        
+    }
+}
+
+pub fn uncharge_socke_wmem(skb:&mut RrosSkBuff){
+    let mut rsk = skb.net_cb_mut().tracker;
+    if rsk.is_null(){
+        return
+    }
+    let rsk = unsafe{&mut *rsk};
+    let flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut rsk.wmem_wait.lock as *const _ as *mut bindings::raw_spinlock_t)};
+
+    skb.net_cb_mut().tracker = ptr::null_mut();
+
+    let count = rsk.wmem_count.fetch_sub(skb.truesize as i32,Ordering::Relaxed);
+    if count < rsk.wmem_max && rsk.wmem_wait.is_active(){
+        rsk.wmem_wait.flush_locked(0);
+    }
+
+    rsk.wmem_drain.up();
+
+    unsafe{bindings::_raw_spin_unlock_irqrestore(&mut rsk.wmem_wait.lock as *const _ as *mut bindings::raw_spinlock_t,flags)};
+
+}
+
+// // default operation for socket
+// const netproto_ops:binding::proto_ops = binding::proto_ops{
+//     family = binding::PF_OOB,
+//     owner = ThisModule,
+//     release = evl_sock_release,
+//     bind = evl_sock_bind,
+//     connect = bindings::sock_no_connect,
+//     socketpair = bindings::sock_no_socketpair,
+//     accept = bindings::sock_no_accept,
+//     getname =	bindings::sock_no_getname,
+// 	// ioctl =	sock_inband_ioctl,  // TODO
+// 	ioctl =	bindings::sock_no_ioctl,
+// 	listen =	bindings::sock_no_listen,
+// 	shutdown =	bindings::sock_no_shutdown,
+// 	sendmsg =	bindings::sock_no_sendmsg,
+// 	recvmsg =	bindings::sock_no_recvmsg,
+// 	mmap =		bindings::sock_no_mmap,
+// 	sendpage =	bindings::sock_no_sendpage,
+// };
+// const evl_af_oob_proto: bindings::proto = bindings::proto{
+// 	name		= "RROS", //TODO: [c_types::c_char; 32usize]
+// 	owner		= ThisModule,
+// 	obj_size	= core::mem::size_of(RrosSocket),
+// };
+
+unsafe extern "C" fn create_evl_socket(net:*mut bindings::net, sock: *mut bindings::socket, protocol:i32, kern:i32) -> i32
+{
+    unimplemented!();
+}
+//     // extern "C"{
+//     //     fn sk_refcnt_debug_inc()
+//     //     fn local_bh_disable();
+//     //     fn sock_prot_inuse_add();
+//     //     fn local_bh_enable();
+//     // }
+//     if kern{
+//         return -(bindings::EOPNOTSUPP as i32);
+//     }
+//     unsafe{(*sock).state = bindings::socket_state_SS_UNCONNECTED;}
+//     let sk : *mut bindings::sock = unsafe{bindings::sk_alloc(net,bindings::PF_OOB,bindings::GFP_KERNEL,0)};
+//     if sk.is_null(){
+//         return -(bindings::ENOBUFS as i32);
+//     }
+//     unsafe{(*sock).ops = &netproto_ops as *const netproto_ops};
+//     unsafe{bindings::sock_init_data(sock,sk);}
+
+//     /*
+// 	 * Protocol is checked for validity when the socket is
+// 	 * attached to the out-of-band core in sock_oob_attach().
+// 	 */
+//     unsafe{(*sock).sk_protocol = protocol};
+//     // sk_refcnt_debug_inc(sk); TODO: 用于Debug的东西
+//     unsafe{(*sock).sk_destruct = &destroy_evl_socket as ::core::option::Option<unsafe extern "C" fn(sk: *mut sock)>};
+
+//     // unsafe{local_bh_disable();}
+// 	// sock_prot_inuse_add(net,unsafe{&evl_af_oob_proto as *mut proto}, 1 as c_types::c_int); # TODO: CONFIG_PROC_FS
+// 	// unsafe{local_bh_enable();}
+//     0
+// }
+
+// #[no_mangle]
+// fn evl_sock_release(sock:*mut bindings::socket) -> i32
+// {
+//     /*
+// 	 * Cleanup happens from sock_oob_detach(), so that PF_OOB
+// 	 * and common protocols sockets we piggybacked on are
+// 	 * released.
+// 	 */
+// 	0
+// }
+
+// #[no_mangle]
+// fn evl_sock_bind(sock: *mut bindings::socket,u_addr:*mut bindings::sockaddr, len:i32)->i32
+// {
+//     let esk = evl_sk(sock).expect("evl_sk(sock) unwrap failed");
+//     esk.proto.as_mut().expect("get proto failed").bind(esk,u_addr,len)
+// }
+
+#[no_mangle]
+fn sock_oob_attach(sock: *mut bindings::socket) -> i32
+{
+    extern "C" {
+        #[allow(improper_ctypes)]
+        fn rust_helper_is_err(ptr: *const c_types::c_void) -> bool;
+
+        #[allow(improper_ctypes)]
+        fn rust_helper_ptr_err(ptr: *const c_types::c_void) -> c_types::c_long;
+        
+        fn rust_helper_sock_net(sk:*const bindings::sock)->*mut bindings::net; 
+
+        fn rust_helper_INIT_LIST_HEAD(ptr: *mut bindings::list_head);
+    }
+
+    let sk = unsafe{(*sock).sk};
+    /*
+	 * Try finding a suitable out-of-band protocol among those
+	 * registered in EVL.
+	 */
+    let proto = find_oob_proto(sk_family!(sk).into(), unsafe{(*sk).sk_type}.into(), be16::from(unsafe{(*sk).sk_protocol}));
+    if proto.is_none(){
+        return - (bindings::EPROTONOSUPPORT as i32);
+    }
+    let proto = proto.unwrap();
+
+    let rsk =if sk_family!(sk) != bindings::PF_OOB as u16{
+        let tmp = c_kzalloc(core::mem::size_of::<RrosSocket>() as u64);
+        if tmp.is_none(){
+            return -(bindings::ENOMEM as i32);
+        }
+        tmp.unwrap() as *mut RrosSocket
+    } else{
+        unsafe{
+            sk as *const _ as *mut RrosSocket
+        }
+    };
+    let mut rsk = unsafe{NonNull::new(rsk).unwrap().as_mut()};
+    rsk.sk = sk;
+    let ret = rsk.efile.rros_open_file(unsafe {
+        (*sock).file
+    });
+    if ret.is_err(){
+        // TODO:
+        unimplemented!();
+    }
+    rsk.net = unsafe {
+        rust_helper_sock_net((*sock).sk)
+    };
+    let pinned = unsafe{Pin::new_unchecked(&mut rsk.lock)};
+    mutex_init!(pinned,"net mutex");
+
+    unsafe{
+        rust_helper_INIT_LIST_HEAD(&mut rsk.input);
+        rust_helper_INIT_LIST_HEAD(&mut rsk.next_sub);
+        rsk.input_wait.init(&mut RROS_MONO_CLOCK, 0);
+        rsk.wmem_wait.init(&mut RROS_MONO_CLOCK,0);
+    }
+	// evl_init_poll_head(&esk->poll_head);
+    let pinned = unsafe{Pin::new_unchecked(&mut rsk.oob_lock)};
+    spinlock_init!(pinned,"net oob spinlock");
+
+    rsk.rmem_max = unsafe{(*sk).sk_rcvbuf};
+    rsk.wmem_max = unsafe{(*sk).sk_sndbuf};
+    rsk.wmem_drain.init();
+    rsk.proto = Some(proto);
+    proto.attach(&mut rsk, unsafe{be16::from((*sk).sk_protocol)});
+    
+    unsafe{(*sk).oob_data = rsk as *const _ as *mut c_types::c_void;}
+    0
+    //TODO: FAILED handling
+//     fail_attach:
+// 	evl_release_file(&esk->efile);
+// fail_open:
+// 	if (sk->sk_family != PF_OOB)
+// 		kfree(esk);
+
+// 	return ret;
+}
+
+#[no_mangle]
+fn sock_oob_detach(sock: *mut bindings::socket)
+{
+    let sk = unsafe{(*sock).sk};
+    let rsk = unsafe{RrosSocket::from_socket(sock).as_mut()};
+    
+    rsk.efile.rros_release_file();
+    rsk.wmem_drain.pass();
+    free_skb_list(&mut rsk.input);
+    if let Some(proto) = rsk.proto{
+        proto.detach(rsk);
+    }
+
+    if (sk_family!(sk) != bindings::PF_OOB as u16){
+        unsafe{
+            bindings::kfree(rsk as *const _ as  *mut c_types::c_void);
+        }
+    }
+    unsafe{(*sk).oob_data = core::ptr::null_mut();}
+}
+
+#[no_mangle]
+pub fn sock_oob_bind(sock: *mut bindings::socket, addr: *const bindings::sockaddr, len: i32) -> i32
+{
+    let sk = unsafe{(*sock).sk};
+    if sk_family!(sk) == bindings::PF_OOB as u16{
+        return 0
+    }
+    let rsk = unsafe{RrosSocket::from_socket(sock).as_mut()};
+    let proto = rsk.proto.unwrap();
+    proto.bind(rsk, unsafe{&*addr}, len)
+    
+}
+
+pub fn sock_inband_ioctl(sock : *mut bindings::socket,cmd:i32,arg:u64) -> i32{
+    // TODO:
+    -(bindings::ENOTTY as i32)
+}
+
+
+#[no_mangle]
+pub fn sock_inband_ioctl_redirect(sock : *mut bindings::socket,cmd:i32,arg:u64) -> i32{
+    let ret = sock_inband_ioctl(sock, cmd, arg);
+
+    if ret == -(bindings::ENOTTY as i32){
+        - (bindings::ENOIOCTLCMD as i32)
+    }else{
+        ret
+    }
+}
+
+
+#[no_mangle]
+pub fn sock_oob_ioctl(filp:*mut bindings::file,cmd:u32,arg:u64) -> i64{
+    let mut rsk = RrosSocket::from_file(filp);
+    if rsk.is_none(){
+        return - (bindings::EBADFD as i64);
+    }
+    let mut rsk = unsafe{rsk.unwrap().as_mut()};
+    let mut ret = 0;
+
+
+    match cmd{
+        // TODO: update IOV
+        EVL_SOCKIOC_RECVMSG | EVL_SOCKIOC_SENDMSG =>{
+            let u_msghdr : *mut UserOobMsghdr = unsafe{
+                transmute(arg)
+            };
+            ret = rsk.send_or_recv(u_msghdr, cmd as i32);
+        },
+        _ => {ret = -(bindings::ENOTTY as i32);}
+    }
+    return ret as i64;
+}
+pub fn do_load_iov(iov:*mut iovec,u_iov:*mut iovec,iovlen:usize) -> i32{
+    extern "C"{
+		fn rust_helper_raw_copy_from_user(dst:*mut u8,src:*const u8,size:usize) -> usize;
+    }
+    unsafe{
+        if rust_helper_raw_copy_from_user(iov as *const _ as *mut u8,u_iov as *const _ as *mut u8,iovlen*core::mem::size_of::<iovec>()) !=0{
+            -(bindings::EFAULT as i32)
+        }else{
+            0
+        }
+    }
+}
+pub fn load_iov(u_iov:*mut iovec,iovlen:usize,fast_iov:*mut iovec)->*mut iovec{
+    assert!(iovlen <= 1024);
+    if iovlen < 8{
+       if do_load_iov(fast_iov,u_iov,iovlen)!=0{
+            panic!()
+       }
+       return fast_iov;
+    }else{
+        unimplemented!()
+    }
+    unimplemented!()
+}
+
+pub fn rros_import_iov(mut iov:&mut bindings::iovec,iovlen:usize,data:*mut u8,mut len:u64,remainder:*mut usize)->i32{
+    extern "C"{
+        fn rust_helper_raw_copy_from_user(to:*mut u8,from:*const u8,n:usize)->usize;
+    }
+    let mut n = 0;
+    let mut nbytes = 0;
+    let mut avail = 0;
+    let mut read = 0;
+    while len > 0 && n <iovlen{
+        if iov.iov_len == 0{
+            n+=1;
+            iov = unsafe{
+                let iov_ptr = (iov as *mut bindings::iovec).offset(1);
+                &mut (*iov_ptr)
+            };
+            continue;
+        }
+        nbytes = iov.iov_len;
+        avail += nbytes;
+        if nbytes > len{
+            nbytes = len;
+        }
+        let ret = unsafe{
+            rust_helper_raw_copy_from_user(data,iov.iov_base as *const u8,nbytes as usize)
+        };
+        if ret != 0{
+            return - (bindings::EFAULT as i32);
+        }
+
+        len -= nbytes;
+        unsafe{data.offset(nbytes as isize)};
+        read += nbytes;
+        if read < 0{
+            return - (bindings::EFAULT as i32);
+        }
+    }
+    
+    if !remainder.is_null(){
+        while n<iovlen{
+            avail += iov.iov_len;
+            n+=1;
+            iov = unsafe{
+                let iov_ptr = (iov as *mut bindings::iovec).offset(1);
+                &mut (*iov_ptr)
+            };
+        }
+        unsafe{*remainder = (avail - read) as usize;}
+    }
+    return read as i32;
+}
+
+pub fn rros_export_iov(mut iov:&bindings::iovec,iovlen:usize,data:*const u8,len:usize)->i32{
+    extern "C"{
+        fn rust_helper_raw_copy_from_user(to:*mut u8,from:*const u8,n:usize)->usize;
+    }
+    let mut n = 0;
+    let mut written = 0;
+    let mut len = len as u64;
+    while len > 0 && n <iovlen{
+        if iov.iov_len == 0{
+            n+=1;
+            iov = unsafe{
+                let iov_ptr = (iov as *const bindings::iovec).offset(1);
+                &(*iov_ptr)
+            };
+            continue;
+        }
+        let mut nbytes = iov.iov_len;
+        if nbytes > len{
+            nbytes = len;
+        }
+        let ret = unsafe{
+            rust_helper_raw_copy_from_user(iov.iov_base as *const _ as *mut u8, data, nbytes as usize)
+        };
+        if ret !=0 {
+            return -(bindings::EFAULT as i32)
+        }
+        len -= nbytes;
+        unsafe{
+            data.offset(nbytes as isize);
+        }
+        written += nbytes;
+        if written < 0{
+            return -(bindings::EINVAL as i32)
+        }
+        n+=1;
+        iov = unsafe{
+            let iov_ptr = (iov  as *const bindings::iovec).offset(1);
+            &(*iov_ptr)
+        };
+    }
+    return written as i32;
+
+}
+// #[no_mangle]
+// fn destroy_evl_socket(sk: *mut bindings::sock)
+// {
+//     // extern "C" {
+//     //     fn rust_helper_sock_net(sk:*const bindings::sock)->*mut bindings::net; 
+//     // }
+// 	// bindings::local_bh_disable();
+// 	// unsafe{bindings::sock_prot_inuse_add(rust_helper_sock_net(sk),sk_prot!(sk), -1 as c_types::c_int)};
+// 	// bindings::local_bh_enable();
+//     // sk_refcnt_debug_dec(sk); // TODO: 没有实现，Evl中用于debug
+// }
+
+
+// // todo: 字符串用什么格式？
+// #[no_mangle]
+// fn sock_oob_read(flip:File, buf:*mut u8, count:usize, pos:off_t) -> ssize_t
+// {
+//     let rsk = evl_sk_from_file(flip);
+//     if rsk.is_none(){
+//         return -(bindings::EBADF as i32);
+//     }
+//     if count == 0{
+//         return 0;
+//     }
+//     let iov : bindings::iovec;
+//     unsafe{
+//         iov.iov_base = buf;
+//         iov.iov_len = count as u64;
+//     }
+//     let rsk = rsk.unwrap();
+//     let proto = rsk.proto.as_mut().unwrap();
+//     // TODO: 接口
+//     proto.oob_receive(rsk,proto,core::ptr::null(),&iov as *const bindings::iovec,1 as c_types::c_int)
+// }
+
+// #[no_mangle]
+// fn sock_oob_write(flip:File, buf:*mut u8, count:usize, pos:off_t) -> ssize_t
+// {
+//     let rsk = evl_sk_from_file(filp);
+//     if rsk.is_none(){
+//         return -(bindings::EBADF as i32);
+//     }
+//     if count == 0{
+//         return 0;
+//     }
+//     let iov : bindings::iovec;
+//     unsafe{
+//         iov.iov_base = buf;
+//         iov.iov_len = count as u64;
+//     }
+//     let proto = rsk.proto.as_mut();
+//     // TODO: oob_send接口
+//     proto.oob_send()
+// }
+
+
+// //
+// // Domain management
+// //
+// init_static_sync!(
+//     static mut DOMAIN_LOCK : Mutex<()> = Mutex::new();
+//     domain_hash:domian_list_head[256]
+// );
+
+// pub struct evl_socket_domain {
+//     pub af_domain: i32,
+//     pub match_: fn(type_:i32, protocol :bindings::__be16) -> impl RrosNetProto,
+//     pub next : *mut list_head
+// }
+
+// struct domian_list_head{
+//     af_domain: i32,
+//     hkey : u32,
+//     hash : *mut hlist_node,
+//     list : *mut list_head
+// }
+
+
+// initialize_lock_hashtable!(domain_hash, 8);
+
+// struct DomainListHead{
+//     af_domain: i32,
+//     hkey : u32,
+//     hash : bindings::hlist_node,
+//     list : 
+// }
+
+// #[inline]
+// fn get_domain_hash(af_domain : i32) -> u32{
+//     /// 计算协议哈希
+//     let hsrc : u32 = af_domain as u32;
+//     extern "C"{
+//         fn rust_helper_jhash2(k :*const u32, length:u32, initval:u32) -> u32;
+//     }
+//     unsafe{
+//         rust_helper_jhash2(&hsrc as *const u32, 1u32, 0u32)
+//     }
+// }
+
+// fn fetch_domain_list(hkey : u32) -> {
+
+// }
+
+fn find_oob_proto(domain:i32,type_:i32,protocol:be16) -> Option<&'static impl RrosNetProto>
+{
+    // TODO: 支持更多协议
+    // let hkey = get_domain_hash(domain);
+    // let gurad = domain_hash.lock();
+    // drop(gurad);
+
+    Some(&ethernet_net_proto)
+}
+
+// #[no_mangle]
+// fn evl_register_socket_domain(domain : evl_socket_domain){
+// 	inband_context_only();
+
+//     let hkey = get_domain_hash(domain.af_domain);
+//     let mut lock = DOMAIN_LOCK.lock();
+//     let mut head = fetch_domain_list(hkey);
+//     drop(lock);
+// }
+
+
+
+
+// /**
+//  * Tools function
+//  */
+// #[inline]
+// #[no_mangle]
+// fn evl_sk_from_file(flip:File) -> Option<RrosSocket>{
+// 	unsafe {
+// 		let data = (*flip.ptr).oob_data;
+//         if data{
+//             Some(kernel::container_of!(data,RrosSocket,efile))
+//         }else{
+//             None
+//         }
+// 	}
+
+
+// }
+// pub const rros_family_ops : bindings::net_proto_family = bindings::net_proto_family{
+//     family : bindings::PF_OOB as i32,
+//     create : Some(create_evl_socket),
+//     owner : unsafe{ &kernel::bindings::__this_module as *const _ as *mut _},
+// };
+// pub static mut rros_af_oob_proto : bindings::proto  = bindings::proto::default();
+
+// pub fn init_rros_af_oob_proto(){
+//     let x:&'static str = "sd";
+
+//     unsafe{
+//         rros_af_oob_proto.name = (*b"RROS\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0").map(|u| u as i8);
+//         rros_af_oob_proto.owner = &kernel::bindings::__this_module as *const _ as *mut _;
+//         rros_af_oob_proto.obj_size = core::mem::size_of::<RrosSocket>() as u32;
+//     }
+// }
+
+pub struct DummpyProto;
+static DUMMY_PROTO : DummpyProto = DummpyProto;
+
+impl RrosNetProto for DummpyProto{
+    fn attach(&self,sock:&mut RrosSocket, protocol: be16) -> i32{
+        0
+    }
+    fn detach(&self,sock:&mut RrosSocket){
+
+    }
+    fn bind(&self, sock:&mut RrosSocket,addr:&sockaddr, len:i32) -> i32{
+        0
+    }
+    // fn ioctl(&mut self, cmd:u32, arg:u64) -> i32;
+    fn oob_send(&self,sock:&mut RrosSocket, msg:*mut UserOobMsghdr, iov:&mut iovec, len:usize) -> isize{
+        0
+    }
+    fn oob_receive(&self,sock:&mut RrosSocket,msg:*mut UserOobMsghdr, iov:&iovec, len:usize) -> isize{
+        0
+    }
+    fn get_netif(&self,sock:&mut RrosSocket) -> Option<NetDevice>{
+        unimplemented!()
+    }
+}
+
diff --git a/kernel/rros/queue.rs b/kernel/rros/queue.rs
index 77e56d201..5d24e55b0 100644
--- a/kernel/rros/queue.rs
+++ b/kernel/rros/queue.rs
@@ -17,7 +17,6 @@ extern "C" {
 //     unsafe { rust_helper_INIT_LIST_HEAD(list) };
 // }
 
-
 // pub fn rros_get_schedq(struct rros_sched_queue *q){
 // 	if (list_empty(&q->head))
 // 		return NULL;
diff --git a/kernel/rros/rros/Kconfig b/kernel/rros/rros/Kconfig
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/rros/Makefile b/kernel/rros/rros/Makefile
new file mode 100644
index 000000000..0e5299896
--- /dev/null
+++ b/kernel/rros/rros/Makefile
@@ -0,0 +1 @@
+# obj-y				+= latmus.o
\ No newline at end of file
diff --git a/kernel/rros/rros/latmus.rs b/kernel/rros/rros/latmus.rs
new file mode 100644
index 000000000..e6a33dc4a
--- /dev/null
+++ b/kernel/rros/rros/latmus.rs
@@ -0,0 +1,210 @@
+#![no_std]
+#![feature(allocator_api, global_asm)]
+use crate::{
+    thread::{self, rros_sleep, KthreadRunner}, timer::{self, rros_stop_timer}, clock,
+};
+use crate::bindings::ETIMEDOUT;
+use kernel::prelude::*;
+use kernel::{c_str, ktime, c_types};
+// use rros::thread_test::KthreadRunner;
+
+static mut kthread_runner_1: KthreadRunner = KthreadRunner::new_empty();
+
+fn kthread_handler(ptr: *mut c_types::c_void) {
+    let k_runner = unsafe{&mut *(ptr as *mut KthreadRunner)};
+// void kthread_handler(void *arg)
+// {
+// 	struct kthread_runner *k_runner = arg;
+// 	ktime_t now;
+// 	int ret = 0;
+    // TODO: add the ret value
+    let kernel_lantency = [0; 1000];
+    let mut ret = 0;
+
+    loop {
+        // TODO: add the should stop function
+        // if thread::should_stop() {
+        //     break;
+        // }
+
+        // TODO: add the wait flag function
+        // ret = evl_wait_flag(&k_runner->barrier);
+        // if (ret)
+        //     break;
+
+        // TODO: change the runner period flag when change
+        thread::rros_set_period(unsafe{&mut clock::RROS_MONO_CLOCK},
+                k_runner.1 as i64,
+                k_runner.2.period as i64, 1);
+        
+        // TODO: error handle
+        // if (ret)
+        //     break;
+
+        for i in 0..1 {
+            pr_info!("I'm going to wait\n");
+            thread::rros_wait_period();
+            pr_info!("I'm in the loop\n");
+            // if (ret && ret != -ETIMEDOUT) {
+            //     // done_sampling(&k_runner.runner, ret);
+            //     rros_stop_kthread(&k_runner.kthread);
+            //     return;
+            // }
+
+            let now = unsafe{clock::rros_read_clock(&clock::RROS_MONO_CLOCK)};
+            // if (k_runner.runner.add_sample(&k_runner.runner, now)) {
+            if (add_measurement_sample(k_runner, now) == 1) {
+                unsafe{thread::rros_set_period(&mut clock::RROS_MONO_CLOCK, 0, 0, 0);}
+                break;
+            }
+        }
+
+        break;
+    }
+
+    pr_info!("k_runner.2.state.min_latency: {}\n", k_runner.2.state.min_latency);
+    pr_info!("k_runner.2.state.max_latency: {}\n", k_runner.2.state.max_latency);
+    pr_info!("k_runner.2.state.avg_latency: {}\n", k_runner.2.state.avg_latency);
+// 	for (;;) {
+// 		if (evl_kthread_should_stop())
+// 			break;
+
+// 		ret = evl_wait_flag(&k_runner->barrier);
+// 		if (ret)
+// 			break;
+
+// 		ret = evl_set_period(&evl_mono_clock,
+// 				k_runner->start_time,
+// 				k_runner->runner.period);
+// 		if (ret)
+// 			break;
+
+// 		for (;;) {
+// 			ret = evl_wait_period(NULL);
+// 			if (ret && ret != -ETIMEDOUT)
+// 				goto out;
+
+// 			now = evl_read_clock(&evl_mono_clock);
+// 			if (k_runner->runner.add_sample(&k_runner->runner, now)) {
+// 				evl_set_period(NULL, 0, 0);
+// 				break;
+// 			}
+// 		}
+// 	}
+// out:
+// 	done_sampling(&k_runner->runner, ret);
+// 	evl_stop_kthread(&k_runner->kthread);
+// }
+}
+
+pub fn test_latmus() {
+    unsafe{
+        kthread_runner_1.init(Box::try_new(move || {
+        let now = unsafe{clock::rros_read_clock(&clock::RROS_MONO_CLOCK)};
+        unsafe{        
+            kthread_runner_1.1 = now as u64;
+            kthread_runner_1.2.period = 1000000;
+            kthread_runner_1.2.state.ideal = now as u64;
+            kthread_runner_1.2.state.offset = 0;
+        }
+        kthread_handler(&mut kthread_runner_1 as *mut KthreadRunner as *mut c_types::c_void);
+    }).unwrap());
+        kthread_runner_1.run(c_str!("latmus_thread"), 30);
+    }
+}
+
+fn add_measurement_sample(runner: &mut KthreadRunner, timestamp: ktime::KtimeT) -> i32{
+    let state = &mut runner.2.state;
+    let delta = ktime::ktime_to_ns(ktime::ktime_sub(timestamp, state.ideal as i64)) as u64;
+	let offset_delta = (delta - state.offset) as u64;
+
+    if (offset_delta < state.min_latency) {
+        state.min_latency = offset_delta;
+    } else if (offset_delta > state.max_latency) {
+        state.max_latency = offset_delta;
+    } 
+
+    state.avg_latency += offset_delta;
+    runner.2.state.ideal = ktime::ktime_add(state.ideal as i64, runner.2.period as i64) as u64;
+    // else if offset_delta > state.allmax_lat {
+        // state.allmax_lat = offset_delta;
+        // trace_evl_latspot(offset_delta);
+        // trace_evl_trigger("latmus");
+    // }
+
+    return 0;
+} 
+
+// static int add_measurement_sample(struct latmus_runner *runner,
+//     ktime_t timestamp)
+// {
+// struct runner_state *state = &runner->state;
+// ktime_t period = runner->period;
+// int delta, cell, offset_delta;
+
+// /* Skip samples in warmup time. */
+// if (runner->warmup_samples < runner->warmup_limit) {
+// runner->warmup_samples++;
+// state->ideal = ktime_add(state->ideal, period);
+// return 0;
+// }
+
+// delta = (int)ktime_to_ns(ktime_sub(timestamp, state->ideal));
+// offset_delta = delta - state->offset;
+// if (offset_delta < state->min_lat)
+// state->min_lat = offset_delta;
+// if (offset_delta > state->max_lat)
+// state->max_lat = offset_delta;
+// if (offset_delta > state->allmax_lat) {
+// state->allmax_lat = offset_delta;
+// trace_evl_latspot(offset_delta);
+// trace_evl_trigger("latmus");
+// }
+
+// if (runner->histogram) {
+// cell = (offset_delta < 0 ? -offset_delta : offset_delta) / 1000; /* us */
+// if (cell >= runner->hcells)
+// cell = runner->hcells - 1;
+// runner->histogram[cell]++;
+// }
+
+// state->sum += offset_delta;
+// state->ideal = ktime_add(state->ideal, period);
+
+// while (delta > 0 &&
+// (unsigned int)delta > ktime_to_ns(period)) { /* period > 0 */
+// state->overruns++;
+// state->ideal = ktime_add(state->ideal, period);
+// delta -= ktime_to_ns(period);
+// }
+
+// if (++state->cur_samples >= state->max_samples)
+// send_measurement(runner);
+
+// return 0;	/* Always keep going. */
+// }
+
+
+// TODO: move this to a file
+// struct Latmus;
+
+// impl KernelModule for Latmus {
+//     fn init() -> Result<Self> {
+//         // unsafe{Arc::try_new(SpinLock::new(rros_thread::new().unwrap())).unwrap()},
+//         unsafe{
+//             kthread_runner_1.init(Box::try_new(move || {
+//                 kthread_handler(&mut kthread_runner_1 as *mut KthreadRunner as *mut c_types::c_void);
+//             }).unwrap());
+//             kthread_runner_1.run(c_str!("latmus_thread"));
+//         }
+
+//         pr_info!("Hello world from latmus!\n");
+//         Ok(Rros)
+//     }
+// }
+
+// impl Drop for Rros {
+//     fn drop(&mut self) {
+//         pr_info!("Bye world from latmus!\n");
+//     }
+// }
diff --git a/kernel/rros/rros/mod.rs b/kernel/rros/rros/mod.rs
new file mode 100644
index 000000000..7d999262c
--- /dev/null
+++ b/kernel/rros/rros/mod.rs
@@ -0,0 +1 @@
+pub mod latmus;
\ No newline at end of file
diff --git a/kernel/rros/sched.rs b/kernel/rros/sched.rs
index 5b9d8702a..8529a11b6 100644
--- a/kernel/rros/sched.rs
+++ b/kernel/rros/sched.rs
@@ -1,47 +1,63 @@
-use crate::{idle, sched};
+use crate::flags::RrosFlag;
+use crate::wait::{RrosWaitChannel, RrosWaitQueue, RROS_WAIT_PRIO};
+use crate::{idle, sched, };
+// tp
 use alloc::rc::Rc;
+use kernel::linked_list::{GetLinks, Links};
 use core::cell::RefCell;
-use core::ptr::{null_mut, null};
 use core::ops::DerefMut;
 use core::ptr::NonNull;
+use core::ptr::{null, null_mut};
 
 #[warn(unused_mut)]
 use kernel::{
+    bindings, c_types, cpumask,c_str,
+    double_linked_list::*,
+    irq_work::IrqWork,
+    ktime::*,
     percpu::alloc_per_cpu,
-    bindings, c_types, cpumask, prelude::*, str::CStr, percpu_defs,percpu_defs::per_cpu, c_str,double_linked_list::*,
-    sync::{SpinLock, Lock, Guard}, spinlock_init, premmpt,ktime::*, irq_work::IrqWork, 
+    percpu_defs,
+    percpu_defs::per_cpu,
+    prelude::*,
+    premmpt, spinlock_init,
+    str::CStr,
+    sync::{Guard, Lock, SpinLock},
 };
 
-use core::mem::{size_of, align_of};
+use core::mem::{align_of, size_of, transmute};
 // use crate::weak;
 use crate::{
-    fifo, thread::*, timer::*, clock,list,
-    timeout::RROS_INFINITE, tick,stat, lock,
+    fifo, thread::*, timer::*, clock::{self,RrosClock},list,
+    timeout::RROS_INFINITE, tick,stat, lock,factory::RrosElement,
 };
 
 extern "C" {
     fn rust_helper_cpumask_of(cpu: i32) -> *const cpumask::CpumaskT;
     fn rust_helper_list_add_tail(new: *mut list_head, head: *mut list_head);
     fn rust_helper_dovetail_current_state() -> *mut bindings::oob_thread_state;
-    fn rust_helper_test_bit(nr:i32,addr:*const u32) -> bool;
+    fn rust_helper_test_bit(nr: i32, addr: *const u32) -> bool;
 }
 
-pub const RQ_SCHED:    u64 = 0x10000000;
-pub const RQ_TIMER:    u64 = 0x00010000;
-pub const RQ_TPROXY:   u64 = 0x00008000;
-pub const RQ_IRQ:      u64 = 0x00004000;
-pub const RQ_TDEFER:   u64 = 0x00002000;
-pub const RQ_IDLE:     u64 = 0x00001000;
+pub const RQ_SCHED: u64 = 0x10000000;
+pub const RQ_TIMER: u64 = 0x00010000;
+pub const RQ_TPROXY: u64 = 0x00008000;
+pub const RQ_IRQ: u64 = 0x00004000;
+pub const RQ_TDEFER: u64 = 0x00002000;
+pub const RQ_IDLE: u64 = 0x00001000;
 pub const RQ_TSTOPPED: u64 = 0x00000800;
 
 pub const SCHED_WEAK: i32 = 43;
 pub const SCHED_IDLE: i32 = 5;
 pub const SCHED_FIFO: i32 = 1;
+pub const SCHED_RR: i32 = 2;
+pub const SCHED_TP: i32 = 45;
 pub const RROS_CLASS_WEIGHT_FACTOR: i32 = 1024;
-pub const RROS_MM_PTSYNC_BIT:i32 = 0;
+pub const RROS_MM_PTSYNC_BIT: i32 = 0;
 
-static mut rros_sched_topmost:*mut rros_sched_class = 0 as *mut rros_sched_class;
-static mut rros_sched_lower:*mut rros_sched_class = 0 as *mut rros_sched_class;
+static mut rros_sched_topmost: *mut rros_sched_class = 0 as *mut rros_sched_class;
+static mut rros_sched_lower: *mut rros_sched_class = 0 as *mut rros_sched_class;
+
+// static mut rros_thread_list: List<Arc<SpinLock<rros_thread>>> = ;
 
 // pub static mut rros_sched_topmost:*mut rros_sched_class = 0 as *mut rros_sched_class;
 // pub static mut rros_sched_lower:*mut rros_sched_class = 0 as *mut rros_sched_class;
@@ -53,6 +69,7 @@ pub struct rros_rq {
     pub curr: Option<Arc<SpinLock<rros_thread>>>,
     pub fifo: rros_sched_fifo,
     pub weak: rros_sched_weak,
+    // pub tp: tp::Rros_sched_tp,
     pub root_thread: Option<Arc<SpinLock<rros_thread>>>,
     pub local_flags: u64,
     pub inband_timer: Option<Arc<SpinLock<rros_timer>>>,
@@ -62,11 +79,12 @@ pub struct rros_rq {
     #[cfg(CONFIG_SMP)]
     pub cpu: i32,
     #[cfg(CONFIG_SMP)]
-    pub resched_cpus :cpumask::CpumaskT,
+    pub resched_cpus: cpumask::CpumaskT,
     #[cfg(CONFIG_EVL_RUNSTATS)]
     pub last_account_switch: KtimeT,
     #[cfg(CONFIG_EVL_RUNSTATS)]
     pub current_account: *mut stat::RrosAccount,
+    pub lock: bindings::hard_spinlock_t,
 }
 
 impl rros_rq {
@@ -76,6 +94,7 @@ impl rros_rq {
             curr: None,
             fifo: rros_sched_fifo::new()?,
             weak: rros_sched_weak::new(),
+            // tp: tp::Rros_sched_tp::new()?,
             root_thread: None,
             // root_thread: unsafe{Some(Arc::try_new(SpinLock::new(rros_thread::new()?))?)},
             local_flags: 0,
@@ -84,13 +103,26 @@ impl rros_rq {
             proxy_timer_name: null_mut(),
             rrb_timer_name: null_mut(),
             #[cfg(CONFIG_SMP)]
-            cpu:0,
+            cpu: 0,
             #[cfg(CONFIG_SMP)]
             resched_cpus: cpumask::CpumaskT::from_int(0 as u64),
             #[cfg(CONFIG_EVL_RUNSTATS)]
             last_account_switch: 0,
             #[cfg(CONFIG_EVL_RUNSTATS)]
             current_account: stat::RrosAccount::new() as *mut stat::RrosAccount,
+            lock: bindings::hard_spinlock_t {
+                rlock: bindings::raw_spinlock {
+                    raw_lock: bindings::arch_spinlock_t {
+                        __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
+                            val: bindings::atomic_t { counter: 0 },
+                        },
+                    },
+                },
+                dep_map: bindings::phony_lockdep_map {
+                    wait_type_outer: 0,
+                    wait_type_inner: 0,
+                },
+            },
         })
     }
 
@@ -128,34 +160,35 @@ impl rros_rq {
 }
 
 #[no_mangle]
-pub static helloworldint :i32 = 5433;
+pub static helloworldint: i32 = 5433;
 
-static mut rros_runqueues :*mut rros_rq = 0 as *mut rros_rq;
+static mut rros_runqueues: *mut rros_rq = 0 as *mut rros_rq;
 
 pub static RROS_CPU_AFFINITY: cpumask::CpumaskT = cpumask::CpumaskT::from_int(0 as u64);
 
-
 pub fn rros_cpu_rq(cpu: i32) -> *mut rros_rq {
-    unsafe{percpu_defs::per_cpu(rros_runqueues,cpu)}
+    unsafe { percpu_defs::per_cpu(rros_runqueues, cpu) }
 }
 
 pub fn this_rros_rq() -> *mut rros_rq {
-    unsafe{percpu_defs::per_cpu_ptr(rros_runqueues as *mut u8, 
-        percpu_defs::smp_processor_id()) as *mut rros_rq}
+    unsafe {
+        percpu_defs::per_cpu_ptr(rros_runqueues as *mut u8, percpu_defs::smp_processor_id())
+            as *mut rros_rq
+    }
 }
 
 pub fn this_rros_rq_thread() -> Option<Arc<SpinLock<rros_thread>>> {
     let rq = this_rros_rq();
-    unsafe{(*rq).curr.clone()}
+    unsafe { (*rq).curr.clone() }
 }
 
 pub fn rros_need_resched(rq: *mut rros_rq) -> bool {
-    unsafe{(*rq).flags & RQ_SCHED != 0x0}
+    unsafe { (*rq).flags & RQ_SCHED != 0x0 }
 }
 
-pub fn rros_set_self_resched(rq: Option<*mut rros_rq>) -> Result<usize>{
+pub fn rros_set_self_resched(rq: Option<*mut rros_rq>) -> Result<usize> {
     match rq {
-        Some(r) => unsafe{
+        Some(r) => unsafe {
             (*r).flags |= RQ_SCHED;
             // (*r).local_flags |= RQ_SCHED;
         },
@@ -167,7 +200,7 @@ pub fn rros_set_self_resched(rq: Option<*mut rros_rq>) -> Result<usize>{
 //测试通过
 #[cfg(CONFIG_SMP)]
 pub fn rros_rq_cpu(rq: *mut rros_rq) -> i32 {
-    unsafe{(*rq).get_cpu()}
+    unsafe { (*rq).get_cpu() }
 }
 
 #[cfg(not(CONFIG_SMP))]
@@ -175,29 +208,58 @@ pub fn rros_rq_cpu(rq: *mut rros_rq) -> i32 {
     return 0;
 }
 
+pub fn rros_protect_thread_priority(thread:Arc<SpinLock<rros_thread>>, prio:i32) -> Result<usize>{
+    unsafe{
+        // raw_spin_lock(&thread->rq->lock);
+        let mut state = (*thread.locked_data().get()).state;
+        if state & T_READY != 0{
+            rros_dequeue_thread(thread.clone());
+        }
+
+        (*thread.locked_data().get()).sched_class = Some(&fifo::rros_sched_fifo);
+        rros_ceil_priority(thread.clone(), prio);
+        
+        state = (*thread.locked_data().get()).state;
+        if state & T_READY != 0{
+            rros_enqueue_thread(thread.clone());
+        }
+
+        let rq = (*thread.locked_data().get()).rq;
+        rros_set_resched(rq.clone());
+
+        // raw_spin_unlock(&thread->rq->lock);
+        Ok(0)
+    }
+}
+
 //测试通过
 #[cfg(CONFIG_SMP)]
-pub fn rros_set_resched(rq_op:Option<*mut rros_rq>) {
+pub fn rros_set_resched(rq_op: Option<*mut rros_rq>) {
     let rq;
     match rq_op {
-        None => return ,
+        None => return,
         Some(x) => rq = x,
     };
     let this_rq = this_rros_rq();
     if this_rq == rq {
-        unsafe{(*this_rq).add_flags(RQ_SCHED);}
+        unsafe {
+            (*this_rq).add_flags(RQ_SCHED);
+        }
     } else if rros_need_resched(rq) == false {
-        unsafe{
+        unsafe {
             (*rq).add_flags(RQ_SCHED);
             (*this_rq).add_local_flags(RQ_SCHED);
-            cpumask::cpumask_set_cpu(rros_rq_cpu(rq) as u32,(*this_rq).resched_cpus.as_cpumas_ptr());
+            cpumask::cpumask_set_cpu(
+                rros_rq_cpu(rq) as u32,
+                (*this_rq).resched_cpus.as_cpumas_ptr(),
+            );
         }
     }
 }
 
 #[cfg(not(CONFIG_SMP))]
-pub fn rros_set_resched(rq:Option<*mut rros_rq>) {
-	rros_set_self_resched(rq_clone)
+pub fn rros_set_resched(rq: Option<*mut rros_rq>) {
+    rros_set_self_resched(rq_clone)
 }
 
 //暂时不用
@@ -213,7 +275,6 @@ pub fn is_threading_cpu(cpu: i32) -> bool {
     return true;
 }
 
-
 #[cfg(not(CONFIG_SMP))]
 pub fn is_rros_cpu(cpu: i32) -> bool {
     return true;
@@ -253,57 +314,60 @@ pub fn rros_cannot_block() -> bool {
 
 #[no_mangle]
 unsafe extern "C" fn this_rros_rq_enter_irq_local_flags() {
-    unsafe{
+    unsafe {
         if rros_runqueues == 0 as *mut rros_rq {
             return;
         }
     }
 
     let rq = this_rros_rq();
-    
-    unsafe{(*rq).local_flags |= RQ_IRQ;}
+
+    unsafe {
+        (*rq).local_flags |= RQ_IRQ;
+    }
 }
 
 #[no_mangle]
 unsafe extern "C" fn this_rros_rq_exit_irq_local_flags() -> c_types::c_int {
-    unsafe{
+    unsafe {
         if rros_runqueues == 0 as *mut rros_rq {
             return 0;
         }
     }
 
     let rq = this_rros_rq();
-	// struct evl_rq *rq = this_evl_rq();
+    // struct evl_rq *rq = this_evl_rq();
 
-    unsafe{(*rq).local_flags &= !RQ_IRQ;}
+    unsafe {
+        (*rq).local_flags &= !RQ_IRQ;
+    }
 
     let flags;
     let local_flags;
 
-    unsafe{
+    unsafe {
         flags = (*rq).flags;
         local_flags = (*rq).local_flags;
     }
 
     // pr_info!("{} cc {} \n", flags, local_flags);
 
-    if  ((flags | local_flags) & RQ_SCHED) != 0x0 {
+    if ((flags | local_flags) & RQ_SCHED) != 0x0 {
         return 1 as c_types::c_int;
     }
 
     0 as c_types::c_int
 }
 
-
-
-
 //#[derive(Copy,Clone)]
 pub struct rros_sched_fifo {
     pub runnable: rros_sched_queue,
 }
 impl rros_sched_fifo {
     fn new() -> Result<Self> {
-        Ok(rros_sched_fifo { runnable: rros_sched_queue::new()? })
+        Ok(rros_sched_fifo {
+            runnable: rros_sched_queue::new()?,
+        })
     }
 }
 
@@ -317,14 +381,14 @@ impl rros_sched_weak {
     }
 }
 
-//#[derive(Copy,Clone)]
+// #[derive(Copy,Clone)]
 pub struct rros_sched_queue {
     pub head: Option<List<Arc<SpinLock<rros_thread>>>>,
 }
 impl rros_sched_queue {
-    fn new() -> Result<Self> {
+    pub fn new() -> Result<Self> {
         Ok(rros_sched_queue {
-            head:None,
+            head: None,
             // head: unsafe{List::new(Arc::try_new(SpinLock::new(rros_rq::new()?))?)},
         })
     }
@@ -333,7 +397,6 @@ impl rros_sched_queue {
 //#[derive(Copy,Clone)]
 // pub type list_head = bindings::list_head;
 
-
 pub struct list_head {
     pub next: *mut list_head,
     pub prev: *mut list_head,
@@ -352,28 +415,28 @@ pub type ssize_t = bindings::__kernel_ssize_t;
 // #[derive(Copy,Clone)]
 pub struct rros_sched_class {
     pub sched_init: Option<fn(rq: *mut rros_rq) -> Result<usize>>,
-    pub sched_enqueue: Option<fn(thread: Arc<SpinLock<rros_thread>>)>,
+    pub sched_enqueue: Option<fn(thread: Arc<SpinLock<rros_thread>>) -> Result<i32>>,
     pub sched_dequeue: Option<fn(thread: Arc<SpinLock<rros_thread>>)>,
     pub sched_requeue: Option<fn(thread: Arc<SpinLock<rros_thread>>)>,
     pub sched_pick: Option<fn(rq: Option<*mut rros_rq>) -> Result<Arc<SpinLock<rros_thread>>>>,
     pub sched_tick: Option<fn(rq: Option<*mut rros_rq>)-> Result<usize>>,
-    pub sched_migrate: Option<fn(thread: Rc<RefCell<rros_thread>>, rq: Rc<RefCell<rros_rq>>)>,
+    pub sched_migrate: Option<fn(thread: Arc<SpinLock<rros_thread>>, rq: *mut rros_rq)->Result<usize>>,
     pub sched_setparam:
-        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Rc<RefCell<rros_sched_param>>>) -> Result<usize>>,
+        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>>,
     pub sched_getparam:
-        Option<fn(thread: Rc<RefCell<rros_thread>>, p: Rc<RefCell<rros_sched_param>>)>,
+        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>)>,
     pub sched_chkparam:
-        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Rc<RefCell<rros_sched_param>>>) -> Result<i32>>,
-    pub sched_trackprio: Option<fn(thread: Rc<RefCell<rros_thread>>, p: *const rros_sched_param)>,
-    pub sched_ceilprio: Option<fn(thread: Rc<RefCell<rros_thread>>, prio: i32)>,
+        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>) -> Result<i32>>,
+    pub sched_trackprio: Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>)>,
+    pub sched_ceilprio: Option<fn(thread: Arc<SpinLock<rros_thread>>, prio: i32)>,
 
     pub sched_declare:
-        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Rc<RefCell<rros_sched_param>>>) -> Result<i32>>,
+        Option<fn(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<i32>>,
     pub sched_forget: Option<fn(thread:Arc<SpinLock<rros_thread>>) -> Result<usize>>,
     pub sched_kick: Option<fn(thread: Rc<RefCell<rros_thread>>)>,
     pub sched_show: Option<
         fn(
-            thread: Rc<RefCell<rros_thread>>,
+            thread: *mut rros_thread,
             buf: *mut c_types::c_char,
             count: ssize_t,
         ) -> Result<usize>,
@@ -381,8 +444,8 @@ pub struct rros_sched_class {
     pub sched_control: Option<
         fn(
             cpu: i32,
-            ctlp: Rc<RefCell<rros_sched_ctlparam>>,
-            infp: Rc<RefCell<rros_sched_ctlinfo>>,
+            ctlp: *mut rros_sched_ctlparam,
+            infp: *mut rros_sched_ctlinfo,
         ) -> Result<ssize_t>,
     >,
     pub nthreads: i32,
@@ -390,7 +453,7 @@ pub struct rros_sched_class {
     pub weight: i32,
     pub policy: i32,
     pub name: &'static str,
-    pub flag:i32, // 标识调度类 1:evl_sched_idle 3:evl_sched_fifo
+    pub flag:i32, // 标识调度类 1:rros_sched_idle 3:rros_sched_fifo 4:rros_sched_tp
 }
 impl rros_sched_class {
     pub fn new() -> Self {
@@ -417,13 +480,13 @@ impl rros_sched_class {
             weight: 0,
             policy: 0,
             name: "sched_class",
-            flag:0,
+            flag: 0,
         }
     }
 }
 
-pub fn evl_cpu_rq(cpu: i32) -> *mut rros_rq{
-    unsafe{percpu_defs::per_cpu_ptr(rros_runqueues as *mut u8, cpu) as *mut rros_rq}
+pub fn evl_cpu_rq(cpu: i32) -> *mut rros_rq {
+    unsafe { percpu_defs::per_cpu_ptr(rros_runqueues as *mut u8, cpu) as *mut rros_rq }
 }
 
 #[derive(Copy, Clone)]
@@ -431,6 +494,7 @@ pub struct rros_sched_param {
     pub idle: rros_idle_param,
     pub fifo: rros_fifo_param,
     pub weak: rros_weak_param,
+    pub tp: rros_tp_param,
 }
 impl rros_sched_param {
     pub fn new() -> Self {
@@ -438,6 +502,7 @@ impl rros_sched_param {
             idle: rros_idle_param::new(),
             fifo: rros_fifo_param::new(),
             weak: rros_weak_param::new(),
+            tp:rros_tp_param::new(),
         }
     }
 }
@@ -462,6 +527,17 @@ impl rros_fifo_param {
     }
 }
 
+#[derive(Copy, Clone)]
+pub struct rros_tp_param {
+    pub prio:i32,
+	pub ptid:i32,	/* partition id. */
+}
+impl rros_tp_param {
+    fn new() -> Self {
+        rros_tp_param { prio: 0, ptid:0, }
+    }
+}
+
 #[derive(Copy, Clone)]
 pub struct rros_weak_param {
     pub prio: i32,
@@ -515,14 +591,14 @@ impl rros_quota_ctlparam {
 pub struct rros_tp_ctlparam {
     pub op: rros_tp_ctlop,
     pub nr_windows: i32,
-    pub windows: Option<Rc<RefCell<__rros_tp_window>>>,
+    pub windows: *mut __rros_tp_window,
 }
 impl rros_tp_ctlparam {
     fn new() -> Self {
         rros_tp_ctlparam {
             op: 0,
             nr_windows: 0,
-            windows: None,
+            windows: 0 as *mut __rros_tp_window,
         }
     }
 }
@@ -548,13 +624,13 @@ impl rros_quota_ctlinfo {
 //#[derive(Copy,Clone)]
 pub struct rros_tp_ctlinfo {
     pub nr_windows: i32,
-    pub windows: Option<Rc<RefCell<__rros_tp_window>>>,
+    pub windows: *mut __rros_tp_window,
 }
 impl rros_tp_ctlinfo {
     fn new() -> Self {
         rros_tp_ctlinfo {
             nr_windows: 0,
-            windows: None,
+            windows: 0 as *mut __rros_tp_window,
         }
     }
 }
@@ -623,15 +699,15 @@ impl get {
 
 //#[derive(Copy,Clone)]
 pub struct __rros_tp_window {
-    pub offset: __rros_timespec,
-    pub duration: __rros_timespec,
+    pub offset: *mut __rros_timespec,
+    pub duration: *mut __rros_timespec,
     pub ptid: i32,
 }
 impl __rros_tp_window {
     fn new() -> Self {
         __rros_tp_window {
-            offset: __rros_timespec::new(),
-            duration: __rros_timespec::new(),
+            offset: 0 as *mut __rros_timespec,
+            duration: 0 as *mut __rros_timespec,
             ptid: 0,
         }
     }
@@ -642,7 +718,7 @@ pub struct __rros_timespec {
     pub tv_nsec: c_types::c_longlong,
 }
 impl __rros_timespec {
-    fn new() -> Self {
+    pub fn new() -> Self {
         __rros_timespec {
             tv_sec: 0,
             tv_nsec: 0,
@@ -652,7 +728,7 @@ impl __rros_timespec {
 pub type __rros_time64_t = c_types::c_longlong;
 use crate::timer::RrosTimer as rros_timer;
 type ktime_t = i64;
-use crate::clock::RrosClock as rros_clock;
+use crate::clock::{RrosClock as rros_clock, RROS_MONO_CLOCK};
 use crate::timer::RrosTimerbase as rros_timerbase;
 
 //#[derive(Copy,Clone)]
@@ -677,7 +753,8 @@ pub struct ops {
     pub set: Option<fn(clock: Rc<RefCell<rros_clock>>, date: ktime_t) -> i32>,
     pub program_local_shot: Option<fn(clock: Rc<RefCell<rros_clock>>)>,
     pub program_remote_shot: Option<fn(clock: Rc<RefCell<rros_clock>>, rq: Rc<RefCell<rros_rq>>)>,
-    pub set_gravity: Option<fn(clock: Rc<RefCell<rros_clock>>, p: *const rros_clock_gravity) -> i32>,
+    pub set_gravity:
+        Option<fn(clock: Rc<RefCell<rros_clock>>, p: *const rros_clock_gravity) -> i32>,
     pub reset_gravity: Option<fn(clock: Rc<RefCell<rros_clock>>)>,
     pub adjust: Option<fn(clock: Rc<RefCell<rros_clock>>)>,
 }
@@ -725,7 +802,7 @@ pub struct rros_stat {
 
 impl rros_stat {
     pub fn new() -> Self {
-        rros_stat{
+        rros_stat {
             isw: stat::RrosCounter::new(),
             csw: stat::RrosCounter::new(),
             sc: stat::RrosCounter::new(),
@@ -736,22 +813,56 @@ impl rros_stat {
     }
 }
 
+pub struct RrosThreadWithLock(SpinLock<rros_thread>);
+impl RrosThreadWithLock{
+    /// transmute back
+    pub unsafe fn transmute_to_original(ptr:Arc<Self>) -> Arc<SpinLock<rros_thread>>{
+        unsafe{
+            let ptr = Arc::into_raw(ptr) as *mut SpinLock<rros_thread>;
+            Arc::from_raw(transmute(NonNull::new_unchecked(ptr).as_ptr()))
+        }
+    }
+
+    pub unsafe fn new_from_curr_thread() -> Arc<Self>{
+        unsafe{
+            let ptr = transmute(NonNull::new_unchecked(rros_current()).as_ptr());
+            let ret = Arc::from_raw(ptr);
+            Arc::increment_strong_count(ptr);
+            ret
+        }
+    }
+    pub fn get_wprio(&self) -> i32{
+       unsafe{
+            (*(*self.0.locked_data()).get()).wprio
+       } 
+    }
+}
+
+impl GetLinks for RrosThreadWithLock{
+    type EntryType = RrosThreadWithLock;
+    fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType> {
+        unsafe {
+            &(*data.0.locked_data().get()).wait_next
+        }  
+    }
+}
+
 //#[derive(Copy,Clone)]
 pub struct rros_thread {
     pub lock: bindings::hard_spinlock_t,
 
     pub rq: Option<*mut rros_rq>,
-    pub base_class:Option<&'static rros_sched_class>,
+    pub base_class: Option<&'static rros_sched_class>,
     pub sched_class: Option<&'static rros_sched_class>,
 
     pub bprio: i32,
     pub cprio: i32,
     pub wprio: i32,
 
-    pub boosters: list_head,
-    pub wchan: rros_wait_channel,
-    pub wait_next: list_head,
-    pub wwake: Option<Rc<RefCell<rros_wait_channel>>>,
+    // pub boosters: *mut List<Arc<SpinLock<RrosMutex>>>,
+    pub wchan: *mut RrosWaitChannel,
+    pub wait_next: Links<RrosThreadWithLock>,
+    pub wwake: *mut RrosWaitChannel,
     pub rtimer: Option<Arc<SpinLock<rros_timer>>>,
     pub ptimer: Option<Arc<SpinLock<rros_timer>>>,
     pub rrperiod: ktime_t,
@@ -761,8 +872,8 @@ pub struct rros_thread {
     // pub rq_next: Option<List<Arc<SpinLock<rros_thread>>>>,
     pub next: *mut Node<Arc<SpinLock<rros_thread>>>,
 
-    pub rq_next: *mut Node<Arc<SpinLock<rros_thread>>>,
-    
+    pub rq_next: Option<NonNull<Node<Arc<SpinLock<rros_thread>>>>>,
+
     pub altsched: bindings::dovetail_altsched_context,
     pub local_info: __u32,
     pub wait_data: *mut c_types::c_void,
@@ -773,21 +884,24 @@ pub struct rros_thread {
     pub stat: rros_stat,
     pub u_window: Option<Rc<RefCell<rros_user_window>>>,
 
-    pub trackers: list_head,
+    // pub trackers: *mut List<Arc<SpinLock<RrosMutex>>>,
     pub tracking_lock: bindings::hard_spinlock_t,
-    // pub element:rros_element,
+    pub element:RrosElement,
     pub affinity: cpumask::CpumaskT,
     pub exited: bindings::completion,
     pub raised_cap: bindings::kernel_cap_t,
     pub kill_next: list_head,
-    pub oob_mm:*mut oob_mm_state,
+    pub oob_mm: *mut bindings::oob_mm_state,
     pub ptsync_next: list_head,
     pub observable: Option<Rc<RefCell<rros_observable>>>,
     pub name: &'static str,
+    // pub tps:*mut tp::rros_tp_rq,
+	pub tp_link: Option<Node<Arc<SpinLock<rros_thread>>>>,
 }
 
 impl rros_thread {
     pub fn new() -> Result<Self> {
+        unsafe{
         Ok(rros_thread {
             lock: bindings::hard_spinlock_t {
                 rlock: bindings::raw_spinlock {
@@ -808,28 +922,22 @@ impl rros_thread {
             bprio: 0,
             cprio: 0,
             wprio: 0,
-            boosters: list_head {
-                next: 0 as *mut list_head,
-                prev: 0 as *mut list_head,
-            },
-            wchan: rros_wait_channel::new(),
-            wait_next: list_head {
-                next: 0 as *mut list_head,
-                prev: 0 as *mut list_head,
-            },
-            wwake: None,
+            // boosters: 0 as *mut List<Arc<SpinLock<RrosMutex>>>,
+            wchan: core::ptr::null_mut(),
+            wait_next: Links::new(),
+            wwake: core::ptr::null_mut(),
             rtimer: None,
             ptimer: None,
             rrperiod: 0,
             state: 0,
             info: 0,
             // rq_next: unsafe{List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?)},
-            rq_next:0 as *mut Node<Arc<SpinLock<rros_thread>>>,
+            rq_next: None,
             // next: list_head {
             //     next: 0 as *mut list_head,
             //     prev: 0 as *mut list_head,
             // },
-            next:0 as *mut Node<Arc<SpinLock<rros_thread>>>, // kernel corrupted bug
+            next: 0 as *mut Node<Arc<SpinLock<rros_thread>>>, // kernel corrupted bug
             altsched: bindings::dovetail_altsched_context {
                 task: null_mut(),
                 active_mm: null_mut(),
@@ -842,10 +950,7 @@ impl rros_thread {
             inband_work: IrqWork::new(),
             stat: rros_stat::new(),
             u_window: None,
-            trackers: list_head {
-                next: 0 as *mut list_head,
-                prev: 0 as *mut list_head,
-            },
+            // trackers: 0 as *mut List<Arc<SpinLock<RrosMutex>>>,
             tracking_lock: bindings::hard_spinlock_t {
                 rlock: bindings::raw_spinlock {
                     raw_lock: bindings::arch_spinlock_t {
@@ -859,6 +964,7 @@ impl rros_thread {
                     wait_type_inner: 0,
                 },
             },
+            element:RrosElement::new()?,
             affinity: cpumask::CpumaskT::from_int(0 as u64),
             exited: bindings::completion {
                 done: 0,
@@ -896,24 +1002,125 @@ impl rros_thread {
             observable: None,
             name: "thread\0",
             oob_mm:null_mut(),
+            // tps: 0 as *mut tp::rros_tp_rq,
+            tp_link: None,
         })
     }
-}
+    }
 
-pub struct oob_mm_state{
-    flags:u32,
-    //todo
-    // struct list_head ptrace_sync;
-	// struct evl_wait_queue ptsync_barrier;
-}
-impl oob_mm_state{
-    fn new() -> Self{
-        oob_mm_state{
-            flags:0,
-        }
+    pub fn init(&mut self) -> Result<usize>{
+        self.lock = bindings::hard_spinlock_t {
+            rlock: bindings::raw_spinlock {
+                raw_lock: bindings::arch_spinlock_t {
+                    __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
+                        val: bindings::atomic_t { counter: 0 },
+                    },
+                },
+            },
+            dep_map: bindings::phony_lockdep_map {
+                wait_type_outer: 0,
+                wait_type_inner: 0,
+            },
+        };
+        self.rq= None;
+        self.base_class= None;
+        self.sched_class= None;
+        self.bprio= 0;
+        self.cprio= 0;
+        self.wprio= 0;
+        self.wchan= core::ptr::null_mut();
+        self.wait_next= Links::new();
+        self.wwake= core::ptr::null_mut();
+        self.rtimer= None;
+        self.ptimer= None;
+        self.rrperiod= 0;
+        self.state= 0;
+        self.info= 0;
+        self.rq_next= None;
+        self.next = 0 as *mut Node<Arc<SpinLock<rros_thread>>>; // kernel;
+        self.altsched = bindings::dovetail_altsched_context {
+            task: null_mut(),
+            active_mm: null_mut(),
+            borrowed_mm: false,
+        };
+        self.local_info = 0;
+        self.wait_data = null_mut();
+        self.poll_context = poll_context::new();
+        self.inband_disable_count = bindings::atomic_t { counter: 0 };
+        self.inband_work = IrqWork::new();
+        self.stat = rros_stat::new();
+        self.u_window = None;
+        self.tracking_lock = bindings::hard_spinlock_t {
+            rlock: bindings::raw_spinlock {
+                raw_lock: bindings::arch_spinlock_t {
+                    __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
+                        val: bindings::atomic_t { counter: 0 },
+                    },
+                },
+            },
+            dep_map: bindings::phony_lockdep_map {
+                wait_type_outer: 0,
+                wait_type_inner: 0,
+            },
+        };
+        self.element =RrosElement::new()?;
+        self.affinity = cpumask::CpumaskT::from_int(0 as u64);
+        self.exited = bindings::completion {
+            done: 0,
+            wait: bindings::swait_queue_head {
+                lock: bindings::raw_spinlock_t {
+                    raw_lock: bindings::arch_spinlock_t {
+                        __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
+                            val: bindings::atomic_t { counter: 0 },
+                            // __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1__bindgen_ty_1{
+                            // 	locked: 0,
+                            // 	pending: 0,
+                            //  },
+                            // __bindgen_anon_2: bindings::qspinlock__bindgen_ty_1__bindgen_ty_2{
+                            // 	locked_pending: 0,
+                            // 	tail: 0,
+                            // },
+                        },
+                    },
+                },
+                task_list: bindings::list_head {
+                    next: null_mut(),
+                    prev: null_mut(),
+                },
+            },
+        };
+        self.raised_cap = bindings::kernel_cap_t { cap: [0, 0] };
+        self.kill_next = list_head {
+            next: 0 as *mut list_head,
+            prev: 0 as *mut list_head,
+        };
+        self.ptsync_next = list_head {
+            next: 0 as *mut list_head,
+            prev: 0 as *mut list_head,
+        };
+        self.observable = None;
+        self.name = "thread\0";
+        self.oob_mm =null_mut();
+        // self.tps = 0 as *mut tp::rros_tp_rq;
+        self.tp_link = None;
+
+        Ok(0)
     }
 }
 
+// TODO: move oob_mm_state to c in the mm_info.h
+// pub struct oob_mm_state {
+//     flags: u32,
+//     //todo
+//     // struct list_head ptrace_sync;
+//     // struct evl_wait_queue ptsync_barrier;
+// }
+// impl oob_mm_state {
+//     fn new() -> Self {
+//         oob_mm_state { flags: 0 }
+//     }
+// }
+
 //#[derive(Copy,Clone)]
 pub struct poll_context {
     pub table: Option<Rc<RefCell<rros_poll_watchpoint>>>,
@@ -938,7 +1145,7 @@ pub struct rros_poll_watchpoint {
     pub events_polled: i32,
     pub pollval: rros_value,
     // pub wait:oob_poll_wait,  //ifdef
-    pub flag: Option<Rc<RefCell<rros_flag>>>,
+    pub flag: Option<Rc<RefCell<RrosFlag>>>,
     // pub filp:*mut file,
     pub node: rros_poll_node,
 }
@@ -970,51 +1177,6 @@ impl rros_value {
     }
 }
 
-//#[derive(Copy,Clone)]
-pub struct rros_flag {
-    pub wait: rros_wait_queue,
-    pub raised: bool,
-}
-impl rros_flag {
-    fn new() -> Self {
-        rros_flag {
-            wait: rros_wait_queue::new(),
-            raised: false,
-        }
-    }
-}
-
-//#[derive(Copy,Clone)]
-pub struct rros_wait_queue {
-    pub flags: i32,
-    pub clock: Option<Rc<RefCell<rros_clock>>>,
-    pub wchan: rros_wait_channel,
-    pub lock: bindings::hard_spinlock_t,
-}
-impl rros_wait_queue {
-    pub fn new() -> Self {
-        rros_wait_queue {
-            flags: 0,
-            clock: None,
-            wchan: rros_wait_channel::new(),
-            lock: bindings::hard_spinlock_t {
-                rlock: bindings::raw_spinlock {
-                    raw_lock: bindings::arch_spinlock_t {
-                        __bindgen_anon_1: bindings::qspinlock__bindgen_ty_1 {
-                            val: bindings::atomic_t { counter: 0 },
-                        },
-                    },
-                },
-                dep_map: bindings::phony_lockdep_map {
-                    wait_type_outer: 0,
-                    wait_type_inner: 0,
-                },
-            },
-        }
-    }
-}
-
-//#[derive(Copy,Clone)]
 pub struct rros_poll_node {
     pub next: list_head,
 }
@@ -1028,41 +1190,6 @@ impl rros_poll_node {
         }
     }
 }
-
-// pub struct stat{
-//     struct rros_counter isw;
-//     struct rros_counter csw;
-//     struct rros_counter sc;
-//     struct rros_counter rwa;
-//     struct rros_account account;
-//     struct rros_account lastperiod;
-// }
-
-//#[derive(Copy,Clone)]
-pub struct rros_wait_channel {
-    pub reorder_wait:
-        Option<fn(waiter: Arc<SpinLock<rros_thread>>, originator: Arc<SpinLock<rros_thread>>) -> i32>,
-    pub follow_depend: Option<
-        fn(wchan: Rc<RefCell<rros_wait_channel>>, originator: Rc<RefCell<rros_thread>>) -> i32,
-    >,
-    pub wait_list: list_head,
-    pub name: *const c_types::c_char,
-}
-impl rros_wait_channel {
-    pub fn new() -> Self {
-        rros_wait_channel {
-            reorder_wait: None,
-            follow_depend: None,
-            wait_list: list_head {
-                next: 0 as *mut list_head,
-                prev: 0 as *mut list_head,
-            },
-            name: null(),
-        }
-    }
-}
-
-//#[derive(Copy,Clone)]
 pub struct rros_user_window {
     pub state: __u32,
     pub info: __u32,
@@ -1078,12 +1205,12 @@ impl rros_user_window {
     }
 }
 
-//#[derive(Copy,Clone)]
+// #[derive(Copy,Clone)]
 pub struct rros_observable {
     // pub element:rros_element,
     pub observers: list_head,
     pub flush_list: list_head,
-    pub oob_wait: rros_wait_queue,
+    pub oob_wait: RrosWaitQueue,
     pub inband_wait: bindings::wait_queue_head_t,
     pub poll_head: rros_poll_head,
     pub wake_irqwork: bindings::irq_work,
@@ -1103,7 +1230,7 @@ impl rros_observable {
                 next: 0 as *mut list_head,
                 prev: 0 as *mut list_head,
             },
-            oob_wait: rros_wait_queue::new(),
+            oob_wait: unsafe{RrosWaitQueue::new(&mut RROS_MONO_CLOCK as *mut RrosClock, RROS_WAIT_PRIO as i32)},
             inband_wait: bindings::wait_queue_head_t {
                 lock: bindings::spinlock_t {
                     _bindgen_opaque_blob: 0,
@@ -1195,7 +1322,7 @@ pub struct rros_init_thread_attr {
     pub observable: Option<Rc<RefCell<rros_observable>>>,
     pub flags: i32,
     pub sched_class: Option<&'static rros_sched_class>,
-    pub sched_param: Option<Rc<RefCell<rros_sched_param>>>,
+    pub sched_param: Option<Arc<SpinLock<rros_sched_param>>>,
 }
 impl rros_init_thread_attr {
     pub fn new() -> Self {
@@ -1208,108 +1335,313 @@ impl rros_init_thread_attr {
         }
     }
 }
-
-pub fn rros_init_sched() -> Result<usize> {
-    unsafe{rros_runqueues =
-        alloc_per_cpu(size_of::<rros_rq>() as usize, align_of::<rros_rq>() as usize) as *mut rros_rq; 
-        if rros_runqueues == 0 as *mut rros_rq {
-            return Err(kernel::Error::ENOMEM);
-        }
-    }
-    
-    let mut rq_ptr = this_rros_rq();
-    unsafe {
-        // pr_info!("{:p}\n", &(*rq_ptr).local_flags);
+fn init_inband_timer(rq_ptr: *mut rros_rq) -> Result<usize> {
+    unsafe{
         (*rq_ptr) = rros_rq::new()?;
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned =  Pin::new_unchecked(&mut x);
         spinlock_init!(pinned, "inband_timer");
         (*rq_ptr).inband_timer =  Some(Arc::try_new(x)?);
+    }
+    Ok(0)
+}
 
+fn init_rrbtimer(rq_ptr: *mut rros_rq)  -> Result<usize> {
+    unsafe{
         let mut y = SpinLock::new(RrosTimer::new(1));
         let pinned = Pin::new_unchecked(&mut y);
         spinlock_init!(pinned, "rrb_timer");
         (*rq_ptr).rrbtimer =  Some(Arc::try_new(y)?);
+    }
+    Ok(0)
+}
 
-        // let mut y = SpinLock::new(RrosTimer::new(1));
-        // let pinned = Pin::new_unchecked(&mut y);
-        // spinlock_init!(pinned, "rrb_timer");
-        // (*rq_ptr).rrbtimer =  Some(Arc::try_new(y)?);
-
-        // let pinned = Pin::new_unchecked(&mut (*rq_ptr).root_thread.unwrap());
-        // spinlock_init!(pinned, "root_thread");
-        
-        let mut thread = SpinLock::new(rros_thread::new()?);
-        let pinned = Pin::new_unchecked(&mut thread);
+fn init_root_thread(rq_ptr: *mut rros_rq) -> Result<usize> {
+    unsafe{
+        let mut tmp = Arc::<SpinLock<rros_thread>>::try_new_uninit()?;
+
+        // let mut thread = SpinLock::new(rros_thread::new()?);
+        // let pinned = Pin::new_unchecked(&mut thread);
+        // spinlock_init!(pinned, "rros_threads");
+        // Arc::get_mut(&mut tmp).unwrap().write(thread);
+
+        let tmp = tmp.assume_init();
+        (*rq_ptr).root_thread =  Some(tmp);//Arc::try_new(thread)?
+        (*(*rq_ptr).root_thread.as_mut().unwrap().locked_data().get()).init()?;
+        let pinned = Pin::new_unchecked(&mut *(Arc::into_raw( (*rq_ptr).root_thread.clone().unwrap()) as *mut SpinLock<rros_thread>));
+        // &mut *Arc::into_raw( *(*rq_ptr).root_thread.clone().as_mut().unwrap()) as &mut SpinLock<rros_thread>
         spinlock_init!(pinned, "rros_threads");
-        (*rq_ptr).root_thread =  Some(Arc::try_new(thread)?);
-        
+        // (*rq_ptr).root_thread.as_mut().unwrap().assume_init();
+    }
+    Ok(0)
+}
+
+fn init_rtimer(rq_ptr: *mut rros_rq) -> Result<usize> {
+    unsafe {
         let mut r = SpinLock::new(rros_timer::new(1));
         let pinned_r =  Pin::new_unchecked(&mut r);
         spinlock_init!(pinned_r, "rtimer");
-        
+        (*rq_ptr).root_thread.as_ref().unwrap().lock().rtimer = Some(Arc::try_new(r)?);
+    }
+    Ok(0)
+}
+
+fn init_ptimer(rq_ptr: *mut rros_rq) -> Result<usize> {
+    unsafe {
         let mut p = SpinLock::new(rros_timer::new(1));
         let pinned_p =  Pin::new_unchecked(&mut p);
         spinlock_init!(pinned_p, "ptimer");
-
-        (*rq_ptr).root_thread.as_ref().unwrap().lock().rtimer = Some(Arc::try_new(r)?);
         (*rq_ptr).root_thread.as_ref().unwrap().lock().ptimer = Some(Arc::try_new(p)?);
+    }
+    Ok(0)
+}
 
-        // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq_ptr).root_thread.clone().unwrap()));
+fn init_rq_ptr(rq_ptr: *mut rros_rq)-> Result<usize>  {
+    unsafe {
+            init_inband_timer(rq_ptr)?;
+            init_rrbtimer(rq_ptr)?;
+            init_root_thread(rq_ptr)?;
+            init_rtimer(rq_ptr)?;
+            init_ptimer(rq_ptr)?;
+                // pr_info!("{:p}\n", &(*rq_ptr).local_flags);
+                // (*rq_ptr) = rros_rq::new()?;
+                // let mut x = SpinLock::new(RrosTimer::new(1));
+                // let pinned =  Pin::new_unchecked(&mut x);
+                // spinlock_init!(pinned, "inband_timer");
+                // (*rq_ptr).inband_timer =  Some(Arc::try_new(x)?);
+        
+                // let mut y = SpinLock::new(RrosTimer::new(1));
+                // let pinned = Pin::new_unchecked(&mut y);
+                // spinlock_init!(pinned, "rrb_timer");
+                // (*rq_ptr).rrbtimer =  Some(Arc::try_new(y)?);
         
+                // let mut y = SpinLock::new(RrosTimer::new(1));
+                // let pinned = Pin::new_unchecked(&mut y);
+                // spinlock_init!(pinned, "rrb_timer");
+                // (*rq_ptr).rrbtimer =  Some(Arc::try_new(y)?);
+        
+                // let pinned = Pin::new_unchecked(&mut (*rq_ptr).root_thread.unwrap());
+                // spinlock_init!(pinned, "root_thread");
+                
+            // let mut thread = SpinLock::new(rros_thread::new()?);
+            // let pinned = Pin::new_unchecked(&mut thread);
+            // spinlock_init!(pinned, "rros_threads");
+            // (*rq_ptr).root_thread =  Some(Arc::try_new(thread)?);
+            
+            // let mut r = SpinLock::new(rros_timer::new(1));
+            // let pinned_r =  Pin::new_unchecked(&mut r);
+            // spinlock_init!(pinned_r, "rtimer");
+            // (*rq_ptr).root_thread.as_ref().unwrap().lock().rtimer = Some(Arc::try_new(r)?);
+
+            // let mut p = SpinLock::new(rros_timer::new(1));
+            // let pinned_p =  Pin::new_unchecked(&mut p);
+            // spinlock_init!(pinned_p, "ptimer");
+            // (*rq_ptr).root_thread.as_ref().unwrap().lock().ptimer = Some(Arc::try_new(p)?);
     }
-    // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq_ptr).root_thread.clone().unwrap()));
-    
-    unsafe{
-        let mut x = SpinLock::new(rros_thread::new()?);
+    Ok(0)
+}
 
-        let pinned =  Pin::new_unchecked(&mut x);
-        spinlock_init!(pinned, "rros_runnable_thread");
-        (*rq_ptr).fifo.runnable.head =  Some(List::new(Arc::try_new(x)?));
-    // unsafe{(*rq_ptr).fifo.runnable.head = Some(List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));}
+
+fn init_rq_ptr_inband_timer(rq_ptr: *mut rros_rq)-> Result<usize>  {
+    unsafe {
+        let mut tmp = Arc::<SpinLock<rros_thread>>::try_new_uninit()?;
+
+        // let mut thread = SpinLock::new(rros_thread::new()?);
+        // let pinned = Pin::new_unchecked(&mut thread);
+        // spinlock_init!(pinned, "rros_threads");
+        // Arc::get_mut(&mut tmp).unwrap().write(thread);
+
+        let tmp = tmp.assume_init();
+        (*rq_ptr).fifo.runnable.head =  Some(List::new(tmp));//Arc::try_new(thread)?
+        (*(*rq_ptr).fifo.runnable.head.as_mut().unwrap().head.value.locked_data().get()).init()?;
+        let pinned = Pin::new_unchecked(&mut *(Arc::into_raw( (*rq_ptr).fifo.runnable.head.as_mut().unwrap().head.value.clone()) as *mut SpinLock<rros_thread>));
+        // &mut *Arc::into_raw( *(*rq_ptr).root_thread.clone().as_mut().unwrap()) as &mut SpinLock<rros_thread>
+        spinlock_init!(pinned, "rros_threads");
+
+        // let mut x = SpinLock::new(rros_thread::new()?);
+
+        // let pinned = Pin::new_unchecked(&mut x);
+        // spinlock_init!(pinned, "rros_runnable_thread");
+        // (*rq_ptr).fifo.runnable.head = Some(List::new(Arc::try_new(x)?));
+        // unsafe{(*rq_ptr).fifo.runnable.head = Some(List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));}
+    }
+    Ok(0)
+}
+
+pub struct rros_sched_attrs {
+	pub sched_policy:i32,
+	pub sched_priority:i32,
+	// union {
+	// 	struct __evl_rr_param rr;
+	// 	struct __evl_quota_param quota;
+	// 	struct __evl_tp_param tp;
+	// } sched_u;
+    pub tp_partition:i32,
+}
+impl rros_sched_attrs{
+    pub fn new() -> Self{
+        rros_sched_attrs{
+            sched_policy:0,
+            sched_priority:0,
+            tp_partition:0,
+        }
+    }
+}
+
+use kernel::prelude::*;
+use kernel::cpumask::{online_cpus, possible_cpus};
+
+pub fn rros_init_sched() -> Result<usize> {
+    unsafe {
+        rros_runqueues = alloc_per_cpu(
+            size_of::<rros_rq>() as usize,
+            align_of::<rros_rq>() as usize,
+        ) as *mut rros_rq;
+        if rros_runqueues == 0 as *mut rros_rq {
+            return Err(kernel::Error::ENOMEM);
+        }
+    }
+
+    for cpu in possible_cpus() {
+        pr_info!("{}\n", cpu);
+
+        // let mut rq_ptr = this_rros_rq();
+        let mut rq_ptr = unsafe{kernel::percpu_defs::per_cpu(rros_runqueues, cpu as i32)};
+        unsafe {
+            init_rq_ptr(rq_ptr)?;
+            // // pr_info!("{:p}\n", &(*rq_ptr).local_flags);
+            // (*rq_ptr) = rros_rq::new()?;
+            // let mut x = SpinLock::new(RrosTimer::new(1));
+            // let pinned = Pin::new_unchecked(&mut x);
+            // spinlock_init!(pinned, "inband_timer");
+            // (*rq_ptr).inband_timer = Some(Arc::try_new(x)?);
+
+            // let mut y = SpinLock::new(RrosTimer::new(1));
+            // let pinned = Pin::new_unchecked(&mut y);
+            // spinlock_init!(pinned, "rrb_timer");
+            // (*rq_ptr).rrbtimer = Some(Arc::try_new(y)?);
+
+            // // let mut y = SpinLock::new(RrosTimer::new(1));
+            // // let pinned = Pin::new_unchecked(&mut y);
+            // // spinlock_init!(pinned, "rrb_timer");
+            // // (*rq_ptr).rrbtimer =  Some(Arc::try_new(y)?);
+
+            // // let pinned = Pin::new_unchecked(&mut (*rq_ptr).root_thread.unwrap());
+            // // spinlock_init!(pinned, "root_thread");
+
+            // let mut thread = SpinLock::new(rros_thread::new()?);
+            // let pinned = Pin::new_unchecked(&mut thread);
+            // spinlock_init!(pinned, "rros_threads");
+            // (*rq_ptr).root_thread = Some(Arc::try_new(thread)?);
+
+            // let mut r = SpinLock::new(rros_timer::new(1));
+            // let pinned_r = Pin::new_unchecked(&mut r);
+            // spinlock_init!(pinned_r, "rtimer");
+
+            // let mut p = SpinLock::new(rros_timer::new(1));
+            // let pinned_p = Pin::new_unchecked(&mut p);
+            // spinlock_init!(pinned_p, "ptimer");
+
+            // (*rq_ptr).root_thread.as_ref().unwrap().lock().rtimer = Some(Arc::try_new(r)?);
+            // (*rq_ptr).root_thread.as_ref().unwrap().lock().ptimer = Some(Arc::try_new(p)?);
+
+            // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq_ptr).root_thread.clone().unwrap()));
+        }
+        // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq_ptr).root_thread.clone().unwrap()));
+
+        init_rq_ptr_inband_timer(rq_ptr)?;
+        // unsafe {
+        //     let mut x = SpinLock::new(rros_thread::new()?);
+
+        //     let pinned = Pin::new_unchecked(&mut x);
+        //     spinlock_init!(pinned, "rros_runnable_thread");
+        //     (*rq_ptr).fifo.runnable.head = Some(List::new(Arc::try_new(x)?));
+        //     // unsafe{(*rq_ptr).fifo.runnable.head = Some(List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));}
+        // }
     }
+
     let ret = 0;
-    let cpu = 0;
+    // let cpu = 0;
     let ret = register_classes();
-	match ret{
-		Ok(_) => pr_info!("register_classes success!"),
-		Err(e) =>{
-			pr_warn!("register_classes error!");
-			return Err(e);
-		},
-	}
+    match ret {
+        Ok(_) => pr_info!("register_classes success!"),
+        Err(e) => {
+            pr_warn!("register_classes error!");
+            return Err(e);
+        }
+    }
+
     // for_each_online_cpu()
-    unsafe{if rros_sched_topmost == 0 as *mut rros_sched_class{
-        pr_info!("rros_sched_topmost is 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
-    }}
-    let ret = unsafe{init_rq(rq_ptr, cpu,)};
-	match ret{
-		Ok(_) => pr_info!("init_rq success!"),
-		Err(e) =>{
-			pr_warn!("init_rq error!");
-			return Err(e);
-		},
-	}
+    #[cfg(CONFIG_SMP)]
+    for cpu in online_cpus(){
+        unsafe {
+            if rros_sched_topmost == 0 as *mut rros_sched_class {
+                pr_info!("rros_sched_topmost is 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+            }
+        }
+        let mut rq_ptr = unsafe{kernel::percpu_defs::per_cpu(rros_runqueues, cpu as i32)};
+        // TODO: fix the i32 problem
+        let ret = unsafe { init_rq(rq_ptr, cpu as i32) };
+        match ret {
+            Ok(_) => pr_info!("init_rq success!"),
+            Err(e) => {
+                pr_warn!("init_rq error!");
+                return Err(e);
+            }
+        }
+    }
     // ret = bindings::__request_percpu_irq();
-	pr_info!("sched init success!");
+    pr_info!("sched init success!");
     Ok(0)
 }
 
+/* oob stalled. */
+// #[cfg(CONFIG_SMP)]
+// unsafe extern "C" fn oob_reschedule_interrupt(irq: i32 , dev_id:*mut c_types::c_void) -> bindings::irqreturn_t {
+// 	// trace_evl_reschedule_ipi(this_evl_rq());
+
+// 	/* Will reschedule from evl_exit_irq(). */
+
+// 	return bindings::IRQ_HANDLED;
+// }
+
+// #[cfg(not(CONFIG_SMP))]
+// unsafe extern "C" fn oob_reschedule_interrupt(irq: i32 , dev_id:*mut c_types::c_void) -> bindings::irqreturn_t {
+// 	// trace_evl_reschedule_ipi(this_evl_rq());
+
+// 	/* Will reschedule from evl_exit_irq(). */
+
+// 	NULL;
+// }
+
 fn register_classes() -> Result<usize> {
     // let rros_sched_idle = unsafe{idle::rros_sched_idle};
     // let rros_sched_fifo = unsafe{fifo::rros_sched_fifo};
-    let res = unsafe{register_one_class(&mut idle::rros_sched_idle, 1,)};
-	pr_info!("after one register_one_class,topmost = {:p}",rros_sched_topmost);
-    match res{
-		Ok(_) => pr_info!("register_one_class(idle) success!"),
-		Err(e) =>{
-			pr_warn!("register_one_class(idle) error!");
-			return Err(e);
-		},
-	}
+    let res = unsafe { register_one_class(&mut idle::rros_sched_idle, 1) };
+    pr_info!(
+        "after one register_one_class,topmost = {:p}",
+        rros_sched_topmost
+    );
+    match res {
+        Ok(_) => pr_info!("register_one_class(idle) success!"),
+        Err(e) => {
+            pr_warn!("register_one_class(idle) error!");
+            return Err(e);
+        }
+    }
     // register_one_class(&mut rros_sched_weak);
+
+    // let res = unsafe{register_one_class(&mut tp::rros_sched_tp, 2,)};
+	// pr_info!("after two register_one_class,topmost = {:p}",rros_sched_topmost);
+    // match res{
+	// 	Ok(_) => pr_info!("register_one_class(tp) success!"),
+	// 	Err(e) =>{
+	// 		pr_warn!("register_one_class(tp) error!");
+	// 		return Err(e);
+	// 	},
+	// }
     let res = unsafe{register_one_class(&mut fifo::rros_sched_fifo, 3,)};
-	pr_info!("after two register_one_class,topmost = {:p}",rros_sched_topmost);
+	pr_info!("after three register_one_class,topmost = {:p}",rros_sched_topmost);
     match res{
 		Ok(_) => pr_info!("register_one_class(fifo) success!"),
 		Err(e) =>{
@@ -1321,35 +1653,34 @@ fn register_classes() -> Result<usize> {
 }
 
 // todo等全局变量实现后，去掉index和topmost
-fn register_one_class(
-    sched_class: &mut rros_sched_class,
-    index:i32,
-) -> Result<usize> {
+fn register_one_class(sched_class: &mut rros_sched_class, index: i32) -> Result<usize> {
     // let mut sched_class_lock = sched_class.lock();
     // let index = sched_class_lock.flag;
     // sched_class_lock.next = Some(rros_sched_topmost);
     // unsafe{sched_class.unlock()};
     unsafe{sched_class.next = rros_sched_topmost};
-    if index ==1{
-        unsafe{rros_sched_topmost = &mut idle::rros_sched_idle as *mut rros_sched_class};
-    }
-    else if index ==3{
+    if index ==1 {
         unsafe{rros_sched_topmost = &mut idle::rros_sched_idle as *mut rros_sched_class};
+    //} else if index ==2 {
+      //  unsafe{rros_sched_topmost = &mut tp::rros_sched_tp as *mut rros_sched_class}; // FIXME: tp取消注释
+        // unsafe{rros_sched_topmost  = 0 as *mut rros_sched_class};
+    } else if index ==3 {
+        unsafe{rros_sched_topmost = &mut fifo::rros_sched_fifo as *mut rros_sched_class};
     }
     pr_info!("in register_one_class,rros_sched_topmost = {:p}",rros_sched_topmost);
     if index != 3 {
-        if index == 1{
+        if index == 1 {
             unsafe{rros_sched_lower = &mut idle::rros_sched_idle as *mut rros_sched_class};
         }
+        // if index == 2 {
+        //     unsafe{rros_sched_lower = &mut tp::rros_sched_tp as *mut rros_sched_class};// FIXME: tp实现后取消注释
+        // }
     }
     Ok(0)
 }
 
 // todo等全局变量实现后，去掉topmost
-fn init_rq(
-    rq: *mut rros_rq,
-    cpu: i32,
-) -> Result<usize> {
+fn init_rq(rq: *mut rros_rq, cpu: i32) -> Result<usize> {
     let mut iattr = rros_init_thread_attr::new();
     let name_fmt: &'static CStr = c_str!("ROOT");
     // let mut rq_ptr = rq.borrow_mut();
@@ -1367,14 +1698,14 @@ fn init_rq(
         )
     };
 
-    let mut p = unsafe{rros_sched_topmost};
+    let mut p = unsafe { rros_sched_topmost };
 
-    pr_info!("before11111111111111111111111111111111111111");
+    // pr_info!("before11111111111111111111111111111111111111");
     while p != 0 as *mut rros_sched_class {
-        pr_info!("p!=0!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
-        if unsafe{(*p).sched_init != None} {
+        // pr_info!("p!=0!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
+        if unsafe { (*p).sched_init != None } {
             let func;
-            unsafe{
+            unsafe {
                 match (*p).sched_init {
                     Some(f) => func = f,
                     None => {
@@ -1385,22 +1716,38 @@ fn init_rq(
             }
             func(rq);
         }
-        unsafe{p = (*p).next};
+        unsafe { p = (*p).next };
     }
     pr_info!("after11111111111111111111111111111111111111");
 
-    unsafe{(*rq).flags = 0};  
-    unsafe{(*rq).local_flags = RQ_IDLE};
+    unsafe { (*rq).flags = 0 };
+    unsafe { (*rq).local_flags = RQ_IDLE };
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq).root_thread.clone().unwrap()));
-    let a = unsafe{(*rq).root_thread.clone()};
+    let a = unsafe { (*rq).root_thread.clone() };
     if a.is_some() {
-        unsafe{(*rq).curr = a.clone()};
+        unsafe { (*rq).curr = a.clone() };
     }
     // pr_info!("The state is {:}\n", (*rq).get_curr().lock().state);
-    unsafe{rros_init_timer_on_rq((*rq).get_inband_timer(), unsafe{&mut clock::RROS_MONO_CLOCK}, None,
-    		rq, c_str!("tick"), RROS_TIMER_IGRAVITY)};
-    unsafe{rros_init_timer_on_rq((*rq).get_rrbtimer(), unsafe{&mut clock::RROS_MONO_CLOCK}, Some(roundrobin_handler),
-            rq, c_str!("rrb"), RROS_TIMER_IGRAVITY)};
+    unsafe {
+        rros_init_timer_on_rq(
+            (*rq).get_inband_timer(),
+            unsafe { &mut clock::RROS_MONO_CLOCK },
+            None,
+            rq,
+            c_str!("tick"),
+            RROS_TIMER_IGRAVITY,
+        )
+    };
+    unsafe {
+        rros_init_timer_on_rq(
+            (*rq).get_rrbtimer(),
+            unsafe { &mut clock::RROS_MONO_CLOCK },
+            Some(roundrobin_handler),
+            rq,
+            c_str!("rrb"),
+            RROS_TIMER_IGRAVITY,
+        )
+    };
     // rros_set_timer_name(&rq->inband_timer, rq->proxy_timer_name);
     // rros_init_timer_on_rq(&rq->rrbtimer, &rros_mono_clock, roundrobin_handler,
     // 		rq, RROS_TIMER_IGRAVITY);
@@ -1410,8 +1757,10 @@ fn init_rq(
     iattr.flags = T_ROOT as i32;
     iattr.affinity = cpumask_of(cpu);
     // todo等全局变量
-    unsafe{iattr.sched_class = Some(& idle::rros_sched_idle);}
-	// 下面多数注释的都是由于evl_init_thread未完成导致的
+    unsafe {
+        iattr.sched_class = Some(&idle::rros_sched_idle);
+    }
+    // 下面多数注释的都是由于evl_init_thread未完成导致的
     // let sched_param_clone;
     // let mut sched_param_ptr;
     // match iattr.sched_param {
@@ -1421,34 +1770,34 @@ fn init_rq(
     // sched_param_ptr = sched_param_clone.borrow_mut();
     // sched_param_ptr.idle.prio = idle::RROS_IDLE_PRIO;
 
-    let sched_param =  Rc::try_new(RefCell::new(rros_sched_param::new()))?;
-    sched_param.borrow_mut().fifo.prio = idle::RROS_IDLE_PRIO;
+    let sched_param =  unsafe{Arc::try_new(SpinLock::new(rros_sched_param::new()))?};
+    unsafe{(*sched_param.locked_data().get()).fifo.prio = idle::RROS_IDLE_PRIO};
     iattr.sched_param = Some(sched_param);
 
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq).root_thread.clone().unwrap()));
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&(*rq).root_thread.clone().unwrap()));
-    unsafe{rros_init_thread(&(*rq).root_thread.clone(), iattr, rq, name_fmt)};//c_str!("0").as_char_ptr()
-    
-    unsafe{let next_add = (*rq).root_thread.clone().unwrap().lock().deref_mut() as *mut rros_thread;
-    // pr_info!("the root thread add is  next_add {:p}", next_add);
+    unsafe { rros_init_thread(&(*rq).root_thread.clone(), iattr, rq, name_fmt) }; //c_str!("0").as_char_ptr()
+
+    unsafe {
+        let next_add = (*rq).root_thread.clone().unwrap().lock().deref_mut() as *mut rros_thread;
+        // pr_info!("the root thread add is  next_add {:p}", next_add);
     }
 
     // pr_info!("The state is {:}\n", (*rq).get_curr().lock().state);
     let rq_root_thread_2;
-    unsafe{match (*rq).root_thread.clone() {
-        Some(rt) => rq_root_thread_2 = rt.clone(),
-        None => {
-				pr_warn!("use rq.root_thread error");
-				return Err(kernel::Error::EINVAL);
-			}
-    }}
-    // let mut rq_root_thread_lock = rq_root_thread_2.lock();
-    let add = &mut rq_root_thread_2.lock().deref_mut().altsched as *mut bindings::dovetail_altsched_context;
     unsafe {
-        bindings::dovetail_init_altsched(
-            add,
-        )
-    };
+        match (*rq).root_thread.clone() {
+            Some(rt) => rq_root_thread_2 = rt.clone(),
+            None => {
+                pr_warn!("use rq.root_thread error");
+                return Err(kernel::Error::EINVAL);
+            }
+        }
+    }
+    // let mut rq_root_thread_lock = rq_root_thread_2.lock();
+    let add = &mut rq_root_thread_2.lock().deref_mut().altsched
+        as *mut bindings::dovetail_altsched_context;
+    unsafe { bindings::dovetail_init_altsched(add) };
 
     // let mut rros_thread_list = list_head::new();
     // list_add_tail(
@@ -1459,22 +1808,26 @@ fn init_rq(
     Ok(0)
 }
 
-fn rros_sched_tick(rq :*mut rros_rq) {
+fn rros_sched_tick(rq: *mut rros_rq) {
     let curr;
-    unsafe{curr = (*rq).get_curr();}
+    unsafe {
+        curr = (*rq).get_curr();
+    }
     let sched_class = curr.lock().sched_class.clone().unwrap();
     let flags = curr.lock().base_class.clone().unwrap().flag;
     let state = curr.lock().state;
     let a = sched_class.flag == flags;
     let b = !sched_class.sched_tick.is_none();
-    let c = state & (RROS_THREAD_BLOCK_BITS|T_RRB) == T_RRB;
+    let c = state & (RROS_THREAD_BLOCK_BITS | T_RRB) == T_RRB;
     let d = rros_preempt_count() == 0;
     // pr_info!("The current root state {}", (*rq).root_thread.as_ref().unwrap().lock().state);
-    // pr_info!("abcd {} {} {} {} state{} 2208 ", a, b, c, d, state);
+    // pr_info!("abcd {} {} {} {} state{} 2208\n", a, b, c, d, state);
     if a && b && c && d {
-            unsafe{sched_class.sched_tick.unwrap()(Some(rq));}
-            // sched_class->sched_tick(rq);
+        unsafe {
+            sched_class.sched_tick.unwrap()(Some(rq));
         }
+        // sched_class->sched_tick(rq);
+    }
 }
 
 pub fn roundrobin_handler(timer: *mut RrosTimer) {
@@ -1519,91 +1872,122 @@ pub fn rros_set_effective_thread_priority(
     Ok(0)
 }
 
-fn rros_calc_weighted_prio(sched_class: &'static rros_sched_class, prio: i32) -> i32 {
-    return prio + sched_class.weight;
+pub fn rros_track_priority(thread:Arc<SpinLock<rros_thread>>,p:Arc<SpinLock<rros_sched_param>>) -> Result<usize>{
+    unsafe{
+        let func;
+        match (*thread.locked_data().get()).sched_class.unwrap().sched_trackprio{
+            Some(f) => func = f,
+            None => {
+                pr_warn!("rros_get_schedparam: sched_trackprio function error");
+                return Err(kernel::Error::EINVAL);
+            }
+        };
+        func(Some(thread.clone()), Some(p.clone()));
+
+        let sched_class = (*thread.locked_data().get()).sched_class.unwrap();
+        let prio = (*thread.locked_data().get()).cprio;
+        (*thread.locked_data().get()).wprio = rros_calc_weighted_prio(sched_class,  prio);
+    }
+    Ok(0)
 }
 
+fn rros_ceil_priority(thread:Arc<SpinLock<rros_thread>>, prio:i32) -> Result<usize>{
+	unsafe{
+        let func;
+        match (*thread.locked_data().get()).sched_class.unwrap().sched_ceilprio{
+            Some(f) => func = f,
+            None => {
+                pr_warn!("rros_ceil_priority:sched_ceilprio function error");
+                return Err(kernel::Error::EINVAL);
+            }
+        }
+        func(thread.clone(), prio);
+        let sched_class = (*thread.locked_data().get()).sched_class.unwrap();
+        let prio = (*thread.locked_data().get()).cprio;
+        (*thread.locked_data().get()).wprio = rros_calc_weighted_prio(sched_class, prio);
+    }
+    Ok(0)
+}
+
+pub fn rros_calc_weighted_prio(sched_class: &'static rros_sched_class, prio: i32) -> i32 {
+    return prio + sched_class.weight;
+}
 
-pub fn rros_putback_thread(thread:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+pub fn rros_putback_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
     let mut state = thread.lock().state;
-	if state & T_READY != 0{
-		rros_dequeue_thread(thread.clone());
-	}
-	else{
+    if state & T_READY != 0 {
+        rros_dequeue_thread(thread.clone());
+    } else {
         thread.lock().state |= T_READY;
-	}
-	rros_enqueue_thread(thread.clone());
+    }
+    rros_enqueue_thread(thread.clone());
     let rq = thread.lock().rq;
-	rros_set_resched(rq);
+    rros_set_resched(rq);
     Ok(0)
 }
 
 //未测试，应该可行
-pub fn rros_dequeue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize>{
-	let sched_class;
-	match thread.lock().sched_class.clone(){
-		Some(c) => sched_class = c,
-		None => return Err(kernel::Error::EINVAL),
-	}
-	if sched_class.flag == 3{
-		fifo::__rros_dequeue_fifo_thread(thread.clone());
-	}
-	else if sched_class.flag != 1{
-		let func;
-		match sched_class.sched_dequeue{
-			Some(f) => func = f,
-			None => return Err(kernel::Error::EINVAL),
-		}
-		func(thread.clone());
-	}
-	Ok(0)
+pub fn rros_dequeue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+    let sched_class;
+    match thread.lock().sched_class.clone() {
+        Some(c) => sched_class = c,
+        None => return Err(kernel::Error::EINVAL),
+    }
+    if sched_class.flag == 3 {
+        fifo::__rros_dequeue_fifo_thread(thread.clone());
+    } else if sched_class.flag != 1 {
+        let func;
+        match sched_class.sched_dequeue {
+            Some(f) => func = f,
+            None => return Err(kernel::Error::EINVAL),
+        }
+        func(thread.clone());
+    }
+    Ok(0)
 }
 
 //未测试，应该可行
-pub fn rros_enqueue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize>{
-	let sched_class;
-	match thread.lock().sched_class.clone(){
-		Some(c) => sched_class = c,
-		None => return Err(kernel::Error::EINVAL),
-	}
-	if sched_class.flag == 3{
-		fifo::__rros_enqueue_fifo_thread(thread.clone());
-	}
-	else if sched_class.flag != 1{
-		let func;
-		match sched_class.sched_enqueue{
-			Some(f) => func = f,
-			None => return Err(kernel::Error::EINVAL),
-		}
-		func(thread.clone());
-	}
-	Ok(0)
+pub fn rros_enqueue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+    let sched_class;
+    match thread.lock().sched_class.clone() {
+        Some(c) => sched_class = c,
+        None => return Err(kernel::Error::EINVAL),
+    }
+    if sched_class.flag == 3 {
+        fifo::__rros_enqueue_fifo_thread(thread.clone());
+    } else if sched_class.flag != 1 {
+        let func;
+        match sched_class.sched_enqueue {
+            Some(f) => func = f,
+            None => return Err(kernel::Error::EINVAL),
+        }
+        func(thread.clone());
+    }
+    Ok(0)
 }
 
 //未测试，应该可行
-pub fn rros_requeue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize>{
-	let sched_class;
-	unsafe{
-        match (*thread.locked_data().get()).sched_class.clone(){
+pub fn rros_requeue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+    let sched_class;
+    unsafe {
+        match (*thread.locked_data().get()).sched_class.clone() {
             Some(c) => sched_class = c,
             None => return Err(kernel::Error::EINVAL),
         }
     }
-	if sched_class.flag == 3{
-		fifo::__rros_requeue_fifo_thread(thread.clone());
-    }
-	else if sched_class.flag != 1{
+    if sched_class.flag == 3 {
+        fifo::__rros_requeue_fifo_thread(thread.clone());
+    } else if sched_class.flag != 1 {
         let func;
-        match sched_class.sched_requeue{
+        match sched_class.sched_requeue {
             Some(f) => func = f,
             None => return Err(kernel::Error::EINVAL),
         }
-		func(thread.clone());
+        func(thread.clone());
     }
     Ok(0)
 }
 
-
 // fn rros_need_resched(rq: *mut rros_rq) -> bool {
 //     unsafe{(*rq).flags & RQ_SCHED != 0x0}
 // }
@@ -1616,15 +2000,15 @@ pub fn rros_requeue_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize>{
 fn test_resched(rq: *mut rros_rq) -> bool {
     let need_resched = rros_need_resched(rq);
     // #ifdef CONFIG_SMP
-	/* Send resched IPI to remote CPU(s). */
-	// if (unlikely(!cpumask_empty(&this_rq->resched_cpus))) {
-	// 	irq_send_oob_ipi(RESCHEDULE_OOB_IPI, &this_rq->resched_cpus);
-	// 	cpumask_clear(&this_rq->resched_cpus);
-	// 	this_rq->local_flags &= ~RQ_SCHED;
-	// }
-    
+    /* Send resched IPI to remote CPU(s). */
+    // if (unlikely(!cpumask_empty(&this_rq->resched_cpus))) {
+    // 	irq_send_oob_ipi(RESCHEDULE_OOB_IPI, &this_rq->resched_cpus);
+    // 	cpumask_clear(&this_rq->resched_cpus);
+    // 	this_rq->local_flags &= ~RQ_SCHED;
+    // }
+
     if need_resched {
-        unsafe{(*rq).flags &= !RQ_SCHED}
+        unsafe { (*rq).flags &= !RQ_SCHED }
         // unsafe{(*rq).local_flags &= !RQ_SCHED}
     }
 
@@ -1651,40 +2035,47 @@ fn test_resched(rq: *mut rros_rq) -> bool {
 //逻辑完整，未测试
 #[no_mangle]
 pub unsafe extern "C" fn rros_schedule() {
-    unsafe{
+    unsafe {
         if rros_runqueues == 0 as *mut rros_rq {
             return;
         }
     }
-    
+
     let this_rq = this_rros_rq();
     let flags;
     let local_flags;
 
-    unsafe{
+    unsafe {
         flags = (*this_rq).flags;
         local_flags = (*this_rq).local_flags;
     }
 
-    pr_info!("rros_schedule: flags is {} local_flags is {}\n", flags, local_flags);
-
+    // pr_info!(
+    //     "rros_schedule: flags is {} local_flags is {}\n",
+    //     flags,
+    //     local_flags
+    // );
 
     //b kernel/rros/sched.rs:1670
-    if  ((flags | local_flags) & (RQ_IRQ|RQ_SCHED)) != RQ_SCHED {
+    if ((flags | local_flags) & (RQ_IRQ | RQ_SCHED)) != RQ_SCHED {
         return;
     }
 
     let res = premmpt::running_inband();
     let r = match res {
         Ok(_o) => true,
-        Err(_e) => false
+        Err(_e) => false,
     };
     if !r {
-        unsafe{__rros_schedule(0 as *mut c_types::c_void);}
-        return ;
+        unsafe {
+            __rros_schedule(0 as *mut c_types::c_void);
+        }
+        return;
     }
 
-    unsafe{bindings::run_oob_call(Some(__rros_schedule), 0 as *mut c_types::c_void);}
+    unsafe {
+        bindings::run_oob_call(Some(__rros_schedule), 0 as *mut c_types::c_void);
+    }
 }
 
 extern "C" {
@@ -1694,261 +2085,347 @@ extern "C" {
     fn rust_helper_preempt_disable();
 }
 
+/// # Find the next thread and switch to it
+/// 
+/// # Requirements
+///     - You need to figure out how to use `bindings::dovetail_context_switch` to switch to the next thread.
+///
+/// # Arguments
+///     - `arg`: not used
+///     
+/// # Return
+///     - 0 all the time
+///  
+/// # Tips: 
+///     - You can find the clue from the struct of `rros_thread`.
+///     - Set the return value of the `bindings::dovetail_context_switch` to `inband_tail`. 
+///       If you figure out the meaning of the `inband_tail`, just contact us for the extra credit!
+///       You can find clues from the `resume_oob_task` and the [evl document](https://evlproject.org/dovetail/altsched/).
+///     - Read the `dovetail_init_altsched` and `bindings::dovetail_context_switch` function for more details.
 #[no_mangle]
 unsafe extern "C" fn __rros_schedule(arg: *mut c_types::c_void) -> i32 {
-    unsafe{
-// fn __rros_schedule() {
-    // pr_info!("sched thread!!!!");
-    // let prev = curr;
-    let prev;
-    let curr;
-    let next;
-    let this_rq = this_rros_rq();
-    let mut leaving_inband;
+    unsafe {
+        // fn __rros_schedule() {
+        // pr_info!("sched thread!!!!");
+        // let prev = curr;
+        let prev;
+        let curr;
+        let next;
+        let this_rq = this_rros_rq();
+        let mut leaving_inband;
+
+        let flags = unsafe { rust_helper_hard_local_irq_save() };
+
+        unsafe {
+            curr = (*this_rq).get_curr();
+        }
 
-    let flags = unsafe{rust_helper_hard_local_irq_save()};
-    
-    unsafe{curr = (*this_rq).get_curr();}
+        let curr_state = unsafe { (*curr.locked_data().get()).state };
+        if curr_state & T_USER != 0x0 {
+            //evl_commit_monitor_ceiling();
+        }
 
-    let curr_state = unsafe{(*curr.locked_data().get()).state};
-    if curr_state & T_USER != 0x0 {
-        //evl_commit_monitor_ceiling();
-    }
+        //这里可以不用自旋锁，因为只有一个cpu，理论上来说没有问题
+        // raw_spin_lock(&curr->lock);
+        // raw_spin_lock(&this_rq->lock);
 
-    //这里可以不用自旋锁，因为只有一个cpu，理论上来说没有问题
-    // raw_spin_lock(&curr->lock);
-	// raw_spin_lock(&this_rq->lock);
+        if !(test_resched(this_rq)) {
+            // raw_spin_unlock(&this_rq->lock);
+            // raw_spin_unlock_irqrestore(&curr->lock, flags);
+            // rust_helper_hard_local_irq_restore(flags);
+            lock::raw_spin_unlock_irqrestore(flags);
+            return 0;
+        }
 
-    if !(test_resched(this_rq)){
-        // raw_spin_unlock(&this_rq->lock);
-		// raw_spin_unlock_irqrestore(&curr->lock, flags);
-        // rust_helper_hard_local_irq_restore(flags);
-        lock::raw_spin_unlock_irqrestore(flags);
-        return 0;
-    }
+        let curr_add = curr.locked_data().get();
+        next = pick_next_thread(Some(this_rq)).unwrap();
+        // unsafe{pr_info!("begin of the rros_schedule uninit_thread: x ref is {}", Arc::strong_count(&next.clone()));}
 
-    let curr_add = curr.locked_data().get();
-    next = pick_next_thread(Some(this_rq)).unwrap();
-    
-    let next_add = next.locked_data().get();
+        let next_add = next.locked_data().get();
 
-    if next_add == curr_add { // if the curr and next are both root, we should call the inband thread
-        pr_info!("__rros_schedule: next_add == curr_add ");
-        let next_state = unsafe{(*next.locked_data().get()).state};
-		if (next_state & T_ROOT as u32) != 0x0 {
+        if next_add == curr_add {
+            // if the curr and next are both root, we should call the inband thread
+            // pr_info!("__rros_schedule: next_add == curr_add ");
+            let next_state = unsafe { (*next.locked_data().get()).state };
+            if (next_state & T_ROOT as u32) != 0x0 {
+                if unsafe { (*this_rq).local_flags & RQ_TPROXY != 0x0 } {
+                    // pr_info!("__rros_schedule: (*this_rq).local_flags & RQ_TPROXY != 0x0 ");
+                    tick::rros_notify_proxy_tick(this_rq);
+                }
+                if unsafe { (*this_rq).local_flags & RQ_TDEFER != 0x0 } {
+                    // pr_info!("__rros_schedule: (*this_rq).local_flags & RQ_TDEFER !=0x0 ");
+                    unsafe {
+                        tick::rros_program_local_tick(
+                            &mut clock::RROS_MONO_CLOCK as *mut clock::RrosClock,
+                        );
+                    }
+                }
+            }
+            // rust_helper_hard_local_irq_restore(flags);
+            lock::raw_spin_unlock_irqrestore(flags);
+            return 0;
+        }
 
-            if unsafe{(*this_rq).local_flags & RQ_TPROXY != 0x0} {
-                pr_info!("__rros_schedule: (*this_rq).local_flags & RQ_TPROXY != 0x0 ");
+        prev = curr.clone();
+        unsafe {
+            (*this_rq).curr = Some(next.clone());
+        }
+        // unsafe{pr_info!("mid of the rros_schedule uninit_thread: x ref is {}", Arc::strong_count(&next.clone()));}
+        leaving_inband = false;
+
+        let prev_state = (*prev.locked_data().get()).state;
+        let next_state = (*next.locked_data().get()).state;
+        if prev_state & T_ROOT as u32 != 0x0 {
+            // leave_inband(prev);
+            leaving_inband = true;
+        } else if (next_state & T_ROOT as u32 != 0x0) {
+            if unsafe { (*this_rq).local_flags & RQ_TPROXY != 0x0 } {
                 tick::rros_notify_proxy_tick(this_rq);
             }
-            if unsafe{(*this_rq).local_flags & RQ_TDEFER !=0x0} {
-                pr_info!("__rros_schedule: (*this_rq).local_flags & RQ_TDEFER !=0x0 ");
-                unsafe{tick::rros_program_local_tick(&mut clock::RROS_MONO_CLOCK as *mut clock::RrosClock);}
+            if unsafe { (*this_rq).local_flags & RQ_TDEFER != 0x0 } {
+                unsafe {
+                    tick::rros_program_local_tick(
+                        &mut clock::RROS_MONO_CLOCK as *mut clock::RrosClock,
+                    );
+                }
             }
-		}
-        // rust_helper_hard_local_irq_restore(flags);
-        lock::raw_spin_unlock_irqrestore(flags);
-        return 0;
-    }
-
-	prev =  curr.clone();
-    unsafe{(*this_rq).curr = Some(next.clone());}
-    leaving_inband = false;
-
-    let prev_state = (*prev.locked_data().get()).state;
-    let next_state = (*next.locked_data().get()).state;
-    if prev_state & T_ROOT as u32 != 0x0 {
-		// leave_inband(prev);
-		leaving_inband = true;
-	} else if (next_state & T_ROOT as u32 != 0x0) {
-        if unsafe{(*this_rq).local_flags& RQ_TPROXY != 0x0 } {
-            tick::rros_notify_proxy_tick(this_rq);
+            // enter_inband(next);
         }
-        if unsafe{(*this_rq).local_flags & RQ_TDEFER !=0x0 } {
-            unsafe{tick::rros_program_local_tick(&mut clock::RROS_MONO_CLOCK as *mut clock::RrosClock);}
-        }
-        // enter_inband(next);
-	}
 
-    // prepare_rq_switch(this_rq, prev, next);
-    
-    let prev_add = prev.locked_data().get();
-    // pr_info!("the run thread add is  prev {:p}", prev_add);
+        // prepare_rq_switch(this_rq, prev, next);
 
-    let next_add = next.locked_data().get();
-    // pr_info!("the run thread add is  next {:p}", next_add);
+        let prev_add = prev.locked_data().get();
+        // pr_info!("the run thread add is  spinlock prev {:p}", prev_add);
 
-    // fix!!!!!
-    let prev_sched = &mut (*prev.locked_data().get()).altsched as *mut bindings::dovetail_altsched_context;
-    let next_sched = &mut (*next.locked_data().get()).altsched as *mut bindings::dovetail_altsched_context;
-    let inband_tail;
-    // pr_info!("before the inband_tail next state is {}", next.lock().state);
-	unsafe{inband_tail = bindings::dovetail_context_switch(prev_sched, next_sched, leaving_inband);}
-    // next.unlock();
-	// finish_rq_switch(inband_tail, flags); //b kernel/rros/sched.rs:1751
+        let next_add = next.locked_data().get();
+        // pr_info!("the run thread add is  spinlock  next {:p}", next_add);
+        // pr_info!("the run thread add is  arc prev {:p}", prev);
+        // pr_info!("the run thread add is  arc next {:p}", next);
+        let mut inband_tail = false;
 
-    // if prev == 
-    // bindings::dovetail_context_switch();
-    // inband_tail = dovetail_context_switch(&prev->altsched,
-    //     &next->altsched, leaving_inband);
+        // fix!!!!!
+        // TODO: your code here
+      
+        // end of your code
+        // next.unlock();
+        // finish_rq_switch(inband_tail, flags); //b kernel/rros/sched.rs:1751
 
-    // rust_helper_hard_local_irq_restore(flags);
-    // pr_info!("before the inband_tail curr state is {}", curr.lock().state);
-    
-    pr_info!("the inband_tail is {}", inband_tail);
-    if inband_tail == false {
-        lock::raw_spin_unlock_irqrestore(flags);
-    }
-    0
-    }
-}
+        // if prev ==
+        // bindings::dovetail_context_switch();
+        // inband_tail = dovetail_context_switch(&prev->altsched,
+        //     &next->altsched, leaving_inband);
 
-fn finish_rq_switch() {
+        // rust_helper_hard_local_irq_restore(flags);
+        // pr_info!("before the inband_tail curr state is {}", curr.lock().state);
 
+        // pr_info!("the inband_tail is {}", inband_tail);
+        if inband_tail == false {
+            lock::raw_spin_unlock_irqrestore(flags);
+        }
+        // unsafe{pr_info!("end of the rros_schedule uninit_thread: x ref is {}", Arc::strong_count(&next.clone()));}
+        0
+    }
 }
 
+// TODO: add this function
+fn finish_rq_switch() {}
 
-pub fn pick_next_thread(rq:Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>>{
-    let mut oob_mm = &mut oob_mm_state::new() as *mut oob_mm_state;
-    let mut next:Option<Arc<SpinLock<rros_thread>>> = None;
-    loop{
+pub fn pick_next_thread(rq: Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>> {
+    let mut oob_mm = &mut bindings::oob_mm_state::default() as *mut bindings::oob_mm_state;
+    let mut next: Option<Arc<SpinLock<rros_thread>>> = None;
+    loop {
         next = __pick_next_thread(rq);
         let next_clone = next.clone().unwrap();
-        oob_mm = unsafe{(*next_clone.locked_data().get()).oob_mm};
-        if oob_mm.is_null(){
-			break;
+        oob_mm = unsafe { (*next_clone.locked_data().get()).oob_mm };
+        if oob_mm.is_null() {
+            break;
         }
-        unsafe{
-            if test_bit(RROS_MM_PTSYNC_BIT, & (*oob_mm).flags as *const u32) == false{
+        unsafe {
+            if test_bit(RROS_MM_PTSYNC_BIT, &(*oob_mm).flags as *const u32) == false {
                 break;
             }
         }
-        let info = unsafe{(*next_clone.locked_data().get()).info};
-		if info & (T_PTSTOP|T_PTSIG|T_KICKED) != 0{
+        let info = unsafe { (*next_clone.locked_data().get()).info };
+        if info & (T_PTSTOP | T_PTSIG | T_KICKED) != 0 {
             break;
         }
-        unsafe{(*next_clone.locked_data().get()).state |= T_PTSYNC};
-		unsafe{(*next_clone.locked_data().get()).state &= !T_READY};
-        
+        unsafe { (*next_clone.locked_data().get()).state |= T_PTSYNC };
+        unsafe { (*next_clone.locked_data().get()).state &= !T_READY };
     }
     set_next_running(rq.clone(), next.clone());
 
-	return next;
-}
-
+    return next;
+}
+
+/// # Enqueue the thread into the fifo rq
+/// 
+/// # Requirements
+///     - Enqueue the thread into the fifo rq by the priority of the thread. The higher priority, the front of the queue.
+///     - The If the priority of the thread is the same as the other threads in the queue, enqueue the new thread at the tail of old threads.
+///
+/// # Arguments
+///     - `thread`: the new thread to be enqueued
+///     
+/// # Return
+///     - Ok on success enqueuing the thread
+///     - Err on failure
+///  
+/// # Tips: 
+///     - The priority of the thread is set in the `cprio` item of the `rros_thread`.
+///     - It's easier to inverted search the queue to find the position to insert the new thread.
+///     - The `rq_next` item of the `rros_thread` is used to remove itself when dequeue. Remeber to set it when you enqueue the thread. Read the `__rros_dequeue_fifo_thread` function for more details.
 //逻辑不完整，但应该没问题，未测试
-pub fn __pick_next_thread(rq: Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>>{
-	let curr = unsafe{(*rq.clone().unwrap()).curr.clone().unwrap()};
-
-	let mut next:Option<Arc<SpinLock<rros_thread>>> = None;
-    
-    let mut state = unsafe{(*curr.locked_data().get()).state};
-	if state & (RROS_THREAD_BLOCK_BITS | T_ZOMBIE) == 0 {
-		if rros_preempt_count() > 0 {
-			rros_set_self_resched(rq);
-			return Some(curr.clone());
-		}
-		if state & T_READY == 0 {
-			rros_requeue_thread(curr.clone());
-			state |= T_READY;
-		}
-	}
+pub fn __pick_next_thread(rq: Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>> {
+    let curr: Arc<SpinLock<rros_thread>> = unsafe { (*rq.clone().unwrap()).curr.clone().unwrap() };
 
-	next = lookup_fifo_class(rq.clone());
-    // pr_info!("next2");
-	if next.is_some(){
-        pr_info!("__pick_next_thread: next.is_some");
-		return next;
-    }
+    let mut next: Option<Arc<SpinLock<rros_thread>>> = None;
 
-    //这里虽然没有循环，但应该没有问题
-    //todo 这里的for循环
-    let func = unsafe{idle::rros_sched_idle.sched_pick.unwrap()};
-    let next = func(rq.clone());
-    match next{
-        Ok(n) => return Some(n),
-        Err(e) => return None,
+    let mut state = unsafe { (*curr.locked_data().get()).state };
+    if state & (RROS_THREAD_BLOCK_BITS | T_ZOMBIE) == 0 {
+        if rros_preempt_count() > 0 {
+            rros_set_self_resched(rq);
+            return Some(curr.clone());
+        }
+        if state & T_READY == 0 {
+            rros_requeue_thread(curr.clone());
+            state |= T_READY;
+        }
     }
-}
 
-//逻辑完整，未测试
-pub fn lookup_fifo_class(rq:Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>>{
-    let q = &mut unsafe{(*rq.clone().unwrap()).fifo.runnable.head.as_mut().unwrap()};
-	if q.is_empty(){
-		return None;
+    next = lookup_fifo_class(rq.clone());
+    // pr_info!("next2");
+    if next.is_some() {
+        // pr_info!("__pick_next_thread: next.is_some");
+        return next;
     }
-    // pr_info!("next0");
-	let thread = q.get_head().unwrap().value.clone();
-    let sched_class = unsafe{(*thread.locked_data().get()).sched_class.clone().unwrap()};
 
-	if sched_class.flag != 3{
-        let func = sched_class.sched_pick.unwrap();
-		return Some(func(rq).unwrap());
+    //这里虽然没有循环，但应该没有问题
+    //TODO: 这里的for循环
+    // 已经加上了
+    let mut next;
+    let mut p = unsafe { rros_sched_lower };
+    while p != 0 as *mut rros_sched_class {
+        // pr_info!("p!=0!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! in sched_pick");
+        if unsafe { (*p).sched_pick != None } {
+            let func;
+            unsafe {
+                match (*p).sched_pick {
+                    Some(f) => func = f,
+                    None => {
+                        pr_warn!("sched_pick function error, this should not happen");
+                        return None;
+                        // return Err(kernel::Error::EINVAL);
+                    }
+                }
+            }
+            next = func(rq.clone());
+            match next {
+                Ok(n) => return Some(n),
+                Err(e) => {
+                    pr_warn!("nothing found");
+                },
+            }
+        }
+        unsafe { p = (*p).next };
     }
-
-    pr_info!("lookup_fifo_class :2");
-    q.de_head();
-	return Some(thread.clone());
+    // let func = unsafe { idle::rros_sched_idle.sched_pick.unwrap() };
+    // let next = func(rq.clone());
+    // match next {
+    //     Ok(n) => return Some(n),
+    //     Err(e) => return None,
+    // }
+    return None;
+}
+
+/// # Find the highest priority thread in the fifo runnable queue(rq)
+/// 
+/// # Requirements
+///     - if there is a thread in the rq, find it and return
+///
+/// # Arguments
+///     - `thread`: the new thread to be enqueued
+///     
+/// # Return
+///     - Some on finding a thread
+///     - None on there is no thread
+///  
+/// # Tips: 
+///     - `rq.fifo.runnable.head` is the head of the rq.
+///     - Be careful! The rq may be empty.
+//逻辑完整，未测试
+pub fn lookup_fifo_class(rq: Option<*mut rros_rq>) -> Option<Arc<SpinLock<rros_thread>>> {
+    // TODO: your code here
+   
+    // end of your code
+    // TODO: If you inmplment the `lookup_fifo_class` function, you can delete the following code.
+    None
 }
 
 //逻辑完整，未测试
-pub fn set_next_running(rq:Option<*mut rros_rq>, next:Option<Arc<SpinLock<rros_thread>>>){
+pub fn set_next_running(rq: Option<*mut rros_rq>, next: Option<Arc<SpinLock<rros_thread>>>) {
     let next = next.unwrap();
-    unsafe{(*next.locked_data().get()).state &= !T_READY};
-    let state = unsafe{(*next.locked_data().get()).state};
-    pr_info!("set_next_running: next.lock().state is {}", unsafe{(*next.locked_data().get()).state});
-    if state & T_RRB != 0{
-        unsafe{
+    unsafe { (*next.locked_data().get()).state &= !T_READY };
+    let state = unsafe { (*next.locked_data().get()).state };
+    // pr_info!("set_next_running: next.lock().state is {}", unsafe {
+        // (*next.locked_data().get()).state
+    // });
+    if state & T_RRB != 0 {
+        unsafe {
             let delta = (*next.locked_data().get()).rrperiod;
-            rros_start_timer((*rq.clone().unwrap()).rrbtimer.clone().unwrap(),
+            rros_start_timer(
+                (*rq.clone().unwrap()).rrbtimer.clone().unwrap(),
                 rros_abs_timeout((*rq.clone().unwrap()).rrbtimer.clone().unwrap(), delta),
-                RROS_INFINITE)
+                RROS_INFINITE,
+            )
         };
-    }
-    else{
-        unsafe{rros_stop_timer((*rq.clone().unwrap()).rrbtimer.clone().unwrap())};
+    } else {
+        unsafe { rros_stop_timer((*rq.clone().unwrap()).rrbtimer.clone().unwrap()) };
     }
 }
 
-fn rros_preempt_count() -> i32{
-    unsafe{return (*rust_helper_dovetail_current_state()).preempt_count};
+fn rros_preempt_count() -> i32 {
+    unsafe { return (*rust_helper_dovetail_current_state()).preempt_count };
 }
 
-fn test_bit(nr: i32, addr: *const u32) -> bool{
-    unsafe{return rust_helper_test_bit(nr, addr)};
+fn test_bit(nr: i32, addr: *const u32) -> bool {
+    unsafe { return rust_helper_test_bit(nr, addr) };
 }
 
 pub fn rros_set_thread_policy(thread: Option<Arc<SpinLock<rros_thread>>>,
     sched_class:Option<&'static rros_sched_class>,
-    p:Option<Rc<RefCell<rros_sched_param>>>) ->Result<usize>
+    p:Option<Arc<SpinLock<rros_sched_param>>>) ->Result<usize>
 {
     let mut flags:c_types::c_ulong = 0;
-    let test = p.clone().unwrap();
+    // let test = p.clone().unwrap();
     let mut rq: Option<*mut rros_rq>;
     rq = rros_get_thread_rq(thread.clone(), &mut flags);
-    rros_set_thread_policy_locked(thread.clone(), sched_class.clone(), p.clone());
+    // pr_info!("rros_get_thread_rq success");
+    rros_set_thread_policy_locked(thread.clone(), sched_class.clone(), p.clone())?;
+    // pr_info!("rros_set_thread_policy_locked success");
     rros_put_thread_rq(thread.clone(), rq.clone(), flags);
+    // pr_info!("rros_put_thread_rq success");
     Ok(0)
 }
 
-
-pub fn rros_get_thread_rq(thread: Option<Arc<SpinLock<rros_thread>>>, flags: &mut c_types::c_ulong) -> Option<*mut rros_rq>{
+pub fn rros_get_thread_rq(
+    thread: Option<Arc<SpinLock<rros_thread>>>,
+    flags: &mut c_types::c_ulong,
+) -> Option<*mut rros_rq> {
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&thread.clone().unwrap()));
     //todo raw_spin_lock_irqsave and raw_spin_lock
-    *flags = unsafe{rust_helper_hard_local_irq_save()};
+    *flags = unsafe { rust_helper_hard_local_irq_save() };
     // unsafe{rust_helper_preempt_disable();}
-    unsafe{(*thread.unwrap().locked_data().get()).rq.clone()}
+    unsafe { (*thread.unwrap().locked_data().get()).rq.clone() }
 }
 
 pub fn rros_put_thread_rq(
     thread: Option<Arc<SpinLock<rros_thread>>>,
-    rq:Option<*mut rros_rq>,
-    flags:c_types::c_ulong) -> Result<usize>
-{
-    
-    unsafe{
-    rust_helper_hard_local_irq_restore(flags);
-    // rust_helper_preempt_enable();
+    rq: Option<*mut rros_rq>,
+    flags: c_types::c_ulong,
+) -> Result<usize> {
+    unsafe {
+        rust_helper_hard_local_irq_restore(flags);
+        // rust_helper_preempt_enable();
     }
     //todo  raw_spin_unlock and raw_spin_unlock_irqrestore
     Ok(0)
@@ -1957,12 +2434,12 @@ pub fn rros_put_thread_rq(
 pub fn rros_set_thread_policy_locked(
     thread: Option<Arc<SpinLock<rros_thread>>>,
     sched_class:Option<&'static rros_sched_class>,
-    p:Option<Rc<RefCell<rros_sched_param>>>) ->Result<usize>
+    p:Option<Arc<SpinLock<rros_sched_param>>>) ->Result<usize>
 {
     let test = p.clone().unwrap();
     let thread_unwrap = thread.clone().unwrap();
-    let orig_effective_class:Option<Rc<RefCell<rros_sched_class>>> = None;
-    let mut effective:Result<usize>;
+    let orig_effective_class: Option<Rc<RefCell<rros_sched_class>>> = None;
+    let mut effective: Result<usize>;
     rros_check_schedparams(thread.clone(), sched_class.clone(), p.clone())?;
     let mut flag_base_class = 0;
     let mut base_class = thread_unwrap.lock().base_class;
@@ -1970,18 +2447,25 @@ pub fn rros_set_thread_policy_locked(
         // pr_info!("baseclass is none!");
         flag_base_class = 1;
     }
-    
-    unsafe{if flag_base_class == 1 || (sched_class.unwrap() as *const rros_sched_class) != (base_class.unwrap() as *const rros_sched_class) {
-        rros_declare_thread(thread.clone(), sched_class.clone(), p.clone());
-    }}
+
+    unsafe {
+        if flag_base_class == 1
+            || (sched_class.unwrap() as *const rros_sched_class)
+                != (base_class.unwrap() as *const rros_sched_class)
+        {
+            rros_declare_thread(thread.clone(), sched_class.clone(), p.clone());
+        }
+    }
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&thread.clone().unwrap()));
-    if base_class.is_some(){
+    if base_class.is_some() {
         let state = thread_unwrap.lock().state;
         if state & T_READY != 0x0 {
             rros_dequeue_thread(thread.clone().unwrap());
         }
 
-        if (sched_class.unwrap() as *const rros_sched_class) != (base_class.unwrap() as *const rros_sched_class) {
+        if (sched_class.unwrap() as *const rros_sched_class)
+            != (base_class.unwrap() as *const rros_sched_class)
+        {
             rros_forget_thread(thread.clone().unwrap());
         }
     }
@@ -1991,7 +2475,7 @@ pub fn rros_set_thread_policy_locked(
     //     orig_effective_class = thread->sched_class;
     //     thread->sched_class = NULL;
     // }
-    let test = p.clone().unwrap();
+    // let test = p.clone().unwrap();
     // pr_info!("! yinyongcishu is {}", Arc::strong_count(&thread.clone().unwrap()));
     effective = rros_set_schedparam(thread.clone(), p.clone());
     // pr_info!("thread after setting {}", thread_unwrap.lock().state);
@@ -2001,19 +2485,18 @@ pub fn rros_set_thread_policy_locked(
         let cprio = thread_unwrap.lock().cprio;
         let wprio = rros_calc_weighted_prio(sched_class.clone().unwrap(), cprio);
         thread_unwrap.lock().wprio = wprio;
-    } 
+    }
     // todo EVL_DEBUG
     // else if (EVL_DEBUG(CORE))
     //     thread->sched_class = orig_effective_class;
     let state = thread_unwrap.lock().state;
-    if state & T_READY != 0x0{
+    if state & T_READY != 0x0 {
         // pr_info!("wwwwwwwwhat the fuck!");
         rros_enqueue_thread(thread.clone().unwrap());
     }
 
-
     let state = thread_unwrap.lock().state;
-    if state & (T_DORMANT|T_ROOT as u32) == 0x0 {
+    if state & (T_DORMANT | T_ROOT as u32) == 0x0 {
         let rq = thread_unwrap.lock().rq;
         rros_set_resched(rq);
     }
@@ -2026,22 +2509,37 @@ pub fn rros_set_thread_policy_locked(
 fn rros_check_schedparams(
     thread: Option<Arc<SpinLock<rros_thread>>>,
     sched_class:Option<&'static rros_sched_class>,
-    p:Option<Rc<RefCell<rros_sched_param>>>) -> Result<usize>
+    p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>
 {
 
     let sched_class_ptr = sched_class.unwrap();
-    if sched_class_ptr.sched_chkparam.is_some(){
+    if sched_class_ptr.sched_chkparam.is_some() {
         let func = sched_class_ptr.sched_chkparam.unwrap();
         func(thread.clone(), p.clone());
     } else {
+        pr_info!("rros_check_schedparams no sched_chkparam functions");
+    }
+    Ok(0)
+}
 
+pub fn rros_get_schedparam(thread:Arc<SpinLock<rros_thread>>,p:Arc<SpinLock<rros_sched_param>>) ->Result<usize>{
+    let func;
+    unsafe{
+        match (*thread.locked_data().get()).sched_class.unwrap().sched_getparam{
+            Some(f) => func = f,
+            None => {
+                pr_warn!("rros_get_schedparam: sched_getparam function error");
+                return Err(kernel::Error::EINVAL);
+            }
+        };
+        func(Some(thread.clone()), Some(p.clone()));
     }
     Ok(0)
 }
 
 fn rros_set_schedparam(
     thread: Option<Arc<SpinLock<rros_thread>>>,
-    p:Option<Rc<RefCell<rros_sched_param>>>) -> Result<usize>
+    p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>
 {
     let thread_clone = thread.clone();
     let thread_unwrap = thread_clone.unwrap();
@@ -2063,39 +2561,47 @@ fn rros_set_schedparam(
 fn rros_declare_thread(
     thread: Option<Arc<SpinLock<rros_thread>>>,
     sched_class: Option<&'static rros_sched_class>,
-    p: Option<Rc<RefCell<rros_sched_param>>>)->Result<usize>
+    p: Option<Arc<SpinLock<rros_sched_param>>>)->Result<usize>
 {
     let thread_clone = thread.clone();
     let thread_unwrap = thread_clone.unwrap();
     let mut sched_class_ptr = sched_class.unwrap();
-    if sched_class_ptr.sched_declare.is_some(){
+    if sched_class_ptr.sched_declare.is_some() {
         let func = sched_class_ptr.sched_declare.unwrap();
         func(thread.clone(), p.clone())?;
     }
     let base_class = thread_unwrap.lock().base_class;
-    unsafe{
-        if base_class.is_none() || (sched_class_ptr as *const rros_sched_class) != (base_class.unwrap() as *const rros_sched_class){
+    unsafe {
+        if base_class.is_none()
+            || (sched_class_ptr as *const rros_sched_class)
+                != (base_class.unwrap() as *const rros_sched_class)
+        {
             let mut sched_class_mutptr = sched_class_ptr as *const rros_sched_class;
             let mut sched_class_mutptr_mut = sched_class_mutptr as *mut rros_sched_class;
             (*sched_class_mutptr_mut).nthreads += 1;
         }
     }
 
+    // pr_info!("rros_declare_thread success!");
     Ok(0)
 }
 
-pub fn rros_forget_thread(thread:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+pub fn rros_forget_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
     let thread_clone = thread.clone();
     // let thread_lock = thread_clone.lock();
     let sched_class = thread_clone.lock().base_class.clone();
     let mut sched_class_ptr = sched_class.unwrap() as *const rros_sched_class;
-	let mut sched_class_ptr = sched_class_ptr as *mut rros_sched_class;
-    unsafe{(*sched_class_ptr).nthreads -= 1;}
+    let mut sched_class_ptr = sched_class_ptr as *mut rros_sched_class;
+    unsafe {
+        (*sched_class_ptr).nthreads -= 1;
+    }
 
-	unsafe{if (*sched_class_ptr).sched_forget.is_some(){
-        let func = (*sched_class_ptr).sched_forget.unwrap();
-		func(thread.clone());
-    }}
+    unsafe {
+        if (*sched_class_ptr).sched_forget.is_some() {
+            let func = (*sched_class_ptr).sched_forget.unwrap();
+            func(thread.clone());
+        }
+    }
 
     Ok(0)
 }
@@ -2107,41 +2613,56 @@ extern "C" {
 
 #[no_mangle]
 unsafe extern "C" fn rust_resume_oob_task(ptr: *mut c_types::c_void) {
-	// struct evl_thread *thread = evl_thread_from_task(p);
+    // struct evl_thread *thread = evl_thread_from_task(p);
 
     // pr_info!("rros evl mutex ptr{:p}", ptr);
     let thread: Arc<SpinLock<rros_thread>>;
-	
-    unsafe{ thread = Arc::from_raw(ptr as *mut SpinLock<rros_thread>);}
+
+    unsafe {
+        thread = Arc::from_raw(ptr as *mut SpinLock<rros_thread>);
+        // unsafe{pr_info!("0600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+        Arc::increment_strong_count(ptr);
+        // unsafe{pr_info!("b600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+        // pr_info!("the ptr in resume address is {:p}", ptr);
+        // unsafe{pr_info!("a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+        // unsafe{pr_info!("600 uninit_thread: x ref is {}", Arc::strong_count(ptr));}
+    }
+    // unsafe{pr_info!("2a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
 
     /*
-	 * Dovetail calls us with hard irqs off, oob stage
-	 * stalled. Clear the stall bit which we don't use for
-	 * protection but keep hard irqs off.
-	 */
-    unsafe{rust_helper_unstall_oob();}
-	// check_cpu_affinity(p);
-	rros_release_thread(thread.clone(), T_INBAND, 0);
-	/*
-	 * If T_PTSTOP is set, pick_next_thread() is not allowed to
-	 * freeze @thread while in flight to the out-of-band stage.
-	 */
-    unsafe{
+     * Dovetail calls us with hard irqs off, oob stage
+     * stalled. Clear the stall bit which we don't use for
+     * protection but keep hard irqs off.
+     */
+    unsafe {
+        rust_helper_unstall_oob();
+    }
+    // check_cpu_affinity(p);
+    // unsafe{pr_info!("3a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+    rros_release_thread(thread.clone(), T_INBAND, 0);
+    /*
+     * If T_PTSTOP is set, pick_next_thread() is not allowed to
+     * freeze @thread while in flight to the out-of-band stage.
+     */
+    // unsafe{pr_info!("4a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+    unsafe {
         sched::rros_schedule();
-	    rust_helper_stall_oob();
+        // unsafe{pr_info!("5a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
+        rust_helper_stall_oob();
     }
+    // unsafe{pr_info!("6a600 uninit_thread: x ref is {}", Arc::strong_count(&thread));}
 }
 
 extern "C" {
-    fn rust_helper_hard_local_irq_disable() ;
+    fn rust_helper_hard_local_irq_disable();
     fn rust_helper_dovetail_leave_oob();
-    fn rust_helper_hard_local_irq_enable() ;
+    fn rust_helper_hard_local_irq_enable();
 }
 
 //基本完整
 pub fn rros_switch_inband(cause: i32) {
     pr_info!("rros_switch_inband: in");
-    let curr = unsafe{&mut *rros_current()};
+    let curr = unsafe { &mut *rros_current() };
     let this_rq: Option<*mut rros_rq>;
     let notify: bool;
     unsafe {
@@ -2157,29 +2678,64 @@ pub fn rros_switch_inband(cause: i32) {
     let info = curr.lock().info;
     if cause == (RROS_HMDIAG_TRAP as i32) {
         pr_info!("evl_switch_inband: cause == EVL_HMDIAG_TRAP");
-        //todo
+        // TODO:
     } else if info & T_PTSIG != 0x0 {
         pr_info!("evl_switch_inband: curr->info & T_PTSIG");
-        //todo
+        // TODO:
     }
 
     curr.lock().info &= !RROS_THREAD_INFO_MASK;
     rros_set_resched(this_rq);
-    unsafe{
+    unsafe {
         rust_helper_dovetail_leave_oob();
         __rros_schedule(0 as *mut c_types::c_void);
         rust_helper_hard_local_irq_enable();
         bindings::dovetail_resume_inband();
     }
-    
+
     // curr.lock().stat.isw.inc_counter();
     // rros_propagate_schedparam_change(curr);
 
     if notify {
-        //todo
+        // TODO:
     }
 
     //evl_sync_uwindow(curr); todo
 }
 
-//111
\ No newline at end of file
+pub enum RrosValue{
+    Val(i32),
+    Lval(i64),
+    Ptr(*mut c_types::c_void),
+}
+
+impl RrosValue{
+    pub fn new() -> Self{
+        RrosValue::Lval(0)
+    }
+    pub fn new_nil() -> Self{
+        RrosValue::Ptr(null_mut())
+    }
+}
+
+#[inline]
+pub fn rros_enable_preempt(){
+    extern "C"{
+        fn rust_helper_rros_enable_preempt_top_part()->bool;
+    }
+    unsafe{
+        if rust_helper_rros_enable_preempt_top_part(){
+            rros_schedule()
+        }  
+    } 
+}
+
+#[inline]
+pub fn rros_disable_preempt(){
+    extern "C"{
+        fn rust_helper_rros_disable_preempt();
+    }
+    unsafe{
+        rust_helper_rros_disable_preempt();
+    }
+}
\ No newline at end of file
diff --git a/kernel/rros/sched_test.rs b/kernel/rros/sched_test.rs
index 1c7d8c391..d1b582217 100644
--- a/kernel/rros/sched_test.rs
+++ b/kernel/rros/sched_test.rs
@@ -1,10 +1,12 @@
-use crate::{
-    thread, timer, clock,tick, 
-    sched,
-};
-use kernel::{bindings, prelude::*, c_str, spinlock_init, sync::{SpinLock, Lock, Guard}, c_types, };
+use crate::{clock, sched, thread, tick, timer};
 use alloc::rc::Rc;
 use core::cell::RefCell;
+use kernel::{
+    bindings, c_str, c_types,
+    prelude::*,
+    spinlock_init,
+    sync::{Guard, Lock, SpinLock},
+};
 
 pub fn test_this_rros_rq_thread() -> Result<usize> {
     pr_info!("~~~test_this_rros_rq_thread begin~~~");
@@ -12,10 +14,10 @@ pub fn test_this_rros_rq_thread() -> Result<usize> {
     match curr {
         None => {
             pr_info!("curr is None");
-        },
+        }
         Some(x) => {
             pr_info!("curr is not None ");
-        },
+        }
     };
     pr_info!("~~~test_this_rros_rq_thread end~~~");
     Ok(0)
@@ -24,7 +26,9 @@ pub fn test_this_rros_rq_thread() -> Result<usize> {
 pub fn test_cpu_smp() -> Result<usize> {
     pr_info!("~~~test_cpu_smp begin~~~");
     let rq = sched::this_rros_rq();
-    unsafe{pr_info!("cpu is {}",(*rq).cpu);}
+    unsafe {
+        pr_info!("cpu is {}", (*rq).cpu);
+    }
     pr_info!("~~~test_cpu_smp end~~~");
     Ok(0)
 }
@@ -32,20 +36,27 @@ pub fn test_cpu_smp() -> Result<usize> {
 pub fn test_rros_set_resched() -> Result<usize> {
     pr_info!("~~~test_rros_set_resched begin~~~");
     let rq = sched::this_rros_rq();
-    unsafe{pr_info!("before this_rros_rq flags is {}",(*rq).flags);}
+    unsafe {
+        pr_info!("before this_rros_rq flags is {}", (*rq).flags);
+    }
     sched::rros_set_resched(Some(rq));
-    unsafe{pr_info!("after this_rros_rq flags is {}",(*rq).flags);}
+    unsafe {
+        pr_info!("after this_rros_rq flags is {}", (*rq).flags);
+    }
     pr_info!("~~~test_rros_set_resched end~~~");
     Ok(0)
 }
 
-
 pub fn test_rros_in_irq() -> Result<usize> {
     pr_info!("~~~test_rros_set_resched begin~~~");
     let rq = sched::this_rros_rq();
-    unsafe{pr_info!("before this_rros_rq flags is {}",(*rq).flags);}
+    unsafe {
+        pr_info!("before this_rros_rq flags is {}", (*rq).flags);
+    }
     sched::rros_set_resched(Some(rq));
-    unsafe{pr_info!("after this_rros_rq flags is {}",(*rq).flags);}
+    unsafe {
+        pr_info!("after this_rros_rq flags is {}", (*rq).flags);
+    }
     pr_info!("~~~test_rros_set_resched end~~~");
     Ok(0)
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/stat.rs b/kernel/rros/stat.rs
index 0280c3528..113caf675 100644
--- a/kernel/rros/stat.rs
+++ b/kernel/rros/stat.rs
@@ -1,14 +1,11 @@
+use crate::{clock::*, sched::rros_rq};
 use kernel::{
-    bindings, c_types, cpumask, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*, 
-};
-use crate::{
-    clock::*,
-    sched::rros_rq,
+    bindings, c_types, cpumask, double_linked_list::*, file_operations::FileOperations, ktime::*,
+    percpu, prelude::*, premmpt, spinlock_init, str::CStr, sync::Lock, sync::SpinLock, sysfs,
+    timekeeping,
 };
 // #[cfg(not(CONFIG_EVL_RUNSTATS))] struct不能加cfg
-pub struct RrosAccount { 
+pub struct RrosAccount {
     #[cfg(CONFIG_EVL_RUNSTATS)]
     start: KtimeT,
     #[cfg(CONFIG_EVL_RUNSTATS)]
@@ -18,14 +15,11 @@ pub struct RrosAccount {
 impl RrosAccount {
     #[cfg(CONFIG_EVL_RUNSTATS)]
     pub fn new(start: KtimeT, total: KtimeT) -> Self {
-        RrosAccount{
-            start,
-            total
-        }
+        RrosAccount { start, total }
     }
 
     #[cfg(CONFIG_EVL_RUNSTATS)]
-    pub fn get_account_total(&self) -> KtimeT{ 
+    pub fn get_account_total(&self) -> KtimeT {
         self.total
     }
 
@@ -46,20 +40,22 @@ impl RrosAccount {
     }
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn new() -> Self {RrosAccount{}}
+    pub fn new() -> Self {
+        RrosAccount {}
+    }
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn get_account_total(&self) -> KtimeT{0}
+    pub fn get_account_total(&self) -> KtimeT {
+        0
+    }
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn reset_account(&mut self) {;}
-
-
+    pub fn reset_account(&mut self) {}
 }
 
 #[cfg(CONFIG_EVL_RUNSTATS)]
 fn rros_get_timestamp() -> KtimeT {
-    unsafe{rros_read_clock(&RROS_MONO_CLOCK)}
+    unsafe { rros_read_clock(&RROS_MONO_CLOCK) }
 }
 
 #[cfg(CONFIG_EVL_RUNSTATS)]
@@ -71,7 +67,8 @@ pub fn rros_update_account(rq: Option<*mut rros_rq>) {
             let now = rros_get_timestamp();
             unsafe {
                 let total = (*x).current_account.get_account_total();
-                (*x).current_account.set_account_total(total + now - (*x).last_account_switch);
+                (*x).current_account
+                    .set_account_total(total + now - (*x).last_account_switch);
                 (*x).last_account_switch = now;
                 // smp_wmb();未实现
             }
@@ -83,59 +80,66 @@ pub fn rros_update_account(rq: Option<*mut rros_rq>) {
 pub fn rros_get_last_account_switch(rq: Option<*mut rros_rq>) -> KtimeT {
     match rq {
         None => return 0,
-        Some(x) => unsafe{return (*x).last_account_switch;}
+        Some(x) => unsafe {
+            return (*x).last_account_switch;
+        },
     }
 }
 
 #[cfg(CONFIG_EVL_RUNSTATS)]
-pub fn rros_set_current_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount) -> *mut RrosAccount {
+pub fn rros_set_current_account(
+    rq: Option<*mut rros_rq>,
+    new_account: *mut RrosAccount,
+) -> *mut RrosAccount {
     match rq {
-        None => return 0 as *mut RrosAccount ,
-        Some(x) => unsafe{
+        None => return 0 as *mut RrosAccount,
+        Some(x) => unsafe {
             let prev = (*x).current_account;
             (*x).current_account = new_account;
             return prev;
-        }
+        },
     }
-}   
+}
 
 #[cfg(CONFIG_EVL_RUNSTATS)]
-pub fn rros_close_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount){
+pub fn rros_close_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount) {
     match rq {
         None => return,
-        Some(x) => unsafe{ 
+        Some(x) => unsafe {
             (*x).last_account_switch = rros_get_timestamp();
             (*x).current_account = new_account;
-        }
+        },
     }
 }
 
 #[cfg(not(CONFIG_EVL_RUNSTATS))]
-fn rros_get_timestamp() -> KtimeT {0}
+fn rros_get_timestamp() -> KtimeT {
+    0
+}
 
 #[cfg(not(CONFIG_EVL_RUNSTATS))]
-pub fn rros_update_account(rq: Option<*mut rros_rq>) {;}
+pub fn rros_update_account(rq: Option<*mut rros_rq>) {}
 
 #[cfg(not(CONFIG_EVL_RUNSTATS))]
-pub fn rros_set_current_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount){;}
+pub fn rros_set_current_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount) {}
 
 #[cfg(not(CONFIG_EVL_RUNSTATS))]
-pub fn rros_close_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount){;}
+pub fn rros_close_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount) {}
 
 #[cfg(not(CONFIG_EVL_RUNSTATS))]
-pub fn rros_get_last_account_switch(rq: Option<*mut rros_rq>) -> KtimeT {0}
+pub fn rros_get_last_account_switch(rq: Option<*mut rros_rq>) -> KtimeT {
+    0
+}
 
 pub struct RrosCounter {
     #[cfg(CONFIG_EVL_RUNSTATS)]
-    counter: u32
+    counter: u32,
 }
 
 impl RrosCounter {
     #[cfg(CONFIG_EVL_RUNSTATS)]
     pub fn new(counter: u32) -> Self {
-        RrosCounter{
-            counter
-        }
+        RrosCounter { counter }
     }
 
     #[cfg(CONFIG_EVL_RUNSTATS)]
@@ -144,7 +148,7 @@ impl RrosCounter {
     }
 
     #[cfg(CONFIG_EVL_RUNSTATS)]
-    pub fn get_counter(&self) -> u32{
+    pub fn get_counter(&self) -> u32 {
         self.counter
     }
 
@@ -155,20 +159,22 @@ impl RrosCounter {
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
     pub fn new() -> Self {
-        RrosCounter{}
+        RrosCounter {}
     }
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn inc_counter(&mut self) {;}
+    pub fn inc_counter(&mut self) {}
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn get_counter(&self) -> u32{0}
+    pub fn get_counter(&self) -> u32 {
+        0
+    }
 
     #[cfg(not(CONFIG_EVL_RUNSTATS))]
-    pub fn set_counter(&mut self, value: u32) {;}
+    pub fn set_counter(&mut self, value: u32) {}
 }
 
 pub fn rros_switch_account(rq: Option<*mut rros_rq>, new_account: *mut RrosAccount) {
     rros_update_account(rq);
-    rros_set_current_account(rq,new_account);
-}
\ No newline at end of file
+    rros_set_current_account(rq, new_account);
+}
diff --git a/kernel/rros/stat_test.rs b/kernel/rros/stat_test.rs
index d289e1735..939273238 100644
--- a/kernel/rros/stat_test.rs
+++ b/kernel/rros/stat_test.rs
@@ -3,19 +3,19 @@
 use crate::factory;
 use crate::list::*;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    sched::*, 
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, sched::*,
+    timer::*, RROS_OOB_CPUS,
 };
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::Deref;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,
+    bindings, c_types, cpumask::CpumaskT, double_linked_list::*, file_operations::FileOperations,
+    ktime::*, percpu, percpu_defs, prelude::*, premmpt, spinlock_init, str::CStr, sync::Guard,
+    sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use core::ops::DerefMut;
-use core::ops::Deref;
 
-// CFG为!CONFIG_EVL_RUNSTATS未测试，理论可行
\ No newline at end of file
+// CFG为!CONFIG_EVL_RUNSTATS未测试，理论可行
diff --git a/kernel/rros/syscall.rs b/kernel/rros/syscall.rs
new file mode 100644
index 000000000..cca7fca0e
--- /dev/null
+++ b/kernel/rros/syscall.rs
@@ -0,0 +1,472 @@
+use core::mem::size_of;
+
+use kernel::{
+    bindings,
+    prelude::*,
+    premmpt::running_inband,
+    sync::{Guard, Lock, SpinLock},
+    task::Task,
+    user_ptr::UserSlicePtr, io_buffer::IoBufferWriter, c_types::c_void,
+    uapi::time_types::{KernelOldTimespec, KernelTimespec},
+};
+
+use crate::{
+    arch::arm64::syscall::*,
+    lock, oob_arg1, oob_arg2, oob_arg3, oob_retval,
+    is_clock_gettime, is_clock_gettime64,
+    sched::{rros_is_inband, rros_switch_inband, rros_thread},
+    thread::*,
+    uapi::rros::syscall::*,
+    uapi::rros::thread::*, clock::{RROS_MONO_CLOCK, RROS_REALTIME_CLOCK, rros_read_clock},
+};
+
+const SYSCALL_PROPAGATE: i32 = 0;
+const SYSCALL_STOP: i32 = 1;
+
+extern "C" {
+    fn rust_helper_current_cap() -> bindings::kernel_cap_t;
+    fn rust_helper_cap_raised(c: bindings::kernel_cap_t, f: i32) -> i32;
+    fn rust_helper_ktime_to_timespec64(kt: bindings::ktime_t) -> bindings::timespec64;
+}
+
+fn rros_read(fd: i32, u_buf: *mut u8, size: isize) -> i32 {
+    pr_info!("read syscall is called");
+    0
+    // 	struct evl_file *efilp = rros_get_file(fd);
+    // 	struct file *filp;
+    // 	ssize_t ret;
+
+    // 	if (efilp == NULL)
+    // 		return -EBADF;
+
+    // 	filp = efilp->filp;
+    // 	if (!(filp->f_mode & FMODE_READ)) {
+    // 		ret = -EBADF;
+    // 		goto out;
+    // 	}
+
+    // 	if (filp->f_op->oob_read == NULL) {
+    // 		ret = -EINVAL;
+    // 		goto out;
+    // 	}
+
+    // 	ret = filp->f_op->oob_read(filp, u_buf, size);
+    // out:
+    // 	evl_put_file(efilp);
+
+    // 	return ret;
+}
+
+fn rros_write(fd: i32, u_buf: *mut u8, size: isize) -> i32 {
+    pr_info!("write syscall is called");
+    0
+}
+
+fn rros_ioctl(fd: i32, request: u32, arg: u32) -> i32 {
+    pr_info!("ioctl syscall is called");
+    0
+}
+
+fn invoke_syscall(nr: u32, regs: *mut bindings::pt_regs) {
+    let mut ret = 0;
+
+    /*
+     * We have only very few syscalls, prefer a plain switch to a
+     * pointer indirection which ends up being fairly costly due
+     * to exploit mitigations.
+     */
+    unsafe {
+        match (nr) {
+            // [TODO: lack __user]
+            sys_evl_read => {
+                ret = rros_read(
+                    oob_arg1!(regs) as i32,
+                    oob_arg2!(regs) as *mut u8,
+                    oob_arg3!(regs) as isize,
+                );
+            }
+            sys_evl_write => {
+                ret = rros_write(
+                    oob_arg1!(regs) as i32,
+                    oob_arg2!(regs) as *mut u8,
+                    oob_arg3!(regs) as isize,
+                );
+            }
+            sys_evl_ioctl => {
+                ret = rros_ioctl(
+                    oob_arg1!(regs) as i32,
+                    oob_arg2!(regs) as u32,
+                    oob_arg3!(regs) as u32,
+                );
+            }
+            _ => {
+                pr_alert!("err!");
+            }
+        }
+    }
+
+    set_oob_retval(regs, ret);
+}
+
+fn prepare_for_signal(
+    p: *mut SpinLock<rros_thread>,
+    curr: *mut SpinLock<rros_thread>,
+    regs: *mut bindings::pt_regs,
+) {
+    let mut flags;
+
+    // /*
+    // * FIXME: no restart mode flag for setting -EINTR instead of
+    // * -ERESTARTSYS should be obtained from curr->local_info on a
+    // * per-invocation basis, not on a per-call one (since we have
+    // * 3 generic calls only).
+    // */
+    // /*
+    // * @curr == this_evl_rq()->curr over oob so no need to grab
+    // * @curr->lock (i.e. @curr cannot go away under out feet).
+    // */
+    // [TODO: use the curr rq lock to make smp work]
+    flags = lock::raw_spin_lock_irqsave();
+    // raw_spin_lock_irqsave(&curr->rq->lock, flags);
+
+    // /*
+    // * We are called from out-of-band mode only to act upon a
+    // * pending signal receipt. We may observe signal_pending(p)
+    // * which implies that T_KICKED was set too
+    // * (handle_sigwake_event()), or T_KICKED alone which means
+    // * that we have been unblocked from a wait for some other
+    // * reason.
+    // */
+    let res2 = unsafe { (*(*curr).locked_data().get()).info & T_KICKED != 0 };
+    unsafe {
+        if (res2) {
+            let res1 = Task::current().signal_pending();
+
+            if (res1) {
+                set_oob_error(regs, -(bindings::ERESTARTSYS as i32));
+                (*(*curr).locked_data().get()).info &= !T_BREAK;
+            }
+            (*(*curr).locked_data().get()).info &= !T_KICKED;
+        }
+    }
+
+    lock::raw_spin_unlock_irqrestore(flags);
+
+    rros_test_cancel();
+
+    rros_switch_inband(EVL_HMDIAG_SYSDEMOTE);
+}
+
+fn handle_vdso_fallback(nr: i32, regs: *mut bindings::pt_regs) -> bool {
+    let u_old_ts;
+    let mut uts: KernelOldTimespec = KernelOldTimespec::new();
+    let u_uts;    
+    let mut old_ts: KernelTimespec = KernelTimespec::new();
+
+    let clock;
+    let mut ts64;
+    let mut clock_id;
+    let mut ret = 0;
+
+    if (!is_clock_gettime!(nr) && !is_clock_gettime64!(nr)) {
+        return false;
+    }        
+
+    clock_id = unsafe{oob_arg1!(regs) as u32};
+    match (clock_id) {
+    	bindings::CLOCK_MONOTONIC => {
+            clock = unsafe{&RROS_MONO_CLOCK};
+        }
+        bindings::CLOCK_REALTIME => {
+            clock = unsafe{&RROS_REALTIME_CLOCK};
+        }
+        _ =>{
+            pr_alert!("error clock");
+            // clock = unsafe{&RROS_MONO_CLOCK};
+            return false
+        }
+    }
+
+    ts64 = unsafe{rust_helper_ktime_to_timespec64(rros_read_clock(clock))};
+
+    if (is_clock_gettime!(nr)) {
+        old_ts.spec.tv_sec = ts64.tv_sec as bindings::__kernel_old_time_t;
+        old_ts.spec.tv_nsec = ts64.tv_nsec;
+        // [TODO: lack the size of u_old_rs]
+        u_old_ts = unsafe { UserSlicePtr::new(oob_arg2!(regs) as *mut c_void, size_of::<KernelTimespec>()) }; 
+        let res = unsafe{u_old_ts.writer().write_raw(&mut old_ts as *mut KernelTimespec as *mut u8 as *const u8, size_of::<KernelTimespec>())};        
+        ret = match res {
+            Ok(()) => 0,
+            Err(e) => -(bindings::EFAULT as i32),
+        };
+    } else if (is_clock_gettime64!(nr)) {
+        uts.spec.tv_sec = ts64.tv_sec;
+        uts.spec.tv_nsec = ts64.tv_nsec;
+        // [TODO: lack the size of u_uts]
+        u_uts = unsafe { UserSlicePtr::new(oob_arg2!(regs) as *mut c_void, size_of::<KernelOldTimespec>()) };
+        let res = unsafe{u_uts.writer().write_raw(&mut uts as *mut KernelOldTimespec as *mut u8 as *const u8, size_of::<KernelOldTimespec>())};        
+        ret = match res {
+            Ok(()) => 0,
+            Err(e) => -(bindings::EFAULT as i32),
+        };
+    }
+
+    set_oob_retval(regs, ret);
+
+    // #undef is_clock_gettime
+    // #undef is_clock_gettime64
+
+    true
+}
+
+fn do_oob_syscall(stage: *mut bindings::irq_stage, regs: *mut bindings::pt_regs) -> i32 {
+    let p;
+    let mut nr: u32 = 0;
+
+    if (!is_oob_syscall(regs)) {
+        if rros_is_inband() {
+            return SYSCALL_PROPAGATE;
+        }
+
+        /*
+         * We don't want to trigger a stage switch whenever the
+         * current request issued from the out-of-band stage is not a
+         * valid in-band syscall, but rather deliver -ENOSYS directly
+         * instead.  Otherwise, switch to in-band mode before
+         * propagating the syscall down the pipeline. CAUTION:
+         * inband_syscall_nr(regs, &nr) is valid only if
+         * !is_oob_syscall(regs), which we checked earlier in
+         * do_oob_syscall().
+         */
+        if inband_syscall_nr(regs, &mut nr as *mut u32) {
+            if handle_vdso_fallback(nr as i32, regs) {
+                return SYSCALL_STOP;
+            }
+
+            rros_switch_inband(EVL_HMDIAG_SYSDEMOTE);
+            return SYSCALL_PROPAGATE;
+        }
+
+        // bad_syscall:
+        // [TODO: add evl warning]
+        pr_info!("Warning: invalid out-of-band syscall {}", nr);
+        // printk(EVL_WARNING "invalid out-of-band syscall <%#x>\n", nr);
+
+        set_oob_error(regs, -(bindings::ENOSYS as i32));
+
+        return SYSCALL_STOP;
+    }
+
+    nr = oob_syscall_nr(regs);
+    if (nr >= crate::uapi::rros::syscall::NR_EVL_SYSCALLS) {
+        pr_info!("invalid out-of-band syscall <{}>", nr);
+
+        set_oob_error(regs, -(bindings::ENOSYS as i32));
+        return SYSCALL_STOP;
+    }
+
+    let curr = rros_current();
+    unsafe {
+        let res1 =
+            !(rust_helper_cap_raised(rust_helper_current_cap(), bindings::CAP_SYS_NICE as i32) != 0);
+        pr_info!("curr is {:p} res is {}", curr, res1);
+        if (curr == 0 as *mut SpinLock<rros_thread> || res1) {
+            // [TODO: lack RROS_DEBUG]
+            pr_info!("ERROR: syscall denied");
+            // if (EVL_DEBUG(CORE))
+            // 	printk(EVL_WARNING
+            // 		"syscall <oob_%s> denied to %s[%d]\n",
+            // 		evl_sysnames[nr], current->comm, task_pid_nr(current));
+            set_oob_error(regs, -(bindings::EPERM as i32));
+            return SYSCALL_STOP;
+        }
+    }
+
+    /*
+     * If the syscall originates from in-band context, hand it
+     * over to handle_inband_syscall() where the caller would be
+     * switched to OOB context prior to handling the request.
+     */
+    unsafe {
+        if (stage != &mut bindings::oob_stage as *mut bindings::irq_stage) {
+            return SYSCALL_PROPAGATE;
+        }
+    }
+
+    // [TODO: lack the trace system]
+    // trace_evl_oob_sysentry(nr);
+
+    invoke_syscall(nr, regs);
+
+    /* Syscall might have switched in-band, recheck. */
+    if (!rros_is_inband()) {
+        p = rros_current();
+        let res1 = Task::current().signal_pending();
+        let res2 = unsafe { (*(*curr).locked_data().get()).info & T_KICKED != 0 };
+        let res3 = (Task::current().state() & T_WEAK) != 0;
+        // [TODO: lack covert atomic in bindings to atomic in rfl]
+        let res4 = unsafe {
+            atomic_read(
+                &mut (*(*curr).locked_data().get()).inband_disable_count as *mut bindings::atomic_t,
+            )
+        } != 0;
+        if (res1 || res2) {
+            prepare_for_signal(p, curr, regs);
+        } else if (res3 && !res4) {
+            rros_switch_inband(RROS_HMDIAG_NONE);
+        }
+    }
+
+    /* Update the stats and user visible info. */
+    // [TODO: lack syncing of user info]
+    // evl_inc_counter(&curr->stat.sc);
+    // evl_sync_uwindow(curr);
+
+    // [TODO: lack trace]
+    // trace_evl_oob_sysexit(oob_retval(regs));
+    // unsafe{
+    //     thread::uthread = None;
+    // }
+
+    return SYSCALL_STOP;
+}
+
+use crate::thread;
+
+fn do_inband_syscall(stage: *mut bindings::irq_stage, regs: *mut bindings::pt_regs) -> i32 {
+    let curr = rros_current();
+    // struct evl_thread *curr = evl_current(); /* Always valid. */
+    let p;
+    // struct task_struct *p;
+    let nr;
+    // unsigned int nr;
+    let ret;
+    // int ret;
+
+    /*
+     * Some architectures may use special out-of-bound syscall
+     * numbers which escape Dovetail's range check, e.g. when
+     * handling aarch32 syscalls over an aarch64 kernel. When so,
+     * assume this is an in-band syscall which we need to
+     * propagate downstream to the common handler.
+     */
+    if (curr == 0 as *mut SpinLock<rros_thread>) {
+        return SYSCALL_PROPAGATE;
+    }
+
+    /*
+     * Catch cancellation requests pending for threads undergoing
+     * the weak scheduling policy, which won't cross
+     * prepare_for_signal() frequently as they run mostly in-band.
+     */
+    rros_test_cancel();
+
+    /* Handle lazy schedparam updates before switching. */
+    // evl_propagate_schedparam_change(curr);
+
+    /* Propagate in-band syscalls. */
+    if (!is_oob_syscall(regs)) {
+        return SYSCALL_PROPAGATE;
+    }
+
+    /*
+     * Process an OOB syscall after switching current to OOB
+     * context.  do_oob_syscall() already checked the syscall
+     * number.
+     */
+    nr = oob_syscall_nr(regs);
+
+    // [TODO: lack trace]
+    // trace_evl_inband_sysentry(nr);
+
+    ret = evl_switch_oob();
+    /*
+     * -ERESTARTSYS might be received if switching oob was blocked
+     * by a pending signal, otherwise -EINTR might be received
+     * upon signal detection after the transition to oob context,
+     * in which case the common logic applies (i.e. based on
+     * T_KICKED and/or signal_pending()).
+     */
+    if (ret == Err(kernel::Error::ERESTARTSYS)) {
+        set_oob_error(regs, -(bindings::ERESTARTSYS as i32));
+
+        let res1 = unsafe { (*(*curr).locked_data().get()).local_info };
+        if res1 & T_IGNOVR == 1 {
+            unsafe {
+                (*(*curr).locked_data().get()).local_info &= !T_IGNOVR;
+            }
+        }
+
+        // [TODO: lack sync user stat]
+        // evl_inc_counter(&curr->stat.sc);
+        // evl_sync_uwindow(curr);
+
+        // [TODO: lack trace]
+        // trace_evl_inband_sysexit(oob_retval(regs));
+
+        return SYSCALL_STOP;
+    }
+
+    invoke_syscall(nr, regs);
+
+    if (!rros_is_inband()) {
+        p = rros_current();
+        let res1 = Task::current().signal_pending();
+        let res2 = unsafe { (*(*curr).locked_data().get()).info & T_KICKED != 0 };
+        let res3 = (Task::current().state() & T_WEAK) != 0;
+        let res4 = unsafe {
+            atomic_read(
+                &mut (*(*curr).locked_data().get()).inband_disable_count as *mut bindings::atomic_t,
+            )
+        } != 0;
+        if (res1 || res2) {
+            prepare_for_signal(p, curr, regs);
+        } else if (res3 && res4) {
+            rros_switch_inband(RROS_HMDIAG_NONE);
+        }
+    }
+
+    let res1 = unsafe { (*(*curr).locked_data().get()).local_info };
+    if (res1 & T_IGNOVR) != 0 {
+        unsafe {
+            (*(*curr).locked_data().get()).local_info &= !T_IGNOVR;
+        }
+    }
+
+    // [TODO: lack sync user stat]
+    // evl_inc_counter(&curr->stat.sc);
+    // evl_sync_uwindow(curr);
+
+    // [TODO: lack trace]
+    // trace_evl_inband_sysexit(oob_retval!(regs));
+
+    return SYSCALL_STOP;
+}
+
+// gcc /root/evl_output/lib/libevl.so write.c -lpthread -g -o write 
+// export C_INCLUDE_PATH=$C_INCLUDE_PATH:/root/evl_output/include
+#[no_mangle]
+unsafe extern "C" fn  handle_pipelined_syscall(stage: *mut bindings::irq_stage, regs: *mut bindings::pt_regs) -> i32 {
+    // [TODO: lack unlikely]
+    let res = running_inband();
+    let r = match res {
+        Ok(_o) => true,
+        Err(_e) => false,
+    };
+    if r {
+        return do_inband_syscall(stage, regs);
+    }
+    return do_oob_syscall(stage, regs);
+}
+
+#[no_mangle]
+unsafe extern "C" fn handle_oob_syscall(regs: *mut bindings::pt_regs) {
+    let ret: i32;
+    // if running_inband().is_ok() {
+    //     // return;
+    // }
+
+    ret = unsafe { do_oob_syscall(&mut bindings::oob_stage as *mut bindings::irq_stage, regs) };
+    // [TODO: lack warn_on]
+    // EVL_WARN_ON(CORE, ret == SYSCALL_PROPAGATE);
+}
diff --git a/kernel/rros/test.rs b/kernel/rros/test.rs
index db297dbf7..d400fb58c 100644
--- a/kernel/rros/test.rs
+++ b/kernel/rros/test.rs
@@ -3,16 +3,146 @@
 use crate::factory;
 use crate::list::*;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    sched, 
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, sched,
+    timer::*, RROS_OOB_CPUS, this_rros_rq
 };
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,
+    bindings, c_types, cpumask::CpumaskT, double_linked_list::*, file_operations::FileOperations,
+    ktime::*, percpu, percpu_defs, prelude::*, premmpt, spinlock_init, str::CStr, sync::Guard,
+    sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use core::ops::DerefMut;
+
+use core::fmt::{Display,Formatter};
+use kernel::{vmalloc, pr_info, pr_crit, mm, container_of};
+use core::fmt;
+
+
+#[derive(Debug, Clone)]
+pub struct TestFailed{
+    pub file: &'static str,
+    pub line: u32,
+    pub col: u32,
+    pub msg: &'static str,
+}
+
+impl TestFailed {
+    fn new_without_msg(file: &'static str, line: u32, col: u32) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg: "",
+        }
+    }
+    fn new(file: &'static str, line: u32, col: u32, msg: &'static str) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg,
+        }
+    }
+}
+
+impl fmt::Display for TestFailed {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        if self.msg.is_empty(){
+            write!(f, "Test failed on {}:{}:{}",self.file, self.line, self.col)
+        }else{
+            write!(f, "Test failed on {}:{}:{}  :  {}",self.file, self.line, self.col, self.msg)
+        }
+    }
+}
+
+#[macro_export]
+macro_rules! test_failed_here {
+    () => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""});
+    };
+    ($msg:expr) => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg});
+    };
+}
+
+#[macro_export]
+macro_rules! test {
+    ($left:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $msg:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! test_eq {
+    ($left:expr, $right:expr $(,)?) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $right:expr, $msg:expr) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+pub type Result<T> = core::result::Result<T, TestFailed>;
+#[inline]
+pub fn handle_and_print_result(test_name:&'static str, r: Result<()>){
+    match r{
+        Ok(())=>{pr_crit!("[RAND]Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+}
+
+// macro_rules! pr_color_crit {
+//     ($color:expr, $($arg:tt)*) => {{
+//         pr_crit!("\x1b[{}m",$color);
+//         pr_crit!($($arg)*);
+//         pr_crit!("\x1b[0m");
+//     }};
+// }
+// macro_rules! pr_green_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(32,$($arg)*);
+//     }};
+// }
+// macro_rules! pr_red_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(31,$($arg)*);
+//     }};
+// }
\ No newline at end of file
diff --git a/kernel/rros/test_thread.rs b/kernel/rros/test_thread.rs
new file mode 100644
index 000000000..44543f87b
--- /dev/null
+++ b/kernel/rros/test_thread.rs
@@ -0,0 +1,41 @@
+use kernel::prelude::*;
+use crate::fifo_test::{test___rros_enqueue_fifo_thread, test___rros_enqueue_fifo_thread_without_dequeue};
+use crate::{test::*, this_rros_rq, clock_test, thread_test};
+use crate::sched::lookup_fifo_class;
+
+fn test_loopup_fifo_class() -> crate::test::Result<()> {
+    let rq_ptr1 = this_rros_rq();
+    let mut length;
+
+    let rq = this_rros_rq();
+    test___rros_enqueue_fifo_thread_without_dequeue();
+    length = unsafe{(*rq_ptr1).fifo.runnable.head.clone().unwrap().len()};
+    test_eq!(length, 3)?;
+
+    let res1 = lookup_fifo_class(Some(rq)).unwrap();
+    length = unsafe{(*rq_ptr1).fifo.runnable.head.clone().unwrap().len()};
+    test_eq!(length, 2)?;
+    let res2 = lookup_fifo_class(Some(rq)).unwrap();
+    length = unsafe{(*rq_ptr1).fifo.runnable.head.clone().unwrap().len()};
+    test_eq!(length, 1)?;
+    let res3 = lookup_fifo_class(Some(rq)).unwrap();
+    length = unsafe{(*rq_ptr1).fifo.runnable.head.clone().unwrap().len()};
+    test_eq!(length, 0)?;
+    Ok(())
+}
+
+fn test___rros_schedule() -> crate::test::Result<()> {
+    thread_test::test_thread_context_switch();
+    Ok(())
+}
+
+pub fn test_rros_thread() {
+    pr_info!("--------------test rros thread begin-----------------");
+    handle_and_print_result("fifo enqueue with the prority", test___rros_enqueue_fifo_thread());
+    handle_and_print_result("lookup fifo class", test_loopup_fifo_class());
+    handle_and_print_result("handle clock tick", clock_test::test_do_clock_tick());
+    // If you pass the `rros_enable_tick` and `setup_proxy` test, you will not see the error message such as
+    // "rust_kernel: init clock_proxy_device error!" and "rros: cpd new error!".
+    handle_and_print_result("context switch between threads", test___rros_schedule());
+    pr_info!("--------------test rros thread end-----------------");
+}
\ No newline at end of file
diff --git a/kernel/rros/thread.rs b/kernel/rros/thread.rs
index 43c956c47..d9014b0c3 100644
--- a/kernel/rros/thread.rs
+++ b/kernel/rros/thread.rs
@@ -1,33 +1,39 @@
+use crate::clock::rros_read_clock;
+use crate::lock::{raw_spin_lock_irqsave, raw_spin_unlock_irqrestore};
+use crate::wait::RrosWaitChannel;
+use crate::{tick, fifo};
+use crate::timer::{RrosRq, rros_start_timer, rros_abs_timeout, rros_stop_timer};
+use crate::{
+    clock, factory, fifo::rros_sched_fifo, idle, list::*, lock, sched::*, stat, timeout, timer,file::RrosFileBinding,
+    RROS_OOB_CPUS,
+};
+// tp::rros_sched_tp
+use alloc::alloc_rros::rros_alloc_zeroed;
 use alloc::rc::Rc;
-use kernel::{c_str, ktime};
+use kernel::bindings::prepare_creds;
+use core::ops::{Deref, DerefMut};
+use core::result::Result::Ok;
+use core::{cell::RefCell, clone::Clone};
 use kernel::completion::Completion;
 use kernel::cpumask::CpumaskT;
+use kernel::error::from_kernel_err_ptr;
 use kernel::irq_work::IrqWork;
+use kernel::premmpt;
 use kernel::str::CStr;
 use kernel::task::Task;
-use kernel::premmpt;
-use kernel::error::from_kernel_err_ptr;
-use core::result::Result::Ok;
-use core::{cell::RefCell, clone::Clone};
-use core::ops::{DerefMut, Deref};
 #[warn(unused_mut)]
-use kernel::{bindings, c_types, cpumask, prelude::*,sync::{SpinLock,Lock}};
-use crate::tick;
-use crate::timer::RrosRq;
-use crate::{
-    factory,
-    RROS_OOB_CPUS,
-    sched::*,
-    list::*,
-    fifo::rros_sched_fifo,
-    idle,
-    clock,
-    timeout,
-    timer,    
-    stat,
-    lock,
+use kernel::{
+    bindings, c_types, cpumask,
+    prelude::*,
+    sync::{Lock, SpinLock},
+    spinlock_init,
+    memory_rros::*,
 };
-
+use kernel::file_operations::FileOperations;
+use kernel::{c_str, ktime};
+use kernel::io_buffer::IoBufferWriter;
+use kernel::file::File;
+use core::mem::size_of;
 
 extern "C" {
     // pub fn kvasprintf(
@@ -36,6 +42,11 @@ extern "C" {
     //     args: va_list,
     // ) -> *mut c_types::c_char;
     fn rust_helper_atomic_set(v:*mut bindings::atomic_t,i:i32);
+    fn rust_helper_atomic_inc(v:*mut bindings::atomic_t);
+    fn rust_helper_atomic_dec_and_test(v:*mut bindings::atomic_t) -> bool;
+    fn rust_helper_atomic_dec_return(v:*mut bindings::atomic_t) -> i32;
+    fn rust_helper_atomic_cmpxchg(v:*mut bindings::atomic_t,old:i32,new:i32) -> i32;
+    fn rust_helper_atomic_read(v:*mut bindings::atomic_t) ->i32;
     fn rust_helper_init_irq_work(work:*mut bindings::irq_work,func:unsafe extern "C" fn(work:*mut bindings::irq_work));
     fn rust_helper_kthread_run(
         threadfn: Option<unsafe extern "C" fn(*mut c_types::c_void) -> c_types::c_int>,
@@ -43,131 +54,237 @@ extern "C" {
         namefmt: *const c_types::c_char,
         ...
     ) -> *mut c_types::c_void;
-    fn rust_helper_dovetail_request_ucall(task:*mut bindings::task_struct);
+    fn rust_helper_dovetail_request_ucall(task: *mut bindings::task_struct);
     fn rust_helper_unstall_oob();
     fn rust_helper_dovetail_current_state() -> *mut bindings::oob_thread_state;
+    fn rust_helper_cap_raise(c: *mut bindings::kernel_cap_t, flag: i32);
 }
 
+pub const SIGDEBUG: i32 = 24;
+pub const sigdebug_marker:u32 = 0xfccf0000;
+pub const SI_QUEUE:i32 = -1;
 pub const T_SUSP: u32 =    0x00000001; /* Suspended */
 pub const T_PEND: u32 =    0x00000002; /* Blocked on a wait_queue/mutex */
 pub const T_DELAY: u32 =   0x00000004; /* Delayed/timed */
 pub const T_WAIT: u32 =    0x00000008; /* Periodic wait */
 pub const T_READY: u32 =   0x00000010; /* Ready to run (in rq) */
 pub const T_DORMANT: u32 = 0x00000020; /* Not started yet */
-pub const T_ZOMBIE: u32 =  0x00000040; /* Dead, waiting for disposal */
-pub const T_INBAND: u32 =  0x00000080; /* Running in-band */
-pub const T_HALT: u32 =    0x00000100; /* Halted */
-pub const T_BOOST: u32 =   0x00000200; /* PI/PP boost undergoing */
-pub const T_PTSYNC: u32 =  0x00000400; /* Synchronizing on ptrace event */
-pub const T_RRB: u32 =     0x00000800; /* Undergoes round-robin scheduling */
-pub const T_ROOT: u32 =    0x00001000; /* Root thread (in-band kernel placeholder) */
-pub const T_WEAK: u32 =    0x00002000; /* Weak scheduling (in-band) */
-pub const T_USER: u32 =    0x00004000; /* Userland thread */
-pub const T_WOSS: u32 =    0x00008000; /* Warn on stage switch (HM) */
-pub const T_WOLI: u32 =    0x00010000; /* Warn on locking inconsistency (HM) */
-pub const T_WOSX: u32 =    0x00020000; /* Warn on stage exclusion (HM) */
-pub const T_PTRACE: u32 =  0x00040000; /* Stopped on ptrace event */
-pub const T_OBSERV: u32 =  0x00080000; /* Observable (only for export to userland) */
-pub const T_HMSIG: u32 =   0x00100000; /* Notify HM events via SIGDEBUG */
-pub const T_HMOBS: u32 =   0x00200000; /* Notify HM events via observable */
-
-pub const T_TIMEO: u32 =   0x00000001; /* Woken up due to a timeout condition */
-pub const T_RMID: u32 =    0x00000002; /* Pending on a removed resource */
-pub const T_BREAK: u32 =   0x00000004; /* Forcibly awaken from a wait state */
-pub const T_KICKED: u32 =  0x00000008; /* Forced out of OOB context */
-pub const T_WAKEN: u32 =   0x00000010; /* Thread waken up upon resource availability */
-pub const T_ROBBED: u32 =  0x00000020; /* Robbed from resource ownership */
+pub const T_ZOMBIE: u32 = 0x00000040; /* Dead, waiting for disposal */
+pub const T_INBAND: u32 = 0x00000080; /* Running in-band */
+pub const T_HALT: u32 = 0x00000100; /* Halted */
+pub const T_BOOST: u32 = 0x00000200; /* PI/PP boost undergoing */
+pub const T_PTSYNC: u32 = 0x00000400; /* Synchronizing on ptrace event */
+pub const T_RRB: u32 = 0x00000800; /* Undergoes round-robin scheduling */
+pub const T_ROOT: u32 = 0x00001000; /* Root thread (in-band kernel placeholder) */
+pub const T_WEAK: u32 = 0x00002000; /* Weak scheduling (in-band) */
+pub const T_USER: u32 = 0x00004000; /* Userland thread */
+pub const T_WOSS: u32 = 0x00008000; /* Warn on stage switch (HM) */
+pub const T_WOLI: u32 = 0x00010000; /* Warn on locking inconsistency (HM) */
+pub const T_WOSX: u32 = 0x00020000; /* Warn on stage exclusion (HM) */
+pub const T_PTRACE: u32 = 0x00040000; /* Stopped on ptrace event */
+pub const T_OBSERV: u32 = 0x00080000; /* Observable (only for export to userland) */
+pub const T_HMSIG: u32 = 0x00100000; /* Notify HM events via SIGDEBUG */
+pub const T_HMOBS: u32 = 0x00200000; /* Notify HM events via observable */
+
+pub const T_TIMEO: u32 = 0x00000001; /* Woken up due to a timeout condition */
+pub const T_RMID: u32 = 0x00000002; /* Pending on a removed resource */
+pub const T_BREAK: u32 = 0x00000004; /* Forcibly awaken from a wait state */
+pub const T_KICKED: u32 = 0x00000008; /* Forced out of OOB context */
+pub const T_WAKEN: u32 = 0x00000010; /* Thread waken up upon resource availability */
+pub const T_ROBBED: u32 = 0x00000020; /* Robbed from resource ownership */
 pub const T_CANCELD: u32 = 0x00000040; /* Cancellation request is pending */
 pub const T_PIALERT: u32 = 0x00000080; /* Priority inversion alert (HM notified) */
-pub const T_SCHEDP: u32 =  0x00000100; /* Schedparam propagation is pending */
-pub const T_BCAST: u32 =   0x00000200; /* Woken up upon resource broadcast */
-pub const T_SIGNAL: u32 =  0x00000400; /* Event monitor signaled */
+pub const T_SCHEDP: u32 = 0x00000100; /* Schedparam propagation is pending */
+pub const T_BCAST: u32 = 0x00000200; /* Woken up upon resource broadcast */
+pub const T_SIGNAL: u32 = 0x00000400; /* Event monitor signaled */
 pub const T_SXALERT: u32 = 0x00000800; /* Stage exclusion alert (HM notified) */
-pub const T_PTSIG: u32 =   0x00001000; /* Ptrace signal is pending */
-pub const T_PTSTOP: u32 =  0x00002000; /* Ptrace stop is ongoing */
-pub const T_PTJOIN: u32 =  0x00004000; /* Ptracee should join ptsync barrier */
-pub const T_NOMEM: u32 =   0x00008000; /* No memory to complete the operation */
+pub const T_PTSIG: u32 = 0x00001000; /* Ptrace signal is pending */
+pub const T_PTSTOP: u32 = 0x00002000; /* Ptrace stop is ongoing */
+pub const T_PTJOIN: u32 = 0x00004000; /* Ptracee should join ptsync barrier */
+pub const T_NOMEM: u32 = 0x00008000; /* No memory to complete the operation */
 
-pub const T_SYSRST: u32 =  0x00000001; /* Thread awaiting syscall restart after signal */
-pub const T_IGNOVR: u32 =  0x00000002; /* Overrun detection temporarily disabled */
+pub const T_SYSRST: u32 = 0x00000001; /* Thread awaiting syscall restart after signal */
+pub const T_IGNOVR: u32 = 0x00000002; /* Overrun detection temporarily disabled */
 pub const T_INFAULT: u32 = 0x00000004; /* In fault handling */
 
-const RROS_INFINITE:i64 = 0;
+const RROS_INFINITE: i64 = 0;
 
-pub const RROS_THREAD_BLOCK_BITS: u32 = (T_SUSP|T_PEND|T_DELAY|T_WAIT|T_DORMANT|T_INBAND|T_HALT|T_PTSYNC);
-pub const RROS_THREAD_INFO_MASK: u32 = (T_RMID|T_TIMEO|T_BREAK|T_WAKEN|T_ROBBED|T_KICKED|T_BCAST|T_NOMEM);
-pub const RROS_THREAD_MODE_BITS: u32 = (T_WOSS|T_WOLI|T_WOSX|T_HMSIG|T_HMOBS);
+pub const RROS_THREAD_BLOCK_BITS: u32 =
+    (T_SUSP | T_PEND | T_DELAY | T_WAIT | T_DORMANT | T_INBAND | T_HALT | T_PTSYNC);
+pub const RROS_THREAD_INFO_MASK: u32 =
+    (T_RMID | T_TIMEO | T_BREAK | T_WAKEN | T_ROBBED | T_KICKED | T_BCAST | T_NOMEM);
+pub const RROS_THREAD_MODE_BITS: u32 = (T_WOSS | T_WOLI | T_WOSX | T_HMSIG | T_HMOBS);
 
 pub const RROS_HMDIAG_NONE: i32 = 0;
 pub const RROS_HMDIAG_TRAP: i32 = -1;
+pub const RROS_HMDIAG_LKDEPEND:i32 = 5;
+pub const RROS_HMDIAG_LKIMBALANCE:i32 = 6;
+
+pub const RROS_THRIOC_SET_SCHEDPARAM:u32 = 1;
+pub const RROS_THRIOC_GET_SCHEDPARAM:u32 = 2;
+pub const RROS_THRIOC_GET_STATE:u32 = 4;
+
+// TODO: move this to the config file
+pub const CONFIG_RROS_NR_THREADS:usize = 8;
+
+pub static mut RROS_TRHEAD_FACTORY: SpinLock<factory::RrosFactory> = unsafe {
+    SpinLock::new(factory::RrosFactory {
+        // TODO: move this and clock factory name to a variable 
+        name: unsafe { CStr::from_bytes_with_nul_unchecked("RROS_THREAD_DEV\0".as_bytes()) },
+        nrdev: CONFIG_RROS_NR_THREADS,
+        // TODO: add the corresponding ops
+        build: Some(thread_factory_build),
+        // TODO: add the corresponding ops
+        dispose: None,
+        // TODO: add the corresponding attr
+        attrs: None, //sysfs::attribute_group::new(),
+        // TODO: rename this flags to the bit level variable RROS_FACTORY_CLONE and RROS_FACTORY_SINGLE
+        flags: 1,
+        inside: Some(factory::RrosFactoryInside {
+            rrtype: None,
+            class: None,
+            cdev: None,
+            device: None,
+            sub_rdev: None,
+            kuid: None,
+            kgid: None,
+            minor_map: None,
+            index: None,
+            name_hash: None,
+            hash_lock: None,
+            register: None,
+        }),
+    })
+};
+
+#[derive(Default)]
+pub struct ThreadOps;
+
+impl FileOperations for ThreadOps {
+    kernel::declare_file_operations!(read);
+
+    fn read<T: IoBufferWriter>(
+        _this: &Self,
+        _file: &File,
+        _data: &mut T,
+        _offset: u64,
+    ) -> Result<usize> {
+        pr_info!("I'm the read ops of the rros thread factory.");
+        Ok(1)
+    }
+}
 
-use core::fmt::{Debug, Formatter, Error};
 use core::fmt;
+use core::fmt::{Debug, Formatter};
 
-pub struct RrosKthread{
+pub struct RrosKthread {
     pub thread: Option<Arc<SpinLock<rros_thread>>>,
     pub done: Completion,
-    pub kthread_fn: Option<fn()>,
+    pub kthread_fn:  Option<Box<dyn FnOnce()->()>>,
     status: i32,
     pub irq_work: IrqWork,
 }
 
-impl Debug for RrosKthread{
+impl Debug for RrosKthread {
     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
         f.write_str("good ew")
     }
 }
 
 impl RrosKthread {
-    pub fn new(kfn: fn())-> Self{
-        RrosKthread { 
+    pub fn new(kfn: Option<Box<dyn FnOnce()->()>>) -> Self {
+        RrosKthread {
             thread: None,
             done: Completion::new(),
-            kthread_fn: Some(kfn),
+            kthread_fn: kfn,
             status: 0,
             irq_work: IrqWork::new(),
         }
     }
 }
 
+pub struct sig_irqwork_data {
+	thread: *mut rros_thread,
+	signo: i32, 
+    sigval: u32,
+	work:IrqWork,
+}
+
+impl sig_irqwork_data{
+    pub fn new()-> Self{
+        sig_irqwork_data{
+            thread: 0 as *mut rros_thread,
+            signo: 0,
+            sigval: 0,
+            work: IrqWork::new(),
+        }
+    }
+}
+
+pub struct rros_thread_state{
+    pub eattrs:rros_sched_attrs,
+	pub cpu:u32,
+	pub state:u32,
+	pub isw:u32,
+	pub csw:u32,
+	pub sc:u32,
+	pub rwa:u32,
+	pub xtime:u32,
+}
+impl rros_thread_state{
+    pub fn new() -> Self{
+        rros_thread_state{
+            eattrs:rros_sched_attrs::new(),
+            cpu:0,
+            state:0,
+            isw:0,
+            csw:0,
+            sc:0,
+            rwa:0,
+            xtime:0,
+        }
+    }
+}
+
+
 pub fn rros_init_thread(
     thread: &Option<Arc<SpinLock<rros_thread>>>,
     // rq_s: Rc<RefCell<rros_rq>>,
     // iattr: Rc<RefCell<rros_init_thread_attr>>,
     iattr: rros_init_thread_attr,
     rq: *mut rros_rq,
-    fmt:&'static CStr,
+    fmt: &'static CStr,
     // args:fmt::Arguments<'_>
-) -> Result<usize>{
+) -> Result<usize> {
     let iattr_ptr = iattr;
     let mut flags = iattr_ptr.flags & (!T_SUSP as i32);
-    let ret: i32; 
+    let ret: i32;
     let gravity: i32;
     let affinity: *mut bindings::cpumask;
-    
 
     if flags & (T_ROOT as i32) == 0x0 {
-        pr_info!("called timesssss");
+        // pr_info!("called timesssss");
         flags |= ((T_DORMANT | T_INBAND) as i32);
     }
 
     //if ((flags & T_USER) && IS_ENABLED(CONFIG_EVL_DEBUG_WOLI))
     //       flags |= T_WOLI;
 
-    if iattr_ptr.observable.is_some(){
-		flags |= (T_OBSERV as i32);
+    if iattr_ptr.observable.is_some() {
+        flags |= (T_OBSERV as i32);
     }
 
     // pr_info!("hello world flags {}", flags);
+    // TODO: add the support for the affinity, this can be related to the `pin_to_initial_cpu()` function
     // if rq.is_none(){
     //     if (!alloc_cpumask_var(&affinity, GFP_KERNEL))
-	// 		return -ENOMEM;
-	// 	cpumask_and(affinity, iattr->affinity, &evl_cpu_affinity);
-	// 	if (!cpumask_empty(affinity))
-	// 		rq = evl_cpu_rq(cpumask_first(affinity));
-	// 	free_cpumask_var(affinity);
-	// 	if (!rq)
-	// 		return -EINVAL;
+    // 		return -ENOMEM;
+    // 	cpumask_and(affinity, iattr->affinity, &evl_cpu_affinity);
+    // 	if (!cpumask_empty(affinity))
+    // 		rq = evl_cpu_rq(cpumask_first(affinity));
+    // 	free_cpumask_var(affinity);
+    // 	if (!rq)
+    // 		return -EINVAL;
     // }
     // va_start(args,fmt);
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&thread.clone().unwrap()));
@@ -190,62 +307,115 @@ pub fn rros_init_thread(
 	thread_unwrap.lock().cprio = idle::RROS_IDLE_PRIO;
 	thread_unwrap.lock().bprio = idle::RROS_IDLE_PRIO;
 	thread_unwrap.lock().rrperiod = RROS_INFINITE;
-	thread_unwrap.lock().wchan = rros_wait_channel::new();
-	thread_unwrap.lock().wwake = None;
+	thread_unwrap.lock().wchan = core::ptr::null_mut();
+	thread_unwrap.lock().wwake = core::ptr::null_mut();
 	thread_unwrap.lock().wait_data = 0 as *mut c_types::c_void;
 	thread_unwrap.lock().u_window = None;
 	thread_unwrap.lock().observable = iattr_ptr.observable.clone();
 
     // thread_lock.rq = Some(rq);
-	// thread_lock.state = flags as __u32;
+    // thread_lock.state = flags as __u32;
     // pr_info!("{}", thread_lock.state);
-	// thread_lock.info = 0;
-	// thread_lock.local_info = 0;
-	// thread_lock.wprio = idle::RROS_IDLE_PRIO;
-	// thread_lock.cprio = idle::RROS_IDLE_PRIO;
-	// thread_lock.bprio = idle::RROS_IDLE_PRIO;
-	// thread_lock.rrperiod = RROS_INFINITE;
-	// thread_lock.wchan = None;
-	// thread_lock.wwake = None;
-	// thread_lock.wait_data = 0 as *mut c_types::c_void;
-	// thread_lock.u_window = None;
-	// thread_lock.observable = iattr_ptr.observable.clone();
+    // thread_lock.info = 0;
+    // thread_lock.local_info = 0;
+    // thread_lock.wprio = idle::RROS_IDLE_PRIO;
+    // thread_lock.cprio = idle::RROS_IDLE_PRIO;
+    // thread_lock.bprio = idle::RROS_IDLE_PRIO;
+    // thread_lock.rrperiod = RROS_INFINITE;
+    // thread_lock.wchan = None;
+    // thread_lock.wwake = None;
+    // thread_lock.wait_data = 0 as *mut c_types::c_void;
+    // thread_lock.u_window = None;
+    // thread_lock.observable = iattr_ptr.observable.clone();
 
     /// fix !!!!!
-    atomic_set(&mut thread.clone().unwrap().lock().deref_mut().inband_disable_count as *mut bindings::atomic_t, 0);
+    atomic_set(
+        &mut thread
+            .clone()
+            .unwrap()
+            .lock()
+            .deref_mut()
+            .inband_disable_count as *mut bindings::atomic_t,
+        0,
+    );
     // memset(&thread->poll_context, 0, sizeof(thread->poll_context));
-	// memset(&thread->stat, 0, sizeof(thread->stat));
-	// memset(&thread->altsched, 0, sizeof(thread->altsched));
+    // memset(&thread->stat, 0, sizeof(thread->stat));
+    // memset(&thread->altsched, 0, sizeof(thread->altsched));
     /// fix !!!!!
-    init_irq_work(thread.clone().unwrap().lock().deref_mut().inband_work.get_ptr(), inband_task_wakeup);
+    init_irq_work(
+        thread
+            .clone()
+            .unwrap()
+            .lock()
+            .deref_mut()
+            .inband_work
+            .get_ptr(),
+        inband_task_wakeup,
+    );
     // pr_info!("yinyongcishu is {}", Arc::strong_count(&thread.clone().unwrap()));
-    rros_set_thread_policy(thread.clone(), iattr_ptr.sched_class.clone(),iattr_ptr.sched_param.clone());
+
+    //tp设置gps
+    // let mut gps = unsafe{evl_system_heap.evl_alloc_chunk((size_of::<rros_tp_schedule>() + 4 * size_of::<rros_tp_window>()) as usize)};
+    // if gps == None{
+    //     return Err(kernel::Error::ENOMEM);
+    // }
+    // unsafe{(*thread_unwrap.lock().rq.unwrap()).tp.gps = gps.unwrap() as *mut rros_tp_schedule};
+
+    rros_set_thread_policy(
+        thread.clone(),
+        iattr_ptr.sched_class.clone(),
+        iattr_ptr.sched_param.clone(),
+    )?;
     // thread_ptr.base_class = Some(iattr_ptr.unwrap());
 
     let rtimer = thread_unwrap.lock().rtimer.as_mut().unwrap().clone();
-    unsafe{timer::rros_init_timer_on_rq(rtimer.clone(), unsafe{&mut clock::RROS_MONO_CLOCK}, Some(timeout_handler),
-    		rq, c_str!("rtimer"), timer::RROS_TIMER_IGRAVITY)};
+    unsafe {
+        timer::rros_init_timer_on_rq(
+            rtimer.clone(),
+            unsafe { &mut clock::RROS_MONO_CLOCK },
+            Some(timeout_handler),
+            rq,
+            c_str!("rtimer"),
+            timer::RROS_TIMER_IGRAVITY,
+        )
+    };
     // let thread_addr = &mut thread_unwrap as *mut ;
     rtimer.lock().thread = Some(thread_unwrap.clone());
-    // let ptimer = thread_unwrap.lock().rtimer.clone().as_mut().unwrap();
-    // unsafe{timer::rros_init_timer_on_rq(ptimer, unsafe{&mut clock::RROS_MONO_CLOCK}, Some(periodic_handler),
-    //         rq, c_str!("ptimer"), RROS_TIMER_IGRAVITY)};
-
+    let ptimer = thread_unwrap.lock().ptimer.as_mut().unwrap().clone();
+    unsafe{timer::rros_init_timer_on_rq(
+        ptimer.clone(), 
+        unsafe{&mut clock::RROS_MONO_CLOCK}, 
+        Some(periodic_handler),
+            rq, 
+            c_str!("ptimer"),
+            timer::RROS_TIMER_IGRAVITY)
+    };
+    ptimer.lock().thread = Some(thread_unwrap.clone());
+    // pr_info!("rros_init_thread success!");
     Ok(0)
 }
 
 pub fn timeout_handler(timer: *mut timer::RrosTimer) {
-    let mut t_thread = unsafe{(*timer).thread()};
+    let mut t_thread = unsafe { (*timer).thread() };
     let thread = t_thread.as_mut().unwrap().clone();
     //  timer.thread().as_mut().unwrap().clone();
     // let rq = this_rros_rq();
-    evl_wakeup_thread(thread, T_DELAY|T_PEND, T_TIMEO as i32);
+    evl_wakeup_thread(thread, T_DELAY | T_PEND, T_TIMEO as i32);
     // rros_sched_tick(rq);
 }
 
-fn evl_wakeup_thread(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, info: i32) {
+pub fn periodic_handler(timer: *mut timer::RrosTimer) {
+    let mut t_thread = unsafe { (*timer).thread() };
+    let thread = t_thread.as_mut().unwrap().clone();
+    // let rq = this_rros_rq();
+    // TODO: adjust all the i32/u32 flags
+    evl_wakeup_thread(thread, T_WAIT, T_TIMEO as i32);
+    // rros_sched_tick(rq);
+}
+
+pub fn evl_wakeup_thread(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, info: i32) {
     let mut flags: c_types::c_ulong = 0;
-    let rq =  rros_get_thread_rq(Some(thread.clone()), &mut flags);
+    let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
     evl_wakeup_thread_locked(thread.clone(), mask, info);
     rros_put_thread_rq(Some(thread.clone()), rq, flags);
 }
@@ -253,48 +423,47 @@ fn evl_wakeup_thread(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, info: i3
 fn evl_wakeup_thread_locked(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, info: i32) {
     let rq = thread.lock().rq;
     // struct evl_rq *rq = thread->rq;
-	// unsigned long oldstate;
+    // unsigned long oldstate;
 
-	// assert_hard_lock(&thread->lock);
-	// assert_hard_lock(&thread->rq->lock);
+    // assert_hard_lock(&thread->lock);
+    // assert_hard_lock(&thread->rq->lock);
 
-	// if (EVL_WARN_ON(CORE, mask & ~(T_DELAY|T_PEND|T_WAIT)))
-	// 	return;
+    // if (EVL_WARN_ON(CORE, mask & ~(T_DELAY|T_PEND|T_WAIT)))
+    // 	return;
 
-	// trace_evl_wakeup_thread(thread, mask, info);
+    // trace_evl_wakeup_thread(thread, mask, info);
 
-	let mut oldstate = thread.lock().state;
-	if (oldstate & mask) != 0x0 {
-	// 	/* Clear T_DELAY along w/ T_PEND in state. */
-		if (mask & T_PEND) != 0x0{
+    let mut oldstate = thread.lock().state;
+    if (oldstate & mask) != 0x0 {
+        // 	/* Clear T_DELAY along w/ T_PEND in state. */
+        if (mask & T_PEND) != 0x0 {
             mask |= T_DELAY;
         }
-	// 		
+        //
         // let mut oldstate = thread.lock();
         thread.lock().state &= !mask;
 
-		if (mask & (T_DELAY|T_PEND)) != 0x0 {
+        if (mask & (T_DELAY | T_PEND)) != 0x0 {
             let rtimer = thread.lock().rtimer.clone();
             timer::rros_stop_timer(rtimer.unwrap());
         }
-			
 
-		// if (mask & T_PEND & oldstate)
-		// 	thread.wchan = None;
+        // if (mask & T_PEND & oldstate)
+        // 	thread.wchan = None;
 
-		thread.lock().info |= info as u32;
+        thread.lock().info |= info as u32;
 
-        let oldstate = thread.lock().state ;
-		if (!(oldstate & RROS_THREAD_BLOCK_BITS)) != 0x0 {
-			rros_enqueue_thread(thread.clone());
-			// oldstate |= T_READY;
+        let oldstate = thread.lock().state;
+        if (!(oldstate & RROS_THREAD_BLOCK_BITS)) != 0x0 {
+            rros_enqueue_thread(thread.clone());
+            // oldstate |= T_READY;
             thread.lock().state |= T_READY;
-			rros_set_resched(rq);
-			// if (rq != this_evl_rq()){
+            rros_set_resched(rq);
+            // if (rq != this_evl_rq()){
             //     rros_inc_counter(&thread->stat.rwa);
             // }
-		}
-	}
+        }
+    }
 }
 // void evl_wakeup_thread(struct evl_thread *thread, int mask, int info)
 // {
@@ -306,66 +475,84 @@ fn evl_wakeup_thread_locked(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, i
 // 	evl_put_thread_rq(thread, rq, flags);
 // }
 
+fn __rros_run_kthread(kthread: &mut RrosKthread, clone_flags: i32) -> Result<usize> {
+    let thread = kthread.thread.clone().unwrap();
+    // struct evl_thread *thread = &kthread->thread;
+    // struct task_struct *p;
+    // int ret;
 
+    // ret = evl_init_element(&thread->element,
+    // 		&evl_thread_factory, clone_flags);
+    // if (ret)
+    // 	goto fail_element;
+
+    // ret = evl_create_core_element_device(&thread->element,
+    // 				&evl_thread_factory,
+    // 				thread->name);
+    // if (ret)
+    // 	goto fail_device;
 
-fn __rros_run_kthread(kthread: &mut RrosKthread, clone_flags:i32) -> Result<usize>{
-    let thread = kthread.thread.clone().unwrap();
-	// struct evl_thread *thread = &kthread->thread;
-	// struct task_struct *p;
-	// int ret;
-
-	// ret = evl_init_element(&thread->element,
-	// 		&evl_thread_factory, clone_flags);
-	// if (ret)
-	// 	goto fail_element;
-
-	// ret = evl_create_core_element_device(&thread->element,
-	// 				&evl_thread_factory,
-	// 				thread->name);
-	// if (ret)
-	// 	goto fail_device;
-  
     let data: *mut c_types::c_void;
     data = kthread as *mut RrosKthread as *mut c_types::c_void;
-    let p = unsafe{kthread_run(Some(kthread_trampoline), data, c_str!("%s").as_char_ptr(), format_args!("{}", (*thread.locked_data().get()).name))};
+    let p = unsafe {
+        kthread_run(
+            Some(kthread_trampoline),
+            data,
+            c_str!("%s").as_char_ptr(),
+            format_args!("{}", (*thread.locked_data().get()).name),
+        )
+    };
     let res = from_kernel_err_ptr(p);
     match res {
         Ok(_o) => (),
         Err(_e) => {
-            pr_info!("your thread creation failed!!!!!!");
-            unsafe{uninit_thread(&mut (*(Arc::into_raw(kthread.thread.clone().as_mut().unwrap().clone()) as *mut SpinLock<rros_thread>)) );}
+            // pr_info!("your thread creation failed!!!!!!");
+            unsafe {
+                // uninit_thread(
+                //     &mut (*(Arc::into_raw(kthread.thread.clone().as_mut().unwrap().clone())
+                //         as *mut SpinLock<rros_thread>)),
+                // );
+                uninit_thread(kthread.thread.clone().unwrap());
+            }
             // unsafe{uninit_thread(kthread.thread.clone().unwrap().get_mut());}
             return Err(_e);
-        },
+        }
     }
     // if  {
     //     pr_info!("your thread creation failed!!!!!!");
     //     return Err();
     // }
-	// p = kthread_run(kthread_trampoline, kthread, "%s", thread->name);
-	// if (IS_ERR(p)) { 
-	// 	ret = PTR_ERR(p);
-	// 	goto fail_spawn;
-	// }
+    // p = kthread_run(kthread_trampoline, kthread, "%s", thread->name);
+    // if (IS_ERR(p)) {
+    // 	ret = PTR_ERR(p);
+    // 	goto fail_spawn;
+    // }
 
-	// evl_index_factory_element(&thread->element);
+    // evl_index_factory_element(&thread->element);
 
-    pr_info!("thread before wait_for_completion");
+    // pr_info!("thread before wait_for_completion");
     kthread.done.wait_for_completion();
-    pr_info!("thread after wait_for_completion ");
-    
+    // pr_info!("thread after wait_for_completion ");
+
     if kthread.status != 0x0 {
-        pr_info!("__rros_run_kthread: kthread.status != 0x0");
+        // pr_info!("__rros_run_kthread: kthread.status != 0x0");
         return Err(kernel::Error::EINVAL);
     }
-    
 
-    pr_info!("thread before release {}", unsafe{(*thread.locked_data().get()).state});
-	rros_release_thread(thread.clone(), T_DORMANT as u32, 0 as u32);
-    pr_info!("thread after release {}", unsafe{(*thread.locked_data().get()).state});
-    
-	unsafe{rros_schedule();}
-    pr_info!("thread after sched {}", unsafe{(*thread.locked_data().get()).state});
+    // pr_info!("thread before release {}", unsafe {
+        // (*thread.locked_data().get()).state
+    // });
+    rros_release_thread(thread.clone(), T_DORMANT as u32, 0 as u32);
+    // pr_info!("thread after release {}", unsafe {
+        // (*thread.locked_data().get()).state
+    // });
+
+    unsafe {
+        rros_schedule();
+    }
+    // pr_info!("thread after sched {}", unsafe {
+        // (*thread.locked_data().get()).state
+    // });
 
     Ok(0)
 }
@@ -377,7 +564,7 @@ const MAX_RT_PRIO: i32 = 100;
 unsafe extern "C" fn kthread_trampoline(arg: *mut c_types::c_void) -> c_types::c_int {
     let kthread: &mut RrosKthread;
     // pr_info!("the thread add is {:p}", arg);
-    unsafe{ 
+    unsafe {
         let tmp = arg as *mut RrosKthread;
         kthread = &mut *tmp;
     }
@@ -386,71 +573,95 @@ unsafe extern "C" fn kthread_trampoline(arg: *mut c_types::c_void) -> c_types::c
     let mut prio;
     let sched_class;
     unsafe {
-        match (*curr.locked_data().get()).sched_class.clone(){
+        match (*curr.locked_data().get()).sched_class.clone() {
             Some(c) => sched_class = c,
             None => {
                 pr_info!("kthread_trampoline: curr.lock().sched_class.clone err");
                 return -1 as c_types::c_int;
-            },
+            }
         }
     }
 
-	if sched_class.flag != 3{ //curr的调度类不是evl_sched_fifo
-		policy = SCHED_NORMAL;
+    if sched_class.flag != 3 {
+        //curr的调度类不是evl_sched_fifo
+        policy = SCHED_NORMAL;
         prio = 0;
-	} else{
-		policy = SCHED_NORMAL;
-        prio = unsafe{(*curr.locked_data().get()).cprio};
+    } else {
+        policy = SCHED_NORMAL;
+        prio = unsafe { (*curr.locked_data().get()).cprio };
         if prio >= MAX_RT_PRIO {
             prio = MAX_RT_PRIO - 1;
         }
-	}
+    }
 
-    unsafe{pr_info!("kthread_trampoline: state1 in the thread{}", (*curr.locked_data().get()).state);}
-    let param = bindings::sched_param{ sched_priority : prio };
-	unsafe{bindings::sched_setscheduler(Task::current_ptr(), policy as c_types::c_int, &param as *const bindings::sched_param);}
+    unsafe {
+        // pr_info!(
+            // "kthread_trampoline: state1 in the thread{}",
+            // (*curr.locked_data().get()).state
+        // );
+    }
+    let param = bindings::sched_param {
+        sched_priority: prio,
+    };
+    unsafe {
+        bindings::sched_setscheduler(
+            Task::current_ptr(),
+            policy as c_types::c_int,
+            &param as *const bindings::sched_param,
+        );
+    }
 
-    unsafe{pr_info!("kthread_trampoline: state2 in the thread{}", (*curr.locked_data().get()).state);}
-	let ret = map_kthread_self(kthread);
+    unsafe {
+        // pr_info!(
+            // "kthread_trampoline: state2 in the thread{}",
+            // (*curr.locked_data().get()).state
+        // );
+    }
+    let ret = map_kthread_self(kthread);
 
-    unsafe{pr_info!("kthread_trampoline: state3 in the thread{}", (*curr.locked_data().get()).state);}
+    unsafe {
+        // pr_info!(
+            // "kthread_trampoline: state3 in the thread{}",
+            // (*curr.locked_data().get()).state
+        // );
+    }
 
-    match ret{
+    match ret {
         Ok(_o) => {
             // pr_info!("bug n");
-            unsafe{
-                let kthread_fn = kthread.kthread_fn;
-                // pr_info!("function {:?}", kthread_fn);
-                match kthread_fn{
-                    Some(a) => a(),
-                    None => {},
-                }
+            if let Some(kfn) =  kthread.kthread_fn.take(){
+                kfn();
             }
-        },
-        Err(_e) => {},
+        }
+        Err(_e) => {}
     }
-    unsafe{pr_info!("kthread_trampoline: state4 in the thread{}", (*curr.locked_data().get()).state);}
-	rros_cancel_thread(kthread.thread.as_mut().unwrap().clone());
+    unsafe {
+        pr_info!(
+            "kthread_trampoline: state4 in the thread{}",
+            (*curr.locked_data().get()).state
+        );
+    }
+    rros_cancel_thread(kthread.thread.as_mut().unwrap().clone());
     0 as c_types::c_int
 }
 
 fn rros_cancel_thread(thread: Arc<SpinLock<rros_thread>>) {
     pr_info!(" in rros_cancel_thread");
-    
+
     let mut flags: c_types::c_ulong = 0;
     //关中断
-    let rq =  rros_get_thread_rq(Some(thread.clone()), &mut flags);
-    
+    let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
+
     let mut state = thread.lock().state;
     let mut info = thread.lock().info;
-    pr_info!("state is {:?}",thread.lock().state);
+    pr_info!("state is {:?}", thread.lock().state);
     state = thread.lock().state;
     if state & T_ZOMBIE != 0x0 {
         pr_info!("thread->state: T_ZOMBIE");
         rros_put_thread_rq(Some(thread.clone()), rq, flags);
         return;
     }
-    
+
     info = thread.lock().info;
     if info & T_CANCELD != 0x0 {
         //goto check_self_cancel;
@@ -459,45 +670,47 @@ fn rros_cancel_thread(thread: Arc<SpinLock<rros_thread>>) {
     thread.lock().info |= T_CANCELD;
 
     state = thread.lock().state;
-    if (state & (T_DORMANT|T_INBAND) as u32) == (T_DORMANT|T_INBAND) as u32 {
+    if (state & (T_DORMANT | T_INBAND) as u32) == (T_DORMANT | T_INBAND) as u32 {
         rros_release_thread_locked(thread.clone(), T_DORMANT, T_KICKED);
-		rros_put_thread_rq(Some(thread.clone()), rq, flags);
-		unsafe{rros_schedule();}
-        return ;
+        rros_put_thread_rq(Some(thread.clone()), rq, flags);
+        unsafe {
+            rros_schedule();
+        }
+        return;
     }
 
     rros_put_thread_rq(Some(thread.clone()), rq, flags);
     let rq = this_rros_rq();
-    let curr = unsafe{(*rq).get_curr()};
+    let curr = unsafe { (*rq).get_curr() };
     let curr_addr = curr.locked_data().get();
     let thread_addr = thread.locked_data().get();
-    pr_info!("curr_addr is {:?}",curr_addr);
-    pr_info!("thread_addr is {:?}",thread_addr);
+    pr_info!("curr_addr is {:?}", curr_addr);
+    pr_info!("thread_addr is {:?}", thread_addr);
 
     if curr_addr == thread_addr {
         pr_info!("evl_current() == thread");
         rros_test_cancel();
-        return ;
+        return;
     }
 
-    state = unsafe{(*thread.locked_data().get()).state};
+    state = unsafe { (*thread.locked_data().get()).state };
     if state & T_USER != 0x0 {
         //evl_demote_thread(thread);
-		// evl_signal_thread(thread, SIGTERM, 0);
-    }else {
+        // evl_signal_thread(thread, SIGTERM, 0);
+    } else {
         pr_info!("evl_kick_thread: no");
         //evl_kick_thread(thread, 0);
     }
-    unsafe{
+    unsafe {
         rros_schedule();
     }
 }
 
 pub fn rros_test_cancel() {
-    let curr_ptr = unsafe{rros_current()};
+    let curr_ptr = unsafe { rros_current() };
     if curr_ptr != 0 as *mut SpinLock<rros_thread> {
-        let curr = unsafe{(&mut *curr_ptr)};
-        let info = unsafe{(*curr.locked_data().get()).info};
+        let curr = unsafe { (&mut *curr_ptr) };
+        let info = unsafe { (*curr.locked_data().get()).info };
         if info & T_CANCELD != 0x0 {
             pr_info!("rros_test_cancel: yes");
             __rros_test_cancel(curr);
@@ -505,24 +718,23 @@ pub fn rros_test_cancel() {
     }
 }
 
-
-
 fn __rros_test_cancel(curr_ptr: *mut SpinLock<rros_thread>) {
-    let curr = unsafe{(&mut *curr_ptr)};
-    let rq_local_flags = unsafe{(& *((*curr.locked_data().get()).rq.clone().unwrap())).local_flags};
+    let curr = unsafe { (&mut *curr_ptr) };
+    let rq_local_flags =
+        unsafe { (&*((*curr.locked_data().get()).rq.clone().unwrap())).local_flags };
     if rq_local_flags & RQ_IRQ != 0x0 {
-        return ;
+        return;
     }
-    let state = unsafe{(*curr.locked_data().get()).state};
+    let state = unsafe { (*curr.locked_data().get()).state };
     if state & T_INBAND == 0x0 {
         pr_info!("__evl_test_cancel:!(curr->state & T_INBAND)");
         rros_switch_inband(RROS_HMDIAG_NONE as i32);
     }
-    unsafe{bindings::do_exit(0 as c_types::c_long);}
+    unsafe {
+        bindings::do_exit(0 as c_types::c_long);
+    }
 }
 
-
-
 // static void wakeup_kthread_parent(struct irq_work *irq_work)
 // {
 // 	struct evl_kthread *kthread;
@@ -532,39 +744,46 @@ fn __rros_test_cancel(curr_ptr: *mut SpinLock<rros_thread>) {
 
 pub fn RrosKthread_kfn_1() {
     // while 1==1 {
-        // pr_info!("hello! from rros~~~~~~~~~~~~");
+    // pr_info!("hello! from rros~~~~~~~~~~~~");
     // }
 }
 
 unsafe extern "C" fn wakeup_kthread_parent(irq_work: *mut bindings::irq_work) {
     // const rk: = RrosKthread::new(RrosKthread_kfn_1);
     let kthread = kernel::container_of!(irq_work, RrosKthread, irq_work);
-    unsafe{(*(kthread as *mut RrosKthread)).done.complete();}
+    unsafe {
+        (*(kthread as *mut RrosKthread)).done.complete();
+    }
 }
 
-
 fn map_kthread_self(kthread: &mut RrosKthread) -> Result<usize> {
     let thread = kthread.thread.clone().unwrap();
-    
-    pr_info!("map_kthread_self:in");
+
+    // pr_info!("map_kthread_self:in");
+
+    // TODO: add the support of the pin_to_initial_cpu
+    // pin_to_initial_cpu(curr);
+
     let ret;
     unsafe {
-        let mut add = &mut (*thread.locked_data().get()).altsched as *mut bindings::dovetail_altsched_context;
-        pr_info!("map_kthread_self: the altched add is {:p}", add);
+        let mut add =
+            &mut (*thread.locked_data().get()).altsched as *mut bindings::dovetail_altsched_context;
+        // pr_info!("map_kthread_self: the altched add is {:p}", add);
         bindings::dovetail_init_altsched(add);
-        pr_info!("map_kthread_self: after dovetail_init_altsched");
+        // pr_info!("map_kthread_self: after dovetail_init_altsched");
 
         set_oob_threadinfo(Arc::into_raw(thread.clone()) as *mut SpinLock<rros_thread> as *mut c_types::c_void);
-
+        // pr_info!("map_kthread_self rros_current address is {:p}",rros_current());
         bindings::dovetail_start_altsched();
 
         rros_release_thread(thread.clone(), T_DORMANT as u32, 0 as u32);
 
-        ret = evl_switch_oob(kthread);
+        // ret = evl_switch_oob(kthread);
+        ret = evl_switch_oob();
         if let Err(_e) = ret {
             kthread.status = -1;
         }
-        pr_info!("map_kthread_self: after evl_switch_oob");
+        // pr_info!("map_kthread_self: after evl_switch_oob");
         //b kernel/rros/thread.rs:531
         kthread.irq_work.init_irq_work(wakeup_kthread_parent);
         kthread.irq_work.irq_work_queue();
@@ -574,42 +793,41 @@ fn map_kthread_self(kthread: &mut RrosKthread) -> Result<usize> {
     return ret;
 }
 
-extern "C"{
+extern "C" {
     fn rust_helper_hard_local_irq_enable();
     fn rust_helper_preempt_enable();
     fn rust_helper_preempt_disable();
 }
 
 pub fn rros_current() -> *mut SpinLock<rros_thread> {
-    unsafe{
-        (*rust_helper_dovetail_current_state()).thread as *mut SpinLock<rros_thread>
-    }
+    unsafe { (*rust_helper_dovetail_current_state()).thread as *mut SpinLock<rros_thread> }
 }
 
-fn evl_switch_oob(kthread: &mut RrosKthread) -> Result<usize> {
+pub fn evl_switch_oob() -> Result<usize> {
+    // fn evl_switch_oob(kthread: &mut RrosKthread) -> Result<usize> {
     let res = premmpt::running_inband()?;
     // pr_info!("res premmpt {:?}" , res);
-	// struct evl_thread *curr = evl_current();
-    
+    // struct evl_thread *curr = evl_current();
+
     // let prio = kthread.thread.as_mut().unwrap().lock().cprio;
 
-    pr_info!("evl_switch_oob: 1");
-    let curr = unsafe{&mut *rros_current()};
+    // pr_info!("evl_switch_oob: 1");
+    let curr = unsafe { &mut *rros_current() };
     // pr_info!("curr state {}" , curr.lock().state);
-    pr_info!("kthread state {}" , kthread.thread.as_mut().unwrap().lock().state);
+    // pr_info!("kthread state {}" , kthread.thread.as_mut().unwrap().lock().state);
 
     // let prio = kthread.thread.as_mut().unwrap().lock().cprio;
-    pr_info!("evl_switch_oob: 2");
+    // pr_info!("evl_switch_oob: 2");
 
-	// struct task_struct *p = current;
-	// unsigned long flags;
-	// int ret;
+    // struct task_struct *p = current;
+    // unsigned long flags;
+    // int ret;
 
-	// inband_context_only();
+    // inband_context_only();
+
+    // if (curr == NULL)
+    // 	return -EPERM;
 
-	// if (curr == NULL)
-	// 	return -EPERM;
-    
     if Task::current().signal_pending() {
         // pr_info!("wrong!!!!!!!!!!!!!!!!!!!");
         return Err(kernel::Error::ERESTARTSYS);
@@ -618,42 +836,43 @@ fn evl_switch_oob(kthread: &mut RrosKthread) -> Result<usize> {
     // let prio = kthread.thread.as_mut().unwrap().lock().cprio;
     // pr_info!("mutex 3.25");
 
-	// if (signal_pending(p))
-	// 	return -ERESTARTSYS;
+    // if (signal_pending(p))
+    // 	return -ERESTARTSYS;
 
-	// trace_evl_switch_oob(curr);b kernel/rros/thread.rs:504
+    // trace_evl_switch_oob(curr);b kernel/rros/thread.rs:504
     //b kernel/rros/thread.rs:604
 
-	// evl_clear_sync_uwindow(curr, T_INBAND);
+    // evl_clear_sync_uwindow(curr, T_INBAND);
 
-	let ret = unsafe{bindings::dovetail_leave_inband()};
+    let ret = unsafe { bindings::dovetail_leave_inband() };
     // pr_info!("2dddddddddddddddddddddddddddddddddddddddddddeee ");
-	if ret != 0x0 {
+    if ret != 0x0 {
         rros_test_cancel();
-	// 	evl_set_sync_uwindow(curr, T_INBAND);
-		return Err(kernel::Error::EINVAL);
-	}
-
+        // 	evl_set_sync_uwindow(curr, T_INBAND);
+        return Err(kernel::Error::EINVAL);
+    }
 
-	// /*
-	//  * On success, dovetail_leave_inband() stalls the oob stage
-	//  * before returning to us: clear this stall bit since we don't
-	//  * use it for protection but keep hard irqs off.
-	//  */
-	unsafe{rust_helper_unstall_oob();}
+    // /*
+    //  * On success, dovetail_leave_inband() stalls the oob stage
+    //  * before returning to us: clear this stall bit since we don't
+    //  * use it for protection but keep hard irqs off.
+    //  */
+    unsafe {
+        rust_helper_unstall_oob();
+    }
     // let prio = kthread.thread.as_mut().unwrap().lock().cprio;
     // pr_info!("mutex 3.4");
 
-	// /*
-	//  * The current task is now running on the out-of-band
-	//  * execution stage, scheduled in by the latest call to
-	//  * __evl_schedule() on this CPU: we must be holding the
-	//  * runqueue lock and hard irqs must be off.
-	//  */
-	// oob_context_only();
+    // /*
+    //  * The current task is now running on the out-of-band
+    //  * execution stage, scheduled in by the latest call to
+    //  * __evl_schedule() on this CPU: we must be holding the
+    //  * runqueue lock and hard irqs must be off.
+    //  */
+    // oob_context_only();
 
-	// finish_rq_switch_from_inband();
-    unsafe{
+    // finish_rq_switch_from_inband();
+    unsafe {
         rust_helper_hard_local_irq_enable();
         // rust_helper_preempt_enable();
     }
@@ -671,43 +890,43 @@ fn evl_switch_oob(kthread: &mut RrosKthread) -> Result<usize> {
     //     }
     // }
 
-	// trace_evl_switched_oob(curr);
-
-	// /*
-	//  * In case check_cpu_affinity() caught us resuming oob from a
-	//  * wrong CPU (i.e. outside of the oob set), we have T_CANCELD
-	//  * set. Check and bail out if so.
-	//  */
-	// if (curr->info & T_CANCELD)
-	// 	evl_test_cancel();
-
-	// /*
-	//  * Since handle_sigwake_event()->evl_kick_thread() won't set
-	//  * T_KICKED unless T_INBAND is cleared, a signal received
-	//  * during the stage transition process might have gone
-	//  * unnoticed. Recheck for signals here and raise T_KICKED if
-	//  * some are pending, so that we switch back in-band asap for
-	//  * handling them.
-	//  */
-	// if (signal_pending(p)) {
-	// 	raw_spin_lock_irqsave(&curr->rq->lock, flags);
-	// 	curr->info |= T_KICKED;
-	// 	raw_spin_unlock_irqrestore(&curr->rq->lock, flags);
-	// }
+    // trace_evl_switched_oob(curr);
+
+    // /*
+    //  * In case check_cpu_affinity() caught us resuming oob from a
+    //  * wrong CPU (i.e. outside of the oob set), we have T_CANCELD
+    //  * set. Check and bail out if so.
+    //  */
+    // if (curr->info & T_CANCELD)
+    // 	evl_test_cancel();
+
+    // /*
+    //  * Since handle_sigwake_event()->evl_kick_thread() won't set
+    //  * T_KICKED unless T_INBAND is cleared, a signal received
+    //  * during the stage transition process might have gone
+    //  * unnoticed. Recheck for signals here and raise T_KICKED if
+    //  * some are pending, so that we switch back in-band asap for
+    //  * handling them.
+    //  */
+    // if (signal_pending(p)) {
+    // 	raw_spin_lock_irqsave(&curr->rq->lock, flags);
+    // 	curr->info |= T_KICKED;
+    // 	raw_spin_unlock_irqrestore(&curr->rq->lock, flags);
+    // }
 
     if Task::current().signal_pending() {
         // pr_info!("wrong!!!!!!!!!!!!!!!!!!!");
         return Err(kernel::Error::ERESTARTSYS);
     }
 
-	// return 0;
+    // return 0;
     Ok(0)
 }
 
-extern "C"{
+extern "C" {
     fn rust_helper_hard_local_irq_save() -> c_types::c_ulong;
     fn rust_helper_hard_local_irq_restore(flags: c_types::c_ulong);
-
+    fn rust_helper_doveail_mm_state() -> *mut bindings::oob_mm_state;
 }
 // pub fn finish_rq_switch_from_inband() {
 //     bindings::_raw_spin_unlock_irq()
@@ -716,8 +935,8 @@ extern "C"{
 //逻辑完整，未测试
 pub fn rros_release_thread(thread: Arc<SpinLock<rros_thread>>, mask: u32, info: u32) {
     let mut flags: c_types::c_ulong = 0;
-    let rq =  rros_get_thread_rq(Some(thread.clone()), &mut flags);
-    rros_release_thread_locked(thread.clone(), mask, info);//对于smp这里需要改，现在没问题
+    let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
+    rros_release_thread_locked(thread.clone(), mask, info); //对于smp这里需要改，现在没问题
     rros_put_thread_rq(Some(thread.clone()), rq, flags);
 }
 
@@ -734,7 +953,7 @@ pub fn rros_release_thread_locked(thread: Arc<SpinLock<rros_thread>>, mask: u32,
             return;
         }
 
-        if ((oldstate & mask) & (T_HALT|T_PTSYNC)) != 0x0 {
+        if ((oldstate & mask) & (T_HALT | T_PTSYNC)) != 0x0 {
             rros_requeue_thread(thread.clone());
             thread.lock().state |= T_READY;
             rros_set_resched(Some(rq));
@@ -756,24 +975,23 @@ pub fn rros_release_thread_locked(thread: Arc<SpinLock<rros_thread>>, mask: u32,
 
 // fn rros_hold_thread(thread: Arc<SpinLock<rros_thread>>, mask: u32) {
 fn rros_hold_thread(kthread: &mut RrosKthread, mask: u32) {
-
     // rros_hold_thread(kthread.thread.clone().unwrap(), T_DORMANT);
     // as_mut().unwrap()
     let thread = kthread.thread.clone().unwrap();
     let mut flags: c_types::c_ulong = 0;
 
     //关中断
-    let rq_op =  rros_get_thread_rq(Some(thread.clone()), &mut flags);
+    let rq_op = rros_get_thread_rq(Some(thread.clone()), &mut flags);
 
     let rq = rq_op.unwrap();
     let oldstate = thread.lock().state;
-    let curr = unsafe{(*rq).get_curr()};
+    let curr = unsafe { (*rq).get_curr() };
     let curr_add = curr.clone().lock().deref_mut() as *mut rros_thread;
     let thread_add = thread.clone().lock().deref_mut() as *mut rros_thread;
     if oldstate & RROS_THREAD_BLOCK_BITS == 0x0 {
         let info = thread.lock().info;
         if info & T_KICKED != 0x0 {
-            thread.lock().info &= !(T_RMID|T_TIMEO);
+            thread.lock().info &= !(T_RMID | T_TIMEO);
             thread.lock().info |= T_BREAK;
             rros_put_thread_rq(Some(thread.clone()), Some(rq), flags);
             return;
@@ -783,28 +1001,30 @@ fn rros_hold_thread(kthread: &mut RrosKthread, mask: u32) {
         }
     }
 
-	if oldstate & T_READY != 0x0 {
-		rros_dequeue_thread(thread.clone());
-        thread.lock().state &= !T_READY; 
-	}
+    if oldstate & T_READY != 0x0 {
+        rros_dequeue_thread(thread.clone());
+        thread.lock().state &= !T_READY;
+    }
 
     thread.lock().state |= mask;
 
     if thread_add == curr_add {
         rros_set_resched(Some(rq));
-    } else if oldstate & (RROS_THREAD_BLOCK_BITS|T_USER) == (T_INBAND as u32|T_USER) {
+    } else if oldstate & (RROS_THREAD_BLOCK_BITS | T_USER) == (T_INBAND as u32 | T_USER) {
         // dovetail_request_ucall(thread->altsched.task);
         todo!();
     }
     rros_put_thread_rq(Some(thread.clone()), Some(rq), flags);
 }
 
-pub fn set_oob_threadinfo(curr: *mut c_types::c_void){
-
+pub fn set_oob_threadinfo(curr: *mut c_types::c_void) {
     // pr_info!("oob thread info {:p}", curr);
     // unsafe{(*Task::current_ptr()).thread_info.oob_state.thread = curr }
-    pr_info!("set_oob_threadinfo: in");
-    unsafe{(*rust_helper_dovetail_current_state()).thread = curr;}
+    // pr_info!("set_oob_threadinfo: in");
+    unsafe {
+        (*rust_helper_dovetail_current_state()).thread = curr;
+    }
+    // unsafe{Arc::decrement_strong_count(curr);}
 }
 
 // static inline void set_oob_threadinfo(struct evl_thread *thread)
@@ -815,12 +1035,24 @@ pub fn set_oob_threadinfo(curr: *mut c_types::c_void){
 // 	p->thread = thread;
 // }
 
-pub fn kthread_run(threadfn: Option<unsafe extern "C" fn(*mut c_types::c_void) -> c_types::c_int>,
-                data: *mut c_types::c_void,
-                namefmt: *const c_types::c_char,
-                msg: fmt::Arguments<'_>, 
-                ) -> *mut c_types::c_void{
-    unsafe{
+
+pub fn set_oob_mminfo(thread: Arc<SpinLock<rros_thread>>) {
+    // pr_info!("set_oob_mminfo: in");
+    unsafe{(*thread.locked_data().get()).oob_mm = rust_helper_doveail_mm_state();}
+}
+
+// static inline void set_oob_mminfo(struct evl_thread *thread)
+// {
+// 	thread->oob_mm = dovetail_mm_state();
+// }
+
+pub fn kthread_run(
+    threadfn: Option<unsafe extern "C" fn(*mut c_types::c_void) -> c_types::c_int>,
+    data: *mut c_types::c_void,
+    namefmt: *const c_types::c_char,
+    msg: fmt::Arguments<'_>,
+) -> *mut c_types::c_void {
+    unsafe {
         rust_helper_kthread_run(
             threadfn,
             data,
@@ -830,70 +1062,104 @@ pub fn kthread_run(threadfn: Option<unsafe extern "C" fn(*mut c_types::c_void) -
     }
 }
 
-pub fn atomic_set(v:*mut bindings::atomic_t,i:i32){
-    unsafe{rust_helper_atomic_set(v, i);}
+pub fn atomic_set(v: *mut bindings::atomic_t, i: i32) {
+    unsafe {
+        rust_helper_atomic_set(v, i);
+    }
+}
+
+pub fn atomic_inc(v:*mut bindings::atomic_t){
+    unsafe{rust_helper_atomic_inc(v)};
+}
+
+pub fn atomic_dec_and_test(v:*mut bindings::atomic_t) -> bool{
+    unsafe{return rust_helper_atomic_dec_and_test(v)};
+}
+
+pub fn atomic_dec_return(v:*mut bindings::atomic_t) -> i32{
+    unsafe{return rust_helper_atomic_dec_return(v)};
+}
+
+pub fn atomic_cmpxchg(v:*mut bindings::atomic_t, old:i32, new:i32) -> i32{
+    unsafe{return rust_helper_atomic_cmpxchg(v,old,new)};
+}
+
+pub fn atomic_read(v:*mut bindings::atomic_t) -> i32{
+    unsafe{return rust_helper_atomic_read(v)};
 }
 
 fn init_irq_work(work:*mut bindings::irq_work,func:unsafe extern "C" fn(work:*mut bindings::irq_work)){
     unsafe{rust_helper_init_irq_work(work,func);}
 }
 
-unsafe extern "C" fn inband_task_wakeup(work:*mut bindings::irq_work){
-    
+
+unsafe extern "C" fn inband_task_wakeup(work: *mut bindings::irq_work) {
+    unsafe{
+        let p = kernel::container_of!(work, rros_thread, inband_work);
+        bindings::wake_up_process((*p).altsched.task);
+    }
 }
 
-pub fn rros_run_kthread(kthread: &mut RrosKthread,
-            fmt:&'static CStr,) -> Result<usize> {
+pub fn rros_run_kthread(kthread: &mut RrosKthread, fmt: &'static CStr, prio: i32) -> Result<usize> {
     let mut iattr = rros_init_thread_attr::new();
     // iattr.flags = T_USER as i32;
-    unsafe{
+    unsafe {
         iattr.affinity = &RROS_OOB_CPUS as *const CpumaskT;
         iattr.sched_class = Some(&rros_sched_fifo);
+        // iattr.sched_class = Some(&rros_sched_tp);
+        let prio = prio;
+        let sched_param =  unsafe{Arc::try_new(SpinLock::new(rros_sched_param::new()))?};
+        (*sched_param.locked_data().get()).fifo.prio = prio;
+        (*sched_param.locked_data().get()).idle.prio = prio;
+        (*sched_param.locked_data().get()).weak.prio = prio;
+        (*sched_param.locked_data().get()).tp.prio = 1;
+        iattr.sched_param = Some(sched_param);
+        // kthread.borrow_mut().done.init_completion();
+        kthread.done.init_completion();
+        // Completion::init_completion(&mut kthread.borrow_mut().done);
+        
+        // kthread.borrow_mut().thread.clone().unwrap().lock().state |= sched::T_READY;
+        // pr_info!("fkkkkk {}", kthread.thread.clone().unwrap().lock().state);
+
+        let thread_unwrap = kthread.thread.clone().unwrap();
+        // pr_info!("here 2");
+        rros_init_thread(& Some(thread_unwrap), iattr, this_rros_rq(), fmt);
+        let next_add = kthread.thread.clone().unwrap().lock().deref_mut() as *mut rros_thread;
+        // pr_info!("the run thread add is  next_add {:p}", next_add);
+        // pr_info!("fkkkkk 176 128 32 16 {}", kthread.thread.clone().unwrap().lock().state);
+        kthread.thread.clone().unwrap().lock().rrperiod = 400000;
+        kthread.thread.clone().unwrap().lock().state |= T_RRB;
+        // let rq = unsafe{(*this_rros_rq()).get_rrbtimer()};
+        // let value = unsafe{rros_abs_timeout((*rq).get_rrbtimer(), 4000)};
+        // let interval = RROS_INFINITE;
+        // rros_start_timer(rq, value, interval);
+        
+
+        // pr_info!("kkkkkk {}", kthread.thread.clone().unwrap().lock().state);
+
+        __rros_run_kthread(kthread, factory::RROS_CLONE_PUBLIC);
+
+        // pr_info!("kkkkkk2 2224 2048 128 32 16 {}", kthread.borrow_mut().thread.clone().unwrap().lock().state);
     }
-    let piro = 98;
-    let sched_param =  Rc::try_new(RefCell::new(rros_sched_param::new()))?;
-    sched_param.borrow_mut().fifo.prio = piro;
-    sched_param.borrow_mut().idle.prio = piro;
-    sched_param.borrow_mut().weak.prio = piro;
-    iattr.sched_param = Some(sched_param);
-    // kthread.borrow_mut().done.init_completion();
-    kthread.done.init_completion();
-    // Completion::init_completion(&mut kthread.borrow_mut().done);
-    
-    // kthread.borrow_mut().thread.clone().unwrap().lock().state |= sched::T_READY;
-    // pr_info!("fkkkkk {}", kthread.thread.clone().unwrap().lock().state);
-
-    let thread_unwrap = kthread.thread.clone().unwrap();
-    rros_init_thread(& Some(thread_unwrap), iattr, this_rros_rq(), fmt);
-    let next_add = kthread.thread.clone().unwrap().lock().deref_mut() as *mut rros_thread;
-    // pr_info!("the run thread add is  next_add {:p}", next_add);
-    // pr_info!("fkkkkk 176 128 32 16 {}", kthread.thread.clone().unwrap().lock().state);
-    // kthread.thread.clone().unwrap().lock().rrperiod = 4000;
-    // kthread.thread.clone().unwrap().lock().state |= T_RRB;
-    
-
-    // pr_info!("kkkkkk {}", kthread.thread.clone().unwrap().lock().state);
-
-    __rros_run_kthread(kthread, factory::RROS_CLONE_PUBLIC);
-
-    // pr_info!("kkkkkk2 2224 2048 128 32 16 {}", kthread.borrow_mut().thread.clone().unwrap().lock().state);
     Ok(0)
 }
 
-pub fn rros_set_thread_schedparam(thread: Arc<SpinLock<rros_thread>>,
+pub fn rros_set_thread_schedparam(
+    thread: Arc<SpinLock<rros_thread>>,
     sched_class: Option<&'static rros_sched_class>,
-    sched_param: Option<Rc<RefCell<rros_sched_param>>>) -> Result<usize>
+
+    sched_param: Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>
 {
     let mut flags: c_types::c_ulong = 0;
     let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
-    rros_set_thread_schedparam_locked(thread.clone(), sched_class, sched_param);
+    rros_set_thread_schedparam_locked(thread.clone(), sched_class, sched_param.clone());
     rros_put_thread_rq(Some(thread.clone()), rq, flags);
     Ok(0)
 }
 
 pub fn rros_set_thread_schedparam_locked(thread:Arc<SpinLock<rros_thread>>,
     sched_class:Option<&'static rros_sched_class>,
-    sched_param:Option<Rc<RefCell<rros_sched_param>>>) -> Result<usize>
+    sched_param:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>
 {
     let mut old_wprio:i32 = 0;
     let mut new_wprio:i32 = 0;
@@ -902,23 +1168,26 @@ pub fn rros_set_thread_schedparam_locked(thread:Arc<SpinLock<rros_thread>>,
     new_wprio = thread.lock().wprio;
 
     let state = thread.lock().state;
-    if old_wprio != new_wprio && (state & T_PEND) != 0{
+    if old_wprio != new_wprio && (state & T_PEND) != 0 {
         let func;
-        match thread.lock().wchan.reorder_wait{
-            Some(f) => func = f,
-            None => {
-                pr_warn!("reorder_wait function error");
-                return Err(kernel::Error::EINVAL);
+        unsafe{
+            // let wchan = (*thread.locked_data().get()).wchan.clone().unwrap();
+            match (*(*thread.locked_data().get()).wchan).reorder_wait{
+                Some(f) => func = f,
+                None => {
+                    pr_warn!("reorder_wait function error");
+                    return Err(kernel::Error::EINVAL);
+                }
             }
         }
         func(thread.clone(), thread.clone());
     }
     thread.lock().info |= T_SCHEDP;
-    
+
     let state = thread.lock().state;
-    if ((state & (T_INBAND as u32|T_USER)) == (T_INBAND as u32|T_USER)){
+    if ((state & (T_INBAND as u32 | T_USER)) == (T_INBAND as u32 | T_USER)) {
         let task = thread.lock().altsched.task as *mut bindings::task_struct;
-        unsafe{rust_helper_dovetail_request_ucall(task)};
+        unsafe { rust_helper_dovetail_request_ucall(task) };
     }
     Ok(0)
 }
@@ -928,106 +1197,117 @@ pub fn rros_set_thread_schedparam_locked(thread:Arc<SpinLock<rros_thread>>,
 //     fn rust_helper_hard_local_irq_restore(flags: c_types::c_ulong);
 // }
 
-pub fn rros_sleep(delay: ktime::KtimeT) -> Result<usize>{
-    let end: ktime::KtimeT = unsafe{ktime::ktime_add(clock::rros_read_clock(&clock::RROS_MONO_CLOCK), delay)};
+pub fn rros_sleep(delay: ktime::KtimeT) -> Result<usize> {
+    let end: ktime::KtimeT =
+        unsafe { ktime::ktime_add(clock::rros_read_clock(&clock::RROS_MONO_CLOCK), delay) };
     // ktime_t end = ktime_add(clock::rros_read_clock(&evl_mono_clock), delay);
-	evl_sleep_until(end)?;
+    evl_sleep_until(end)?;
     Ok(0)
 }
 
 fn evl_sleep_until(timeout: ktime::KtimeT) -> Result<usize> {
-    // let 
-    let res = unsafe{evl_delay(timeout, timeout::rros_tmode::RROS_ABS, & clock::RROS_MONO_CLOCK)};
+    // let
+    let res = unsafe {
+        evl_delay(
+            timeout,
+            timeout::rros_tmode::RROS_ABS,
+            &clock::RROS_MONO_CLOCK,
+        )
+    };
     match res {
-        Ok(o)=>Ok(0),
-        Err(e)=>Err(kernel::Error::EINVAL),
+        Ok(o) => Ok(0),
+        Err(e) => Err(kernel::Error::EINVAL),
     }
 }
 
-fn evl_delay(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode, clock: & clock::RrosClock) -> Result<usize> {
+fn evl_delay(
+    timeout: ktime::KtimeT,
+    timeout_mode: timeout::rros_tmode,
+    clock: &clock::RrosClock,
+) -> Result<usize> {
     // let curr = evl_current();
     // struct evl_thread *curr = evl_current();
 
-	evl_sleep_on(timeout, timeout_mode, clock);
+	evl_sleep_on(timeout, timeout_mode, clock, 0 as *mut RrosWaitChannel);
 	unsafe{rros_schedule();}
 
     Ok(0)
 }
 
-fn evl_sleep_on(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode, clock: & clock::RrosClock) {
+pub fn evl_sleep_on(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode, clock: & clock::RrosClock,wchan:*mut RrosWaitChannel) {
     let mut flags: c_types::c_ulong = 0;
-    let curr = unsafe{&mut *rros_current()};
-    let thread = unsafe{Arc::from_raw(curr as *const SpinLock<rros_thread>)};
+    let curr = unsafe { &mut *rros_current() };
+    let thread = unsafe { Arc::from_raw(curr as *const SpinLock<rros_thread>) };
     // pr_info!("evl_sleep_on: x ref is {}", Arc::strong_count(&thread.clone()));
+    unsafe{Arc::increment_strong_count(curr);}
 	let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
-    evl_sleep_on_locked(timeout, timeout_mode, clock);
+    evl_sleep_on_locked(timeout, timeout_mode, clock,wchan);
     rros_put_thread_rq(Some(thread.clone()), rq, flags);
 }
 
 // fn evl_current() -> Result<u64> {
-    
+
 //     Ok(0)
 // }
 
-fn evl_sleep_on_locked(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode, clock: & clock::RrosClock) -> Result<u64> {
+pub fn evl_sleep_on_locked(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode, clock: & clock::RrosClock,wchan:*mut RrosWaitChannel) -> Result<u64> {
 	let rq = this_rros_rq();
     let curr = unsafe{(*rq).get_curr()};
 
     let next_add = curr.clone().lock().deref_mut() as *mut rros_thread;
     // pr_info!("the tram thread add {:p}", next_add);
 
-	// /* Sleeping while preemption is disabled is a bug. */
-	// EVL_WARN_ON(CORE, evl_preempt_count() != 0);
+    // /* Sleeping while preemption is disabled is a bug. */
+    // EVL_WARN_ON(CORE, evl_preempt_count() != 0);
 
-	// assert_hard_lock(&curr->lock);
-	// assert_hard_lock(&rq->lock);
+    // assert_hard_lock(&curr->lock);
+    // assert_hard_lock(&rq->lock);
+
+    // trace_evl_sleep_on(timeout, timeout_mode, clock, wchan);
 
-	// trace_evl_sleep_on(timeout, timeout_mode, clock, wchan);
-    
     let oldstate = curr.lock().state;
 
     // pr_info!("thread before0 sleep {}", oldstate);
-	// /*
-	//  * If a request to switch to in-band context is pending
-	//  * (T_KICKED), raise T_BREAK then return immediately.
-	//  */
+    // /*
+    //  * If a request to switch to in-band context is pending
+    //  * (T_KICKED), raise T_BREAK then return immediately.
+    //  */
     if oldstate & RROS_THREAD_BLOCK_BITS != 0x0 {
         if (curr.lock().info & T_KICKED) != 0x0 {
-            curr.lock().info &= !(T_RMID|T_TIMEO) as u32;
+            curr.lock().info &= !(T_RMID | T_TIMEO) as u32;
             curr.lock().info |= T_BREAK as u32;
-                return Ok(0);
-            }
+            return Ok(0);
+        }
         curr.lock().info &= !RROS_THREAD_BLOCK_BITS;
     }
 
-	// /*
-	//  *  wchan + timeout: timed wait for a resource (T_PEND|T_DELAY)
-	//  *  wchan + !timeout: unbounded sleep on resource (T_PEND)
-	//  * !wchan + timeout: timed sleep (T_DELAY)
-	//  * !wchan + !timeout: periodic wait (T_WAIT)
-	//  */
+    // /*
+    //  *  wchan + timeout: timed wait for a resource (T_PEND|T_DELAY)
+    //  *  wchan + !timeout: unbounded sleep on resource (T_PEND)
+    //  * !wchan + timeout: timed sleep (T_DELAY)
+    //  * !wchan + !timeout: periodic wait (T_WAIT)
+    //  */
     // curr.lock().state |= sched::T_DELAY as u32;
     // pr_info!("thread before sleep {}", curr.lock().state);
 
-	if (timeout_mode != timeout::rros_tmode::RROS_REL || !timeout::timeout_infinite(timeout)) {
-		// timer::evl_prepare_timed_wait(curr.lock().rtimer, clock,
-				// evl_thread_rq(curr));
-		if (timeout_mode == timeout::rros_tmode::RROS_REL) {
-	        // timeout = evl_abs_timeout(&curr->rtimer, timeout);
-        } else if (timeout <= clock::rros_read_clock(clock)) { //
-			curr.lock().info |= T_TIMEO as u32;
-			return Ok(0);
-		}
+    if (timeout_mode != timeout::rros_tmode::RROS_REL || !timeout::timeout_infinite(timeout)) {
+        // timer::evl_prepare_timed_wait(curr.lock().rtimer, clock,
+        // evl_thread_rq(curr));
+        if (timeout_mode == timeout::rros_tmode::RROS_REL) {
+            // timeout = evl_abs_timeout(&curr->rtimer, timeout);
+        } else if (timeout <= clock::rros_read_clock(clock)) {
+            //
+            curr.lock().info |= T_TIMEO as u32;
+            return Ok(0);
+        }
         let timer = curr.lock().rtimer.as_ref().unwrap().clone();
-		timer::rros_start_timer(timer.clone(), timeout, timeout::RROS_INFINITE);
-		curr.lock().state |= T_DELAY as u32;
+        timer::rros_start_timer(timer.clone(), timeout, timeout::RROS_INFINITE);
+        curr.lock().state |= T_DELAY as u32;
         // pr_info!("thread 2before sleep {}", curr.lock().state);
 	} 
-
-    // else if (!wchan) {
-		// evl_prepare_timed_wait(&curr->ptimer, clock,
-				// evl_thread_rq(curr));
-		// curr.lock().state |= T_WAIT;
+    // else if wchan == 0 as *mut RrosWaitChannel {
+	// 	// evl_prepare_timed_wait(&curr->ptimer, clock,evl_thread_rq(curr));
+	// 	curr.lock().state |= T_WAIT;
 	// }
 
 	if (oldstate & T_READY) != 0x0 {
@@ -1035,9 +1315,15 @@ fn evl_sleep_on_locked(timeout: ktime::KtimeT, timeout_mode: timeout::rros_tmode
 		curr.lock().state &= !T_READY;
 	}
 
-	// if (wchan) {
-	// 	curr->wchan = wchan;
-	// 	curr->state |= T_PEND;
+    if !wchan.is_null(){
+        unsafe{
+            (*curr.locked_data().get()).wchan = wchan as *mut RrosWaitChannel;
+            (*curr.locked_data().get()).state |= T_PEND;
+        }
+    }
+	// if wchan != 0 as *mut RrosWaitChannel {
+	// 	curr.lock().wchan = wchan;
+	// 	curr.lock().state |= T_PEND;
 	// }
 
 	rros_set_resched(Some(rq));
@@ -1057,141 +1343,1182 @@ pub fn __rros_propagate_schedparam_change(curr: &mut SpinLock<rros_thread>) {
 }
 
 #[no_mangle]
-unsafe extern "C" fn rust_handle_inband_event(event: bindings::inband_event_type, data: *mut c_types::c_void){
-	match(event){
+unsafe extern "C" fn rust_handle_inband_event(
+    event: bindings::inband_event_type,
+    data: *mut c_types::c_void,
+) {
+    match (event) {
         // case INBAND_TASK_SIGNAL:
         // 	handle_sigwake_event(data);
         // 	break;
-        INBAND_TASK_EXIT =>{
+        INBAND_TASK_EXIT => {
             // pr_info!("{}",rust_helper_dovetail_current_state().subscriber);
             // rros_drop_subscriptions(rros_get_subscriber()); //evl中的sbr为NULL，这里先注释
-            if rros_current()!=0 as *mut SpinLock<rros_thread>{
-            	put_current_thread();
+            if rros_current() != 0 as *mut SpinLock<rros_thread> {
+                put_current_thread();
             }
-        }
-		
-        // case INBAND_TASK_MIGRATION:
-        // 	handle_migration_event(data);
-        // 	break;
-        // case INBAND_TASK_RETUSER:
-        // 	handle_retuser_event();
-        // 	break;
-        // case INBAND_TASK_PTSTOP:
-        // 	handle_ptstop_event();
-        // 	break;
-        // case INBAND_TASK_PTCONT:
-        // 	handle_ptcont_event();
-        // 	break;
-        // case INBAND_TASK_PTSTEP:
-        // 	handle_ptstep_event(data);
-        // 	break;
-        // case INBAND_PROCESS_CLEANUP:
-        // 	handle_cleanup_event(data);
-        // 	break;
-	}
+        } // case INBAND_TASK_MIGRATION:
+          // 	handle_migration_event(data);
+          // 	break;
+          // case INBAND_TASK_RETUSER:
+          // 	handle_retuser_event();
+          // 	break;
+          // case INBAND_TASK_PTSTOP:
+          // 	handle_ptstop_event();
+          // 	break;
+          // case INBAND_TASK_PTCONT:
+          // 	handle_ptcont_event();
+          // 	break;
+          // case INBAND_TASK_PTSTEP:
+          // 	handle_ptstep_event(data);
+          // 	break;
+          // case INBAND_PROCESS_CLEANUP:
+          // 	handle_cleanup_event(data);
+          // 	break;
+    }
 }
 
-fn put_current_thread() -> Result<usize>{
-	let curr = unsafe{&mut *rros_current()};
+fn put_current_thread() -> Result<usize> {
+    let curr = unsafe { &mut *rros_current() };
 
     let state = curr.lock().state;
-	if state & T_USER !=0 {
+    if state & T_USER != 0 {
         pr_info!("000000000000000000000000000000000000000000000000000000");
-	// 	skip_ptsync(curr);
+        // 	skip_ptsync(curr);
     }
-	cleanup_current_thread();
-	// evl_put_element(&curr->element);
+    cleanup_current_thread();
+    // evl_put_element(&curr->element);
+    // unsafe{pr_info!("600 uninit_thread: x ref is {}", Arc::strong_count(uthread.as_ref().unwrap()));}
+    unsafe{Arc::decrement_strong_count(curr);}
+    unsafe{uthread = None;}
     Ok(0)
 }
 
-fn cleanup_current_thread() -> Result<usize>{
-	let p = unsafe{rust_helper_dovetail_current_state()};
-	let curr = unsafe{&mut *rros_current()};
-
-	// trace_evl_thread_unmap(curr);
-	unsafe{bindings::dovetail_stop_altsched()};
-	do_cleanup_current(curr);
-
-	unsafe{(*p).thread = 0 as *mut c_types::c_void};
+fn cleanup_current_thread() -> Result<usize> {
+    // unsafe{pr_info!("00 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    let p = unsafe { rust_helper_dovetail_current_state() };
+    let curr = unsafe { rros_current() };
+    let thread = unsafe { Arc::from_raw(curr as *const SpinLock<rros_thread>) };
+    unsafe{Arc::increment_strong_count(curr);}
+    // unsafe{pr_info!("01 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // trace_evl_thread_unmap(curr);
+    unsafe { bindings::dovetail_stop_altsched() };
+    do_cleanup_current(thread);
+    // unsafe{pr_info!("02 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+
+    unsafe { (*p).thread = 0 as *mut c_types::c_void };
+    // unsafe{Arc::decrement_strong_count(curr);}
+    // unsafe{pr_info!("090 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // unsafe{pr_info!("60 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
     Ok(0)
 }
 
-fn do_cleanup_current(curr: &mut SpinLock<rros_thread>) -> Result<usize>{
-	// struct cred *newcap;
-	let mut flags: c_types::c_ulong = 0;
+fn do_cleanup_current(curr: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+// fn do_cleanup_current(curr: &mut SpinLock<rros_thread>) -> Result<usize> {
+    // struct cred *newcap;
+    // unsafe{pr_info!("03 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    let mut flags: c_types::c_ulong = 0;
     let mut rq: Option<*mut rros_rq>;
-    
+
     // 这里的if暂时是进不去的，observable为空
     // let observable = curr.lock().observable.clone();
     // if observable.is_some(){
-        // evl_flush_observable(curr->observable);
+    // evl_flush_observable(curr->observable);
     // }
-    	
 
-	// evl_drop_tracking_mutexes(curr);
+    // evl_drop_tracking_mutexes(curr);
 
-	// evl_unindex_factory_element(&curr->element);
+    // evl_unindex_factory_element(&curr->element);
     let state = curr.lock().state;
-	if state & T_USER !=0 {
+    if state & T_USER != 0 {
         pr_info!("000000000000000000000000000000000000000000000000000000");
-	// 	evl_free_chunk(&evl_shared_heap, curr->u_window);
-	// 	curr->u_window = NULL;
-	// 	evl_drop_poll_table(curr);
-	// 	newcap = prepare_creds();
-	// 	if (newcap) {
-	// 		drop_u_cap(curr, newcap, CAP_SYS_NICE);
-	// 		drop_u_cap(curr, newcap, CAP_IPC_LOCK);
-	// 		drop_u_cap(curr, newcap, CAP_SYS_RAWIO);
-	// 		commit_creds(newcap);
-	// 	}
-	}
-    
+        // 	evl_free_chunk(&evl_shared_heap, curr->u_window);
+        // 	curr->u_window = NULL;
+        // 	evl_drop_poll_table(curr);
+        // 	newcap = prepare_creds();
+        // 	if (newcap) {
+        // 		drop_u_cap(curr, newcap, CAP_SYS_NICE);
+        // 		drop_u_cap(curr, newcap, CAP_IPC_LOCK);
+        // 		drop_u_cap(curr, newcap, CAP_SYS_RAWIO);
+        // 		commit_creds(newcap);
+        // 	}
+    }
+
     pr_info!("before dequeue_old_thread");
-	// dequeue_old_thread(curr);
+    // dequeue_old_thread(curr);
     pr_info!("after dequeue_old_thread,before rros_get_thread_rq");
-    let x = unsafe{Arc::from_raw(curr as *const SpinLock<rros_thread>)};
-	rq = unsafe{rros_get_thread_rq(Some(x.clone()), &mut flags)};
+    // let x = unsafe { Arc::from_raw(curr as *const SpinLock<rros_thread>) };
+    // unsafe{pr_info!("b 6 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    let x = curr.clone();
+    // unsafe{pr_info!("c 6 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    rq = unsafe { rros_get_thread_rq(Some(x.clone()), &mut flags) };
     pr_info!("after rros_get_thread_rq1111111111");
     let state = curr.lock().state;
-	if state & T_READY != 0{
+    if state & T_READY != 0 {
         pr_info!("before rros_dequeue_thread in thread.rs");
-		// EVL_WARN_ON(CORE, (curr->state & EVL_THREAD_BLOCK_BITS));
-		unsafe{rros_dequeue_thread(Arc::from_raw(curr as *const SpinLock<rros_thread>))};
-		curr.lock().state &= !T_READY;
-	}
+        // EVL_WARN_ON(CORE, (curr->state & EVL_THREAD_BLOCK_BITS));
+        // unsafe { rros_dequeue_thread(Arc::from_raw(curr as *const SpinLock<rros_thread>)) };
+        unsafe { rros_dequeue_thread(x.clone()) };
+        curr.lock().state &= !T_READY;
+    }
 
-	unsafe{(*curr).lock().state |= T_ZOMBIE};
+    unsafe { (*curr).lock().state |= T_ZOMBIE };
     pr_info!("before uninit_thread");
-    unsafe{rros_put_thread_rq(Some(x.clone()), rq, flags)};
-	uninit_thread(curr);
+    unsafe { rros_put_thread_rq(Some(x.clone()), rq, flags) };
+    // unsafe{pr_info!("a 6 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    uninit_thread(curr.clone());
+    // unsafe{pr_info!("9090 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
     pr_info!("after uninit_thread");
     Ok(0)
 }
 
-fn dequeue_old_thread(thread: &mut SpinLock<rros_thread>) -> Result<usize>{
-	let flags = lock::raw_spin_lock_irqsave();
+fn dequeue_old_thread(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+// fn dequeue_old_thread(thread: &mut SpinLock<rros_thread>) -> Result<usize> {
+    let flags = lock::raw_spin_lock_irqsave();
     // kernel corrupted bug is here: next is 0 at initialization, but uses * to get its value
     // let next = unsafe{&mut *thread.lock().next};
     // next.remove();
-    
-	// 	evl_nrthreads--;
-	lock::raw_spin_unlock_irqrestore(flags);
+
+    // 	evl_nrthreads--;
+    lock::raw_spin_unlock_irqrestore(flags);
     Ok(0)
 }
 
-fn uninit_thread(thread: &mut SpinLock<rros_thread>){
-	let mut flags: c_types::c_ulong = 0;
-	let mut rq :Option<*mut rros_rq>;
+// fn uninit_thread(thread: &mut SpinLock<rros_thread>) {
+fn uninit_thread(thread: Arc<SpinLock<rros_thread>>) {
+    pr_info!("the thread address is {:p}", thread);
+    // pr_info!("the uthread address is {:p}", uthread.clone().unwrap());
+            // unsafe{pr_info!("d 6 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    let mut flags: c_types::c_ulong = 0;
+    let mut rq: Option<*mut rros_rq>;
     let rtimer = thread.lock().rtimer.clone();
     let ptimer = thread.lock().ptimer.clone();
-	timer::rros_destroy_timer(rtimer.unwrap());
-	timer::rros_destroy_timer(ptimer.unwrap());
-    let x = unsafe{Arc::from_raw(thread as *const SpinLock<rros_thread>)};
-	unsafe{rq = rros_get_thread_rq(Some(x.clone()), &mut flags)};
+            // unsafe{pr_info!("7 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    timer::rros_destroy_timer(rtimer.clone().unwrap());
+            // unsafe{pr_info!("8 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // thread.lock().rtimer = None;
+    rtimer.as_ref().unwrap().lock().thread = None;
+    timer::rros_destroy_timer(ptimer.unwrap());
+            // unsafe{pr_info!("9 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // let x = unsafe { Arc::from_raw(thread as *const SpinLock<rros_thread>) };
+    let x = thread.clone();
+            // unsafe{pr_info!("10 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    unsafe { rq = rros_get_thread_rq(Some(x.clone()), &mut flags) };
+            // unsafe{pr_info!("11 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
     pr_info!("uninit_thread: x ref is {}", Arc::strong_count(&x.clone()));
-	unsafe{rros_forget_thread(Arc::from_raw(thread as *const SpinLock<rros_thread>))};
-	rros_put_thread_rq(Some(x.clone()), rq, flags);
+            // unsafe{pr_info!("12 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // unsafe { rros_forget_thread(Arc::from_raw(thread as *const SpinLock<rros_thread>)) };
+    unsafe { rros_forget_thread(x.clone()) };
+            // unsafe{pr_info!("13 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    rros_put_thread_rq(Some(x.clone()), rq, flags);
+            // unsafe{pr_info!("14 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
 
     // let name = thread.lock().name as *const c_types::c_void;
-	// bindings::kfree(name);
+    // bindings::kfree(name);
+}
+
+use kernel::error::Error;
+use crate::factory::{rros_init_element, RrosElement, RrosFactory};
+use crate::{sched};
+use core::ptr;
+
+pub static mut uthread: Option<Arc<SpinLock<rros_thread>>> = None;
+
+// TODO: update the __user attribute and modify the `*mut` type
+fn thread_factory_build(fac: &'static mut SpinLock<RrosFactory>,
+    uname: &'static CStr,
+    u_attrs: Option<*mut u8>,
+    clone_flags: i32,
+    state_offp: &u32) -> *mut RrosElement {
+// static struct evl_element *
+// thread_factory_build(struct evl_factory *fac, const char __user *u_name,
+// 		void __user *u_attrs, int clone_flags, u32 *state_offp)
+// {
+    // let observalbe;
+// 	struct evl_observable *observable = NULL;
+    let mut tsk = Task::current_ptr();
+// 	struct task_struct *tsk = current;
+    let mut iattr = rros_init_thread_attr::new();
+// 	struct evl_init_thread_attr iattr;
+// 	struct evl_thread *curr;
+// 	int ret;
+
+    if rros_current() != 0 as *mut SpinLock<rros_thread> {
+        pr_info!("this condition should not be met!!!!!!!!!");
+        // return Err(Error::EBUSY);
+    }
+
+// 	if (evl_current())
+// 		return ERR_PTR(-EBUSY);
+
+    if clone_flags & !factory::RROS_THREAD_CLONE_FLAGS != 0 {
+        pr_info!("I'm in this function this condition should not be met!!!!!!!!!");
+        // return Err(Error::EINVAL);
+    }
+// 	if (clone_flags & ~EVL_THREAD_CLONE_FLAGS)
+// 		return ERR_PTR(-EINVAL);
+
+// 	/* @current must open the control device first. */
+    // TODO: check the flags
+    // let flags = unsafe{rust_helper_doveail_mm_state().flags};
+    // if !test_bit(RROS_MM_ACTIVE_BIT, &flags) {
+    //     return Err(Error::EPERM);
+    // }
+// 	if (!test_bit(EVL_MM_ACTIVE_BIT, &dovetail_mm_state()->flags))
+// 		return ERR_PTR(-EPERM);
+
+    // TODO: update all the unwrap() to return error
+
+    let mut curr;
+    
+    unsafe {
+        // kthread_runner_1 = Some(KthreadRunner::new(kfn_1));
+        let mut thread = SpinLock::new(sched::rros_thread::new().unwrap());
+        pr_info!("at the thread build thread address is {:p} ", &thread);
+        let pinned = Pin::new_unchecked(&mut thread);
+        // pr_info!("at the thread build thread address is {:p} ", &thread);
+        spinlock_init!(pinned, "test_threads1");
+        curr = Arc::try_new(thread).unwrap();
+        // pr_info!("at the thread build thread address is {:p} ", &thread);
+
+        let mut r = SpinLock::new(timer::RrosTimer::new(1));
+        let pinned_r = Pin::new_unchecked(&mut r);
+        spinlock_init!(pinned_r, "rtimer_1");
+
+        let mut p = SpinLock::new(timer::RrosTimer::new(1));
+        let pinned_p = Pin::new_unchecked(&mut p);
+        spinlock_init!(pinned_p, "ptimer_1");
+
+        curr
+            .lock()
+            .rtimer = Some(Arc::try_new(r).unwrap());
+        curr
+            .lock()
+            .ptimer = Some(Arc::try_new(p).unwrap());
+    }
+    pr_info!("at the thread build curr is {:p} ", curr);
+    unsafe{uthread = Some(curr.clone());}
+    pr_info!("at the thread build curr is {:p} ", curr);
+    pr_info!("at the thread build curr is {:p} ", uthread.as_ref().unwrap());
+    pr_info!("at the thread build curr is {:p} ", uthread.clone().unwrap());
+// 	curr = kzalloc(sizeof(*curr), GFP_KERNEL);
+// 	if (curr == NULL)
+// 		return ERR_PTR(-ENOMEM);
+
+    // TODO: add the rros_init_user_element
+    // let mut ret = rros_init_element(e, fac, clone_flags);
+// 	ret = evl_init_user_element(&curr->element, &evl_thread_factory,
+// 				u_name, clone_flags);
+// 	if (ret)
+// 		goto fail_element;
+
+    iattr.flags = T_USER as i32;
+// 	iattr.flags = T_USER;
+
+    if clone_flags & factory::RROS_CLONE_OBSERVABLE != 0 {
+        pr_info!("This should not happen for now!!!!!!!")
+        // observalbe = rros_observable::new().unwrap();
+    } else if clone_flags & factory::RROS_CLONE_MASTER != 0 {
+        pr_info!("This should not happen for now!!!!!!!")
+        // observalbe = rros_observable::new().unwrap();
+    }
+// 	if (clone_flags & EVL_CLONE_OBSERVABLE) {
+// 		/*
+// 		 * Accessing the observable is done via the thread
+// 		 * element (if public), so clear the public flag for
+// 		 * the observable itself.
+// 		 */
+// 		observable = evl_alloc_observable(
+// 			u_name, clone_flags & ~EVL_CLONE_PUBLIC);
+// 		if (IS_ERR(observable)) {
+// 			ret = PTR_ERR(observable);
+// 			goto fail_observable;
+// 		}
+// 		/*
+// 		 * Element name was already set from user input by
+// 		 * evl_alloc_observable(). evl_create_core_element_device()
+// 		 * is told to skip name assignment (NULL name).
+// 		 */
+// 		ret = evl_create_core_element_device(
+// 			&observable->element,
+// 			&evl_observable_factory, NULL);
+// 		if (ret)
+// 			goto fail_observable_dev;
+// 		observable = observable;
+// 	} else if (clone_flags & EVL_CLONE_MASTER) {
+// 		ret = -EINVAL;
+// 		goto fail_observable;
+// 	}
+
+    // TODO: update the affinity to cpu_possible_mask
+    iattr.affinity = unsafe{&RROS_OOB_CPUS as *const CpumaskT};
+// 	iattr.affinity = cpu_possible_mask;
+    // iattr.observable = observalbe;
+// 	iattr.observable = observable;
+    // FIXME: alter sched_fifo with sched_weak, but for now we just use fifo
+    iattr.sched_class = unsafe{Some(&fifo::rros_sched_fifo)};
+// 	iattr.sched_class = &evl_sched_weak;
+    // FIXME: alter fifo with weak, but for now we just use fifo
+    unsafe{
+        let sched_param =  unsafe{Arc::try_new(SpinLock::new(rros_sched_param::new())).unwrap()};
+        (*sched_param.locked_data().get()).fifo.prio = 10;
+        iattr.sched_param = Some(sched_param);
+    }    
+    // iattr.sched_param.weak.prio = 0;
+// 	iattr.sched_param.weak.prio = 0;
+    // TODO: update the rq parameter
+    unsafe{pr_info!("2 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    rros_init_thread(&Some(curr.clone()), iattr, this_rros_rq(), uname);
+    unsafe{pr_info!("uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+// 	ret = evl_init_thread(curr, &iattr, NULL, "%s",
+// 			evl_element_name(&curr->element));
+// 	if (ret)
+// 		goto fail_thread;
+
+    map_uthread_self(curr.clone()).unwrap();
+    unsafe{pr_info!("3 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+// 	ret = map_uthread_self(curr);
+// 	if (ret)
+// 		goto fail_map;
+
+    // TODO: set u_window
+    // let state_offp = evl_shared_offset(curr.u_window);
+// 	*state_offp = evl_shared_offset(curr->u_window);
+    // TODO: add the index `evl_index_factory_element` function
+// 	evl_index_factory_element(&curr->element);
+
+// 	/*
+// 	 * Unlike most elements, a thread may exist in absence of any
+// 	 * file reference, so we get a reference on the emerging
+// 	 * thread here to block automatic disposal on last file
+// 	 * release. put_current_thread() drops this reference when the
+// 	 * thread exits, or voluntarily detaches by sending the
+// 	 * EVL_THRIOC_DETACH_SELF control request.
+// 	 */
+    // TODO: add the index `evl_get_element` function
+// 	evl_get_element(&curr->element);
+
+    // TODO: modify the tsk name
+    let comm_name_i:[u8;16] = *b"new_thread_lhy0\0";
+    let comm_name:[i8;16] = unsafe{core::mem::transmute(comm_name_i)};
+// let comm_name_slice = comm_name.as_bytes();
+    unsafe{(*tsk).comm.clone_from(&comm_name);}
+    // tsk.comm = factory::rros_element_name(curr.element);
+// 	strncpy(tsk->comm, evl_element_name(&curr->element),
+// 		sizeof(tsk->comm));
+// 	tsk->comm[sizeof(tsk->comm) - 1] = '\0';
+
+    unsafe{&mut (*curr.locked_data().get()).element as *mut RrosElement}
+// 	return &curr->element;
+
+// fail_map:
+// 	discard_unmapped_uthread(curr);
+// fail_thread:
+// 	if (observable)
+// fail_observable_dev:
+// 		evl_put_element(&observable->element); /* ->dispose() */
+// fail_observable:
+// 	evl_destroy_element(&curr->element);
+// fail_element:
+// 	kfree(curr);
+
+// 	return ERR_PTR(ret);
+// }
+}
+
+fn map_uthread_self(thread: Arc<SpinLock<rros_thread>>) -> Result<usize> {
+// static int map_uthread_self(struct evl_thread *thread)
+// { 
+//    mkdir /dev/evl
+//    mkdir /dev/evl/thread
+//    mknod /dev/evl/thread/clone c 245 1
+// 	struct mm_struct *mm = current->mm;
+// 	struct evl_user_window *u_window;
+// 	struct cred *newcap;
+
+    // TODO: add the mm check
+// 	/* mlockall(MCL_FUTURE) required. */
+// 	if (!(mm->def_flags & VM_LOCKED))
+// 		return -EINVAL;
+
+    // TODO: add the u_window to support statistics
+    // let u_window = rros_alloc_zeroed(layout::size_of::<evl_user_window>()) as *mut evl_user_window;
+// 	u_window = evl_zalloc_chunk(&evl_shared_heap, sizeof(*u_window));
+// 	if (u_window == NULL)
+// 		return -ENOMEM;
+
+// 	/*
+// 	 * Raise capababilities of user threads when attached to the
+// 	 * core. Filtering access to /dev/evl/control can be used to
+// 	 * restrict attachment.
+// 	 */
+    unsafe{(*thread.locked_data().get()).raised_cap = bindings::kernel_cap_t { cap: [0, 0] };}
+// 	thread->raised_cap = CAP_EMPTY_SET;
+    // TODO: add the cred wrappers/ maybe first check the lastest RFL wrap
+    let new_cap = unsafe{prepare_creds()};
+    if new_cap == 0 as *mut bindings::cred {
+        return Err(Error::ENOMEM);
+    }
+// 	newcap = prepare_creds();
+// 	if (newcap == NULL)
+// 		return -ENOMEM;
+
+    add_u_cap(thread.clone(), new_cap, bindings::CAP_SYS_NICE);
+    add_u_cap(thread.clone(), new_cap, bindings::CAP_IPC_LOCK);
+    add_u_cap(thread.clone(), new_cap, bindings::CAP_SYS_RAWIO);
+    // TODO: add the commit_creds wrappers
+    unsafe{bindings::commit_creds(new_cap);}
+// 	add_u_cap(thread, newcap, CAP_SYS_NICE);
+// 	add_u_cap(thread, newcap, CAP_IPC_LOCK);
+// 	add_u_cap(thread, newcap, CAP_SYS_RAWIO);
+// 	commit_creds(newcap);
+
+// 	/*
+// 	 * CAUTION: From that point, we assume the mapping won't fail,
+// 	 * therefore there is no added capability to drop in
+// 	 * discard_unmapped_uthread().
+// 	 */
+    // TODO: add the support of u_window
+// 	thread->u_window = u_window;
+    
+    // TODO: add the support of pin_to_initial_cpu. This can be omitted as shown in the map_kthread_self
+// 	pin_to_initial_cpu(thread);
+    // TODO: add the trace function
+// 	trace_evl_thread_map(thread);
+
+    unsafe{bindings::dovetail_init_altsched(&mut (*thread.locked_data().get()).altsched);}
+// 	dovetail_init_altsched(&thread->altsched);
+    unsafe{pr_info!("new_1 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    set_oob_threadinfo(Arc::into_raw(thread.clone()) as *mut SpinLock<rros_thread> as *mut c_types::c_void);
+    
+    unsafe{pr_info!("new_2 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+    // Arc::into_raw(thread.clone()) as *mut SpinLock<rros_thread> as *mut c_types::c_void;
+    unsafe{pr_info!("new_3 uninit_thread: x ref is {}", Arc::strong_count(&uthread.clone().unwrap()));}
+// 	set_oob_threadinfo(thread);
+    set_oob_mminfo(thread.clone());
+// 	set_oob_mminfo(thread);
+
+// 	/*
+// 	 * CAUTION: we enable dovetailing only when *thread is
+// 	 * consistent, so that we won't trigger false positive in
+// 	 * debug code from handle_schedule_event() and friends.
+// 	 */
+    unsafe{bindings::dovetail_start_altsched();}
+// 	dovetail_start_altsched();
+
+// 	/*
+// 	 * A user-space thread is already started EVL-wise since we
+// 	 * have an underlying in-band context for it, so we can
+// 	 * enqueue it now.
+// 	 */
+    
+    // TODO: add the enqueue_new_thread wrappers. This can be omitted as shown in the map_kthread_self. 
+// 	enqueue_new_thread(thread); 
+    // FIXME: Maybe we could use this function to add the thread to the thread list avoiding hanging the thread. If the null pointer problem occurs, it can be solved by adding the thread to the thread list in the `enqueue_new_thread` function.
+    rros_release_thread(thread.clone(), T_DORMANT, 0);
+// 	evl_release_thread(thread, T_DORMANT, 0);
+
+    // TODO: update the thread u_window
+// 	evl_sync_uwindow(thread);
+
+    Ok(0)
+// 	return 0;
+// }
+}
+
+// TODO: conditional compilation
+#[inline]
+#[cfg(CONFIG_MULTIUSER)]
+fn capable(cap: i32) -> bool {
+    unsafe{bindings::capable(cap)}
+}
+
+#[inline]
+#[cfg(not(CONFIG_MULTIUSER))]
+fn capable(cap: i32) -> bool {
+    true
+}
+
+// TODO: update the cred wrappers
+fn add_u_cap(thread: Arc<SpinLock<rros_thread>>, new_cap: *mut bindings::cred, cap: bindings::u_int) {
+    // TODO: add the cap_raise&&capable wrappers
+    if unsafe{!capable(cap as i32)} {
+        unsafe{rust_helper_cap_raise(&mut (*new_cap).cap_effective as *mut bindings::kernel_cap_struct, cap as i32)};
+        unsafe{rust_helper_cap_raise(&mut (*thread.locked_data().get()).raised_cap as *mut bindings::kernel_cap_struct, cap as i32)};
+    }
+}
+// static inline void add_u_cap(struct evl_thread *thread,
+//     struct cred *newcap,
+//     int cap)
+// {
+// if (!capable(cap)) {
+// cap_raise(newcap->cap_effective, cap);
+// cap_raise(thread->raised_cap, cap);
+// }
+// }
+pub fn rros_notify_thread(thread:*mut rros_thread, tag:u32, details: RrosValue) -> Result<usize>{
+    unsafe{
+        if (*thread).state & T_HMSIG != 0{
+            rros_signal_thread(thread, SIGDEBUG, tag);
+        }
+        // if (*thread).state & T_HMOBS !=0 {
+        //     if (!rros_send_observable(thread->observable, tag, details) == 0){
+        //         printk_ratelimited(EVL_WARNING
+        //             "%s[%d] could not receive HM event #%d",
+        //             evl_element_name(&thread->element),
+        //                 rros_get_inband_pid(thread),
+        //                 tag);
+        //     }
+        // }
+    }
+    Ok(0)
+}
+
+fn rros_signal_thread(thread: *mut rros_thread, sig:i32, arg:u32) -> Result<usize>{
+	let mut sigd = sig_irqwork_data::new();
+
+	// if (EVL_WARN_ON(CORE, !(thread->state & T_USER)))
+		// return;
+
+	if premmpt::running_inband() == Ok(0) {
+		// do_inband_signal(thread, sig, arg);
+		return Ok(0);
+	}
+
+    unsafe{
+	    init_irq_work(&mut sigd.work.0 as *mut bindings::irq_work, sig_irqwork);
+        sigd.thread = thread;
+	    sigd.signo = sig;
+	    if sig == SIGDEBUG {
+            sigd.sigval = arg | sigdebug_marker;
+        }else{
+            sigd.sigval = arg;
+        }
+        // evl_get_element(&thread->element);
+        IrqWork::irq_work_queue(&mut sigd.work);
+    }
+    Ok(0)
+}
+
+unsafe extern "C" fn sig_irqwork(work:*mut bindings::irq_work){
+	let sigd = sig_irqwork_data::new();
+	// unsafe{do_inband_signal(sigd.thread, sigd.signo, sigd.sigval)};
+	// evl_put_element(&sigd->thread->element);
+}
+
+// fn do_inband_signal(thread:*mut rros_thread, signo:i32, sigval:i32){
+// 	let p = unsafe{(*thread)}.altsched.task;
+//     let si = bindings::kernel_siginfo::default();
+
+// 	if signo == SIGDEBUG {
+// 		si.si_signo = signo;
+// 		si.si_code = SI_QUEUE;
+// 		si.si_int = sigval;
+// 		bindings::send_sig_info(signo, &mut si as *mut bindings::kernel_siginfo, p);
+// 	} else{
+// 		bindings::send_sig(signo, p, 1);
+//     }
+// }
+
+fn rros_get_inband_pid(thread:*mut rros_thread) -> i32{
+	unsafe{
+        if (*thread).state & (T_ROOT|T_DORMANT|T_ZOMBIE) != 0{
+            return 0;
+        }
+
+        if (*thread).altsched.task == 0 as *mut bindings::task_struct {
+            return -1;
+        }
+
+        return (*(*thread).altsched.task).pid;
+    }
+}
+
+pub fn rros_track_thread_policy(thread:Arc<SpinLock<rros_thread>>,target:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+	unsafe{
+        let mut param = rros_sched_param::new();
+	    // rros_double_rq_lock((*thread).rq, (*target).rq);
+
+        let mut state = (*thread.locked_data().get()).state;
+        if state & T_READY != 0{
+		    rros_dequeue_thread(thread.clone());
+        }
+        
+        let thread_ptr = Arc::into_raw(thread.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+        let target_ptr = Arc::into_raw(target.clone()) as *mut SpinLock<rros_thread> as *mut rros_thread;
+        if target_ptr == thread_ptr {
+            (*thread.locked_data().get()).sched_class = (*thread.locked_data().get()).base_class;
+            rros_track_priority(thread.clone(), Arc::try_new(SpinLock::new(rros_sched_param::new()))?);
+            state = (*thread.locked_data().get()).state;
+            if state & T_READY != 0{
+                rros_requeue_thread(thread.clone());
+            }
+        } else {
+            rros_get_schedparam(target.clone(), Arc::try_new(SpinLock::new(param))?);
+            (*thread.locked_data().get()).sched_class = (*target.locked_data().get()).sched_class;
+            rros_track_priority(thread.clone(), Arc::try_new(SpinLock::new(param))?);
+            state = (*thread.locked_data().get()).state;
+            if state & T_READY != 0{
+                rros_enqueue_thread(thread.clone());
+            }
+        }
+
+        let rq = (*thread.locked_data().get()).rq;
+        rros_set_resched(rq.clone());
+        // rros_double_rq_unlock(thread->rq, target->rq);
+        Ok(0)
+    }
+}
+
+pub fn rros_wakeup_thread(thread:Arc<SpinLock<rros_thread>>, mask:u32, info:u32) -> Result<usize>{
+	unsafe{
+        let mut flags = 0 as c_types::c_ulong;
+        let rq = rros_get_thread_rq(Some(thread.clone()), &mut flags);
+        rros_wakeup_thread_locked(Some(thread.clone()), mask, info);
+        rros_put_thread_rq(Some(thread.clone()), rq, flags);
+        Ok(0)
+    }
+}
+
+fn rros_wakeup_thread_locked(thread:Option<Arc<SpinLock<rros_thread>>>, mut mask:u32, info:u32){
+	unsafe{
+        let thread = thread.clone().unwrap();
+        let rq = (*thread.locked_data().get()).rq;
+        let mut oldstate:u32 = 0;
+
+        // if (EVL_WARN_ON(CORE, mask & ~(T_DELAY|T_PEND|T_WAIT)))
+        // 	return;
+
+        oldstate = (*thread.locked_data().get()).state;
+        if oldstate & mask != 0 {
+            if mask & T_PEND != 0{
+                mask |= T_DELAY;
+            }
+
+            (*thread.locked_data().get()).state &= !mask;
+
+            if mask & (T_DELAY|T_PEND) != 0{
+                let rtimer = (*thread.locked_data().get()).rtimer.clone().unwrap();
+                timer::rros_stop_timer(rtimer.clone());
+            }
+            if mask & T_PEND & oldstate != 0{
+                (*thread.locked_data().get()).wchan = core::ptr::null_mut();
+            }
+
+            (*thread.locked_data().get()).info |= info;
+            
+            if (*thread.locked_data().get()).state & RROS_THREAD_BLOCK_BITS == 0 {
+                rros_enqueue_thread(thread.clone());
+                (*thread.locked_data().get()).state |= T_READY;
+                rros_set_resched(rq);
+                // if rq != this_rros_rq(){
+                    // rros_inc_counter(&thread->stat.rwa);
+                // }
+            }
+        }
+    }
+}
+
+pub fn thread_oob_ioctl(filp:*mut bindings::file, cmd:u32, arg:u32) -> Result<usize>{
+    let __fbind = unsafe{(*filp).private_data as *mut RrosFileBinding};
+    let thread = unsafe{kernel::container_of!((*__fbind).rros_element, rros_thread, element) as *mut rros_thread};
+
+	// let curr = rros_current();
+
+	unsafe{
+        if (*thread).state & T_ZOMBIE != 0{
+		    return Err(kernel::Error::ESTALE);
+        }
+    }
+
+	// switch (cmd) {
+	// case EVL_THRIOC_SWITCH_OOB:
+	// 	if (thread == curr)
+	// 		ret = 0;	/* Already there. */
+	// 	break;
+	// case EVL_THRIOC_SWITCH_INBAND:
+	// 	if (thread == curr) {
+	// 		evl_switch_inband(EVL_HMDIAG_NONE);
+	// 		ret = 0;
+	// 	}
+	// 	break;
+	// case EVL_THRIOC_SIGNAL:
+	// 	ret = raw_get_user(monfd, (__u32 *)arg);
+	// 	if (ret)
+	// 		return -EFAULT;
+	// 	ret = evl_signal_monitor_targeted(thread, monfd);
+	// 	break;
+	// case EVL_THRIOC_YIELD:
+	// 	evl_release_thread(curr, 0, 0);
+	// 	evl_schedule();
+	// 	ret = 0;
+	// 	break;
+	// default:
+		return thread_common_ioctl(thread, cmd, arg);
+	// }
 }
+
+pub fn thread_common_ioctl(thread:*mut rros_thread, cmd:u32, arg:u32) -> Result<usize>{
+	let mut statebuf = rros_thread_state::new();
+	let mut attrs = rros_sched_attrs::new();
+	// __u32 mask, oldmask;
+	let mut ret:Result<usize> = Ok(0);
+
+	match (cmd) {
+        RROS_THRIOC_SET_SCHEDPARAM =>{
+            // ret = raw_copy_from_user(&attrs,
+            //         (struct evl_sched_attrs *)arg, sizeof(attrs));
+            // if (ret)
+            //     return -EFAULT;
+            ret = set_sched_attrs(thread, attrs);
+        }
+        RROS_THRIOC_GET_SCHEDPARAM =>{
+            get_sched_attrs(thread, &mut attrs);
+            // ret = raw_copy_to_user((struct evl_sched_attrs *)arg,
+            //         &attrs, sizeof(attrs));
+            // if (ret)
+            //     return -EFAULT;
+        }
+        RROS_THRIOC_GET_STATE =>{
+            rros_get_thread_state(thread, &mut statebuf);
+            // ret = raw_copy_to_user((struct evl_thread_state *)arg,
+            //         &statebuf, sizeof(statebuf));
+            // if (ret)
+            //     return -EFAULT;
+        }
+        // case EVL_THRIOC_SET_MODE:
+        // case EVL_THRIOC_CLEAR_MODE:
+        // 	ret = raw_get_user(mask, (__u32 *)arg);
+        // 	if (ret)
+        // 		return -EFAULT;
+        // 	ret = update_mode(thread, mask, &oldmask,
+        // 			cmd == EVL_THRIOC_SET_MODE);
+        // 	if (ret)
+        // 		return ret;
+        // 	ret = raw_put_user(oldmask, (__u32 *)arg);
+        // 	if (ret)
+        // 		return -EFAULT;
+        // 	break;
+        // case EVL_THRIOC_UNBLOCK:
+        // 	evl_unblock_thread(thread, 0);
+        // 	break;
+        // case EVL_THRIOC_DEMOTE:
+        // 	evl_demote_thread(thread);
+        // 	break;
+        _ =>{
+            ret = Err(kernel::Error::ENOTTY);
+        }
+	}
+
+	unsafe{rros_schedule()};
+
+	return ret;
+}
+
+
+pub fn set_sched_attrs(thread:*mut rros_thread, attrs:rros_sched_attrs) -> Result<usize>{
+	let mut param = rros_sched_param::new();
+	let mut flags:c_types::c_ulong = 0;
+	let mut ret:Result<usize> = Ok(0);
+    let mut tslice = unsafe{(*thread).rrperiod};
+    let thread:Option<Arc<SpinLock<rros_thread>>> = unsafe{Some(Arc::from_raw(thread as *mut SpinLock<rros_thread>))};
+	let rq = rros_get_thread_rq(thread.clone(), &mut flags);
+    let sched_class = rros_find_sched_class(&mut param, attrs, tslice);
+
+	// if (IS_ERR(sched_class)) {
+	// 	ret = PTR_ERR(sched_class);
+	// 	rros_put_thread_rq(thread, rq, flags);
+	//     return ret;
+	// }
+
+	ret = set_time_slice(thread.clone(), tslice);
+	if ret != Ok(0){
+        rros_put_thread_rq(thread, rq, flags);
+        return ret;
+    }
+
+	unsafe{ret = rros_set_thread_schedparam_locked(thread.clone().unwrap(), Some(sched_class), Some(Arc::try_new(SpinLock::new(param))?))};
+
+	rros_put_thread_rq(thread, rq, flags);
+
+	return ret;
+}
+
+pub fn rros_find_sched_class(param:&mut rros_sched_param,attrs:rros_sched_attrs,mut tslice_r:ktime::KtimeT) -> &'static rros_sched_class{
+	let sched_class;
+	// int prio, policy;
+	// ktime_t tslice;
+
+	// policy = attrs->sched_policy;
+	// prio = attrs->sched_priority;
+	// tslice = RROS_INFINITE;
+	// sched_class = &evl_sched_fifo;
+	param.fifo.prio = 47;
+
+    // TODO 目前policy这里只写了tp
+	// switch (policy) {
+    //     case SCHED_NORMAL:
+    //         if (prio)
+    //             return ERR_PTR(-EINVAL);
+    //         fallthrough;
+    //     case SCHED_WEAK:
+    //         if (prio < EVL_WEAK_MIN_PRIO ||	prio > EVL_WEAK_MAX_PRIO)
+    //             return ERR_PTR(-EINVAL);
+    //         param->weak.prio = prio;
+    //         sched_class = &evl_sched_weak;
+    //         break;
+    //     case SCHED_RR:
+    //         /* if unspecified, use current one. */
+    //         tslice = u_timespec_to_ktime(attrs->sched_rr_quantum);
+    //         if (timeout_infinite(tslice) && tslice_r)
+    //             tslice = *tslice_r;
+    //         fallthrough;
+    //     case SCHED_FIFO:
+    //         /*
+    //         * This routine handles requests submitted from
+    //         * user-space exclusively, so a SCHED_FIFO priority
+    //         * must be in the [FIFO_MIN..FIFO_MAX] range.
+    //         */
+    //         if (prio < EVL_FIFO_MIN_PRIO ||	prio > EVL_FIFO_MAX_PRIO)
+    //             return ERR_PTR(-EINVAL);
+    //         break;
+    //     case SCHED_QUOTA:
+    // #ifdef CONFIG_EVL_SCHED_QUOTA
+    //         param->quota.prio = attrs->sched_priority;
+    //         param->quota.tgid = attrs->sched_quota_group;
+    //         sched_class = &evl_sched_quota;
+    //         break;
+    // #else
+    //         return ERR_PTR(-EOPNOTSUPP);
+    // #endif
+    //     case SCHED_TP:
+    // #ifdef CONFIG_EVL_SCHED_TP
+    //         param->tp.prio = attrs->sched_priority;
+    //         param->tp.ptid = attrs->sched_tp_partition;
+    //         sched_class = &rros_sched_tp;
+    //         break;
+    // #else
+    //         return ERR_PTR(-EOPNOTSUPP);
+    // #endif
+    //     default:
+    //         return ERR_PTR(-EINVAL);
+	// }
+    
+    // 按照libevl写的，prio+ptid应为50，part取3
+    param.tp.prio = 47;
+    param.tp.ptid = 3;
+    // sched_class = unsafe{&rros_sched_tp}; //FIXME: tp未实现
+    panic!();
+	tslice_r = RROS_INFINITE;
+
+	return sched_class;
+}
+
+pub fn set_time_slice(thread:Option<Arc<SpinLock<rros_thread>>>, quantum:ktime::KtimeT) -> Result<usize>{
+    let thread = thread.clone().unwrap();
+	let rq = unsafe{(*thread.locked_data().get()).rq.unwrap()};
+
+	// assert_hard_lock(&thread->lock);
+	// assert_hard_lock(&rq->lock);
+
+	unsafe{(*thread.locked_data().get()).rrperiod = quantum};
+
+    let curr_ptr = unsafe{(*rq).curr.clone().unwrap().locked_data().get()};
+    let thread_ptr = thread.clone().locked_data().get();
+	if quantum!=0 {
+        unsafe{
+            if quantum <= (&clock::RROS_MONO_CLOCK).get_gravity_user(){
+                return Err(kernel::Error::EINVAL);
+            }
+        }
+
+        let sched_tick = unsafe{(*thread.locked_data().get()).base_class.unwrap().sched_tick};
+		if sched_tick.is_none(){
+			return Err(kernel::Error::EINVAL);
+        }
+
+		unsafe{(*thread.locked_data().get()).state |= T_RRB};
+        unsafe{
+            if curr_ptr == thread_ptr {
+                timer::rros_start_timer((*rq).rrbtimer.clone().unwrap(),timer::rros_abs_timeout((*rq).rrbtimer.clone().unwrap(), quantum),RROS_INFINITE);
+            }
+        }
+	} else {
+		unsafe{
+            (*thread.locked_data().get()).state &= !T_RRB;
+            if curr_ptr == thread_ptr {
+                timer::rros_stop_timer((*rq).rrbtimer.clone().unwrap());
+            }
+        }
+	}
+
+	Ok(0)
+}
+
+pub fn get_sched_attrs(thread:*mut rros_thread, attrs:&mut rros_sched_attrs) -> Result<usize>{
+    let mut flags:c_types::c_ulong = 0;
+    let thread_ptr = thread;
+    let thread:Option<Arc<SpinLock<rros_thread>>> = unsafe{Some(Arc::from_raw(thread as *mut SpinLock<rros_thread>))};
+	let rq = rros_get_thread_rq(thread.clone(), &mut flags);
+	/* Get the base scheduling attributes. */
+	attrs.sched_priority = unsafe{(*thread.clone().unwrap().locked_data().get()).bprio};
+    let base_class = unsafe{(*thread.clone().unwrap().locked_data().get()).base_class};
+	__get_sched_attrs(base_class.clone(), thread_ptr, attrs);
+	rros_put_thread_rq(thread.clone(), rq.clone(), flags);
+    Ok(0)
+}
+
+// 目前只写了tp
+pub fn __get_sched_attrs(sched_class:Option<&'static rros_sched_class>,thread:*mut rros_thread, attrs:&mut rros_sched_attrs) -> Result<usize>{
+    let param = unsafe{Some(Arc::try_new(SpinLock::new(rros_sched_param::new()))?)};
+	attrs.sched_policy = sched_class.unwrap().policy;
+
+    match (sched_class.unwrap().sched_getparam){
+        Some(f) =>{
+            let thread_option = unsafe{Some(Arc::from_raw(thread as *mut SpinLock<rros_thread>))};
+            f(thread_option.clone(), param.clone());
+        }
+        None =>{
+            pr_info!("__get_sched_attrs sched_getparam is none");
+        }
+    }
+	if sched_class.unwrap().flag == 3 {
+        unsafe{
+            if (*thread).state & T_RRB !=0 {
+                // attrs->sched_rr_quantum =
+                // 	ktime_to_u_timespec(thread->rrperiod);
+                attrs.sched_policy = SCHED_RR;
+            }
+        }
+	}
+
+// #ifdef CONFIG_EVL_SCHED_QUOTA
+// 	if (sched_class == &evl_sched_quota) {
+// 		attrs->sched_quota_group = param.quota.tgid;
+// 		goto out;
+// 	}
+// #endif
+
+// #ifdef CONFIG_EVL_SCHED_TP
+	if sched_class.unwrap().flag == 4 {
+		attrs.tp_partition= unsafe{(*param.unwrap().locked_data().get()).tp.ptid};
+	}
+// #endif
+
+// out:
+// 	trace_evl_thread_getsched(thread, attrs);
+    Ok(0)
+}
+
+pub fn rros_get_thread_state(thread:*mut rros_thread,statebuf:&mut rros_thread_state) -> Result<usize>{
+	let mut flags:c_types::c_ulong = 0;
+    let thread_option = unsafe{Some(Arc::from_raw(thread as *mut SpinLock<rros_thread>))};
+	let rq = rros_get_thread_rq(thread_option.clone(), &mut flags);
+	statebuf.eattrs.sched_priority = unsafe{(*thread).cprio};
+	unsafe{__get_sched_attrs((*thread).sched_class.clone(), thread, &mut statebuf.eattrs)};
+	unsafe{
+        statebuf.cpu = rros_rq_cpu((*thread).rq.unwrap()) as u32;
+        statebuf.state = rros_rq_cpu((*thread).rq.unwrap()) as u32;
+        statebuf.isw = (*thread).stat.isw.get_counter();
+        statebuf.csw = (*thread).stat.csw.get_counter();
+        statebuf.sc = (*thread).stat.sc.get_counter();
+        statebuf.rwa = (*thread).stat.rwa.get_counter();
+	    statebuf.xtime = (*thread).stat.account.get_account_total() as u32;
+    }
+	rros_put_thread_rq(thread_option.clone(), rq.clone(), flags);
+    Ok(0)
+}
+
+pub struct LatmusRunner {
+    pub period: u64,
+    pub state: RunnerState,
+}
+
+pub struct RunnerState{
+    pub min_latency: u64,
+    pub max_latency: u64,
+    pub avg_latency: u64,
+    pub ideal: u64,
+    pub offset: u64,
+}
+
+// TODO: fix the kthreadrunner with more flags to adjust the latmus and move this struct to latmus
+pub struct KthreadRunner(pub Option<RrosKthread>, pub u64, pub LatmusRunner);
+impl KthreadRunner {
+    pub const fn new_empty() -> Self {
+        KthreadRunner(None, 0, LatmusRunner{period: 0, state: RunnerState{min_latency: 0, max_latency: 0, avg_latency: 0, ideal: 0, offset: 0}})
+    }
+    // pub fn new(kfn:Box<dyn FnOnce()>) -> Self{
+    //     let mut r = Self::new_empty();
+    //     r.init(kfn);
+    //     return r;
+    // }
+    pub fn init(&mut self, kfn:Box<dyn FnOnce()>){
+        let mut kthread = RrosKthread::new(Some(kfn));
+        // let mut thread = unsafe{SpinLock::new(rros_thread::new().unwrap())};
+        // let pinned: Pin<&mut SpinLock<rros_thread>> = unsafe{Pin::new_unchecked(&mut thread)};
+        // spinlock_init!(pinned, "test_threads2");
+        // kthread.thread = Some(Arc::try_new(thread).unwrap());
+
+        let mut tmp = Arc::<SpinLock<rros_thread>>::try_new_uninit().unwrap();
+        let tmp = unsafe{tmp.assume_init()};
+        kthread.thread =  Some(tmp);//Arc::try_new(thread)?
+        unsafe{(*kthread.thread.as_mut().unwrap().locked_data().get()).init().unwrap();}
+        let pinned = unsafe{Pin::new_unchecked(&mut *(Arc::into_raw( kthread.thread.clone().unwrap()) as *mut SpinLock<rros_thread>))};
+        // &mut *Arc::into_raw( *(*rq_ptr).root_thread.clone().as_mut().unwrap()) as &mut SpinLock<rros_thread>
+        spinlock_init!(pinned, "rros_threads");
+
+
+        let mut r = unsafe{SpinLock::new(timer::RrosTimer::new(1))};
+        let pinned_r = unsafe{Pin::new_unchecked(&mut r)};
+        spinlock_init!(pinned_r, "rtimer_3");
+        let mut p = unsafe{SpinLock::new(timer::RrosTimer::new(1))};
+        let pinned_p = unsafe{Pin::new_unchecked(&mut p)};
+        spinlock_init!(pinned_p, "ptimer_3");
+        
+        kthread.thread.as_mut().map(|thread|{
+            unsafe{
+                let mut t = &mut (*(*thread).locked_data().get());
+                t.rtimer = Some(Arc::try_new(r).unwrap());
+                t.ptimer = Some(Arc::try_new(p).unwrap());
+            }
+        });
+        self.0 = Some(kthread);
+    }
+
+    pub fn run(&mut self,name:&'static CStr, prio: i32){
+        unsafe{
+            let x = self.0.as_mut().unwrap().thread.as_mut().unwrap();
+            (*(*x).locked_data().get()).name = core::str::from_utf8(name.as_bytes()).unwrap()
+        }
+        if let Some(t) = self.0.as_mut(){
+            rros_run_kthread(t, name, prio);
+        }
+    }
+
+    fn cancel(&mut self){
+        if let Some(t) = self.0.as_mut(){
+            // rros_stop_kthread( t.thread.clone().unwrap());
+            self.0 = None;
+        }
+    }
+}
+
+pub fn rros_set_period(clock: &mut clock::RrosClock, idate: ktime::KtimeT, period:ktime::KtimeT, flag: i32) {
+// int evl_set_period(struct evl_clock *clock,
+//     ktime_t idate, ktime_t period)
+// {
+    let curr = rros_current();
+    let mut thread;
+    unsafe{
+        thread = Arc::from_raw(curr as *mut SpinLock<rros_thread>);
+        unsafe{Arc::increment_strong_count(curr);}
+    }
+    let timer = unsafe{(*thread.locked_data().get()).ptimer.as_ref().unwrap().clone()};
+    
+// struct evl_thread *curr = evl_current();
+// unsigned long flags;
+// int ret = 0;
+
+    // TODO: add the error handling
+// if (curr == NULL)
+//     return -EPERM;
+
+    // TODO: add the error handling
+    if flag == 0 {
+        rros_stop_timer(timer);
+        return;
+    }
+// if (clock == NULL || period == EVL_INFINITE) {
+//     evl_stop_timer(&curr->ptimer);
+//     return 0;
+// }
+
+    // TODO: add the error handling
+// /*
+//  * LART: detect periods which are shorter than the target
+//  * clock gravity for kernel thread timers. This can't work,
+//  * caller must have messed up arguments.
+//  */
+// if (period < evl_get_clock_gravity(clock, kernel))
+//     return -EINVAL;
+
+    let flags = raw_spin_lock_irqsave();
+// raw_spin_lock_irqsave(&curr->lock, flags);
+
+    // TODO: when add the real smp function, we nned to add the move
+// evl_prepare_timed_wait(&curr->ptimer, clock, evl_thread_rq(curr));
+
+   // TODO: add this function 
+// if (timeout_infinite(idate))
+//     idate = evl_abs_timeout(&curr->ptimer, period);
+
+    timer::rros_start_timer(timer.clone(), idate, period);
+// evl_start_timer(&curr->ptimer, idate, period);
+
+    raw_spin_unlock_irqrestore(flags);
+// raw_spin_unlock_irqrestore(&curr->lock, flags);
+
+// return ret;
+// }
+// EXPORT_SYMBOL_GPL(evl_set_period);
+}
+
+pub fn rros_wait_period() {
+    let curr = rros_current();
+    let thread = unsafe{Arc::from_raw(curr as *mut SpinLock<rros_thread>)};
+    unsafe{Arc::increment_strong_count(curr);}
+// int evl_wait_period(unsigned long *overruns_r)
+// {
+// 	unsigned long overruns, flags;
+// 	struct evl_thread *curr;
+// 	struct evl_clock *clock;
+// 	ktime_t now;
+
+// 	curr = evl_current();
+    // TODO: add the evl error handling function
+// 	if (unlikely(!evl_timer_is_running(&curr->ptimer)))
+// 		return -EAGAIN;
+
+// 	trace_evl_thread_wait_period(curr);
+
+    let flags = unsafe{rust_helper_hard_local_irq_save()};
+// 	flags = hard_local_irq_save();
+
+    let clock = unsafe{(*(*thread.locked_data().get()).ptimer.as_ref().unwrap().locked_data().get()).get_clock()};
+// 	clock = curr->ptimer.clock;
+    let now = unsafe{rros_read_clock(&mut *clock as &mut clock::RrosClock)};
+// 	now = evl_read_clock(clock);
+    let timer = unsafe{(*thread.locked_data().get()).ptimer.as_ref().unwrap()};
+    if (now < timer::rros_get_timer_next_date(timer.clone())) {
+// 	if (likely(now < evl_get_timer_next_date(&curr->ptimer))) {
+        unsafe{evl_sleep_on(RROS_INFINITE, timeout::rros_tmode::RROS_REL, &mut *clock as &clock::RrosClock, 0 as *mut RrosWaitChannel);    }
+// 		evl_sleep_on(EVL_INFINITE, EVL_REL, clock, NULL); /* T_WAIT */
+        unsafe{rust_helper_hard_local_irq_restore(flags)};
+// 		hard_local_irq_restore(flags);
+        unsafe{sched::rros_schedule();}
+// 		evl_schedule();
+
+        // TODO: add the evl error handling function
+// 		if (unlikely(curr->info & T_BREAK))
+// 			return -EINTR;
+    } else {
+        unsafe{rust_helper_hard_local_irq_restore(flags)};
+    }
+// 	} else
+// 		hard_local_irq_restore(flags);
+
+    // TODO: overruns is zero for this situation. 
+    // let overruns = rros_get_timer_overruns(timer.clone());
+// 	overruns = evl_get_timer_overruns(&curr->ptimer);
+// 	if (overruns) {
+// 		if (likely(overruns_r != NULL))
+// 			*overruns_r = overruns;
+// 		trace_evl_thread_missed_period(curr);
+// 		return -ETIMEDOUT;
+// 	}
+
+// 	return 0;
+// }
+// EXPORT_SYMBOL_GPL(evl_wait_period);
+}
\ No newline at end of file
diff --git a/kernel/rros/thread_test.rs b/kernel/rros/thread_test.rs
index 7036692cb..a1d68631f 100644
--- a/kernel/rros/thread_test.rs
+++ b/kernel/rros/thread_test.rs
@@ -1,116 +1,184 @@
 use crate::{
-    thread, timer, clock,tick, 
+    thread::{self, rros_sleep, KthreadRunner}, timer, clock,tick, 
     sched::{self, this_rros_rq}
 };
-use kernel::{bindings, prelude::*, c_str, spinlock_init, sync::{SpinLock, Lock, Guard}, c_types, };
+use alloc::alloc_rros::*;
+use alloc::alloc::*;
+use alloc::alloc_rros::*;
+use alloc::alloc::*;
 use alloc::rc::Rc;
-use core::cell::RefCell;
-
-struct KthreadRunner {
-    pub kthread: thread::RrosKthread
-    // struct evl_kthread kthread;
-	// struct evl_flag barrier;
-	// ktime_t start_time;
-	// struct latmus_runner runner;
-}
-
+use core::{cell::RefCell, mem::size_of};
+use kernel::{
+    bindings, c_str, c_types::{self, c_void},
+    prelude::*,
+    spinlock_init,
+    sync::{Guard, Lock, SpinLock}, vmalloc,
+};
 
+// struct KthreadRunner {
+//     pub kthread: thread::RrosKthread, // struct evl_kthread kthread;
+//                                       // struct evl_flag barrier;
+//                                       // ktime_t start_time;
+//                                       // struct latmus_runner runner;
+// }
 
-impl KthreadRunner {
-    pub fn new(kfn: fn()) -> Self{
-        // let mut thread = unsafe{SpinLock::new(thread::RrosKthread::new(kfn))};
-        // let pinned = unsafe{Pin::new_unchecked(&mut thread)};
+// impl KthreadRunner {
+//     pub fn new(kfn:Box<dyn FnOnce()>) -> Self {
+//         // let mut thread = unsafe{SpinLock::new(thread::RrosKthread::new(kfn))};
+//         // let pinned = unsafe{Pin::new_unchecked(&mut thread)};
 
-        // unsafe{Self { kthread: Arc::try_new(thread).unwrap()}}
-        unsafe{Self { kthread: thread::RrosKthread::new(kfn)}}
-    }
-}
+//         // unsafe{Self { kthread: Arc::try_new(thread).unwrap()}}
+//         unsafe {
+//             Self {
+//                 kthread: thread::RrosKthread::new(Some(kfn)),
+//             }
+//         }
+//     }
+// }
 
-// static mut kthread_runner_1:Option<> = ;
-static mut kthread_runner_1:Option<KthreadRunner> = None;
-static mut kthread_runner_2:Option<KthreadRunner> = None;
+// // static mut kthread_runner_1:Option<> = ;
+// static mut kthread_runner_1: Option<KthreadRunner> = None;
+// static mut kthread_runner_2: Option<KthreadRunner> = None;
+static mut kthread_runner_1: KthreadRunner = KthreadRunner::new_empty();
+static mut kthread_runner_2: KthreadRunner = KthreadRunner::new_empty();
 
 pub fn test_thread_context_switch() {
     let rq = this_rros_rq();
-    
-    // let 
-    
-    let rq_len = unsafe{(*rq).fifo.runnable.head.clone().unwrap().len()};
+
+    // // let
+
+    let rq_len = unsafe { (*rq).fifo.runnable.head.clone().unwrap().len() };
+
+    unsafe{
+        kthread_runner_1.init(Box::try_new(kfn_1).unwrap());
+        kthread_runner_2.init(Box::try_new(kfn_2).unwrap());
+        kthread_runner_1.run(c_str!("kthread_1"), 10);
+        kthread_runner_2.run(c_str!("kthread_2"), 90);
+    }
     // pr_info!("the init xxx length0 {}", rq_len);
 
     // unsafe{Arc::try_new(SpinLock::new(rros_thread::new().unwrap())).unwrap()},
-    unsafe{
-        kthread_runner_1 = Some(KthreadRunner::new(kfn_1));
-        let mut thread = SpinLock::new(sched::rros_thread::new().unwrap());
-        let pinned = Pin::new_unchecked(&mut thread);
-        spinlock_init!(pinned, "test_threads1");
-        kthread_runner_1.as_mut().unwrap().kthread.thread =  Some(Arc::try_new(thread).unwrap());
-
-        let mut r = SpinLock::new(timer::RrosTimer::new(1));
-        let pinned_r =  Pin::new_unchecked(&mut r);
-        spinlock_init!(pinned_r, "rtimer_1");
-        
-        let mut p = SpinLock::new(timer::RrosTimer::new(1));
-        let pinned_p =  Pin::new_unchecked(&mut p);
-        spinlock_init!(pinned_p, "ptimer_1");
+    // unsafe {
+    //     // let x = "hello";
+    //     // let y = "world";
+    //     // let kfn_3 = move||{
+    //     //     loop{
+    //     //         pr_info!("{}",x);
+    //     //         thread::rros_sleep(1000000000);
+    //     //         pr_info!("{}",y);
+    //     //         thread::rros_sleep(1000000000);
+    //     //     }
+    //     // };
+    //     kthread_runner_1 = Some(KthreadRunner::new(Box::try_new(kfn_1).unwrap()));
+    //     let mut thread = SpinLock::new(sched::rros_thread::new().unwrap());
+    //     let pinned = Pin::new_unchecked(&mut thread);
+    //     spinlock_init!(pinned, "test_threads1");
+    //     kthread_runner_1.as_mut().unwrap().kthread.thread = Some(Arc::try_new(thread).unwrap());
 
-        kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
-        kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
-        
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
-        // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
-        // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
-
-        let temp:&mut thread::RrosKthread;
-        temp = &mut kthread_runner_1.as_mut().unwrap().kthread;
-        thread::rros_run_kthread(temp, c_str!("hongyu1")) ;
-        // thread::rros_run_kthread(&mut kthread_runner_1.as_mut().unwrap().kthread, c_str!("hongyu1")) ;
-        
-    }
-    
-    let rq_len = unsafe{(*rq).fifo.runnable.head.clone().unwrap().len()};
-    // pr_info!("length1 {}", rq_len);
+    //     let mut r = SpinLock::new(timer::RrosTimer::new(1));
+    //     let pinned_r = Pin::new_unchecked(&mut r);
+    //     spinlock_init!(pinned_r, "rtimer_1");
 
-    // // let 
-    unsafe{
-        kthread_runner_2 = Some( KthreadRunner::new(kfn_2));
-        let mut thread = SpinLock::new(sched::rros_thread::new().unwrap());
-        let pinned = Pin::new_unchecked(&mut thread);
-        spinlock_init!(pinned, "test_threads2");
-        kthread_runner_2.as_mut().unwrap().kthread.thread =  Some(Arc::try_new(thread).unwrap());
-
-        let mut r = SpinLock::new(timer::RrosTimer::new(1));
-        let pinned_r =  Pin::new_unchecked(&mut r);
-        spinlock_init!(pinned_r, "rtimer_2");
-        
-        let mut p = SpinLock::new(timer::RrosTimer::new(1));
-        let pinned_p =  Pin::new_unchecked(&mut p);
-        spinlock_init!(pinned_p, "ptimer_2");
+    //     let mut p = SpinLock::new(timer::RrosTimer::new(1));
+    //     let pinned_p = Pin::new_unchecked(&mut p);
+    //     spinlock_init!(pinned_p, "ptimer_1");
 
-        kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
-        kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
+    //     kthread_runner_1
+    //         .as_mut()
+    //         .unwrap()
+    //         .kthread
+    //         .thread
+    //         .as_mut()
+    //         .unwrap()
+    //         .lock()
+    //         .rtimer = Some(Arc::try_new(r).unwrap());
+    //     kthread_runner_1
+    //         .as_mut()
+    //         .unwrap()
+    //         .kthread
+    //         .thread
+    //         .as_mut()
+    //         .unwrap()
+    //         .lock()
+    //         .ptimer = Some(Arc::try_new(p).unwrap());
 
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
-        // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
-        // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
-        
-        pr_info!("kfn_2 rros_run_kthread in");
-        thread::rros_run_kthread(&mut kthread_runner_2.as_mut().unwrap().kthread, c_str!("hongyu2"));
-        pr_info!("kfn_2 rros_run_kthread out");
-    }
+    //     // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
+    //     // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
+    //     // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
+
+    //     let temp: &mut thread::RrosKthread;
+    //     temp = &mut kthread_runner_1.as_mut().unwrap().kthread;
+    //     thread::rros_run_kthread(temp, c_str!("hongyu1"));
+    //     // thread::rros_run_kthread(&mut kthread_runner_1.as_mut().unwrap().kthread, c_str!("hongyu1")) ;
+    // }
+
+    // let rq_len = unsafe { (*rq).fifo.runnable.head.clone().unwrap().len() };
+    // // pr_info!("length1 {}", rq_len);
+
+    // // // let
+    // unsafe {
+    //     kthread_runner_2 = Some(KthreadRunner::new(Box::try_new(kfn_2).unwrap()));
+    //     let mut thread = SpinLock::new(sched::rros_thread::new().unwrap());
+    //     let pinned: Pin<&mut SpinLock<sched::rros_thread>> = Pin::new_unchecked(&mut thread);
+    //     spinlock_init!(pinned, "test_threads2");
+    //     kthread_runner_2.as_mut().unwrap().kthread.thread = Some(Arc::try_new(thread).unwrap());
+
+    //     let mut r = SpinLock::new(timer::RrosTimer::new(1));
+    //     let pinned_r = Pin::new_unchecked(&mut r);
+    //     spinlock_init!(pinned_r, "rtimer_2");
+
+    //     let mut p = SpinLock::new(timer::RrosTimer::new(1));
+    //     let pinned_p = Pin::new_unchecked(&mut p);
+    //     spinlock_init!(pinned_p, "ptimer_2");
 
-    let rq_len = unsafe{(*rq).fifo.runnable.head.clone().unwrap().len()};
+    //     kthread_runner_2
+    //         .as_mut()
+    //         .unwrap()
+    //         .kthread
+    //         .thread
+    //         .as_mut()
+    //         .unwrap()
+    //         .lock()
+    //         .rtimer = Some(Arc::try_new(r).unwrap());
+    //     kthread_runner_2
+    //         .as_mut()
+    //         .unwrap()
+    //         .kthread
+    //         .thread
+    //         .as_mut()
+    //         .unwrap()
+    //         .lock()
+    //         .ptimer = Some(Arc::try_new(p).unwrap());
 
-    pr_info!("length2 {}", rq_len);
+    //     // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
+    //     // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
+    //     // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
+
+    //     pr_info!("kfn_2 rros_run_kthread in");
+    //     thread::rros_run_kthread(
+    //         &mut kthread_runner_2.as_mut().unwrap().kthread,
+    //         c_str!("hongyu2"),
+    //     );
+    //     pr_info!("kfn_2 rros_run_kthread out");
+    // }
+
+    // let rq_len = unsafe { (*rq).fifo.runnable.head.clone().unwrap().len() };
+
+    // pr_info!("length2 {}", rq_len);
 
     // let mut a = 0 as u64;
     // while 1==1 {
     //     a += 1;
-        // if a == 10000000000{
-        //     break;
-        // }
-        // let a = 1;
-        // pr_info!("good");
+    // if a == 10000000000{
+    //     break;
+    // }
+    // let a = 1;
+    // pr_info!("good");
+    // if a == 10000000000{
+    //     break;
+    // }
+    // let a = 1;
+    // pr_info!("good");
     // }
     // let mut RrosKthread1 = kthread::RrosKthread::new(fn1);
     // let mut RrosKthread2 = kthread::RrosKthread::new(fn2);
@@ -141,7 +209,6 @@ pub fn test_thread_context_switch() {
 //     unsafe {
 //         bindings::dovetail_init_altsched(
 
-
 //             &mut (*kthread).altsched as *mut bindings::dovetail_altsched_context,
 //         )
 //     };
@@ -156,49 +223,85 @@ pub fn test_thread_context_switch() {
 // }
 
 
+fn kfn_1() {
+    thread::rros_sleep(100000000);
+    for i in 0..20 {
+        pr_emerg!("hello! from rros~~~~~~~~~~~~");
+    }
+    // for t in 0..2000000 {
+        // thread::rros_sleep(1000000000);
+        // pr_info!("mutex test in~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // pr_info!("test_mutex rros_current address is {:p}",rros_current());
+        // let mut kmutex = RrosKMutex::new();
+        // let mut kmutex = &mut kmutex as *mut RrosKMutex;
+        // let mut mutex = RrosMutex::new();
+        // unsafe{(*kmutex).mutex = &mut mutex as *mut RrosMutex};
+        // rros_init_kmutex(kmutex);
+        // pr_info!("init ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // rros_lock_kmutex(kmutex);
+        // pr_info!("lock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // unsafe{aa+=1};
+        // pr_info!("in fn 1 t is {}, a is {}",t,a);
+        // rros_unlock_kmutex(kmutex);
+        // pr_info!("unlock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
 
+    // }
+    // pr_info!("fn 1, a is {}",aa);
+        // unsafe { pr_info!("kfn1: time end is {}", clock::RROS_REALTIME_CLOCK.read())};
+        // pr_info!("kfn1: waste time is {}",y-x);
+        // pr_emerg!("hello! from rros~~~~~~~~~~~~");
+        
+    // }
 
-pub fn kfn_1() {
-    // while 1==1 {
-        // unsafe{rust_helper_hard_local_irq_enable()};
+        // pr_info!("mutex test in~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // pr_info!("test_mutex rros_current address is {:p}",rros_current());
+        // let mut kmutex = RrosKMutex::new();
+        // let mut kmutex = &mut kmutex as *mut RrosKMutex;
+        // let mut mutex = RrosMutex::new();
+        // unsafe{(*kmutex).mutex = &mut mutex as *mut RrosMutex};
+        // rros_init_kmutex(kmutex);
+        // pr_info!("init ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // rros_lock_kmutex(kmutex);
+        // pr_info!("lock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+        // unsafe{aa+=1};
+        // pr_info!("in fn 1 t is {}, a is {}",t,a);
+        // rros_unlock_kmutex(kmutex);
+        // pr_info!("unlock ok~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
 
-        thread::rros_sleep(1000000000);//sleep有大问题 暂时不用了
-        
-        
-        // unsafe{
-        //     let mut tmb = timer::rros_this_cpu_timers(&clock::RROS_MONO_CLOCK);
-        //     if (*tmb).q.is_empty() == true {
-        //         // tick
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        // //         // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
-        //     }
-        // //     // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
-        // }
-        // // unsafe{
-        //     let this_rq = this_rros_rq();
-        //     tick::rros_notify_proxy_tick(this_rq);
-        // }
-        // pr_info!("hello! from rros~~~~~~~~~~~~");
-        pr_emerg!("hello! from rros~~~~~~~~~~~~");
+    // }
+    // pr_info!("fn 1, a is {}",aa);
+        // unsafe { pr_info!("kfn1: time end is {}", clock::RROS_REALTIME_CLOCK.read())};
+        // pr_info!("kfn1: waste time is {}",y-x);
+        // pr_emerg!("hello! from rros~~~~~~~~~~~~");
         
     // }
 }
 
+// static mut aa:i32 = 0;
+// static mut aa:i32 = 0;
 pub fn kfn_2() {
-    // while 1==1 {
-        thread::rros_sleep(1000000000);
-        pr_info!("world! from rros~~~~~~~~~~~~");
+    thread::rros_sleep(100000000);
+//     for t in 0..1000000 {
+//         // thread::rros_sleep(1000000000);
+//         // pr_info!("world! from rros~~~~~~~~~~~~");
         
-    // }
-}
\ No newline at end of file
+//         unsafe{a+=1};
+//         // pr_info!("in fn 2 t is {}, a is {}",t,a);
+        
+//     }
+//     pr_info!("fn 2, a is {}",a);
+// }
+    for i in 0..20 {
+    // thread::rros_sleep(1000000000);
+        // let x = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        // pr_info!("kfn2: x is {}",x);
+        // // for i in 1..100 {
+        //     let a = Arc::try_new(1000);
+        // // }
+        // let y = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        // pr_info!("kfn2: y is {}",y);
+        // pr_info!("kfn_2: waste time is {}",y-x);
+        pr_info!("world! from rros~~~~~~~~~~~~");
+    }
+
+}
diff --git a/kernel/rros/tick.rs b/kernel/rros/tick.rs
index 4a4087f30..cb2b14891 100644
--- a/kernel/rros/tick.rs
+++ b/kernel/rros/tick.rs
@@ -1,104 +1,114 @@
 use kernel::{
-    bindings,
-    c_types,clockchips,
-    percpu::alloc_per_cpu,
-    prelude::*,
-    double_linked_list::*,
-    sync::SpinLock, 
-    irq_pipeline::*,
-    percpu_defs,
-    container_of,
-    cpumask,
-    str::CStr,
-    ktime::*,
-    sync::Lock,
+    bindings, c_types, clockchips, container_of, cpumask, double_linked_list::*, irq_pipeline::*,
+    ktime::*, percpu::alloc_per_cpu, percpu_defs, prelude::*, str::CStr, sync::Lock,
+    sync::SpinLock,
 };
 
+use crate::clock::RROS_MONO_CLOCK;
+use crate::SCHED_FLAG;
 use crate::{
-    clock::*,
-    sched::*,
-    timer::*,
-    thread::*,
-    RROS_MACHINE_CPUDATA,
-    timeout::*,
-    lock::*,
+    clock::*, lock::*, sched::*, thread::*, timeout::*, timer::*, RROS_MACHINE_CPUDATA,
     RROS_OOB_CPUS,
 };
 use core::cmp;
-use crate::SCHED_FLAG;
-use core::sync::atomic::{AtomicU8, Ordering};
-use crate::clock::RROS_MONO_CLOCK;
 use core::ops::DerefMut;
+use core::sync::atomic::{AtomicU8, Ordering};
 
-fn __this_cpu_write(pcp:*mut bindings::clock_proxy_device, val:*mut bindings::clock_proxy_device){
-    unsafe{rust_helper__this_cpu_write(pcp, val);}
+fn __this_cpu_write(
+    pcp: *mut bindings::clock_proxy_device,
+    val: *mut bindings::clock_proxy_device,
+) {
+    unsafe {
+        rust_helper__this_cpu_write(pcp, val);
+    }
 }
 
-fn __this_cpu_read(pcp:*mut bindings::clock_proxy_device,) -> *mut bindings::clock_proxy_device{
-    unsafe{rust_helper__this_cpu_read(pcp)}
+fn __this_cpu_read(pcp: *mut bindings::clock_proxy_device) -> *mut bindings::clock_proxy_device {
+    unsafe { rust_helper__this_cpu_read(pcp) }
 }
 
-extern "C"{
-    fn rust_helper__this_cpu_write(pcp:*mut bindings::clock_proxy_device, val:*mut bindings::clock_proxy_device);
-    fn rust_helper__this_cpu_read(pcp:*mut bindings::clock_proxy_device) -> *mut bindings::clock_proxy_device;
-    fn rust_helper_proxy_set_next_ktime(expires: ktime_t, dev: *mut  bindings::clock_event_device) -> c_types::c_int;
-    fn rust_helper_proxy_set(expires: ktime_t, dev: *mut  bindings::clock_event_device) -> c_types::c_int;
+extern "C" {
+    fn rust_helper__this_cpu_write(
+        pcp: *mut bindings::clock_proxy_device,
+        val: *mut bindings::clock_proxy_device,
+    );
+    fn rust_helper__this_cpu_read(
+        pcp: *mut bindings::clock_proxy_device,
+    ) -> *mut bindings::clock_proxy_device;
+    fn rust_helper_proxy_set_next_ktime(
+        expires: ktime_t,
+        dev: *mut bindings::clock_event_device,
+    ) -> c_types::c_int;
+    fn rust_helper_proxy_set(
+        expires: ktime_t,
+        dev: *mut bindings::clock_event_device,
+    ) -> c_types::c_int;
     fn rust_helper_hard_local_irq_save() -> c_types::c_ulong;
     fn rust_helper_hard_local_irq_restore(flags: c_types::c_ulong);
     fn rust_helper_tick_notify_proxy();
     fn rust_helper_IRQF_OOB() -> c_types::c_ulong;
 }
 
+static mut PROXY_DEVICE: *mut *mut bindings::clock_proxy_device =
+    0 as *mut *mut bindings::clock_proxy_device;
 
-static mut PROXY_DEVICE :*mut *mut bindings::clock_proxy_device = 0 as *mut *mut bindings::clock_proxy_device;
-    
-use core::mem::{size_of,align_of};
+use core::mem::{align_of, size_of};
 use core::ptr::null;
 
-pub const CLOCK_EVT_FEAT_KTIME:u32 = 0x000004;
+pub const CLOCK_EVT_FEAT_KTIME: u32 = 0x000004;
 type ktime_t = i64;
 
 pub fn rros_program_local_tick(clock: *mut RrosClock) {
-    unsafe{(*(*clock).get_master()).program_local_shot()};
+    unsafe { (*(*clock).get_master()).program_local_shot() };
 }
 
 pub fn rros_program_remote_tick(clock: *mut RrosClock, rq: *mut RrosRq) {
     #[cfg(CONFIG_SMP)]
-    unsafe{(*(*clock).get_master()).program_remote_shot(rq)};
+    unsafe {
+        (*(*clock).get_master()).program_remote_shot(rq)
+    };
 }
 
 pub fn rros_notify_proxy_tick(rq: *mut RrosRq) {
-    unsafe{(*rq).local_flags &= !RQ_TPROXY};
-    unsafe{rust_helper_tick_notify_proxy()};
+    unsafe { (*rq).local_flags &= !RQ_TPROXY };
+    unsafe { rust_helper_tick_notify_proxy() };
 }
 
-
 //未测试
-pub unsafe extern "C" fn proxy_set_next_ktime(expires: KtimeT, arg1: *mut bindings::clock_event_device) -> i32{
+pub unsafe extern "C" fn proxy_set_next_ktime(
+    expires: KtimeT,
+    arg1: *mut bindings::clock_event_device,
+) -> i32 {
     //pr_info!("proxy_set_next_ktime: in");
     let delta = ktime_sub(expires, ktime_get());
-    let flags = unsafe{rust_helper_hard_local_irq_save()};
+    let flags = unsafe { rust_helper_hard_local_irq_save() };
 
     let rq = this_rros_rq();
-    
+
     // let inband_timer = unsafe{(*rq).get_inband_timer()};
     //unsafe{rros_program_proxy_tick(&RROS_MONO_CLOCK)};
-    unsafe{rros_start_timer((*rq).get_inband_timer(),
-                rros_abs_timeout((*rq).get_inband_timer(), delta),
-                RROS_INFINITE)};
-    unsafe{rust_helper_hard_local_irq_restore(flags)};
+    unsafe {
+        rros_start_timer(
+            (*rq).get_inband_timer(),
+            rros_abs_timeout((*rq).get_inband_timer(), delta),
+            RROS_INFINITE,
+        )
+    };
+    unsafe { rust_helper_hard_local_irq_restore(flags) };
     // pr_info!("proxy_set_next_ktime: end");
     return 0;
 }
 
 //未测试
-unsafe extern "C" fn proxy_set_oneshot_stopped(proxy_dev: *mut bindings::clock_event_device) -> c_types::c_int {
+unsafe extern "C" fn proxy_set_oneshot_stopped(
+    proxy_dev: *mut bindings::clock_event_device,
+) -> c_types::c_int {
     pr_info!("proxy_set_oneshot_stopped: in");
     let dev = container_of!(proxy_dev, bindings::clock_proxy_device, proxy_device);
 
-    let flags = unsafe{rust_helper_hard_local_irq_save()};
+    let flags = unsafe { rust_helper_hard_local_irq_save() };
     let rq = this_rros_rq();
-    unsafe{
+    unsafe {
         rros_stop_timer((*rq).get_inband_timer());
         (*rq).local_flags |= RQ_TSTOPPED;
         if (*rq).local_flags & RQ_IDLE != 0 {
@@ -106,122 +116,139 @@ unsafe extern "C" fn proxy_set_oneshot_stopped(proxy_dev: *mut bindings::clock_e
             (*real_dev).set_state_oneshot_stopped.unwrap()(real_dev);
         }
     }
-    unsafe{rust_helper_hard_local_irq_restore(flags)};
+    unsafe { rust_helper_hard_local_irq_restore(flags) };
     pr_info!("proxy_set_oneshot_stopped: end");
     return 0;
 }
 
 #[cfg(CONFIG_SMP)]
-unsafe extern "C" fn clock_ipi_handler(irq: c_types::c_int, dev_id: *mut c_types::c_void) -> bindings::irqreturn_t {
+unsafe extern "C" fn clock_ipi_handler(
+    irq: c_types::c_int,
+    dev_id: *mut c_types::c_void,
+) -> bindings::irqreturn_t {
     pr_info!("god nn");
-    unsafe{rros_core_tick(0 as *mut bindings::clock_event_device)};
+    unsafe { rros_core_tick(0 as *mut bindings::clock_event_device) };
     return bindings::irqreturn_IRQ_HANDLED;
 }
 
+/// # Enable the tick in the rros 
+/// 
+/// # Requirements
+///     - Enable the proxy device to take core of the physical device
+///
+/// # Arguments
+///     - None
+///     
+/// # Return
+///     - Ok on success enabling the tick
+///     - Err on failure
+///  
+/// # Tips: 
+///     - The `tick_install_proxy` and `setup_proxy` is used in the implementation. The `tick_instakk_proxy` is defined in the linux, which should be used by using `bindings::`.
+///     - You may find RROS_OOB_CPUS useful.
+///     - Read the code of `tick_install_proxy` and `setup_proxy` to get more information. The comment of the `tick_install_proxy` is helpful.
 pub fn rros_enable_tick() -> Result<usize> {
+    unsafe {
+        PROXY_DEVICE = alloc_per_cpu(
+            size_of::<*mut bindings::clock_proxy_device>() as usize,
+            align_of::<*mut bindings::clock_proxy_device>() as usize,
+        ) as *mut *mut bindings::clock_proxy_device;
+        if PROXY_DEVICE == 0 as *mut *mut bindings::clock_proxy_device {
+            return Err(kernel::Error::ENOMEM);
+        }
+        pr_info!("PROXY_DEVICE alloc success");
+    }
+
     pr_info!("rros_enable_tick: in");
     #[cfg(CONFIG_SMP)]
     if cpumask::num_possible_cpus() > 1 {
         pr_info!("rros_enable_tick123");
-        let ret = unsafe{bindings::__request_percpu_irq(
-            irq_get_TIMER_OOB_IPI() as c_types::c_uint,
-            Some(clock_ipi_handler),
-            rust_helper_IRQF_OOB(),
-            CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_REALTIME_DEV\0".as_bytes()).as_char_ptr(),
-            RROS_MACHINE_CPUDATA as *mut c_types::c_void
-        )};
+        let ret = unsafe {
+            bindings::__request_percpu_irq(
+                irq_get_TIMER_OOB_IPI() as c_types::c_uint,
+                Some(clock_ipi_handler),
+                rust_helper_IRQF_OOB(),
+                CStr::from_bytes_with_nul_unchecked("RROS_CLOCK_REALTIME_DEV\0".as_bytes())
+                    .as_char_ptr(),
+                RROS_MACHINE_CPUDATA as *mut c_types::c_void,
+            )
+        };
         if ret != 0 {
             return Err(kernel::Error::ENOMEM);
         }
     }
-    
-    
-    unsafe{
-        PROXY_DEVICE =
-            alloc_per_cpu(size_of::<*mut bindings::clock_proxy_device>() as usize, align_of::<*mut bindings::clock_proxy_device>() as usize) as *mut *mut bindings::clock_proxy_device; 
-        if PROXY_DEVICE == 0 as *mut *mut bindings::clock_proxy_device {
-            return Err(kernel::Error::ENOMEM);
-        }
-        pr_info!("PROXY_DEVICE alloc success");
-    }
 
-    let ret = unsafe{bindings::tick_install_proxy(Some(setup_proxy), RROS_OOB_CPUS.as_cpumas_ptr() as *const bindings::cpumask_t)};
+    // TODO: your code here
+
+    // end of your code
+
     pr_info!("enable tick success!");
-	// if (ret && IS_ENABLED(CONFIG_SMP) && num_possible_cpus() > 1) {
-	// 	free_percpu_irq(TIMER_OOB_IPI, &evl_machine_cpudata);
-	// 	return ret;
-	// }
+    // if (ret && IS_ENABLED(CONFIG_SMP) && num_possible_cpus() > 1) {
+    // 	free_percpu_irq(TIMER_OOB_IPI, &evl_machine_cpudata);
+    // 	return ret;
+    // }
 
     Ok(0)
 }
 
-// 新的setup
-unsafe extern "C" fn setup_proxy(_dev: *mut bindings::clock_proxy_device) {
-    let dev;
-    match clockchips::Clock_Proxy_Device::new(_dev){
-        Ok(v) => dev = v,
-        Err(e) => {
-            pr_warn!("dev new error!"); 
-            return;
-        }
-    }
-
-    let real_dev;
-    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_real_device()){
-        Ok(v) => real_dev = v,
-        Err(e) => {
-            pr_warn!("1setup real ced new error!");
-            return;
-        }
-    }
-
-    let proxy_dev;
-    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_proxy_device()){
-        Ok(v) => proxy_dev = v,
-        Err(e) => {
-            pr_warn!("1setup proxy ced new error!");
-            return;
-        }
-    }
-
-    dev.set_handle_oob_event(rros_core_tick);
-    let mut temp = proxy_dev.get_features();
-    temp |= CLOCK_EVT_FEAT_KTIME;
-    proxy_dev.set_features(temp);
-
-	proxy_dev.set_set_next_ktime(proxy_set_next_ktime);
-	if proxy_dev.get_set_state_oneshot_stopped().is_some(){
-		proxy_dev.set_set_state_oneshot_stopped(proxy_set_oneshot_stopped);
-    }
-
-    unsafe{
-        let mut tmp_dev = percpu_defs::per_cpu_ptr(PROXY_DEVICE as *mut u8, 
-            percpu_defs::smp_processor_id()) as *mut *mut bindings::clock_proxy_device;
-        *tmp_dev = dev.get_ptr();
-    } 
-
+/// # Setup the elements of the `clock_proxy_device`
+/// 
+/// # Requirements
+///     - Set the `handle_oob_event` in the `clock_proxy_device` with `rros_core_tick` and `proxy_set_next_ktime` in the proxy `clock_event_device` with `proxy_set_next_ktime`.
+///     - The feature CLOCK_EVT_FEAT_KTIME of proxy clock_event_device should be set.
+///
+/// # Arguments
+///     - `pdev`: the ptr of the `clock_proxy_device`. 
+///     
+/// # Return
+///     - None
+///  
+/// # Tips: 
+///     - `proxy_set_next_ktime` and `rros_core_tick` is used.
+///     - You may find some assosiated functions in the Clock_Proxy_Device and Clock_Event_Device.
+///     - The relationship of related structs: The `clock_proxy_device` contains the proxy `clock_event_device` and the real `clock_event_device`. Rros controls the proxy `clock_event_device` and the real `clock_event_device` is controlled by the linux.
+///     - The `rros_core_tick` is used to handle tick. The `proxy_set_next_ktime` is used to set the next tick. 
+unsafe extern "C" fn setup_proxy(pdev: *mut bindings::clock_proxy_device) {
+    // TODO: If you implement the code below, uncomment this.
+    // let dev;
+    // match clockchips::Clock_Proxy_Device::new(pdev) {
+    //     Ok(v) => dev = v,
+    //     Err(e) => {
+    //         pr_warn!("dev new error!");
+    //         return;
+    //     }
+    // }
+    // TODO: your code here
+ 
+    // end of your code
+    // TODO: If you implement the code above, uncomment this.
+    // unsafe {
+    //     let mut tmp_dev =
+    //         percpu_defs::per_cpu_ptr(PROXY_DEVICE as *mut u8, percpu_defs::smp_processor_id())
+    //             as *mut *mut bindings::clock_proxy_device;
+    //     *tmp_dev = dev.get_ptr();
+    // }
 }
 
-
-
 pub fn rros_program_proxy_tick(clock: &RrosClock) {
     let tmp_dev;
-    unsafe{
-        tmp_dev = *(percpu_defs::per_cpu_ptr(PROXY_DEVICE as *mut u8, 
-            percpu_defs::smp_processor_id()) as *mut *mut bindings::clock_proxy_device);
+    unsafe {
+        tmp_dev =
+            *(percpu_defs::per_cpu_ptr(PROXY_DEVICE as *mut u8, percpu_defs::smp_processor_id())
+                as *mut *mut bindings::clock_proxy_device);
     }
 
     let dev;
-    match clockchips::Clock_Proxy_Device::new(tmp_dev){
+    match clockchips::Clock_Proxy_Device::new(tmp_dev) {
         Ok(v) => dev = v,
         Err(e) => {
-            pr_warn!("cpd new error!"); 
+            pr_warn!("cpd new error!");
             return;
         }
     }
-    
+
     let real_dev;
-    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_real_device()){
+    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_real_device()) {
         Ok(v) => real_dev = v,
         Err(e) => {
             pr_warn!("real ced new error!");
@@ -229,7 +256,7 @@ pub fn rros_program_proxy_tick(clock: &RrosClock) {
         }
     }
     let proxy_device;
-    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_proxy_device()){
+    match clockchips::Clock_Event_Device::from_proxy_device(dev.get_proxy_device()) {
         Ok(v) => proxy_device = v,
         Err(e) => {
             pr_warn!("proxy ced new error!");
@@ -237,27 +264,26 @@ pub fn rros_program_proxy_tick(clock: &RrosClock) {
         }
     }
     let mut this_rq = this_rros_rq();
-    unsafe{
+    unsafe {
         if (*this_rq).local_flags & RQ_TIMER != 0x0 {
             return;
         }
     }
-    
 
     let mut tmb = rros_this_cpu_timers(&clock);
-    unsafe{
+    unsafe {
         if (*tmb).q.is_empty() {
             (*this_rq).add_local_flags(RQ_IDLE);
             return;
         }
-        (*this_rq).change_local_flags(!(RQ_TDEFER|RQ_IDLE|RQ_TSTOPPED));
+        (*this_rq).change_local_flags(!(RQ_TDEFER | RQ_IDLE | RQ_TSTOPPED));
     }
-   
-    let mut timer = unsafe{(*tmb).q.get_head().unwrap().value.clone()};
-    let inband_timer_addr = unsafe{(*this_rq).get_inband_timer().locked_data().get()};
-    let timer_addr = unsafe{timer.locked_data().get()};
+
+    let mut timer = unsafe { (*tmb).q.get_head().unwrap().value.clone() };
+    let inband_timer_addr = unsafe { (*this_rq).get_inband_timer().locked_data().get() };
+    let timer_addr = unsafe { timer.locked_data().get() };
     if (timer_addr == inband_timer_addr) {
-        unsafe{
+        unsafe {
             let state = (*(*this_rq).get_curr().locked_data().get()).state;
             if rros_need_resched(this_rq) || state & T_ROOT == 0x0 {
                 if (*tmb).q.len() > 1 {
@@ -266,27 +292,27 @@ pub fn rros_program_proxy_tick(clock: &RrosClock) {
                 }
             }
         }
-	}
-    let t = unsafe{(*timer.locked_data().get()).get_date()};
-	let mut delta = ktime_to_ns(ktime_sub(t, clock.read()));
-    if real_dev.get_features() as u32 & CLOCK_EVT_FEAT_KTIME != 0{
+    }
+    let t = unsafe { (*timer.locked_data().get()).get_date() };
+    let mut delta = ktime_to_ns(ktime_sub(t, clock.read()));
+    if real_dev.get_features() as u32 & CLOCK_EVT_FEAT_KTIME != 0 {
         real_dev.set_next_ktime(t, real_dev.get_ptr());
-    }else {
-        if delta <= 0{
+    } else {
+        if delta <= 0 {
             delta = real_dev.get_min_delta_ns() as i64;
         } else {
-			delta = cmp::min(delta, real_dev.get_max_delta_ns() as i64);
-			delta = cmp::max(delta, real_dev.get_min_delta_ns() as i64);
-		}
-		let cycles = (delta as u64 * (real_dev.get_mult() as u64)) >> real_dev.get_shift();
-		let ret = real_dev.set_next_event(cycles, dev.get_real_device());
-		if ret != 0 {
-			real_dev.set_next_event(real_dev.get_min_delta_ticks(), dev.get_real_device());
-		}
+            delta = cmp::min(delta, real_dev.get_max_delta_ns() as i64);
+            delta = cmp::max(delta, real_dev.get_min_delta_ns() as i64);
+        }
+        let cycles = (delta as u64 * (real_dev.get_mult() as u64)) >> real_dev.get_shift();
+        let ret = real_dev.set_next_event(cycles, dev.get_real_device());
+        if ret != 0 {
+            real_dev.set_next_event(real_dev.get_min_delta_ticks(), dev.get_real_device());
+        }
     }
 }
 
 #[cfg(CONFIG_SMP)]
 pub fn rros_send_timer_ipi(clock: &RrosClock, rq: *mut RrosRq) {
     irq_send_oob_ipi(irq_get_TIMER_OOB_IPI(), cpumask_of(rros_rq_cpu(rq)));
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/timeout.rs b/kernel/rros/timeout.rs
index a1927f9e6..0332d4ca8 100644
--- a/kernel/rros/timeout.rs
+++ b/kernel/rros/timeout.rs
@@ -1,5 +1,5 @@
-use kernel::prelude::*;
 use kernel::ktime::*;
+use kernel::prelude::*;
 pub const RROS_INFINITE: KtimeT = 0;
 pub const RROS_NONBLOCK: KtimeT = i64::MAX;
 
@@ -15,8 +15,8 @@ pub fn timeout_valid(kt: KtimeT) -> bool {
     kt > 0
 }
 
-#[derive(Clone, PartialEq, Eq, Debug)]
+#[derive(Clone, PartialEq, Eq, Debug,Copy)]
 pub enum rros_tmode {
     RROS_REL,
     RROS_ABS,
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/timer.rs b/kernel/rros/timer.rs
index 50d34d957..2348bcd14 100644
--- a/kernel/rros/timer.rs
+++ b/kernel/rros/timer.rs
@@ -1,40 +1,36 @@
 #![allow(warnings, unused)]
-use crate::{
-    sched::*,
-    clock::*,
-    tick::*, 
-    timeout::*,
-    stat::*,
-    lock::*,
-};
+use crate::{clock::*, lock::*, sched::*, stat::*, tick::*, timeout::*};
 
-use kernel::{str::CStr, c_str, prelude::*, sync::SpinLock, bindings, spinlock_init, 
-    percpu_defs, double_linked_list::*, ktime::*, sync::Lock,
-};
 use core::ops::DerefMut;
+use kernel::{
+    bindings, c_str, double_linked_list::*, ktime::*, percpu_defs, prelude::*, spinlock_init,
+    str::CStr, sync::Lock, sync::SpinLock,
+};
 pub type RrosRq = rros_rq;
 
 /* Timer status */
 pub const RROS_TIMER_DEQUEUED: i32 = 0x00000001;
-pub const RROS_TIMER_KILLED:   i32 = 0x00000002;
+pub const RROS_TIMER_KILLED: i32 = 0x00000002;
 pub const RROS_TIMER_PERIODIC: i32 = 0x00000004;
-pub const RROS_TIMER_FIRED:    i32 = 0x00000010;
-pub const RROS_TIMER_RUNNING:  i32 = 0x00000020;
+pub const RROS_TIMER_FIRED: i32 = 0x00000010;
+pub const RROS_TIMER_RUNNING: i32 = 0x00000020;
 pub const RROS_TIMER_KGRAVITY: i32 = 0x00000040;
 pub const RROS_TIMER_UGRAVITY: i32 = 0x00000080;
 pub const RROS_TIMER_IGRAVITY: i32 = 0; /* most conservative */
-pub const RROS_TIMER_GRAVITY_MASK:i32 =	(RROS_TIMER_KGRAVITY|RROS_TIMER_UGRAVITY);
-pub const RROS_TIMER_INIT_MASK:i32 = RROS_TIMER_GRAVITY_MASK;
+pub const RROS_TIMER_GRAVITY_MASK: i32 = (RROS_TIMER_KGRAVITY | RROS_TIMER_UGRAVITY);
+pub const RROS_TIMER_INIT_MASK: i32 = RROS_TIMER_GRAVITY_MASK;
 pub struct RrosTimerbase {
     pub lock: SpinLock<i32>,
     pub q: List<Arc<SpinLock<RrosTimer>>>,
 }
 
 pub fn rros_this_cpu_timers(clock: &RrosClock) -> *mut RrosTimerbase {
-    unsafe{percpu_defs::raw_cpu_ptr(clock.get_timerdata_addr() as *mut u8) as *mut RrosTimerbase}
+    unsafe { percpu_defs::raw_cpu_ptr(clock.get_timerdata_addr() as *mut u8) as *mut RrosTimerbase }
 }
 
-pub fn rros_timer_null_handler(timer: *mut RrosTimer) {pr_info!("i am in rros_timer_null_handler");}
+pub fn rros_timer_null_handler(timer: *mut RrosTimer) {
+    // pr_info!("i am in rros_timer_null_handler");
+}
 
 pub struct RrosTimer {
     clock: *mut RrosClock,
@@ -60,7 +56,7 @@ pub struct RrosTimer {
 impl RrosTimer {
     pub fn new(date: KtimeT) -> Self {
         RrosTimer {
-            clock: 0 as *mut RrosClock,//?
+            clock: 0 as *mut RrosClock, //?
             date: date,
             status: 0,
             interval: 0,
@@ -68,11 +64,11 @@ impl RrosTimer {
             pexpect_ticks: 0,
             periodic_ticks: 0,
             base: 0 as *mut RrosTimerbase,
-            handler: rros_timer_null_handler,//?
-            name: c_str!(""),//?
+            handler: rros_timer_null_handler, //?
+            name: c_str!(""),                 //?
             #[cfg(CONFIG_SMP)]
             rq: 0 as *mut RrosRq,
-            thread: None
+            thread: None,
         }
     }
 
@@ -86,7 +82,7 @@ impl RrosTimer {
 
     pub fn set_status(&mut self, status: i32) {
         self.status = status;
-    }   
+    }
 
     pub fn add_status(&mut self, status: i32) {
         self.status |= status;
@@ -108,7 +104,7 @@ impl RrosTimer {
         self.pexpect_ticks = value;
     }
 
-    pub fn set_periodic_ticks(&mut self, value: u64)  {
+    pub fn set_periodic_ticks(&mut self, value: u64) {
         self.periodic_ticks = value;
     }
 
@@ -122,7 +118,7 @@ impl RrosTimer {
 
     pub fn set_name(&mut self, name: &'static CStr) {
         self.name = name;
-    }  
+    }
 
     pub fn set_rq(&mut self, rq: *mut RrosRq) {
         self.rq = rq;
@@ -157,11 +153,11 @@ impl RrosTimer {
     //         return None;
     //     }
     //     Some(self.handler.unwrap())
-    // } 
+    // }
 
     pub fn get_handler(&self) -> fn(*mut RrosTimer) {
         self.handler
-    } 
+    }
 
     pub fn get_clock(&self) -> *mut RrosClock {
         self.clock
@@ -214,8 +210,8 @@ pub fn rros_insert_tnode(tq: &mut List<Arc<SpinLock<RrosTimer>>>, timer: Arc<Spi
     let mut l = tq.len();
     while l >= 1 {
         let x = tq.get_by_index(l).unwrap().value.clone();
-        let x_date = unsafe{(*x.locked_data().get()).get_date()};
-        let timer_date = unsafe{(*timer.locked_data().get()).get_date()};
+        let x_date = unsafe { (*x.locked_data().get()).get_date() };
+        let timer_date = unsafe { (*timer.locked_data().get()).get_date() };
         if x_date <= timer_date {
             tq.enqueue_by_index(l, timer.clone());
             return;
@@ -227,26 +223,29 @@ pub fn rros_insert_tnode(tq: &mut List<Arc<SpinLock<RrosTimer>>>, timer: Arc<Spi
 
 //测试通过
 pub fn rros_get_timer_gravity(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
-    let status = unsafe{(*timer.locked_data().get()).get_status()};
+    let status = unsafe { (*timer.locked_data().get()).get_status() };
     if status & RROS_TIMER_KGRAVITY != 0 {
-        return unsafe{(*(*timer.locked_data().get()).get_clock()).get_gravity_kernel()}
+        return unsafe { (*(*timer.locked_data().get()).get_clock()).get_gravity_kernel() };
     }
 
     if status & RROS_TIMER_UGRAVITY != 0 {
-        return unsafe{(*(*timer.locked_data().get()).get_clock()).get_gravity_user()}
+        return unsafe { (*(*timer.locked_data().get()).get_clock()).get_gravity_user() };
     }
 
-    return unsafe{(*(*timer.locked_data().get()).get_clock()).get_gravity_irq()}
+    return unsafe { (*(*timer.locked_data().get()).get_clock()).get_gravity_irq() };
 }
 
 //测试通过
 pub fn rros_update_timer_date(timer: Arc<SpinLock<RrosTimer>>) {
-    unsafe{
+    unsafe {
         let start_date = (*timer.locked_data().get()).get_start_date();
         let periodic_ticks = (*timer.locked_data().get()).get_periodic_ticks();
         let interval = ktime_to_ns((*timer.locked_data().get()).get_interval());
         let gravity = ktime_to_ns(rros_get_timer_gravity(timer.clone()));
-        (*timer.locked_data().get()).set_date(ktime_add_ns(start_date, ((periodic_ticks as i64 * interval) - gravity) as u64));
+        (*timer.locked_data().get()).set_date(ktime_add_ns(
+            start_date,
+            ((periodic_ticks as i64 * interval) - gravity) as u64,
+        ));
     }
 }
 
@@ -259,7 +258,8 @@ pub fn rros_get_timer_next_date(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
 }
 
 #[cfg(CONFIG_RROS_RUNSTATS)]
-pub fn rros_reset_timer_stats(timer: Arc<SpinLock<RrosTimer>>) {//条件不足未修改
+pub fn rros_reset_timer_stats(timer: Arc<SpinLock<RrosTimer>>) {
+    //条件不足未修改
     timer.lock().get_scheduled().set_counter(0);
     timer.lock().get_fired().set_counter(0);
 }
@@ -275,7 +275,7 @@ pub fn rros_account_timer_fired(timer: Arc<SpinLock<RrosTimer>>) {
 }
 
 #[cfg(not(CONFIG_RROS_RUNSTATS))]
-pub fn rros_reset_timer_stats(timer: Arc<SpinLock<RrosTimer>>){}//条件不足未修改
+pub fn rros_reset_timer_stats(timer: Arc<SpinLock<RrosTimer>>) {} //条件不足未修改
 
 #[cfg(not(CONFIG_RROS_RUNSTATS))]
 pub fn rros_account_timer_scheduled(timer: Arc<SpinLock<RrosTimer>>) {}
@@ -286,21 +286,21 @@ pub fn rros_account_timer_fired(timer: Arc<SpinLock<RrosTimer>>) {}
 //测试通过
 pub fn rros_timer_deactivate(timer: Arc<SpinLock<RrosTimer>>) -> bool {
     let mut heading = true;
-    let tmb = unsafe{(*timer.locked_data().get()).get_base()};
-    let status = unsafe{(*timer.locked_data().get()).get_status()};
+    let tmb = unsafe { (*timer.locked_data().get()).get_base() };
+    let status = unsafe { (*timer.locked_data().get()).get_status() };
     if status & RROS_TIMER_DEQUEUED != 0 {
         heading = timer_at_front(timer.clone());
-        unsafe{rros_dequeue_timer(timer.clone(), &mut (*tmb).q)};
+        unsafe { rros_dequeue_timer(timer.clone(), &mut (*tmb).q) };
     }
 
-    unsafe{(*timer.locked_data().get()).change_status(!(RROS_TIMER_FIRED | RROS_TIMER_RUNNING))};
+    unsafe { (*timer.locked_data().get()).change_status(!(RROS_TIMER_FIRED | RROS_TIMER_RUNNING)) };
 
     return heading;
 }
 
 #[cfg(CONFIG_SMP)]
 pub fn rros_timer_on_rq(timer: Arc<SpinLock<RrosTimer>>, rq: *mut RrosRq) -> bool {
-    unsafe{(*timer.locked_data().get()).get_rq() == rq}
+    unsafe { (*timer.locked_data().get()).get_rq() == rq }
 }
 
 #[cfg(not(CONFIG_SMP))]
@@ -311,11 +311,11 @@ pub fn rros_timer_on_rq(timer: Arc<SpinLock<RrosTimer>>, rq: *mut RrosRq) -> boo
 //测试通过
 pub fn stop_timer_locked(timer: Arc<SpinLock<RrosTimer>>) {
     // let timer_lock = timer.lock();
-    let is_running = unsafe{(*timer.locked_data().get()).is_running()};
+    let is_running = unsafe { (*timer.locked_data().get()).is_running() };
     if is_running {
         let heading = rros_timer_deactivate(timer.clone());
         if heading && rros_timer_on_rq(timer.clone(), this_rros_rq()) {
-            let clock = unsafe{(*timer.locked_data().get()).get_clock()};
+            let clock = unsafe { (*timer.locked_data().get()).get_clock() };
             rros_program_local_tick(clock);
         }
     }
@@ -324,13 +324,13 @@ pub fn stop_timer_locked(timer: Arc<SpinLock<RrosTimer>>) {
 //默认通过
 pub fn __rros_stop_timer(timer: Arc<SpinLock<RrosTimer>>) {
     //base = lock_timer_base(timer, &flags);
-	stop_timer_locked(timer);
-	//unlock_timer_base(base, flags);
+    stop_timer_locked(timer);
+    //unlock_timer_base(base, flags);
 }
 
 //默认通过
 pub fn rros_stop_timer(timer: Arc<SpinLock<RrosTimer>>) {
-    unsafe{
+    unsafe {
         let is_running = (*timer.locked_data().get()).is_running();
         if is_running {
             __rros_stop_timer(timer.clone());
@@ -338,12 +338,51 @@ pub fn rros_stop_timer(timer: Arc<SpinLock<RrosTimer>>) {
     }
 }
 
+#[cfg(CONFIG_SMP)]
+fn lock_timer_base(timer: Arc<SpinLock<RrosTimer>>, flags: &mut u32) -> *mut RrosTimerbase {
+    unsafe{
+    let mut base = (*timer.locked_data().get()).get_base();
+        while true {
+        // let new_flags = lock::right_raw_spin_lock_irqsave();
+        // raw_spin_lock_irqsave(&base.lock, flags);
+        base = (*timer.locked_data().get()).get_base();
+        timer.irq_lock_noguard(flags);
+        let base2 = (*timer.locked_data().get()).get_base();
+        if (base == base2) {
+            break;
+        }            
+        timer.irq_unlock_noguard(*flags);
+        // lock::right_raw_spin_unlock_irqrestore(&base.lock, flags);
+        }
+    base
+    }
+}
+
+#[cfg(not(CONFIG_SMP))]
+fn lock_timer_base(timer: Arc<SpinLock<RrosTimer>>, flags: &mut u32) -> *mut RrosTimerbase {
+    pr_info!("!!!!!!!!!!!! this is wrong. lock_timer_base");
+    (*timer.locked_data().get()).get_base()
+}
+
+#[cfg(CONFIG_SMP)]
+fn unlock_timer_base(timer: Arc<SpinLock<RrosTimer>>, flags: u32) {
+    timer.irq_unlock_noguard(flags);
+}
+
+#[cfg(not(CONFIG_SMP))]
+fn unlock_timer_base(timer: Arc<SpinLock<RrosTimer>>, flags: &mut u32) {
+    pr_info!("!!!!!!!!!!!! this is wrong. lock_timer_base");
+}
+
 //测试通过
-pub fn rros_dequeue_timer(timer: Arc<SpinLock<RrosTimer>>, tq: &mut List<Arc<SpinLock<RrosTimer>>>) {
+pub fn rros_dequeue_timer(
+    timer: Arc<SpinLock<RrosTimer>>,
+    tq: &mut List<Arc<SpinLock<RrosTimer>>>,
+) {
     // pr_info!("len tq is {}", tq.len());
-    let timer_addr =unsafe{ timer.clone().locked_data().get()};
+    let timer_addr = unsafe { timer.clone().locked_data().get() };
     // pr_info!("the run timer add is {:p}", timer_addr);
-    unsafe{
+    unsafe {
         for i in 1..=tq.len() {
             let mut _x = tq.get_by_index(i).unwrap().value.clone();
             let x = _x.locked_data().get();
@@ -353,12 +392,14 @@ pub fn rros_dequeue_timer(timer: Arc<SpinLock<RrosTimer>>, tq: &mut List<Arc<Spi
             }
         }
     }
-    unsafe{(*timer.locked_data().get()).add_status(RROS_TIMER_DEQUEUED);}
+    unsafe {
+        (*timer.locked_data().get()).add_status(RROS_TIMER_DEQUEUED);
+    }
 }
 
 //测试通过
 pub fn rros_get_timer_expiry(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
-    let date = unsafe{(*timer.locked_data().get()).get_date()};
+    let date = unsafe { (*timer.locked_data().get()).get_date() };
     let gravity = rros_get_timer_gravity(timer.clone());
     return ktime_add(date, gravity);
 }
@@ -366,7 +407,7 @@ pub fn rros_get_timer_expiry(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
 //测试通过
 pub fn __rros_get_timer_delta(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
     let expiry = rros_get_timer_expiry(timer.clone());
-    let now = unsafe{(*timer.lock().get_clock()).read()};
+    let now = unsafe { (*timer.lock().get_clock()).read() };
     if expiry <= now {
         return ktime_set(0, 1);
     }
@@ -377,7 +418,7 @@ pub fn __rros_get_timer_delta(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
 //测试通过
 pub fn rros_get_timer_delta(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
     let timer_clone = timer.clone();
-    let is_running = unsafe{(*timer_clone.locked_data().get()).is_running()};
+    let is_running = unsafe { (*timer_clone.locked_data().get()).is_running() };
     if is_running == false {
         return RROS_INFINITE;
     }
@@ -386,22 +427,35 @@ pub fn rros_get_timer_delta(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
 
 //测试通过
 pub fn rros_percpu_timers(clock: &RrosClock, cpu: i32) -> *mut RrosTimerbase {
-    unsafe { percpu_defs::per_cpu_ptr(clock.get_timerdata_addr() as *mut u8, cpu) as *mut RrosTimerbase }
+    unsafe {
+        percpu_defs::per_cpu_ptr(clock.get_timerdata_addr() as *mut u8, cpu) as *mut RrosTimerbase
+    }
 }
 
+extern "C" {
+    fn rust_helper_cpumask_test_cpu(cpu: i32, cpumask: *const bindings::cpumask) -> i32;
+}
 
 //测试通过
 #[cfg(CONFIG_SMP)]
-fn get_clock_cpu(clock: &RrosClock, cpu:i32) ->i32 {
+fn get_clock_cpu(clock: &RrosClock, cpu: i32) -> i32 {
     return 0;
+    // unsafe{
+    //     if (rust_helper_cpumask_test_cpu(cpu, clock.affinity.as_mut() as *mut bindings::cpumask)) {
+
+    //     }
+    // }
+    // if (cpumask_test_cpu(cpu, &clock->affinity))
+        // return cpu;
+
+    // return cpumask_first(&clock->affinity);
 }
 
 #[cfg(not(CONFIG_SMP))]
-fn get_clock_cpu(clock: &RrosClock, cpu:i32) ->i32 {
+fn get_clock_cpu(clock: &RrosClock, cpu: i32) -> i32 {
     return 0;
 }
 
-
 // fn get_clock_cpu(clock: &RrosClock, cpu:i32) ->i32{
 //     return 0;
 // }
@@ -411,14 +465,17 @@ use core::borrow::BorrowMut;
 use core::cell::RefCell;
 
 pub fn rros_init_timer_on_rq(
-    timer: Arc<SpinLock<RrosTimer>>, 
+    timer: Arc<SpinLock<RrosTimer>>,
     clock: &mut RrosClock,
     handler: Option<fn(*mut RrosTimer)>,
     rq: *mut RrosRq,
     name: &'static CStr,
-    flags: i32,) {
+    flags: i32,
+) {
     timer.lock().set_date(RROS_INFINITE);
-    timer.lock().set_status(RROS_TIMER_DEQUEUED|(flags & RROS_TIMER_INIT_MASK));
+    timer
+        .lock()
+        .set_status(RROS_TIMER_DEQUEUED | (flags & RROS_TIMER_INIT_MASK));
     //timer.set_handler(handler);
     if handler.is_some() {
         timer.lock().set_handler(handler.unwrap());
@@ -433,7 +490,7 @@ pub fn rros_init_timer_on_rq(
     #[cfg(CONFIG_SMP)]
     timer.lock().set_rq(rros_cpu_rq(cpu));
 
-    timer.lock().set_base(rros_percpu_timers(clock,cpu));
+    timer.lock().set_base(rros_percpu_timers(clock, cpu));
     timer.lock().set_clock(clock as *mut RrosClock);
     //timer.set_name();
     rros_reset_timer_stats(timer.clone());
@@ -442,16 +499,15 @@ pub fn rros_init_timer_on_rq(
 //测试通过
 pub fn program_timer(timer: Arc<SpinLock<RrosTimer>>, tq: &mut List<Arc<SpinLock<RrosTimer>>>) {
     rros_enqueue_timer(timer.clone(), tq);
-    let rq = unsafe{(*timer.locked_data().get()).get_rq()};
-    let local_flags = unsafe{(*rq).local_flags};
-    if (local_flags & RQ_TSTOPPED) == 0
-            && timer_at_front(timer.clone()) == false {
-                return;
-    } 
-    let clock = unsafe{(*timer.locked_data().get()).get_clock()};
+    let rq = unsafe { (*timer.locked_data().get()).get_rq() };
+    let local_flags = unsafe { (*rq).local_flags };
+    if (local_flags & RQ_TSTOPPED) == 0 && timer_at_front(timer.clone()) == false {
+        return;
+    }
+    let clock = unsafe { (*timer.locked_data().get()).get_clock() };
     if rq != this_rros_rq() {
         rros_program_remote_tick(clock, rq);
-    }else {
+    } else {
         rros_program_local_tick(clock);
     }
 }
@@ -461,68 +517,73 @@ pub fn rros_start_timer(timer: Arc<SpinLock<RrosTimer>>, value: KtimeT, interval
     // pr_info!("yinyongcishu is {}",Arc::strong_count(&timer));
     // pr_info!("rros_start_timer: 1");
     // pr_info!("the start timer{:?} {:?}", value, interval);
-    unsafe{
-    let mut tmb = (*timer.locked_data().get()).get_base();
-    let status = (*timer.locked_data().get()).get_status();
-    if status & RROS_TIMER_DEQUEUED == 0 {
-        unsafe{rros_dequeue_timer(timer.clone(), &mut (*tmb).q)};
-    }   
-    (*timer.locked_data().get()).change_status(!(RROS_TIMER_FIRED | RROS_TIMER_PERIODIC));
-    let date = ktime_sub(value, unsafe{(*(*timer.locked_data().get()).get_clock()).get_offset()});
-    let gravity = rros_get_timer_gravity(timer.clone());
-    (*timer.locked_data().get()).set_date(ktime_sub(date, gravity));
-    (*timer.locked_data().get()).set_interval(RROS_INFINITE);
-    if timeout_infinite(interval) == false  {
-        (*timer.locked_data().get()).set_interval(interval);
-        (*timer.locked_data().get()).set_start_date(value);
-        (*timer.locked_data().get()).set_pexpect_ticks(0);
-        (*timer.locked_data().get()).set_periodic_ticks(0);
-        (*timer.locked_data().get()).add_status(RROS_TIMER_PERIODIC);
-    }
+    unsafe {
+        let mut flags = 0;
+        let mut tmb = lock_timer_base(timer.clone(), &mut flags);
+        // let mut tmb = (*timer.locked_data().get()).get_base();
+        let status = (*timer.locked_data().get()).get_status();
+        if status & RROS_TIMER_DEQUEUED == 0 {
+            unsafe { rros_dequeue_timer(timer.clone(), &mut (*tmb).q) };
+        }
+        (*timer.locked_data().get()).change_status(!(RROS_TIMER_FIRED | RROS_TIMER_PERIODIC));
+        let date = ktime_sub(value, unsafe {
+            (*(*timer.locked_data().get()).get_clock()).get_offset()
+        });
+        let gravity = rros_get_timer_gravity(timer.clone());
+        (*timer.locked_data().get()).set_date(ktime_sub(date, gravity));
+        (*timer.locked_data().get()).set_interval(RROS_INFINITE);
+        if timeout_infinite(interval) == false {
+            (*timer.locked_data().get()).set_interval(interval);
+            (*timer.locked_data().get()).set_start_date(value);
+            (*timer.locked_data().get()).set_pexpect_ticks(0);
+            (*timer.locked_data().get()).set_periodic_ticks(0);
+            (*timer.locked_data().get()).add_status(RROS_TIMER_PERIODIC);
+        }
 
-    (*timer.locked_data().get()).add_status(RROS_TIMER_RUNNING);
-    // pr_info!("rros_start_timer: 2");
-    unsafe{program_timer(timer, &mut (*tmb).q)};
+        (*timer.locked_data().get()).add_status(RROS_TIMER_RUNNING);
+        // pr_info!("rros_start_timer: 2");
+        unsafe { program_timer(timer.clone(), &mut (*tmb).q) };
+        unlock_timer_base(timer, flags);
     }
 }
 
 //测试通过
 pub fn timer_at_front(timer: Arc<SpinLock<RrosTimer>>) -> bool {
-    unsafe{
-    let tmb = (*timer.locked_data().get()).get_base();
-    unsafe{
-        if (*tmb).q.is_empty(){
-            return false;
-        }
-    }
-    let mut _head = unsafe{(*tmb).q.get_head().unwrap().value.clone()};
-    let head = _head.locked_data().get();
-    let timer_addr = timer.clone().locked_data().get();
-    if head == timer_addr {
-        return true;
-    }
     unsafe {
-        if (*tmb).q.len() < 2 {
-            return false;
+        let tmb = (*timer.locked_data().get()).get_base();
+        unsafe {
+            if (*tmb).q.is_empty() {
+                return false;
+            }
         }
-    }
-    let local_flags = unsafe{(*(*timer.locked_data().get()).get_rq()).local_flags};
-    if (local_flags & RQ_TDEFER) != 0x0 {
-        let _next = unsafe{(*tmb).q.get_by_index(2).unwrap().value.clone()};
-        let next = _next.locked_data().get();
-        if next == timer_addr {
+        let mut _head = unsafe { (*tmb).q.get_head().unwrap().value.clone() };
+        let head = _head.locked_data().get();
+        let timer_addr = timer.clone().locked_data().get();
+        if head == timer_addr {
             return true;
         }
-    }
-    return false;
+        unsafe {
+            if (*tmb).q.len() < 2 {
+                return false;
+            }
+        }
+        let local_flags = unsafe { (*(*timer.locked_data().get()).get_rq()).local_flags };
+        if (local_flags & RQ_TDEFER) != 0x0 {
+            let _next = unsafe { (*tmb).q.get_by_index(2).unwrap().value.clone() };
+            let next = _next.locked_data().get();
+            if next == timer_addr {
+                return true;
+            }
+        }
+        return false;
     }
 }
 
 //测试通过
 pub fn rros_get_timer_date(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
     let mut expiry = 0;
-    let is_running = unsafe{(*timer.locked_data().get()).is_running()};
-    if is_running == false{
+    let is_running = unsafe { (*timer.locked_data().get()).is_running() };
+    if is_running == false {
         expiry = RROS_INFINITE;
     } else {
         expiry = rros_get_timer_expiry(timer.clone());
@@ -543,13 +604,18 @@ pub fn rros_get_stopped_timer_delta(timer: Arc<SpinLock<RrosTimer>>) -> KtimeT {
         return RROS_INFINITE;
     }
 
-	return t;
+    return t;
 }
 
 //测试通过
-pub fn rros_enqueue_timer(timer: Arc<SpinLock<RrosTimer>>, tq: &mut List<Arc<SpinLock<RrosTimer>>>) {
+pub fn rros_enqueue_timer(
+    timer: Arc<SpinLock<RrosTimer>>,
+    tq: &mut List<Arc<SpinLock<RrosTimer>>>,
+) {
     rros_insert_tnode(tq, timer.clone());
-    unsafe{(*timer.locked_data().get()).change_status(!RROS_TIMER_DEQUEUED);}
+    unsafe {
+        (*timer.locked_data().get()).change_status(!RROS_TIMER_DEQUEUED);
+    }
     rros_account_timer_scheduled(timer.clone());
 }
 
@@ -565,17 +631,13 @@ pub fn rros_destroy_timer(timer: Arc<SpinLock<RrosTimer>>) {
 
 //未测试，默认通过
 pub fn rros_abs_timeout(timer: Arc<SpinLock<RrosTimer>>, delta: KtimeT) -> KtimeT {
-    unsafe{
-        ktime_add(
-            (*(*timer.locked_data().get()).get_clock()).read(), delta
-        ) 
-    }
+    unsafe { ktime_add((*(*timer.locked_data().get()).get_clock()).read(), delta) }
 }
 
 // fn evl_prepare_timed_wait(timer: rros_timer, clock: evl_clock, rq: evl_rq) {
 /* We may change the reference clock before waiting. */
-    // if (rq != timer->rq || clock != timer->clock) {
-        // evl_move_timer(timer, clock, rq);
-    // }
-        
+// if (rq != timer->rq || clock != timer->clock) {
+// evl_move_timer(timer, clock, rq);
+// }
+
 // }
\ No newline at end of file
diff --git a/kernel/rros/timer_test.rs b/kernel/rros/timer_test.rs
index e164606be..48a853fdc 100644
--- a/kernel/rros/timer_test.rs
+++ b/kernel/rros/timer_test.rs
@@ -3,20 +3,20 @@
 use crate::factory;
 use crate::list::*;
 use crate::{
-    factory::RrosFactory, factory::RustFile, factory::RrosElement, lock::*, timer::*, clock::*, RROS_OOB_CPUS as RROS_OOB_CPUS,
-    sched::*, 
+    clock::*, factory::RrosElement, factory::RrosFactory, factory::RustFile, lock::*, sched::*,
+    timer::*, RROS_OOB_CPUS,
 };
+use core::borrow::{Borrow, BorrowMut};
 use core::cell::RefCell;
-use core::{mem::size_of, todo, mem::align_of};
 use core::cell::UnsafeCell;
+use core::ops::Deref;
+use core::ops::DerefMut;
+use core::{mem::align_of, mem::size_of, todo};
 use kernel::{
-    bindings, c_types, cpumask::CpumaskT, file_operations::FileOperations, spinlock_init, percpu,
-    prelude::*, premmpt, str::CStr, sync::Lock, sync::SpinLock, sysfs, timekeeping, double_linked_list::*,
-    ktime::*,percpu_defs, sync::Guard,
+    bindings, c_types, cpumask::CpumaskT, double_linked_list::*, file_operations::FileOperations,
+    ktime::*, percpu, percpu_defs, prelude::*, premmpt, spinlock_init, str::CStr, sync::Guard,
+    sync::Lock, sync::SpinLock, sysfs, timekeeping,
 };
-use core::borrow::{Borrow, BorrowMut};
-use core::ops::DerefMut;
-use core::ops::Deref;
 
 //测试通过
 pub fn test_rros_insert_tnode() -> Result<usize> {
@@ -36,7 +36,6 @@ pub fn test_rros_insert_tnode() -> Result<usize> {
         let pinned = unsafe { Pin::new_unchecked(&mut a) };
         spinlock_init!(pinned, "a");
 
-
         let mut xx = Arc::try_new(x)?;
         let mut yy = Arc::try_new(y)?;
         let mut zz = Arc::try_new(z)?;
@@ -48,11 +47,11 @@ pub fn test_rros_insert_tnode() -> Result<usize> {
         rros_insert_tnode(&mut (*tmb).q, zz);
         rros_insert_tnode(&mut (*tmb).q, aa);
 
-        pr_info!("len is {}",(*tmb).q.len());
+        pr_info!("len is {}", (*tmb).q.len());
 
         for i in 1..=(*tmb).q.len() {
             let mut _x = (*tmb).q.get_by_index(i).unwrap().value.clone();
-            pr_info!("data of x is {}",_x.lock().get_date());
+            pr_info!("data of x is {}", _x.lock().get_date());
         }
     }
     pr_info!("~~~test_rros_insert_tnode end~~~");
@@ -77,7 +76,6 @@ pub fn test_rros_enqueue_timer() -> Result<usize> {
         let pinned = unsafe { Pin::new_unchecked(&mut a) };
         spinlock_init!(pinned, "a");
 
-
         let mut xx = Arc::try_new(x)?;
         let mut yy = Arc::try_new(y)?;
         let mut zz = Arc::try_new(z)?;
@@ -89,13 +87,13 @@ pub fn test_rros_enqueue_timer() -> Result<usize> {
         rros_enqueue_timer(zz, &mut (*tmb).q);
         rros_enqueue_timer(aa, &mut (*tmb).q);
 
-        pr_info!("len is {}",(*tmb).q.len());
+        pr_info!("len is {}", (*tmb).q.len());
 
         for i in 1..=(*tmb).q.len() {
             let mut _x = (*tmb).q.get_by_index(i).unwrap().value.clone();
-            pr_info!("data of x is {}",_x.lock().get_date());
+            pr_info!("data of x is {}", _x.lock().get_date());
         }
-        pr_info!("qufan RROS_TIMER_DEQUEUED is {}",!RROS_TIMER_DEQUEUED);
+        pr_info!("qufan RROS_TIMER_DEQUEUED is {}", !RROS_TIMER_DEQUEUED);
     }
     pr_info!("~~~test_rros_insert_tnode end~~~");
     Ok(0)
@@ -104,22 +102,21 @@ pub fn test_rros_enqueue_timer() -> Result<usize> {
 //测试通过
 pub fn test_rros_get_timer_gravity() -> Result<usize> {
     pr_info!("~~~test_rros_get_timer_gravity begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
         let mut xx = Arc::try_new(x)?;
         xx.lock().set_clock(&mut RROS_MONO_CLOCK as *mut RrosClock);
-    
 
         xx.lock().set_status(RROS_TIMER_KGRAVITY);
-        pr_info!("kernel gravity is {}",rros_get_timer_gravity(xx.clone()));
+        pr_info!("kernel gravity is {}", rros_get_timer_gravity(xx.clone()));
 
         xx.lock().set_status(RROS_TIMER_UGRAVITY);
-        pr_info!("user gravity is {}",rros_get_timer_gravity(xx.clone()));
+        pr_info!("user gravity is {}", rros_get_timer_gravity(xx.clone()));
 
         xx.lock().set_status(0);
-        pr_info!("irq gravity is {}",rros_get_timer_gravity(xx.clone()));
+        pr_info!("irq gravity is {}", rros_get_timer_gravity(xx.clone()));
     }
     pr_info!("~~~test_rros_get_timer_gravity end~~~");
     Ok(0)
@@ -128,7 +125,7 @@ pub fn test_rros_get_timer_gravity() -> Result<usize> {
 //测试通过
 pub fn test_rros_update_timer_date() -> Result<usize> {
     pr_info!("~~~test_rros_update_timer_date begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -141,7 +138,7 @@ pub fn test_rros_update_timer_date() -> Result<usize> {
         xx.lock().set_status(RROS_TIMER_UGRAVITY);
 
         rros_update_timer_date(xx.clone());
-        pr_info!("xx date is {}",xx.lock().get_date());
+        pr_info!("xx date is {}", xx.lock().get_date());
     }
     pr_info!("~~~test_rros_update_timer_date end~~~");
     Ok(0)
@@ -150,7 +147,7 @@ pub fn test_rros_update_timer_date() -> Result<usize> {
 //测试通过
 pub fn test_rros_get_timer_next_date() -> Result<usize> {
     pr_info!("~~~test_rros_get_timer_next_date begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -160,7 +157,7 @@ pub fn test_rros_get_timer_next_date() -> Result<usize> {
         xx.lock().set_periodic_ticks(3);
         xx.lock().set_interval(8);
 
-        pr_info!("xx next date is {}",rros_get_timer_next_date(xx.clone()));
+        pr_info!("xx next date is {}", rros_get_timer_next_date(xx.clone()));
     }
     pr_info!("~~~test_rros_get_timer_next_date end~~~");
     Ok(0)
@@ -250,11 +247,14 @@ pub fn test_rros_timer_deactivate() -> Result<usize> {
 
         if rros_timer_deactivate(zz.clone()) {
             pr_info!("test_rros_timer_deactivate: success");
-        }else {
+        } else {
             pr_info!("test_rros_timer_deactivate: failed");
         }
 
-        pr_info!("test_rros_timer_deactivate: len of tmb is {}",(*tmb).q.len());
+        pr_info!(
+            "test_rros_timer_deactivate: len of tmb is {}",
+            (*tmb).q.len()
+        );
     }
     Ok(0)
 }
@@ -262,7 +262,7 @@ pub fn test_rros_timer_deactivate() -> Result<usize> {
 //测试通过
 pub fn test_rros_get_timer_expiry() -> Result<usize> {
     pr_info!("~~~test_rros_get_timer_expiry begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -272,7 +272,7 @@ pub fn test_rros_get_timer_expiry() -> Result<usize> {
         xx.lock().set_date(11);
         xx.lock().set_status(0);
 
-        pr_info!("xx next date is {}",rros_get_timer_expiry(xx.clone()));
+        pr_info!("xx next date is {}", rros_get_timer_expiry(xx.clone()));
     }
     pr_info!("~~~test_rros_get_timer_expiry end~~~");
     Ok(0)
@@ -281,7 +281,7 @@ pub fn test_rros_get_timer_expiry() -> Result<usize> {
 //测试通过
 pub fn test___rros_get_timer_delta() -> Result<usize> {
     pr_info!("~~~test___rros_get_timer_delta begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -291,13 +291,12 @@ pub fn test___rros_get_timer_delta() -> Result<usize> {
         xx.lock().set_date(1111111111111);
         xx.lock().set_status(0);
 
-        pr_info!("xx delta is {}",__rros_get_timer_delta(xx.clone()));
+        pr_info!("xx delta is {}", __rros_get_timer_delta(xx.clone()));
 
         xx.lock().set_date(0);
         xx.lock().set_status(RROS_TIMER_UGRAVITY);
 
-        pr_info!("xx delta is {}",__rros_get_timer_delta(xx.clone()));
-
+        pr_info!("xx delta is {}", __rros_get_timer_delta(xx.clone()));
     }
     pr_info!("~~~test___rros_get_timer_delta end~~~");
     Ok(0)
@@ -306,7 +305,7 @@ pub fn test___rros_get_timer_delta() -> Result<usize> {
 //测试通过
 pub fn test_rros_get_timer_delta() -> Result<usize> {
     pr_info!("~~~test_rros_get_timer_delta begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -316,13 +315,12 @@ pub fn test_rros_get_timer_delta() -> Result<usize> {
         xx.lock().set_date(1111111111111);
         xx.lock().set_status(RROS_TIMER_RUNNING);
 
-        pr_info!("xx delta is {}",rros_get_timer_delta(xx.clone()));
+        pr_info!("xx delta is {}", rros_get_timer_delta(xx.clone()));
 
         xx.lock().set_date(0);
         xx.lock().set_status(RROS_TIMER_PERIODIC);
 
-        pr_info!("xx delta is {}",rros_get_timer_delta(xx.clone()));
-
+        pr_info!("xx delta is {}", rros_get_timer_delta(xx.clone()));
     }
     pr_info!("~~~test_rros_get_timer_delta end~~~");
     Ok(0)
@@ -331,7 +329,7 @@ pub fn test_rros_get_timer_delta() -> Result<usize> {
 //测试通过
 pub fn test_rros_get_timer_date() -> Result<usize> {
     pr_info!("~~~test_rros_get_timer_date begin~~~");
-    unsafe{
+    unsafe {
         let mut x = SpinLock::new(RrosTimer::new(1));
         let pinned = unsafe { Pin::new_unchecked(&mut x) };
         spinlock_init!(pinned, "x");
@@ -341,15 +339,13 @@ pub fn test_rros_get_timer_date() -> Result<usize> {
         xx.lock().set_date(11);
         xx.lock().set_status(RROS_TIMER_RUNNING);
 
-        pr_info!("xx next date is {}",rros_get_timer_date(xx.clone()));
+        pr_info!("xx next date is {}", rros_get_timer_date(xx.clone()));
 
         xx.lock().set_status(RROS_TIMER_PERIODIC);
-        pr_info!("xx next date is {}",rros_get_timer_date(xx.clone()));
-
+        pr_info!("xx next date is {}", rros_get_timer_date(xx.clone()));
     }
     pr_info!("~~~test_rros_get_timer_date end~~~");
     Ok(0)
-
 }
 
 //测试通过
@@ -362,7 +358,7 @@ pub fn test_program_timer() -> Result<usize> {
         spinlock_init!(pinned, "x");
 
         let mut xx = Arc::try_new(x)?;
-        
+
         let mut _rq = rros_rq::new()?;
         let mut rq = &mut _rq as *mut rros_rq;
 
@@ -373,10 +369,10 @@ pub fn test_program_timer() -> Result<usize> {
 
         program_timer(xx.clone(), &mut (*tmb1).q);
 
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_program_timer end~~~");
-     Ok(0)
+    Ok(0)
 }
 
 //测试通过
@@ -399,12 +395,12 @@ pub fn test_rros_start_timer() -> Result<usize> {
         pr_info!("before program_timer");
         rros_start_timer(xx.clone(), 333, 222);
 
-        pr_info!("timer date is {}",xx.lock().get_date());
-        pr_info!("timer start date is {}",xx.lock().get_start_date());
-        pr_info!("timer interval is {}",xx.lock().get_interval());
-        pr_info!("timer status is {}",xx.lock().get_status());
+        pr_info!("timer date is {}", xx.lock().get_date());
+        pr_info!("timer start date is {}", xx.lock().get_start_date());
+        pr_info!("timer interval is {}", xx.lock().get_interval());
+        pr_info!("timer status is {}", xx.lock().get_status());
 
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_rros_start_timer end~~~");
     Ok(0)
@@ -420,7 +416,7 @@ pub fn test_stop_timer_locked() -> Result<usize> {
         spinlock_init!(pinned, "x");
 
         let mut xx = Arc::try_new(x)?;
-        
+
         let mut _rq = rros_rq::new()?;
         let mut rq = &mut _rq as *mut rros_rq;
 
@@ -430,7 +426,7 @@ pub fn test_stop_timer_locked() -> Result<usize> {
         xx.lock().set_status(RROS_TIMER_RUNNING);
         pr_info!("before stop_timer_locked");
         stop_timer_locked(xx.clone());
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_stop_timer_locked end~~~");
     Ok(0)
@@ -446,7 +442,7 @@ pub fn test_rros_destroy_timer() -> Result<usize> {
         spinlock_init!(pinned, "x");
 
         let mut xx = Arc::try_new(x)?;
-        
+
         let mut _rq = rros_rq::new()?;
         let mut rq = &mut _rq as *mut rros_rq;
 
@@ -458,17 +454,16 @@ pub fn test_rros_destroy_timer() -> Result<usize> {
         rros_destroy_timer(xx.clone());
         let xx_lock_rq = xx.lock().get_rq();
         let xx_lock_base = xx.lock().get_base();
-        if xx_lock_rq == 0 as *mut RrosRq{
+        if xx_lock_rq == 0 as *mut RrosRq {
             pr_info!("xx rq is none");
         }
         if xx_lock_base == 0 as *mut RrosTimerbase {
             pr_info!("xx base is none");
         }
-        pr_info!("len of tmb is {}",(*tmb).q.len());
+        pr_info!("len of tmb is {}", (*tmb).q.len());
     }
     pr_info!("~~~test_rros_destroy_timer end~~~");
     Ok(0)
-
 }
 
 pub fn handler(timer: &RrosTimer) {
@@ -490,4 +485,4 @@ pub fn test_get_handler() -> Result<usize> {
     }
     pr_info!("~~~test_get_handler end~~~");
     Ok(0)
-}
\ No newline at end of file
+}
diff --git a/kernel/rros/tp.rs b/kernel/rros/tp.rs
new file mode 100644
index 000000000..63e06d47b
--- /dev/null
+++ b/kernel/rros/tp.rs
@@ -0,0 +1,701 @@
+use kernel::{
+    bindings, c_types, prelude::*, ktime,str::CStr, c_str,double_linked_list::*,sync::{SpinLock, Lock, Guard},memory_rros::*,};
+use crate::{
+    sched::*,
+    clock::*,
+	thread::*,
+    timer::*,
+    timeout::*,
+    fifo::*,
+};
+use core::mem::size_of;
+
+pub static mut rros_sched_tp:rros_sched_class = rros_sched_class{
+    sched_init:	Some(tp_init),
+    sched_enqueue:	Some(tp_enqueue),
+    sched_dequeue:	Some(tp_dequeue),
+    sched_requeue:	Some(tp_requeue),
+    sched_pick:	Some(tp_pick),
+    sched_migrate:	Some(tp_migrate),
+    sched_chkparam:	Some(tp_chkparam),
+    sched_setparam:	Some(tp_setparam),
+    sched_getparam:	Some(tp_getparam),
+    sched_trackprio: Some(tp_trackprio),
+    sched_ceilprio:	Some(tp_ceilprio),
+    sched_declare:	None,//Some(tp_declare)
+    sched_forget:	Some(tp_forget),
+    sched_show:	Some(tp_show),
+    sched_control:	Some(tp_control),
+    sched_kick: None,
+    sched_tick: None,
+    nthreads: 0,
+    next: 0 as *mut rros_sched_class,
+    weight: 3 * RROS_CLASS_WEIGHT_FACTOR,
+    policy:	SCHED_TP,
+    name:	"tp",
+    flag:4,
+};
+
+pub const CONFIG_RROS_SCHED_TP_NR_PART: i32 = 5;// 先默认设置为5
+pub const RROS_TP_MAX_PRIO:i32  = RROS_FIFO_MAX_PRIO;
+pub const RROS_TP_MIN_PRIO:i32 = RROS_FIFO_MIN_PRIO;
+pub const RROS_TP_NR_PRIO:i32 =	RROS_TP_MAX_PRIO - RROS_TP_MIN_PRIO + 1;
+
+type ktime_t = i64;
+
+extern "C"{
+    fn rust_helper_timespec64_to_ktime(ts:bindings::timespec64) -> ktime_t;
+}
+
+
+pub struct rros_tp_rq {
+    pub runnable: rros_sched_queue,
+}
+impl rros_tp_rq{
+    pub fn new() -> Result<Self>{
+        Ok(rros_tp_rq { 
+            runnable: rros_sched_queue::new()?,
+        })
+    }
+}
+
+pub struct rros_tp_window {
+	pub w_offset:ktime_t,
+	pub w_part:i32,
+}
+
+pub struct rros_tp_schedule {
+	pub pwin_nr:i32,
+	pub tf_duration:ktime_t,
+	pub refcount:*mut bindings::atomic_t,
+	pub pwins:[rros_tp_window;1 as usize],
+}
+
+pub struct Rros_sched_tp {
+	pub partitions:Option<[rros_tp_rq; CONFIG_RROS_SCHED_TP_NR_PART as usize]>,
+	pub idle:rros_tp_rq,
+	pub tps:*mut rros_tp_rq,
+	pub tf_timer:Option<Arc<SpinLock<RrosTimer>>>,
+	pub gps:*mut rros_tp_schedule,
+	pub wnext:i32,
+	pub tf_start:ktime_t,
+	pub threads:Option<List<Arc<SpinLock<rros_thread>>>>,
+}
+impl Rros_sched_tp{
+    pub fn new() -> Result<Self>{
+        unsafe{
+        Ok(Rros_sched_tp { 
+            partitions: None, 
+            idle: rros_tp_rq::new()?, 
+            tps: 0 as *mut rros_tp_rq,
+            tf_timer: None, 
+            gps: 0 as *mut rros_tp_schedule, 
+            wnext: 0, 
+            tf_start: 0, 
+            threads: None,
+        })
+        }
+    }
+}
+
+pub fn tp_schedule_next(tp:&mut Rros_sched_tp) -> Result<usize>{
+    let mut w  = 0 as *mut rros_tp_window;
+    let mut rq = 0 as *mut rros_rq;
+    let mut t:ktime_t = 0; 
+    let mut now:ktime_t = 0;
+    let mut p_next:i32 = 0;
+
+    rq = kernel::container_of!(tp as *mut Rros_sched_tp, rros_rq, tp) as *mut rros_rq;
+    // assert_hard_lock(&rq->lock);
+    unsafe{
+        w = &mut ((*tp.gps).pwins[tp.wnext as usize]) as *mut rros_tp_window;
+        p_next = (*w).w_part;
+        if p_next < 0{
+            tp.tps = &mut tp.idle as *mut rros_tp_rq;
+        }else{
+            tp.tps = &mut tp.partitions.as_mut().unwrap()[p_next as usize] as *mut rros_tp_rq;
+        }
+        tp.wnext = (tp.wnext + 1) % (*tp.gps).pwin_nr;
+        w = &mut (*tp.gps).pwins[tp.wnext as usize] as *mut rros_tp_window;
+        t = ktime::ktime_add(tp.tf_start, (*w).w_offset);
+    }
+
+    loop{
+        unsafe{
+            now = rros_read_clock(&RROS_MONO_CLOCK);
+            if ktime::ktime_compare(now, t) <= 0{
+                break;
+            }
+            t = ktime::ktime_add(tp.tf_start, (*tp.gps).tf_duration);
+            tp.tf_start = t;
+            tp.wnext = 0;
+        }
+    }
+
+    unsafe{rros_start_timer(tp.tf_timer.as_mut().unwrap().clone(), t, RROS_INFINITE)};
+    rros_set_resched(Some(rq));
+    Ok(0)
+}
+
+pub fn tp_tick_handler(timer:*mut RrosTimer){
+    unsafe{
+        // 这里的container_of有问题
+        let rq = kernel::container_of!(timer, rros_rq, tp.tf_timer) as *mut rros_rq;
+        let mut tp = &mut (*rq).tp;
+
+        // raw_spin_lock(&rq->lock);
+        if tp.wnext + 1 == (*tp.gps).pwin_nr{
+            tp.tf_start = ktime::ktime_add(tp.tf_start, (*tp.gps).tf_duration);
+        }
+        tp_schedule_next(tp);
+
+        // raw_spin_unlock(&rq->lock);
+    }
+}
+
+pub fn tp_init(rq:*mut rros_rq) -> Result<usize>{
+    unsafe{
+        let mut tp = &mut (*rq).tp;
+        let r1 = rros_tp_rq::new()?;
+        let r2 = rros_tp_rq::new()?;
+        let r3 = rros_tp_rq::new()?;
+        let r4 = rros_tp_rq::new()?;
+        let r5 = rros_tp_rq::new()?;
+        let mut temp :[rros_tp_rq;CONFIG_RROS_SCHED_TP_NR_PART as usize] = [r1,r2,r3,r4,r5];
+        for n in 0..CONFIG_RROS_SCHED_TP_NR_PART{
+            // temp[n as usize].runnable.head = Some(List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));
+            let mut tmp = Arc::<SpinLock<rros_thread>>::try_new_uninit()?;
+
+            // let mut thread = SpinLock::new(rros_thread::new()?);
+            // let pinned = Pin::new_unchecked(&mut thread);
+            // spinlock_init!(pinned, "rros_threads");
+            // Arc::get_mut(&mut tmp).unwrap().write(thread);
+    
+            let tmp = tmp.assume_init();
+            temp[n as usize].runnable.head =  Some(List::new(tmp));//Arc::try_new(thread)?
+            // temp[n as usize].runnable.head.as_mut().unwrap().head.value.lock().init()?;
+            (*temp[n as usize].runnable.head.as_mut().unwrap().head.value.locked_data().get()).init().unwrap();
+            // let pinned = Pin::new_unchecked(&mut *(Arc::into_raw( temp[n as usize].runnable.head.as_mut().unwrap().head.value.clone()) as *mut SpinLock<rros_thread>));
+            // // &mut *Arc::into_raw( *(*rq_ptr).root_thread.clone().as_mut().unwrap()) as &mut SpinLock<rros_thread>
+            // spinlock_init!(pinned, "rros_threads");
+        }
+        tp.partitions = Some(temp);
+        tp.tps = 0 as *mut rros_tp_rq;
+        tp.gps = 0 as *mut rros_tp_schedule;
+        tp.tf_timer = Some(Arc::try_new(SpinLock::new(RrosTimer::new(0)))?);
+        rros_init_timer_on_rq(tp.tf_timer.clone().as_mut().unwrap().clone(), 
+            &mut RROS_MONO_CLOCK, Some(tp_tick_handler),
+            rq, c_str!("[tp-tick]"),RROS_TIMER_IGRAVITY);
+        // rros_set_timer_name(&tp->tf_timer, "[tp-tick]");
+        pr_info!("tp_init ok");
+        Ok(0)
+    }
+}
+
+pub fn tp_setparam(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<usize>{
+    unsafe{
+        let thread_clone = thread.clone();
+        let thread_clone = thread_clone.unwrap();
+        let rq = (*thread_clone.locked_data().get()).rq.unwrap();
+        let p = p.unwrap();
+        (*thread_clone.locked_data().get()).tps = &mut (*rq).tp.partitions.as_mut().unwrap()[(*p.locked_data().get()).tp.ptid as usize] as *mut rros_tp_rq;
+        (*thread_clone.locked_data().get()).state &= !T_WEAK;
+        let prio = (*p.locked_data().get()).tp.prio;
+        return rros_set_effective_thread_priority(thread.clone(), prio);
+        pr_info!("tp_setparam success!");
+    }
+}
+
+pub fn tp_getparam(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>){
+    let thread = thread.unwrap();
+    let p = p.unwrap();
+    unsafe{
+        (*p.locked_data().get()).tp.prio = (*thread.locked_data().get()).cprio;
+        let p1 = (*thread.locked_data().get()).tps;
+        let p2 = &mut (*(*thread.locked_data().get()).rq.unwrap()).tp.partitions.as_mut().unwrap()[0] as *mut rros_tp_rq;
+        (*p.locked_data().get()).tp.ptid = p1.offset_from(p2) as i32;
+    }
+}
+
+pub fn tp_trackprio(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>){
+    let thread = thread.unwrap();
+    unsafe{
+        if p.is_some(){
+            // EVL_WARN_ON(CORE,
+            //     thread->base_class == &evl_sched_tp &&
+            //     thread->tps - evl_thread_rq(thread)->tp.partitions
+            //     != p->tp.ptid);
+            let p = p.unwrap();
+            (*thread.locked_data().get()).cprio = (*p.locked_data().get()).tp.prio;
+        } else{
+            (*thread.locked_data().get()).cprio = (*thread.locked_data().get()).bprio;
+        }
+    }
+}
+
+pub fn tp_ceilprio(thread: Arc<SpinLock<rros_thread>>, mut prio: i32){
+    if prio > RROS_TP_MAX_PRIO{
+        prio = RROS_TP_MAX_PRIO;
+    }
+
+    unsafe{(*thread.locked_data().get()).cprio = prio};
+}
+
+pub fn tp_chkparam(thread: Option<Arc<SpinLock<rros_thread>>>, p: Option<Arc<SpinLock<rros_sched_param>>>) -> Result<i32>{
+    unsafe{
+        let thread = thread.unwrap();
+        let p = p.unwrap();
+        let rq = (*thread.locked_data().get()).rq.unwrap();
+        let tp = &(*rq).tp;
+
+        let prio = (*p.locked_data().get()).tp.prio;
+        let ptid = (*p.locked_data().get()).tp.ptid;
+        pr_info!("in tp_chkparam,gps = {:p}",tp.gps);
+        pr_info!("in tp_chkparam,prio = {}",prio);
+        pr_info!("in tp_chkparam,ptid = {}",ptid);
+        if (tp.gps == 0 as *mut rros_tp_schedule||
+            prio < RROS_TP_MIN_PRIO ||
+            prio > RROS_TP_MAX_PRIO ||
+            ptid < 0 ||
+            ptid >= CONFIG_RROS_SCHED_TP_NR_PART){
+            pr_warn!("tp_chkparam error");
+            return Err(kernel::Error::EINVAL);
+        }
+        // if tp.gps == 0 as *mut rros_tp_schedule{
+        //     pr_warn!("in tp_chkparam,tp.gps == 0 as *mut rros_tp_schedule");
+        //     return Err(kernel::Error::EINVAL);
+        // }
+    }
+    pr_info!("tp_chkparam success");
+    Ok(0)
+}
+
+pub fn tp_declare(thread: Option<Arc<SpinLock<rros_thread>>>,p:Option<Arc<SpinLock<rros_sched_param>>>) -> Result<i32>{
+    let thread = thread.unwrap();
+    // let p = p.unwrap();
+    unsafe{
+        let rq = (*thread.locked_data().get()).rq.unwrap();
+        (*thread.locked_data().get()).tp_link = Some(Node::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));
+        let mut tp_link = (*thread.locked_data().get()).tp_link.clone();
+        (*rq).tp.threads = Some(List::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));
+        if (*rq).tp.threads.clone().as_mut().unwrap().is_empty(){
+            pr_info!("tp.threads is empty!");
+        }
+        (*rq).tp.threads.clone().as_mut().unwrap().add_tail(tp_link.clone().as_mut().unwrap().value.clone());
+    }
+    pr_info!("tp_declare success!");
+    Ok(0)
+}
+
+pub fn tp_forget(thread:Arc<SpinLock<rros_thread>>) -> Result<usize>{
+    unsafe{
+        (*thread.locked_data().get()).tp_link.clone().as_mut().unwrap().remove();
+        (*thread.locked_data().get()).tps = 0 as *mut rros_tp_rq;
+    }
+    Ok(0)
+}
+
+pub fn tp_enqueue(thread: Arc<SpinLock<rros_thread>>) -> Result<i32>{
+    unsafe{
+        let mut head =  (*((*thread.locked_data().get()).tps)).runnable.head.as_mut().unwrap();
+        if head.is_empty(){
+            (*thread.locked_data().get()).rq_next = Some(Node::new(Arc::try_new(SpinLock::new(rros_thread::new()?))?));
+            let mut rq_next = (*thread.locked_data().get()).rq_next.clone();
+            head.add_head(rq_next.clone().as_mut().unwrap().value.clone());
+        }else{
+            let mut flag = 1; // flag指示是否到头
+            for i in head.len()..=1{
+                let thread_cprio = (*thread.locked_data().get()).cprio;
+                let cprio_in_list = (*head.get_by_index(i).unwrap().value.clone().locked_data().get()).cprio;
+                if thread_cprio <= cprio_in_list{
+                    flag = 0;
+                    let rq_next = (*thread.locked_data().get()).rq_next.clone();
+                    head.enqueue_by_index(i,rq_next.clone().as_mut().unwrap().value.clone());
+                    break;
+                }
+            }
+            if flag == 1{
+                let rq_next = (*thread.locked_data().get()).rq_next.clone();
+                head.add_head(rq_next.clone().as_mut().unwrap().value.clone());
+            }
+        }
+        Ok(0)
+    }
+}
+
+pub fn tp_dequeue(thread: Arc<SpinLock<rros_thread>>){
+    unsafe{
+        (*thread.locked_data().get()).rq_next.clone().as_mut().unwrap().remove();
+    }
+}
+
+pub fn tp_requeue(thread: Arc<SpinLock<rros_thread>>){
+    unsafe{
+        let mut head =  (*((*thread.locked_data().get()).tps)).runnable.head.as_mut().unwrap();
+        if head.is_empty(){
+            let rq_next = (*thread.locked_data().get()).rq_next.clone();
+            head.add_head(rq_next.clone().as_mut().unwrap().value.clone());
+        }else{
+            let mut flag = 1; // flag指示是否到头
+            for i in head.len()..=1{
+                let thread_cprio = (*thread.locked_data().get()).cprio;
+                let cprio_in_list = (*head.get_by_index(i).unwrap().value.clone().locked_data().get()).cprio;
+                if thread_cprio < cprio_in_list{
+                    flag = 0;
+                    let rq_next = (*thread.locked_data().get()).rq_next.clone();
+                    head.enqueue_by_index(i,rq_next.clone().as_mut().unwrap().value.clone());
+                    break;
+                }
+            }
+            if flag == 1{
+                let rq_next = (*thread.locked_data().get()).rq_next.clone();
+                head.add_head(rq_next.clone().as_mut().unwrap().value.clone());
+            }
+        }
+    }
+}
+
+pub fn tp_pick(rq: Option<*mut rros_rq>) -> Result<Arc<SpinLock<rros_thread>>>{
+    let rq = rq.unwrap();
+    unsafe{
+        let timer = Arc::into_raw((*rq).tp.tf_timer.as_mut().unwrap().clone()) as *mut SpinLock<RrosTimer> as *mut RrosTimer;
+        if rros_timer_is_running (timer) == false{
+            return Err(kernel::Error::EINVAL);
+        }
+        let head = (*(*rq).tp.tps).runnable.head.as_mut().unwrap();
+        if head.is_empty(){
+            return Err(kernel::Error::EINVAL);
+        }
+
+		let __item = head.get_head().unwrap().value.clone();	
+        (*__item.locked_data().get()).rq_next.clone().as_mut().unwrap().remove();				
+        return Ok(__item);						
+    }
+}
+
+pub fn tp_migrate(thread: Arc<SpinLock<rros_thread>>, rq: *mut rros_rq) -> Result<usize>{
+    let mut param = rros_sched_param::new();
+    unsafe{
+        param.fifo.prio = (*thread.locked_data().get()).cprio;
+        rros_set_thread_schedparam_locked(thread.clone(), Some(&rros_sched_fifo), Some(Arc::try_new(SpinLock::new(param))?));
+    }
+    Ok(0)
+}
+
+pub fn tp_show(
+    thread: *mut rros_thread,
+    buf: *mut c_types::c_char,
+    count: ssize_t,
+) -> Result<usize>{
+
+    unsafe{
+        let p1 = (*thread).tps;
+        let p2 = &mut (*(*thread).rq.unwrap()).tp.partitions.as_mut().unwrap()[0] as *mut rros_tp_rq;
+        let ptid = p1.offset_from(p2) as i32;
+        // return snprintf(buf, count, "%d\n", ptid);
+        Ok(0)
+    }
+}
+
+pub fn start_tp_schedule(rq:*mut rros_rq){
+    unsafe{
+        let mut tp = &mut (*rq).tp;
+
+        // assert_hard_lock(&rq->lock);
+
+        if tp.gps == 0 as *mut rros_tp_schedule{
+            return;
+        }
+        tp.wnext = 0;
+        tp.tf_start = rros_read_clock(&RROS_MONO_CLOCK);
+        tp_schedule_next(tp);
+    }
+}
+
+pub fn stop_tp_schedule(rq:*mut rros_rq) -> Result<usize>{
+    unsafe{
+        let tp = &mut (*rq).tp;
+        // assert_hard_lock(&rq->lock);
+        if tp.gps != 0 as *mut rros_tp_schedule{
+            rros_stop_timer(tp.tf_timer.as_mut().unwrap().clone());
+        }
+        Ok(0)
+    }
+}
+
+pub fn set_tp_schedule(rq:*mut rros_rq, gps:*mut rros_tp_schedule) -> Result<*mut rros_tp_schedule>{
+    unsafe{
+        let mut tp = &mut (*rq).tp;
+        let mut thread = Arc::try_new(SpinLock::new(rros_thread::new()?))?;
+        let mut old_gps = 0 as *mut rros_tp_schedule;
+        let mut param = rros_sched_param::new();
+        // assert_hard_lock(&rq->lock);
+        // if (EVL_WARN_ON(CORE, gps != NULL &&
+        //     (gps->pwin_nr <= 0 || gps->pwins[0].w_offset != 0)))
+        //     return tp->gps;
+        stop_tp_schedule(rq);
+        if tp.threads.clone().as_mut().unwrap().is_empty() == true{
+            old_gps = tp.gps;
+            tp.gps = gps;
+            return Ok(old_gps);
+        }
+
+        for i in 1..=tp.threads.clone().as_mut().unwrap().len(){
+            thread = tp.threads.clone().as_mut().unwrap().get_by_index(i).unwrap().value.clone();
+            param.fifo.prio = (*thread.locked_data().get()).cprio;
+            rros_set_thread_schedparam_locked(thread.clone(), Some(&rros_sched_fifo), Some(Arc::try_new(SpinLock::new(param))?));
+        }
+        old_gps = tp.gps;
+        tp.gps = gps;
+        return Ok(old_gps);
+    }
+}
+
+pub fn get_tp_schedule(rq:*mut rros_rq) -> *mut rros_tp_schedule{
+    let gps = unsafe{(*rq).tp.gps};
+
+    // assert_hard_lock(&rq->lock);
+
+    if gps == 0 as *mut rros_tp_schedule{
+        return 0 as *mut rros_tp_schedule;
+    }
+
+    unsafe{atomic_inc((*gps).refcount)};
+
+    return gps;
+}
+
+pub fn put_tp_schedule(gps:*mut rros_tp_schedule){
+    unsafe{
+        if atomic_dec_and_test((*gps).refcount) != false{
+            evl_system_heap.evl_free_chunk(gps as *mut u8);
+        }
+    }
+}
+
+pub fn tp_control(cpu: i32, ctlp: *mut rros_sched_ctlparam, infp: *mut rros_sched_ctlinfo) -> Result<ssize_t>{
+    let pt = unsafe{&(*ctlp).tp};
+    let mut offset:ktime_t = 0;
+    let mut duration:ktime_t = 0;
+    let mut next_offset:ktime_t = 0;
+    let mut gps = None;
+    let mut ogps = 0 as *mut rros_tp_schedule;
+    let mut p= 0 as *mut __rros_tp_window;
+    let mut pp= 0 as *mut __rros_tp_window;
+    let mut w = 0 as *mut rros_tp_window;
+    let mut pw = 0 as *mut rros_tp_window;
+    let mut it = 0 as *mut rros_tp_ctlinfo;
+    let flags:u32 = 0;
+    let mut rq = 0 as *mut rros_rq;
+    let mut n:i32 = 0; 
+    let mut nr_windows:i32 = 0;
+
+
+    if (cpu < 0 || !cpu_present(cpu) || !is_threading_cpu(cpu)){
+        return Err(kernel::Error::EINVAL);
+    }
+
+    rq = rros_cpu_rq(cpu);
+
+    // raw_spin_lock_irqsave(&rq->lock, flags);
+    unsafe{
+    match (pt.op){
+    0 =>{
+        if pt.nr_windows > 0{
+            // raw_spin_unlock_irqrestore(&rq->lock, flags);
+            // TODO Sizeof
+            gps = evl_system_heap.evl_alloc_chunk((8 + pt.nr_windows * 8) as usize);
+            if gps == None{
+                return Err(kernel::Error::ENOMEM);
+            }
+            let mut gps = gps.unwrap() as *mut rros_tp_schedule;
+            let mut loop_n = 0;
+            loop{
+                if loop_n == 0{
+                    n = 0;
+                    p = pt.windows;
+                    w = &mut (*gps).pwins[0] as *mut rros_tp_window;
+                    next_offset = 0;
+                }else{
+                    n += 1;
+                    p = p.offset(1);
+                    w = w.offset(1);
+                }
+                if n >= pt.nr_windows{
+                    break;
+                }
+                offset = u_timespec_to_ktime((*p).offset);
+                if offset != next_offset{
+                    evl_system_heap.evl_free_chunk(gps as *mut u8);
+                    return Err(kernel::Error::EINVAL);
+                }
+
+                duration = u_timespec_to_ktime((*p).duration);
+                if duration <= 0{
+                    evl_system_heap.evl_free_chunk(gps as *mut u8);
+                    return Err(kernel::Error::EINVAL);
+                }
+
+                if (*p).ptid < -1 || (*p).ptid >= CONFIG_RROS_SCHED_TP_NR_PART {
+                    evl_system_heap.evl_free_chunk(gps as *mut u8);
+                    return Err(kernel::Error::EINVAL);
+                }
+
+                (*w).w_offset = next_offset;
+                (*w).w_part = (*p).ptid;
+                next_offset = ktime::ktime_add(next_offset, duration);
+                loop_n += 1;
+            }
+            atomic_set((*gps).refcount, 1);
+            (*gps).pwin_nr = n;
+            (*gps).tf_duration = next_offset;
+            // raw_spin_lock_irqsave(&rq->lock, flags);
+
+            ogps = set_tp_schedule(rq, gps).unwrap();
+            // raw_spin_unlock_irqrestore(&rq->lock, flags);
+            if ogps != 0 as *mut rros_tp_schedule{
+                put_tp_schedule(ogps);
+            }
+            rros_schedule();
+            return Ok(0);
+        }
+        let mut gps = 0 as *mut rros_tp_schedule;
+        ogps = set_tp_schedule(rq, gps).unwrap();
+        // raw_spin_unlock_irqrestore(&rq->lock, flags);
+        if ogps != 0 as *mut rros_tp_schedule{
+            put_tp_schedule(ogps);
+        }
+        rros_schedule();
+        return Ok(0);
+    },
+    1 =>{
+        let mut gps = 0 as *mut rros_tp_schedule;
+        ogps = set_tp_schedule(rq, gps).unwrap();
+        // raw_spin_unlock_irqrestore(&rq->lock, flags);
+        if ogps != 0 as *mut rros_tp_schedule{
+            put_tp_schedule(ogps);
+        }
+        rros_schedule();
+        return Ok(0);
+    },
+    2 =>{
+        start_tp_schedule(rq);
+        // raw_spin_unlock_irqrestore(&rq->lock, flags);
+        rros_schedule();
+        return Ok(0);
+    },
+    3 =>{
+        stop_tp_schedule(rq);
+        // raw_spin_unlock_irqrestore(&rq->lock, flags);
+        rros_schedule();
+        return Ok(0);
+    },
+    4 =>(),
+    _ =>
+        return Err(kernel::Error::EINVAL),
+    }
+
+    let mut gps = get_tp_schedule(rq);
+    // raw_spin_unlock_irqrestore(&rq->lock, flags);
+    if gps == 0 as *mut rros_tp_schedule{
+        rros_schedule();
+        return Ok(0);
+    }
+
+    if infp == 0 as *mut rros_sched_ctlinfo {
+        put_tp_schedule(gps);
+        return Err(kernel::Error::EINVAL);
+    }
+
+    it = &mut (*infp).tp as *mut rros_tp_ctlinfo;
+    if pt.nr_windows < (*gps).pwin_nr{
+        nr_windows = pt.nr_windows;
+    }else{
+        nr_windows = (*gps).pwin_nr;
+    }
+    (*it).nr_windows = (*gps).pwin_nr;
+    let mut loop_n = 0;
+    loop{
+        if loop_n == 0{
+            n = 0;
+            p = (*it).windows;
+            pp = p;
+            w = &mut (*gps).pwins[0] as *mut rros_tp_window;
+            pw = w;
+        }else{
+            pp = p;
+            p = p.offset(1);
+            pw = w;
+            w = w.offset(1);
+            n += 1;
+        }
+        if n >= nr_windows{
+            break;
+        }
+        (*p).offset = ktime_to_u_timespec((*w).w_offset);
+        (*pp).duration = ktime_to_u_timespec(ktime::ktime_sub((*w).w_offset, (*pw).w_offset));
+        (*p).ptid = (*w).w_part;
+        loop_n += 1;
+    }
+
+    (*pp).duration = ktime_to_u_timespec(ktime::ktime_sub((*gps).tf_duration, (*pw).w_offset));
+    put_tp_schedule(gps);
+    let ret = size_of::<rros_tp_ctlinfo>() + size_of::<__rros_tp_window>() * nr_windows as usize;
+    return Ok(ret as i64);
+    }
+}
+
+// 下面几个函数应移动到clock.rs中
+pub fn u_timespec_to_ktime(u_ts: *mut __rros_timespec ) -> ktime_t{
+    unsafe{
+        let ts64 = bindings::timespec64{
+            tv_sec: (*u_ts).tv_sec,
+            tv_nsec: (*u_ts).tv_nsec,
+        };
+        return timespec64_to_ktime(ts64);
+    }
+}
+
+pub fn timespec64_to_ktime(ts:bindings::timespec64) -> ktime_t{
+	unsafe{return rust_helper_timespec64_to_ktime(ts);}
+}
+
+pub fn ktime_to_u_timespec(t:ktime_t) -> *mut __rros_timespec{
+	let ts64 = ktime_to_timespec64(t);
+
+	return &mut __rros_timespec{
+		tv_sec: ts64.tv_sec,
+		tv_nsec: ts64.tv_nsec,
+	} as *mut __rros_timespec;
+}
+
+pub fn ktime_to_timespec64(kt:ktime_t) -> bindings::timespec64{
+    unsafe{return bindings::ns_to_timespec64(kt)};
+}
+
+pub fn rros_timer_is_running(timer: *mut RrosTimer) -> bool{
+    unsafe{
+        if (*timer).get_status() & RROS_TIMER_RUNNING != 0{
+            return true;
+        }else{
+            return false;
+        }
+    }
+}
+
+
+pub fn test_tp(){
+    // pr_info!("test_tp in ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~");
+	
+}
+
+#[cfg(CONFIG_SMP)]
+fn cpu_present(cpu:i32) -> bool{
+	return cpu == 0;
+}
+
+
+#[cfg(not(CONFIG_SMP))]
+fn cpu_present(cpu:i32) -> bool{
+	return cpu == 0;
+}
\ No newline at end of file
diff --git a/kernel/rros/types.rs b/kernel/rros/types.rs
new file mode 100644
index 000000000..0ab598337
--- /dev/null
+++ b/kernel/rros/types.rs
@@ -0,0 +1,349 @@
+use kernel::bindings::{self, _raw_spin_lock_irqsave, _raw_spin_unlock_irqrestore};
+use kernel::c_types::c_ulong;
+use kernel::{container_of,init_static_sync, Opaque};
+use kernel::sync::Mutex;
+
+/// Unofficial Wrap for some binding struct.
+/// Now are mostly used in net module.
+/// Here's a simple list:
+/// * macro for list_head
+/// 
+/// 
+
+
+
+/// List macro and method
+/// 
+#[macro_export]
+macro_rules! list_entry {
+    ($ptr:expr, $type:ty, $($f:tt)*) => {{
+        unsafe{kernel::container_of!($ptr, $type, $($f)*) as *mut $type}
+    }}
+}
+
+#[macro_export]
+macro_rules! list_first_entry {
+    ($ptr:expr, $type:ty, $($f:tt)*) => {{
+        list_entry!((*$ptr).next, $type, $($f)*)
+    }}
+}
+
+#[macro_export]
+macro_rules! list_last_entry {
+    ($ptr:expr, $type:ty, $($f:tt)*) => {{
+        list_entry!((*$ptr).prev, $type, $($f)*)
+    }}
+}
+
+#[macro_export]
+macro_rules! list_next_entry {
+    ($pos:expr, $type:ty,$($f:tt)*) => {
+        unsafe {
+            let ptr = ((*$pos).$($f)*).next;
+            list_entry!(ptr,$type, $($f)*)
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_prev_entry {
+    ($pos:expr, $type:ty,$($f:tt)*) => {
+        unsafe {
+            let ptr = ((*$pos).$($f)*).prev;
+            list_entry!(ptr,$type, $($f)*)
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_entry_is_head {
+    ($pos:expr,$head:expr,$($f:tt)*) => {
+        unsafe{
+            core::ptr::eq(&(*$pos).$($f)*,$head)
+        }
+    };
+}
+
+
+#[macro_export]
+macro_rules! init_list_head {
+    ($list:expr) => {
+        extern "C"{
+            fn rust_helper_INIT_LIST_HEAD(list: *mut $crate::bindings::list_head);
+        }
+        unsafe{
+            rust_helper_INIT_LIST_HEAD($list as *mut $crate::bindings::list_head);
+        }
+    };
+}
+
+#[inline]
+pub fn list_empty(list:*const bindings::list_head) -> bool{
+    unsafe{
+        (*list).next as *const bindings::list_head == list
+    }
+}
+
+#[macro_export]
+macro_rules! list_empty {
+    ($list_head_ptr:expr) => {
+        extern "C"{
+            fn rust_helper_list_empty(list: *const $crate::bindings::list_head) -> bool;
+        }
+        unsafe{
+            rust_helper_list_empty($list_head_ptr as *const $crate::bindings::list_head)
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_del {
+    ($list:expr) => {
+        extern "C"{
+            fn rust_helper_list_del(list: *mut $crate::bindings::list_head);
+        }
+        unsafe{
+            rust_helper_list_del($list as *mut $crate::bindings::list_head);
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_del_init {
+    ($list:expr) => {
+        extern "C"{
+            fn rust_helper_list_del_init(list: *mut $crate::bindings::list_head);
+        }
+        unsafe{
+            rust_helper_list_del_init($list as *mut $crate::bindings::list_head);
+        }
+    };
+}
+
+
+/// 获取当前链表节点，并将其从链表中移出去
+#[macro_export]
+macro_rules! list_get_entry{
+    ($head:expr,$type:ty,$($f:tt)*) => {
+        {
+            let item = $crate::list_entry!($head,$type,$($f)*);
+            $crate::list_del!(&mut (*item).$($f)*);
+            item
+        }
+    };
+}
+#[macro_export]
+macro_rules! list_add_tail{
+    ($new:expr,$head:expr) => {
+        extern "C"{
+            fn rust_helper_list_add_tail(new: *mut $crate::bindings::list_head, head: *mut $crate::bindings::list_head);
+        }
+        unsafe{
+            rust_helper_list_add_tail($new as *mut $crate::bindings::list_head,$head as *mut $crate::bindings::list_head);
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_add{
+    ($new:expr,$head:expr) => {
+        extern "C"{
+            fn rust_helper_list_add(new: *mut $crate::bindings::list_head, head: *mut $crate::bindings::list_head);
+        }
+        unsafe{
+            rust_helper_list_add($new as *mut $crate::bindings::list_head,$head as *mut $crate::bindings::list_head);
+        }
+    };
+}
+
+// 常规实现
+#[macro_export]
+macro_rules! list_for_each_entry{
+    ($pos:ident,$head:expr,$type:ty,$e:block,$($f:tt)*) => {
+        let mut $pos = list_first_entry!($head,$type,$($f)*);
+        while !list_entry_is_head!($pos,$head,$($f)*){
+            $e;
+            $pos = list_next_entry!($pos,$type,$($f)*);
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_for_each_entry_safe {
+    ($pos:ident,$n:ident,$head:expr,$type:ty,$e:block,$($f:tt)*) => {
+        let mut $pos = list_first_entry!($head,$type,$($f)*);
+        let mut $n = list_next_entry!($pos,$type,$($f)*);
+        while !list_entry_is_head!($pos,$head,$($f)*){
+            $e;
+            $pos = $n;
+            $n = list_next_entry!($n,$type,$($f)*);
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_for_each_entry_reverse {
+    ($pos:ident,$head:expr,$type:ty,$e:block,$($f:tt)*) => {
+        let mut $pos = list_last_entry!($head,$type,$($f)*);
+        while !list_entry_is_head!($pos,$head,$($f)*){
+            $e;
+            $pos = list_prev_entry!($pos,$type,$($f)*);
+        }
+    };
+}
+
+#[macro_export]
+macro_rules! list_add_priff {
+    ($new:expr,$head:expr, $member_pri:ident,$member_next:ident,$tp:ty) => {
+        {
+            extern "C"{
+                fn rust_helper_list_add(new: *mut $crate::bindings::list_head, head: *mut $crate::bindings::list_head);
+            }
+            let next = unsafe{(*$head).next};
+            if core::ptr::eq(next,$head){
+                unsafe{rust_helper_list_add(unsafe{&mut (*$new).$member_next} as *mut $crate::bindings::list_head,$head as *mut $crate::bindings::list_head)};
+            }else{
+                let mut pos : *mut $tp;
+                $crate::list_for_each_entry_reverse!(pos,$head,$tp,{
+                    if unsafe{(*$new).$member_pri} <= unsafe{(*pos).$member_pri} {
+                        break;
+                    }
+                },$member_next);
+                unsafe{rust_helper_list_add(unsafe{&mut (*$new).$member_next} as *mut $crate::bindings::list_head,unsafe{&mut (*pos).$member_next} as *mut $crate::bindings::list_head)};
+            }
+        }        
+    };
+}
+
+
+
+pub struct Hashtable<const N:usize>{
+    pub table : [bindings::hlist_head; N]
+}
+
+unsafe impl<const N:usize> Sync for Hashtable<N> {}
+unsafe impl<const N:usize> Send for Hashtable<N> {}
+impl<const N:usize> Hashtable<N>{
+    pub const fn new() -> Self{
+        let table = [bindings::hlist_head{first:core::ptr::null_mut()}; N];
+        Self{
+            table:table
+        }
+    }
+    pub fn add(&mut self,node:&mut bindings::hlist_node, key: u32) {
+        extern "C" {
+            fn rust_helper_hash_add(ht: *mut bindings::hlist_head,length:usize,node: *mut bindings::hlist_node,key: u32);
+        }
+        unsafe {
+            rust_helper_hash_add(&self.table as *const _ as *mut bindings::hlist_head,N,node as *mut bindings::hlist_node,key);
+        }
+    }
+
+    pub fn del(&self,node:&mut bindings::hlist_node){
+        extern "C"{
+            fn rust_helper_hash_del(node:*mut bindings::hlist_node);
+        }
+        unsafe{
+            rust_helper_hash_del(node as *mut bindings::hlist_node);
+        }
+    }
+    pub fn head(&mut self,key :u32)->*const bindings::hlist_head{
+        extern "C"{
+            fn rust_helper_get_hlist_head(ht: *const bindings::hlist_head,length:usize,key:u32)->*const bindings::hlist_head;
+        }
+        unsafe{
+            rust_helper_get_hlist_head(&self.table as *const bindings::hlist_head,N,key)
+        }
+    }
+}
+
+#[macro_export]
+macro_rules! initialize_lock_hashtable{
+    ($name:ident,$bits_to_shift:expr) => {
+        kernel::init_static_sync! {
+            static $name: kernel::sync::Mutex<Hashtable::<$bits_to_shift>> = Hashtable::<$bits_to_shift>::new();
+        }
+    }
+}
+
+#[macro_export]
+macro_rules! hlist_entry{
+    ($ptr:expr,$type:ty,$($f:tt)*) =>{
+        kernel::container_of!($ptr,$type,$($f)*)
+    }
+}
+#[macro_export]
+macro_rules! hlist_entry_safe{
+    ($ptr:expr,$type:ty,$($f:tt)*) =>{
+        if ($ptr).is_null(){
+            core::ptr::null()
+        }else{
+            kernel::container_of!($ptr,$type,$($f)*)
+        }
+    }
+}
+
+#[macro_export]
+macro_rules! hash_for_each_possible {
+    ($pos:ident,$head:expr,$type:ty,$member:ident,{ $($block:tt)* } ) => {
+        let mut $pos = $crate::hlist_entry_safe!(unsafe{(*$head).first},$type,$member);
+        while(!$pos.is_null()){
+            // $code
+            $($block)*
+            $pos = $crate::hlist_entry_safe!(unsafe{(*$pos).$member.next},$type,$member);
+        }
+    };
+}
+
+macro_rules! bits_to_long {
+    ($bits:expr) => {
+        ((($bits) + (64) - 1) / (64)) 
+    };
+}
+
+#[macro_export]
+macro_rules! DECLARE_BITMAP {
+    ($name:ident,$bits:expr) => {
+        static mut $name: [usize;bits_to_long!($bits)] = [0;bits_to_long!($bits)];
+    };
+}
+
+// pub struct HardSpinlock<T>{
+//     spin_lock: Opaque<bindings::spinlock>,
+//     flags:usize,
+//     _pin: PhantomPinned,
+//     data: UnsafeCell<T>,
+// }
+
+// unsafe impl Sync for HardSpinlock {}
+// unsafe impl Send for HardSpinlock {}
+
+// impl<T> HardSpinlock<T>{
+//     pub fn new(data:T) -> Self{
+//         extern "C"{
+//             fn rust_helper_raw_spin_lock_init(lock:*mut bindings::spinlock_t);
+//         }
+//         let t = bindings::hard_spinlock_t::default();
+//         unsafe{
+//             rust_helper_raw_spin_lock_init(&t as *const _ as *mut bindings::spinlock_t);
+//         }
+//         Self{
+//             spin_lock : Opaque(t),
+//             flags:0,
+//             _pin:PhantomPinned,
+//             data:UnsafeCell::new(data),
+//         }
+//     }
+
+//     pub fn lock(&mut self) -> usize{
+//         unsafe{
+//             _raw_spin_lock_irqsave(&mut self.0 as *const _ as *mut bindings::raw_spinlock_t) as usize
+//         }
+//     }
+
+//     pub fn unlock(&mut self,flags:usize){
+//         unsafe{
+//             _raw_spin_unlock_irqrestore(&mut self.0 as *const _ as *mut bindings::raw_spinlock_t,flags as c_ulong);
+//         }
+//     }
+// }
\ No newline at end of file
diff --git a/kernel/rros/types_test.rs b/kernel/rros/types_test.rs
new file mode 100644
index 000000000..db73536f1
--- /dev/null
+++ b/kernel/rros/types_test.rs
@@ -0,0 +1,392 @@
+use core::ptr::NonNull;
+
+use crate::types::{Hashtable};
+use kernel::bindings;
+use kernel::sync::SpinLock;
+use kernel::pr_info;
+
+use kernel::linked_list::{Links, GetLinks, List};
+use kernel::prelude::*;
+use crate::types::list_empty;
+
+
+struct HashEntry{
+    pub data : u32,
+    pub hash : bindings::hlist_node
+}
+
+impl HashEntry{
+    pub fn new(val:u32) -> Self{
+        HashEntry { data: val, hash: bindings::hlist_node::default() }
+    }
+}
+
+initialize_lock_hashtable!(table,256);
+
+fn test_hashtable(){
+    let mut result = [0u32;10];
+    let mut entry1 = HashEntry::new(1);
+    let mut entry2 = HashEntry::new(2);
+    let mut entry3 = HashEntry::new(3);
+    let mut table_guard = table.lock();
+    table_guard.add(&mut entry1.hash, 1);
+    table_guard.add(&mut entry2.hash, 1);
+    table_guard.add(&mut entry3.hash, 1);
+    let mut counter = 0;
+    hash_for_each_possible!(cur,table_guard.head(1),HashEntry,hash,{
+        result[counter] = unsafe{(*cur).data};
+        counter+=1;
+    });
+    assert_eq!(result[0],3);
+    assert_eq!(result[1],2);
+    assert_eq!(result[2],1);
+    table_guard.del(&mut entry2.hash);
+
+    counter = 0;
+    hash_for_each_possible!(cur,table_guard.head(1),HashEntry,hash,{
+        result[counter] = unsafe{(*cur).data};
+        counter+=1;
+    });
+    assert_eq!(result[0],3);
+    assert_eq!(result[1],1);
+
+    pr_info!("hash table test ok.");
+}   
+
+
+struct ListTest2 {
+    num: i32,
+    head: kernel::bindings::list_head,
+}
+
+impl ListTest2 {
+    fn new(num: i32) -> ListTest2 {
+        ListTest2 {
+            num,
+            head: kernel::bindings::list_head::default(),
+        }
+    }
+    fn init_list_head(&mut self){
+        init_list_head!(&mut self.head);
+    }
+}
+
+pub fn test_list_macro(){
+    extern "C"{
+        fn rust_helper_list_add_tail(new: *mut kernel::bindings::list_head, head: *mut kernel::bindings::list_head);
+        fn rust_helper_list_del_init(list: *mut kernel::bindings::list_head);
+    }
+    let mut t1 = ListTest2::new(
+        1,
+    );
+    let mut t2 = ListTest2::new(
+        2,
+    );
+    let mut t3 = ListTest2::new(
+        3,
+    );
+    t1.init_list_head();
+    t2.init_list_head();
+    t3.init_list_head();
+
+    let mut head = kernel::bindings::list_head::default();
+    init_list_head!(&mut head);
+    unsafe{
+        rust_helper_list_add_tail(&mut t1.head,&mut head);
+        rust_helper_list_add_tail(&mut t2.head,&mut head);
+        rust_helper_list_add_tail(&mut t3.head,&mut head);
+    }
+    let x= list_prev_entry!(&mut t2,ListTest2,head);
+    assert_eq!(unsafe{(*x).num},1);
+    let x= list_next_entry!(&mut t1,ListTest2,head);
+    assert_eq!(unsafe{(*x).num},2);
+
+
+    let mut array = [0;10];
+    let mut counter = 0;
+    list_for_each_entry!(cur,&head,ListTest2,{
+        // unsafe{
+        //     pr_info!("cur num is {}",(*cur).num);
+        // }
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+
+    },head);
+    assert_eq!(counter,3);
+    assert_eq!(array[0],1);
+    assert_eq!(array[1],2);
+    assert_eq!(array[2],3);
+
+    counter = 0;
+    list_for_each_entry_reverse!(cur,&head,ListTest2,{
+        // unsafe{
+        //     pr_info!("cur num is {}",(*cur).num);
+        // }
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+    },head);
+    assert_eq!(counter,3);
+    assert_eq!(array[0],3);
+    assert_eq!(array[1],2);
+    assert_eq!(array[2],1);
+
+
+    list_for_each_entry_safe!(cur,next,&head,ListTest2,{
+        unsafe{
+            // pr_info!("cur num is {}",(*cur).num);
+            if (*cur).num == 2 {
+                rust_helper_list_del_init(&mut (*cur).head);
+            }
+        }
+    },head);
+
+    counter = 0;
+    list_for_each_entry!(cur,&head,ListTest2,{
+        // unsafe{
+        //     pr_info!("cur num is {}",(*cur).num);
+        // }
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+    },head);
+    assert_eq!(counter,2);
+    assert_eq!(array[0],1);
+    assert_eq!(array[1],3);
+
+    counter = 0;
+    list_for_each_entry_reverse!(cur,&head,ListTest2,{
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+    },head);
+    assert_eq!(counter,2);
+    assert_eq!(array[0],3);
+    assert_eq!(array[2],1);
+
+    pr_info!("list_for_each ok!. ")
+}
+
+// struct KernelListExample{
+//     num: i32,
+//     head: Links<KernelListExample>
+// }
+
+// impl KernelListExample{
+//     fn new(num: i32) -> KernelListExample{
+//         KernelListExample{
+//             num,
+//             head: Links::new()
+//         }
+//     }
+// }
+
+// impl GetLinks for KernelListExample{
+//     type EntryType = KernelListExample;
+//     fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType>{
+//         &data.head
+//     }
+// }
+
+
+// pub fn test_kernel_list(){
+//     let mut list: List<&KernelListExample> = List::new();
+//     let data1 = KernelListExample::new(5);
+//     let data2 = KernelListExample::new(6);
+//     let data3 = KernelListExample::new(7);
+
+//     list.push_back(&data1);
+//     list.push_back(&data2);
+//     list.push_back(&data3);
+
+// }
+
+
+pub fn test_evl_list_add_priff(){
+    extern "C"{
+        fn rust_helper_INIT_LIST_HEAD(list: *mut kernel::bindings::list_head);
+    }
+    let mut t1 = ListTest2::new(
+        1,
+    );
+    let mut t2 = ListTest2::new(
+        2,
+    );
+    let mut t3 = ListTest2::new(
+        3,
+    );
+    let mut head = kernel::bindings::list_head::default();
+
+    // test 1
+    t1.init_list_head();
+    t2.init_list_head();
+    t3.init_list_head();
+    unsafe{
+        rust_helper_INIT_LIST_HEAD(&mut head as *mut kernel::bindings::list_head);
+    }
+
+    list_add_priff!(&mut t3,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t2,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t1,&mut head,num,head,ListTest2);
+
+    let mut array = [0;10];
+    let mut counter = 0;
+    list_for_each_entry!(cur,&head,ListTest2,{
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+
+    },head);
+    assert_eq!(counter,3);
+    assert_eq!(array[0],3);
+    assert_eq!(array[1],2);
+    assert_eq!(array[2],1);
+
+    // test2 
+    t1.init_list_head();
+    t2.init_list_head();
+    t3.init_list_head();
+    unsafe{
+        rust_helper_INIT_LIST_HEAD(&mut head as *mut kernel::bindings::list_head);
+    }
+
+    list_add_priff!(&mut t2,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t3,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t1,&mut head,num,head,ListTest2);
+
+    let mut array = [0;10];
+    let mut counter = 0;
+    list_for_each_entry!(cur,&head,ListTest2,{
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+
+    },head);
+    assert_eq!(counter,3);
+    assert_eq!(array[0],3);
+    assert_eq!(array[1],2);
+    assert_eq!(array[2],1);
+
+    // test3 
+    t1.init_list_head();
+    t2.init_list_head();
+    t3.init_list_head();
+    unsafe{
+        rust_helper_INIT_LIST_HEAD(&mut head as *mut kernel::bindings::list_head);
+    }
+
+    list_add_priff!(&mut t1,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t3,&mut head,num,head,ListTest2);
+    list_add_priff!(&mut t2,&mut head,num,head,ListTest2);
+
+    let mut array = [0;10];
+    let mut counter = 0;
+    list_for_each_entry!(cur,&head,ListTest2,{
+        array[counter] = unsafe{(*cur).num};
+        counter += 1;
+
+    },head);
+    assert_eq!(counter,3);
+    assert_eq!(array[0],3);
+    assert_eq!(array[1],2);
+    assert_eq!(array[2],1);
+
+    pr_info!("test list_add_priff(evl priority function) ok!")
+}
+
+struct KernelListTest {
+    pub prio: i32,
+    head: Links<KernelListTest>,
+}
+impl GetLinks for KernelListTest {
+    type EntryType = KernelListTest;
+    fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType> {
+        &data.head
+    }
+}
+
+impl KernelListTest {
+    fn new(prio: i32) -> KernelListTest {
+        KernelListTest {
+            prio,
+            head: Links::new(),
+        }
+    }
+}
+
+fn add_by_prio(list:&mut List<Box<KernelListTest>>,item:Box<KernelListTest>){
+    if list.is_empty(){
+        list.push_back(item);
+        return;
+    }else {
+        let mut last = list.cursor_back_mut();
+        while let Some(cur) = last.current(){
+            if item.prio <= cur.prio{
+                let cur = NonNull::new(cur as *const _ as *mut KernelListTest).unwrap();
+                unsafe{list.insert_after(cur, item)};
+                return;
+            }
+            last.move_prev();
+        }
+        list.push_front(item);
+    }
+}
+pub fn test_kernel_list(){
+    {
+        let prio0 = Box::try_new(KernelListTest::new(0)).unwrap();
+
+        let prio1 = Box::try_new(KernelListTest::new(1)).unwrap();
+        let prio2 = Box::try_new(KernelListTest::new(2)).unwrap();
+        let prio3 = Box::try_new(KernelListTest::new(3)).unwrap();
+    
+        let mut list: List<Box<KernelListTest>> = List::new();
+        list.push_front(prio0);
+        list.push_front(prio1);
+        list.push_front(prio2);
+        list.push_front(prio3);
+    
+        let mut data = [0;5];
+        let mut counter = 0;
+        let mut start = list.cursor_front();
+        while let Some(item) = start.current(){
+            data[counter] = item.prio;
+            counter += 1;
+            start.move_next();
+        }  
+        assert_eq!(counter,4);
+        assert_eq!(data[0],3);
+        assert_eq!(data[1],2);
+        assert_eq!(data[2],1);
+        assert_eq!(data[3],0);
+    }
+    
+    {
+        let prio0 = Box::try_new(KernelListTest::new(0)).unwrap();
+
+        let prio1 = Box::try_new(KernelListTest::new(1)).unwrap();
+        let prio2 = Box::try_new(KernelListTest::new(2)).unwrap();
+        let prio3 = Box::try_new(KernelListTest::new(3)).unwrap();
+
+        let mut list: List<Box<KernelListTest>> = List::new();
+        add_by_prio(&mut list,prio3);
+        add_by_prio(&mut list,prio0);
+        add_by_prio(&mut list,prio2);
+        add_by_prio(&mut list,prio1);
+
+
+        let mut data = [0;5];
+        let mut counter = 0;
+        let mut start = list.cursor_front();
+        while let Some(item) = start.current(){
+            data[counter] = item.prio;
+            counter += 1;
+            start.move_next();
+        }  
+    }
+    pr_info!("test kernel list ok!");
+
+}
+
+
+pub fn run_tests(){
+    test_hashtable();
+    test_list_macro();
+    test_kernel_list();
+}
+
+
diff --git a/kernel/rros/uapi/Makefile b/kernel/rros/uapi/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/uapi/mod.rs b/kernel/rros/uapi/mod.rs
new file mode 100644
index 000000000..effc1be49
--- /dev/null
+++ b/kernel/rros/uapi/mod.rs
@@ -0,0 +1 @@
+pub mod rros;
diff --git a/kernel/rros/uapi/rros/Makefile b/kernel/rros/uapi/rros/Makefile
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/uapi/rros/mod.rs b/kernel/rros/uapi/rros/mod.rs
new file mode 100644
index 000000000..41cfad97b
--- /dev/null
+++ b/kernel/rros/uapi/rros/mod.rs
@@ -0,0 +1,2 @@
+pub mod syscall;
+pub mod thread;
diff --git a/kernel/rros/uapi/rros/syscall.rs b/kernel/rros/uapi/rros/syscall.rs
new file mode 100644
index 000000000..9ac360569
--- /dev/null
+++ b/kernel/rros/uapi/rros/syscall.rs
@@ -0,0 +1,5 @@
+pub const sys_evl_read: u32 = 0;
+pub const sys_evl_write: u32 = 1;
+pub const sys_evl_ioctl: u32 = 2;
+
+pub const NR_EVL_SYSCALLS: u32 = 3;
diff --git a/kernel/rros/uapi/rros/thread.rs b/kernel/rros/uapi/rros/thread.rs
new file mode 100644
index 000000000..81f47bee0
--- /dev/null
+++ b/kernel/rros/uapi/rros/thread.rs
@@ -0,0 +1 @@
+pub const EVL_HMDIAG_SYSDEMOTE: i32 = 2;
diff --git a/kernel/rros/wait.rs b/kernel/rros/wait.rs
index 2aaf6bf63..b8718ec2f 100644
--- a/kernel/rros/wait.rs
+++ b/kernel/rros/wait.rs
@@ -1,23 +1,292 @@
-use crate::{bindings, clock::RrosClock, thread::RrosKthread};
-use alloc::rc::Rc;
-use core::cell::RefCell;
+use crate::{clock::{RrosClock, RROS_MONO_CLOCK}, thread::{RrosKthread, T_RMID,evl_wakeup_thread, T_PEND, rros_current, T_NOMEM, T_TIMEO, T_BREAK, T_WOLI, rros_notify_thread, evl_sleep_on, T_BCAST, KthreadRunner}, sched::{rros_thread, RrosThreadWithLock, rros_schedule, RrosValue},uapi::rros::thread::EVL_HMDIAG_SYSDEMOTE, timeout::{self, timeout_nonblock, RROS_INFINITE}, types::list_empty};
+use alloc::{rc::Rc, sync::Arc, boxed::Box};
+use core::{cell::RefCell, ptr::NonNull, clone::Clone, ops::FnOnce, sync::atomic::AtomicBool};
+use core::ops::FnMut;
 
 use kernel::{
-    str::CStr,
-    double_linked_list::List
+    bindings,
+    str::CStr, sync::{SpinLock, Lock}, ktime::KtimeT, linked_list::{List, GetLinks},
+    Result
 };
 
-struct WaitChannelNode {
-    pub reorder_wait: Option<fn(Rc<RefCell<RrosKthread>>, Rc<RefCell<RrosKthread>>) -> i32>, // int
-    pub follow_depend: Option<fn(Rc<RefCell<WaitChannel>>, Rc<RefCell<RrosKthread>>) -> i32>, // int
-    pub name: &'static CStr,
+pub const RROS_WAIT_PRIO : usize = 1 << 0;
+
+pub struct RrosWaitChannel {
+    // pub name: &'static CStr,
+    pub wait_list : List<Arc<RrosThreadWithLock>>,
+    pub reorder_wait:Option<fn(waiter: Arc<SpinLock<rros_thread>>, originator: Arc<SpinLock<rros_thread>>) -> Result<i32>>,
+    pub follow_depend: Option<fn(wchan: Arc<SpinLock<RrosWaitChannel>>, originator: Arc<SpinLock<rros_thread>>) -> Result<i32>>
 }
 
-pub type WaitChannel = List<WaitChannelNode>;
+
 
 pub struct RrosWaitQueue {
     pub flags: i32,
     pub clock: *mut RrosClock,
-    pub wchan: Rc<RefCell<WaitChannel>>,
-    pub lock: bindings::hard_spinlock_t,
+    pub wchan: RrosWaitChannel,
+    pub lock : bindings::hard_spinlock,
+}
+
+
+impl RrosWaitQueue{
+    pub fn new(clock:*mut RrosClock,flags:i32) -> Self{
+        let mut wait = RrosWaitQueue{
+            flags,
+            clock,
+            wchan: RrosWaitChannel{
+                wait_list: List::new(),
+                reorder_wait: None,
+                follow_depend: None
+            },
+            lock: bindings::hard_spinlock::default()
+        };
+        wait.init(clock,flags);
+        wait
+    }
+    // 删掉了key:*mut bindings::lock_class_key,和name:&'static CStr
+    pub fn init(&mut self,clock:*mut RrosClock,flags:i32){
+        extern "C"{
+            fn rust_helper_raw_spin_lock_init(lock: *mut bindings::hard_spinlock_t);
+        }
+        self.flags = flags;
+        self.clock = clock;
+        unsafe{rust_helper_raw_spin_lock_init(&mut self.lock)}
+        // self.wchan.name = name;
+        // self.wchan.wait_list.
+        // init_list_head!(&self.wchan.wait_list);
+	    // lockdep_set_class_and_name(&wq->lock, key, name);
+        // 这个是调试时候用的
+    }
+    pub fn flush(&mut self,reason:i32){
+        // evl_flush_wait
+        let flags = unsafe{
+            bindings::_raw_spin_lock_irqsave(&mut self.lock as *const _ as *mut bindings::raw_spinlock)
+        };
+        self.flush_locked(reason);
+        unsafe{bindings::_raw_spin_unlock_irqrestore(&mut self.lock as *const _ as *mut bindings::raw_spinlock, flags)};
+    }
+    pub fn flush_locked(&mut self,reason:i32){
+        // evl_flush_wait_locked
+	    // trace_evl_flush_wait(wq);
+        let list =  &mut self.wchan.wait_list;
+        while let Some(waiter) = list.pop_front(){
+            // locked_thread  
+            // rewrap    
+            let waiter = unsafe{RrosThreadWithLock::transmute_to_original(waiter)};
+            evl_wakeup_thread(waiter,T_PEND,reason);
+        }
+    }
+
+    pub fn destory(&mut self){
+        self.flush(T_RMID as i32);
+        unsafe{rros_schedule()};
+    }
+
+    pub fn wake_up(&mut self,waiter:*mut rros_thread,reason:i32) -> Option<Arc<SpinLock<rros_thread>>>{
+	    // trace_evl_wake_up(wq);
+        // assert!(self.lock) //TODO:
+        if self.wchan.wait_list.is_empty(){
+            return None
+        }else{
+            if waiter.is_null(){
+                let list = &mut self.wchan.wait_list;
+                let waiter = list.pop_front().unwrap();
+                let waiter = unsafe{RrosThreadWithLock::transmute_to_original(waiter)};
+                evl_wakeup_thread(waiter.clone(),T_PEND,reason);
+                Some(waiter)
+            }else{
+                unimplemented!()
+                // let mut list = self.wchan.wait_list;
+                // let thread = RrosThreadWithLock();
+                // unsafe{list.remove(&thread)};
+                // evl_wakeup_thread(thread, T_PEND, reason);
+            }
+        }
+    }
+
+    pub fn locked_add(&mut self,timeout:KtimeT,timeout_mode:timeout::rros_tmode){
+        // evl_add_wait_queue
+        extern "C"{
+            fn rust_helper_atomic_read(v:*mut bindings::atomic_t) ->i32;
+        }
+        let curr = unsafe{&mut *(*rros_current()).locked_data().get()};
+
+        // assert!(self.lock) //TODO:
+        
+        if curr.state & T_WOLI != 0  && 
+            unsafe{rust_helper_atomic_read(&mut curr.inband_disable_count) > 0}{
+                rros_notify_thread(unsafe{curr as *const _ as *mut rros_thread},EVL_HMDIAG_SYSDEMOTE as u32,RrosValue::new_nil());
+            }
+        if self.flags as usize & RROS_WAIT_PRIO == 0{
+            let current = unsafe {
+                RrosThreadWithLock::new_from_curr_thread()
+            };
+
+            self.wchan.wait_list.push_back(current);
+        }else{
+            if self.wchan.wait_list.is_empty(){
+                let current = unsafe {
+                    RrosThreadWithLock::new_from_curr_thread()
+                };
+                self.wchan.wait_list.push_back(current)
+            }else{
+                // 按优先级加入，可以看下types_test.rs的add_by_prio
+                let prio = curr.wprio;
+                let mut last = self.wchan.wait_list.cursor_back_mut();
+                let mut stop_flag = false;
+                while let Some(cur) = last.current(){
+                    if prio <= (*cur).get_wprio(){
+                        let cur = NonNull::new(cur as *const _ as *mut RrosThreadWithLock).unwrap();
+                        let item = unsafe {
+                            RrosThreadWithLock::new_from_curr_thread()
+                        };
+                        unsafe{self.wchan.wait_list.insert_after(cur, item)};
+                        stop_flag = true;
+                        break;
+                    }
+                    last.move_prev();
+                }
+                if !stop_flag{
+                    let item = unsafe {
+                        RrosThreadWithLock::new_from_curr_thread()
+                    };
+                    self.wchan.wait_list.push_front(item);
+                }
+            }
+        }
+        evl_sleep_on(timeout, timeout_mode, unsafe{&*self.clock}, &mut self.wchan as *mut RrosWaitChannel); // 必须保证wchan不会被释放
+    }
+
+    pub fn wait_schedule(&mut self) -> i32{
+        /// evl_wait_schedule
+        let curr: *mut SpinLock<rros_thread> = rros_current();
+
+        unsafe{rros_schedule()};
+
+    	// trace_evl_finish_wait(wq);
+
+        let info = unsafe{(*(*rros_current()).locked_data().get()).info};
+        if info & T_RMID != 0{
+            return -(bindings::EIDRM as i32);
+        }
+        if info & T_NOMEM != 0{
+            return -(bindings::ENOMEM as i32)
+        }
+        
+        let mut ret = 0;
+        if info & (T_TIMEO | T_BREAK) != 0{
+            let flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut self.lock as *const _ as *mut bindings::raw_spinlock)};
+            if !self.wchan.wait_list.is_empty(){
+                let r = unsafe{RrosThreadWithLock::new_from_curr_thread()};
+                unsafe{self.wchan.wait_list.remove(&r)};
+                if info & T_TIMEO != 0{
+                    ret = -(bindings::ETIMEDOUT as i32);
+                }else if (info & T_BREAK != 0){
+                    
+                    ret = -(bindings::EINTR as i32);
+                }
+            }
+            unsafe{bindings::_raw_spin_unlock_irqrestore(&mut self.lock as *const _ as *mut bindings::raw_spinlock, flags)}
+        }
+        return ret;
+        // else if (IS_ENABLED(CONFIG_EVL_DEBUG_CORE)) {
+        //     bool empty;
+        //     raw_spin_lock_irqsave(&wq->lock, flags);
+        //     empty = list_empty(&curr->wait_next);
+        //     raw_spin_unlock_irqrestore(&wq->lock, flags);
+        //     EVL_WARN_ON_ONCE(CORE, !empty);
+        // }
+    }
+
+    #[inline]
+    pub fn wait_timeout(&mut self,timeout:KtimeT,time_mode:timeout::rros_tmode,mut get_cond:impl FnMut()->bool)->i32{
+        // implementation of evl_wait_event_timeout
+        
+        let mut flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut self.lock as *const _ as *mut bindings::raw_spinlock)};
+        let mut ret : i32 = 0;
+        if !get_cond(){
+            if timeout_nonblock(timeout){
+                ret = -(bindings::EAGAIN as i32);
+            } else{
+                let mut bcast = 0;
+                loop{
+                    self.locked_add(timeout, time_mode);
+                    unsafe{bindings::_raw_spin_unlock_irqrestore(&mut self.lock as *const _ as *mut bindings::raw_spinlock, flags)};
+                    ret = self.wait_schedule();
+                    bcast = unsafe{  (*(*rros_current()).locked_data().get()).info } & T_BCAST;
+                    flags = unsafe{bindings::_raw_spin_lock_irqsave(&mut self.lock as *const _ as *mut bindings::raw_spinlock)};
+                    if(ret!=0 || get_cond() || bcast != 0){
+                        break;
+                    }
+                }
+            }
+        }
+        unsafe{bindings::_raw_spin_unlock_irqrestore(&mut self.lock as *const _ as *mut bindings::raw_spinlock, flags)};
+        ret
+    }
+    pub fn is_active(&self) ->bool{
+        // evl_wait_active
+        !self.wchan.wait_list.is_empty()
+    }
+    
+    pub fn wake_up_head(&mut self) -> Option<Arc<SpinLock<rros_thread>>>{
+        self.wake_up(core::ptr::null_mut(), 0)
+    }
+
+    // pub fn add_wait_queue(&mut self,timeout:KtimeT,timeout_mode:timeout::rros_tmode){
+    //     unsafe{
+    //         fn rust_helper_atomic_read(v: *mut bindings::atomic_t) -> i32;
+    //     }
+    //     //evl_add_wait_queue, wq->lock held, hard irqs off
+    //     let curr = rros_current();
+
+    // 	// assert_hard_lock(&wq->lock);
+    //     let ref_curr = &mut unsafe{(*(*curr).locked_data().get())};
+    //     if (ref_curr.state & T_WOLI != 0  && rust_helper_atomic_read(&ref_curr.inband_disable_count) > 0){
+    //         rros_notify_thread(ref_curr, EVL_HMDIAG_LKSLEEP as u32, RrosValue::new_nil());
+    //     }
+
+    //     if self.flags & RROS_WAIT_PRIO ==0{
+    //         list_add
+    //     }
+    // }
 }
+
+
+pub fn wait_test(){
+    use kernel::prelude::*;
+    use kernel::c_str;
+    use core::sync::atomic::Ordering::Relaxed;
+    let mut queue = unsafe{RrosWaitQueue::new(&mut RROS_MONO_CLOCK as *mut RrosClock, RROS_WAIT_PRIO as i32)};
+    let ptr_queue = unsafe{&mut queue as *mut RrosWaitQueue};
+    let mut flag = AtomicBool::new(false);
+    let flag_ptr = &mut flag as *mut AtomicBool;
+    let mut runner = KthreadRunner::new_empty();
+    runner.init(Box::try_new(move||{
+        for i in 0..4{
+            unsafe{(*ptr_queue).wait_timeout(RROS_INFINITE, timeout::rros_tmode::RROS_REL, ||{
+                if flag.load(Relaxed)==true{
+                    flag.store(false, Relaxed);
+                    true
+                }else{
+                    false
+                }
+            })};
+        }
+    }).unwrap());
+    runner.run(c_str!("test wait"), 30);
+    for i in 0..4{
+        unsafe{bindings::usleep_range(10000,20000)};
+        pr_info!("try to wake up thread");
+        let flags = unsafe{
+            bindings::_raw_spin_lock_irqsave(&mut (*ptr_queue).lock as *const _ as *mut bindings::raw_spinlock)
+        };
+        unsafe{(*flag_ptr).store(true, Relaxed)};
+        
+        unsafe{
+            (*ptr_queue).flush_locked(0)
+        }
+        unsafe{bindings::_raw_spin_unlock_irqrestore(&mut (*ptr_queue).lock as *const _ as *mut bindings::raw_spinlock, flags)};
+    }
+    pr_info!("wait test done")
+}
\ No newline at end of file
diff --git a/kernel/rros/weak.rs b/kernel/rros/weak.rs
index dd5ed86b5..4878204c0 100644
--- a/kernel/rros/weak.rs
+++ b/kernel/rros/weak.rs
@@ -1,9 +1,7 @@
-use core::ptr::{null_mut, null};
-use crate::{
-    queue, sched, thread::*,
-};
+use crate::{queue, sched, thread::*};
 use alloc::rc::Rc;
 use core::cell::RefCell;
+use core::ptr::{null, null_mut};
 use kernel::{prelude::*, Error};
 
 pub static mut rros_sched_weak: Rc<RefCell<sched::rros_sched_class>> = sched::rros_sched_class {
@@ -29,6 +27,8 @@ pub static mut rros_sched_weak: Rc<RefCell<sched::rros_sched_class>> = sched::rr
     sched_control: None,
     nthreads: 0,
     next: None,
+    // FIXME: make sure is this correct? 
+    flag: 0,
 };
 
 const RROS_WEAK_MIN_PRIO: i32 = 0;
diff --git a/kernel/rros/work.rs b/kernel/rros/work.rs
new file mode 100644
index 000000000..e33ab35d6
--- /dev/null
+++ b/kernel/rros/work.rs
@@ -0,0 +1,84 @@
+use core::mem::MaybeUninit;
+
+use kernel::{bindings, c_types::c_void, irq_work::IrqWork, container_of, Result};
+
+use crate::factory::RrosElement;
+pub struct RrosWork{
+    irq_work : IrqWork,
+    wq_work : bindings::work_struct,
+    wq : *mut bindings::workqueue_struct,
+    pub handler : Option<fn (arg:&mut RrosWork) ->i32>,
+    element :Option<*mut RrosElement> // 目前用不到，置为None
+}
+
+
+fn do_wq_work(wq_work : *mut bindings::work_struct){
+    let work = container_of!(wq_work, RrosWork, wq_work);
+    let handler =unsafe{
+        (*work).handler.unwrap()
+    };
+    let work = unsafe{&mut *(work as *mut RrosWork)};
+    handler(work);
+
+    // TODO:     
+    // if (work->element)
+    // evl_put_element(work->element);
+}
+
+unsafe extern "C" fn do_irq_work(irq_work : *mut bindings::irq_work){
+    extern "C"{
+        fn rust_helper_queue_work(wq:*mut bindings::workqueue_struct,work:*mut bindings::work_struct) -> bool;
+    }
+    let work = container_of!(irq_work, RrosWork, irq_work);
+    // TODO: 没有实现evl_put_element
+    // if unsafe{rust_helper_queue_work((*work).wq,&mut (*work).wq_work)} && unsafe{(*)}
+    // if (!queue_work(work->wq, &work->wq_work) && work->element)
+    // evl_put_element(work->element);
+}
+
+impl RrosWork{
+    pub const fn new() -> Self{
+        unsafe{
+            core::mem::transmute::<[u8; core::mem::size_of::<Self>()], Self>( [0; core::mem::size_of::<Self>()])
+        }
+        // RrosWork{
+        //     element : None,
+        //     handler : None,
+        //     wq : core::ptr::null_mut(),
+        //     wq_work : bindings::work_struct{
+        //         data : bindings::atomic64_t { counter: 0 },
+        //         entry : bindings::list_head{
+        //             next : core::ptr::null_mut(),
+        //             prev : core::ptr::null_mut(),
+        //         },
+        //         func : None
+        //     },
+        //     irq_work : IrqWork::new()
+        // }
+    }
+    pub fn init(&mut self,handler : fn (arg:&mut RrosWork) ->i32){
+        extern "C"{
+            fn rust_helper_init_work(work:*mut bindings::work_struct,func:fn(*mut bindings::work_struct));
+        }
+        self.irq_work.init_irq_work(do_irq_work);
+        unsafe{rust_helper_init_work(&mut self.wq_work, do_wq_work)};
+        self.handler = Some(handler);
+        self.element = None;
+
+    }
+    pub fn call_inband_from(&mut self,wq:*mut bindings::workqueue_struct){
+        self.wq = wq;
+        // TODO: 没有实现evl_put_element
+        // if (work->element)
+		// evl_get_element(work->element);
+        // if (!irq_work_queue(&work->irq_work) && work->element)
+		// evl_put_element(work->element);
+        // unsafe{rust_helper_queue_work(wq,&mut self.wq_work)};
+    }
+
+    #[inline]
+    pub fn call_inband(&mut self){
+        self.call_inband_from(unsafe{bindings::system_wq});
+    }
+}
+
diff --git a/mm/slub.c b/mm/slub.c
index 61bd40e3e..728760b33 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1770,8 +1770,8 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	flags &= gfp_allowed_mask;
 
-	if (gfpflags_allow_blocking(flags))
-		local_irq_enable();
+	// if (gfpflags_allow_blocking(flags))
+	// 	local_irq_enable();
 
 	flags |= s->allocflags;
 
diff --git a/rust/alloc/alloc_rros.rs b/rust/alloc/alloc_rros.rs
new file mode 100644
index 000000000..a461a3662
--- /dev/null
+++ b/rust/alloc/alloc_rros.rs
@@ -0,0 +1,210 @@
+// SPDX-License-Identifier: Apache-2.0 OR MIT
+
+//! Memory allocation APIs
+
+#![stable(feature = "alloc_module", since = "1.28.0")]
+
+#[cfg(not(test))]
+use core::intrinsics;
+use core::intrinsics::{min_align_of_val, size_of_val};
+
+use core::ptr::Unique;
+#[cfg(not(test))]
+use core::ptr::{self, NonNull};
+
+#[stable(feature = "alloc_module", since = "1.28.0")]
+#[doc(inline)]
+pub use core::alloc::*;
+
+#[cfg(test)]
+mod tests;
+
+extern "Rust" {
+    #[rustc_allocator]
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc(size: usize, align: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_dealloc(ptr: *mut u8, size: usize, align: usize);
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc_zerod(size: usize, align: usize) -> *mut u8;
+}
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[derive(Copy, Clone, Default, Debug)]
+#[cfg(not(test))]
+pub struct RrosMem;
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc(layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_dealloc(ptr: *mut u8, layout: Layout) {
+    unsafe { __rros_sys_heap_dealloc(ptr, layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+    unsafe { __rros_sys_heap_realloc(ptr, layout.size(), layout.align(), new_size) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc_zeroed(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc_zerod(layout.size(), layout.align()) }
+}
+
+#[cfg(not(test))]
+impl RrosMem {
+    #[inline]
+    fn alloc_impl(&self, layout: Layout, zeroed: bool) -> Result<NonNull<[u8]>, AllocError> {
+        match layout.size() {
+            0 => Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0)),
+            size => unsafe {
+                let raw_ptr = if zeroed { rros_alloc_zeroed(layout) } else { rros_alloc(layout) };
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, size))
+            },
+        }
+    }
+
+    // SAFETY: Same as `Allocator::grow`
+    #[inline]
+    unsafe fn grow_impl(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+        zeroed: bool,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() >= old_layout.size(),
+            "`new_layout.size()` must be greater than or equal to `old_layout.size()`"
+        );
+
+        match old_layout.size() {
+            0 => self.alloc_impl(new_layout, zeroed),
+
+            // SAFETY: `new_size` is non-zero as `old_size` is greater than or equal to `new_size`
+            // as required by safety conditions. Other conditions must be upheld by the caller
+            old_size if old_layout.align() == new_layout.align() => unsafe {
+                let new_size = new_layout.size();
+
+                // `realloc` probably checks for `new_size >= old_layout.size()` or something similar.
+                intrinsics::assume(new_size >= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                if zeroed {
+                    raw_ptr.add(old_size).write_bytes(0, new_size - old_size);
+                }
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_layout.size()` must be greater than or equal to `old_size`,
+            // both the old and new memory allocation are valid for reads and writes for `old_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            old_size => unsafe {
+                let new_ptr = self.alloc_impl(new_layout, zeroed)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), old_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
+
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[cfg(not(test))]
+unsafe impl Allocator for RrosMem {
+    #[inline]
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, false)
+    }
+
+    #[inline]
+    fn allocate_zeroed(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, true)
+    }
+
+    #[inline]
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
+        if layout.size() != 0 {
+            // SAFETY: `layout` is non-zero in size,
+            // other conditions must be upheld by the caller
+            unsafe { rros_dealloc(ptr.as_ptr(), layout) }
+        }
+    }
+
+    #[inline]
+    unsafe fn grow(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, false) }
+    }
+
+    #[inline]
+    unsafe fn grow_zeroed(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, true) }
+    }
+
+    #[inline]
+    unsafe fn shrink(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() <= old_layout.size(),
+            "`new_layout.size()` must be smaller than or equal to `old_layout.size()`"
+        );
+
+        match new_layout.size() {
+            // SAFETY: conditions must be upheld by the caller
+            0 => unsafe {
+                self.deallocate(ptr, old_layout);
+                Ok(NonNull::slice_from_raw_parts(new_layout.dangling(), 0))
+            },
+
+            // SAFETY: `new_size` is non-zero. Other conditions must be upheld by the caller
+            new_size if old_layout.align() == new_layout.align() => unsafe {
+                // `realloc` probably checks for `new_size <= old_layout.size()` or something similar.
+                intrinsics::assume(new_size <= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_size` must be smaller than or equal to `old_layout.size()`,
+            // both the old and new memory allocation are valid for reads and writes for `new_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            new_size => unsafe {
+                let new_ptr = self.allocate(new_layout)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), new_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
\ No newline at end of file
diff --git a/rust/alloc/lib.rs b/rust/alloc/lib.rs
index f109e7902..84a37e361 100644
--- a/rust/alloc/lib.rs
+++ b/rust/alloc/lib.rs
@@ -161,7 +161,7 @@ mod macros;
 // Heaps provided for low-level allocation strategies
 
 pub mod alloc;
-
+pub mod alloc_rros;
 // Primitive types using the heaps above
 
 // Need to conditionally define the mod from `boxed.rs` to avoid
diff --git a/rust/alloc/rc.rs b/rust/alloc/rc.rs
index 7344cd9a4..6ac5ecb70 100644
--- a/rust/alloc/rc.rs
+++ b/rust/alloc/rc.rs
@@ -1,5 +1,3 @@
-// SPDX-License-Identifier: Apache-2.0 OR MIT
-
 //! Single-threaded reference-counting pointers. 'Rc' stands for 'Reference
 //! Counted'.
 //!
@@ -43,7 +41,7 @@
 //! use std::rc::Rc;
 //!
 //! let my_rc = Rc::new(());
-//! Rc::downgrade(&my_rc);
+//! let my_weak = Rc::downgrade(&my_rc);
 //! ```
 //!
 //! `Rc<T>`'s implementations of traits like `Clone` may also be called using
@@ -143,7 +141,7 @@
 //! ```
 //!
 //! If our requirements change, and we also need to be able to traverse from
-//! `Owner` to `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
+//! `Owner` to `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
 //! to `Gadget` introduces a cycle. This means that their
 //! reference counts can never reach 0, and the allocation will never be destroyed:
 //! a memory leak. In order to get around this, we can use [`Weak`]
@@ -264,6 +262,7 @@ use core::marker::{self, PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw, forget};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 #[cfg(not(no_global_oom_handling))]
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
@@ -306,23 +305,51 @@ struct RcBox<T: ?Sized> {
 /// [get_mut]: Rc::get_mut
 #[cfg_attr(not(test), rustc_diagnostic_item = "Rc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Rc<T: ?Sized> {
+#[rustc_insignificant_dtor]
+pub struct Rc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<RcBox<T>>,
     phantom: PhantomData<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Send for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Rc<T, A> {}
+
+// Note that this negative impl isn't strictly necessary for correctness,
+// as `Rc` transitively contains a `Cell`, which is itself `!Sync`.
+// However, given how important `Rc`'s `!Sync`-ness is,
+// having an explicit negative impl is nice for documentation purposes
+// and results in nicer error messages.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Sync for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Rc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Rc<T, A> {}
+// #[stable(feature = "rc_ref_unwind_safe", since = "1.58.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> RefUnwindSafe for Rc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Rc<U>> for Rc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Rc<U, A>> for Rc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Rc<U>> for Rc<T> {}
 
 impl<T: ?Sized> Rc<T> {
+    #[inline]
+    unsafe fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
+    }
+
+    #[inline]
+    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
+        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     #[inline(always)]
     fn inner(&self) -> &RcBox<T> {
         // This unsafety is ok because while this Rc is alive we're guaranteed
@@ -330,12 +357,12 @@ impl<T: ?Sized> Rc<T> {
         unsafe { self.ptr.as_ref() }
     }
 
-    fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner_in(ptr: NonNull<RcBox<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
     }
 
-    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
-        Self::from_inner(unsafe { NonNull::new_unchecked(ptr) })
+    unsafe fn from_ptr_in(ptr: *mut RcBox<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
@@ -356,50 +383,83 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Self::from_inner(
-            Box::leak(box RcBox { strong: Cell::new(1), weak: Cell::new(1), value }).into(),
-        )
+        unsafe {
+            Self::from_inner(
+                Box::leak(Box::new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value }))
+                    .into(),
+            )
+        }
     }
 
-    /// Constructs a new `Rc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Rc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
+    ///
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Rc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Rc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Rc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Rc<T>` is not fully-constructed until `Rc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
+    ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// # #![allow(dead_code)]
     /// use std::rc::{Rc, Weak};
     ///
     /// struct Gadget {
-    ///     self_weak: Weak<Self>,
-    ///     // ... more fields
+    ///     me: Weak<Gadget>,
     /// }
+    ///
     /// impl Gadget {
-    ///     pub fn new() -> Rc<Self> {
-    ///         Rc::new_cyclic(|self_weak| {
-    ///             Gadget { self_weak: self_weak.clone(), /* ... */ }
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Rc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Rc` we're constructing.
+    ///         Rc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
     ///         })
     ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Rc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
     /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Rc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Rc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box RcBox {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(RcBox {
             strong: Cell::new(0),
             weak: Cell::new(1),
             value: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
 
         let init_ptr: NonNull<RcBox<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -409,16 +469,16 @@ impl<T> Rc<T> {
         // otherwise.
         let data = data_fn(&weak);
 
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).value), data);
 
             let prev_value = (*inner).strong.get();
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
             (*inner).strong.set(1);
-        }
 
-        let strong = Rc::from_inner(init_ptr);
+            Rc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -438,17 +498,16 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -481,6 +540,7 @@ impl<T> Rc<T> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -508,10 +568,12 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Ok(Self::from_inner(
-            Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
-                .into(),
-        ))
+        unsafe {
+            Ok(Self::from_inner(
+                Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
+                    .into(),
+            ))
+        }
     }
 
     /// Constructs a new `Rc` with uninitialized contents, returning an error if the allocation fails
@@ -526,12 +588,10 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -584,9 +644,229 @@ impl<T> Rc<T> {
     /// `value` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(value: T) -> Pin<Rc<T>> {
         unsafe { Pin::new_unchecked(Rc::new(value)) }
     }
+}
+
+impl<T, A: Allocator> Rc<T, A> {
+    /// Constructs a new `Rc` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(value: T, alloc: A) -> Rc<T, A> {
+        // NOTE: Prefer match over unwrap_or_else since closure sometimes not inlineable.
+        // That would make code size bigger.
+        match Self::try_new_in(value, alloc) {
+            Ok(m) => m,
+            Err(_) => handle_alloc_error(Layout::new::<RcBox<T>>()),
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc<T>` in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::try_new_in(5, System);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(value: T, alloc: A) -> Result<Self, AllocError> {
+        // There is an implicit weak pointer owned by all the strong
+        // pointers, which ensures that the weak destructor never frees
+        // the allocation while the strong destructor is running, even
+        // if the weak pointer is stored inside the strong one.
+        let (ptr, alloc) = Box::into_unique(Box::try_new_in(
+            RcBox { strong: Cell::new(1), weak: Cell::new(1), value },
+            alloc,
+        )?);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, in the provided allocator, returning an
+    /// error if the allocation fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    //#[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Pin<Rc<T>>` in the provided allocator. If `T` does not implement `Unpin`, then
+    /// `value` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(value: T, alloc: A) -> Pin<Self> {
+        unsafe { Pin::new_unchecked(Rc::new_in(value, alloc)) }
+    }
 
     /// Returns the inner value, if the `Rc` has exactly one strong reference.
     ///
@@ -613,13 +893,14 @@ impl<T> Rc<T> {
         if Rc::strong_count(&this) == 1 {
             unsafe {
                 let val = ptr::read(&*this); // copy the contained object
+                let alloc = ptr::read(&this.alloc); // copy the allocator
 
                 // Indicate to Weaks that they can't be promoted by decrementing
                 // the strong count, and then remove the implicit "strong weak"
                 // pointer while also handling drop logic by just crafting a
                 // fake Weak.
                 this.inner().dec_strong();
-                let _weak = Weak { ptr: this.ptr };
+                let _weak = Weak { ptr: this.ptr, alloc };
                 forget(this);
                 Ok(val)
             }
@@ -642,19 +923,19 @@ impl<T> Rc<[T]> {
     ///
     /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe { Rc::from_ptr(Rc::allocate_for_slice(len)) }
     }
@@ -681,6 +962,7 @@ impl<T> Rc<[T]> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -695,7 +977,84 @@ impl<T> Rc<[T]> {
     }
 }
 
-impl<T> Rc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Constructs a new reference-counted slice with uninitialized contents.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Rc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe { Rc::from_ptr_in(Rc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Rc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut RcBox<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Rc<mem::MaybeUninit<T>, A> {
     /// Converts to `Rc<T>`.
     ///
     /// # Safety
@@ -718,70 +1077,185 @@ impl<T> Rc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<T> {
-        Rc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Rc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
+    }
+}
+
+impl<T, A: Allocator> Rc<[mem::MaybeUninit<T>], A> {
+    /// Converts to `Rc<[T]>`.
+    ///
+    /// # Safety
+    ///
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    ///
+    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Rc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Rc<T> {
+    /// Constructs an `Rc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// and alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The raw pointer must point to a block of memory allocated by the global allocator
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Rc<T>` is never accessed.
+    ///
+    /// [into_raw]: Rc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let x = Rc::new("hello".to_owned());
+    /// let x_ptr = Rc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Rc` to prevent leak.
+    ///     let x = Rc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let five = Rc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
+    ///
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Self::increment_strong_count_in(ptr, Global) }
     }
-}
 
-impl<T> Rc<[mem::MaybeUninit<T>]> {
-    /// Converts to `Rc<[T]>`.
+    /// Decrements the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator. This method can be used to release the final `Rc` and
+    /// backing storage, but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::rc::Rc;
     ///
-    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Rc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    ///     Rc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Rc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<[T]> {
-        unsafe { Rc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Self::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Rc<T> {
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Consumes the `Rc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Rc` using
-    /// [`Rc::from_raw`][from_raw].
-    ///
-    /// [from_raw]: Rc::from_raw
+    /// [`Rc::from_raw`].
     ///
     /// # Examples
     ///
@@ -825,36 +1299,41 @@ impl<T: ?Sized> Rc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).value) }
     }
 
-    /// Constructs an `Rc<T>` from a raw pointer.
+    /// Constructs an `Rc<T, A>` from a raw pointer in the provided allocator.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// [`Rc<U, A>::into_raw`][into_raw] where `U` must have the same size
     /// and alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
-    /// [`mem::transmute`][transmute] for more information on what
+    /// [`mem::transmute`] for more information on what
     /// restrictions apply in this case.
     ///
+    /// The raw pointer must point to a block of memory allocated by `alloc`
+    ///
     /// The user of `from_raw` has to make sure a specific value of `T` is only
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Rc<T>` is never accessed.
+    /// even if the returned `Rc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Rc::into_raw
-    /// [transmute]: core::mem::transmute
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let x = Rc::new("hello".to_owned());
+    /// let x = Rc::new_in("hello".to_owned(), System);
     /// let x_ptr = Rc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Rc` to prevent leak.
-    ///     let x = Rc::from_raw(x_ptr);
+    ///     let x = Rc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
@@ -862,15 +1341,15 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         let offset = unsafe { data_offset(ptr) };
 
         // Reverse the offset to find the original RcBox.
         let rc_ptr =
             unsafe { (ptr as *mut RcBox<T>).set_ptr_value((ptr as *mut u8).offset(-offset)) };
 
-        unsafe { Self::from_ptr(rc_ptr) }
+        unsafe { Self::from_ptr_in(rc_ptr, alloc) }
     }
 
     /// Creates a new [`Weak`] pointer to this allocation.
@@ -884,12 +1363,17 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// let weak_five = Rc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Rc`"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         this.inner().inc_weak();
         // Make sure we do not create a dangling Weak
         debug_assert!(!is_dangling(this.ptr.as_ptr()));
-        Weak { ptr: this.ptr }
+        Weak { ptr: this.ptr, alloc: this.alloc.clone() }
     }
 
     /// Gets the number of [`Weak`] pointers to this allocation.
@@ -933,30 +1417,37 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Rc, but don't touch refcount by wrapping in ManuallyDrop
-        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T>::from_raw(ptr)) };
+        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T, A>::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _rc_clone: mem::ManuallyDrop<_> = rc.clone();
     }
@@ -966,33 +1457,36 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release
-    /// the final `Rc` and backing storage, but **should not** be called after
-    /// the final `Rc` has been released.
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final `Rc` and backing storage,
+    /// but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
-    ///     Rc::decrement_strong_count(ptr);
+    ///     Rc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Rc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Rc::from_raw_in(ptr, alloc)) };
     }
 
     /// Returns `true` if there are no other `Rc` or [`Weak`] pointers to
@@ -1009,7 +1503,7 @@ impl<T: ?Sized> Rc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Rc` pointers.
     ///
     /// [make_mut]: Rc::make_mut
     /// [clone]: Clone::clone
@@ -1084,24 +1578,24 @@ impl<T: ?Sized> Rc<T> {
     /// assert!(Rc::ptr_eq(&five, &same_five));
     /// assert!(!Rc::ptr_eq(&five, &other_five));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
 }
 
-impl<T: Clone> Rc<T> {
+impl<T: Clone, A: Allocator + Clone> Rc<T, A> {
     /// Makes a mutable reference into the given `Rc`.
     ///
     /// If there are other `Rc` pointers to the same allocation, then `make_mut` will
     /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
     /// referred to as clone-on-write.
     ///
-    /// If there are no other `Rc` pointers to this allocation, then [`Weak`]
-    /// pointers to this allocation will be disassociated.
+    /// However, if there are no other `Rc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be disassociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or diassociating [`Weak`] pointers.
     ///
     /// [`clone`]: Clone::clone
     /// [`get_mut`]: Rc::get_mut
@@ -1113,11 +1607,11 @@ impl<T: Clone> Rc<T> {
     ///
     /// let mut data = Rc::new(5);
     ///
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// let mut other_data = Rc::clone(&data);    // Won't clone inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Clones inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// *Rc::make_mut(&mut other_data) *= 2;  // Won't clone anything
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// let mut other_data = Rc::clone(&data); // Won't clone inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Clones inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// *Rc::make_mut(&mut other_data) *= 2;   // Won't clone anything
     ///
     /// // Now `data` and `other_data` point to different allocations.
     /// assert_eq!(*data, 8);
@@ -1147,7 +1641,7 @@ impl<T: Clone> Rc<T> {
         if Rc::strong_count(this) != 1 {
             // Gotta clone the data, there are other Rcs.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1155,7 +1649,7 @@ impl<T: Clone> Rc<T> {
             }
         } else if Rc::weak_count(this) != 0 {
             // Can just steal the data, all that's left is Weaks
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1174,11 +1668,44 @@ impl<T: Clone> Rc<T> {
         // reference to the allocation.
         unsafe { &mut this.ptr.as_mut().value }
     }
-}
 
-impl Rc<dyn Any> {
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `rc_t` is of type `Rc<T>`, this function is functionally equivalent to
+    /// `(*rc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, rc::Rc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let rc = Rc::new(inner);
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let rc = Rc::new(inner);
+    /// let rc2 = rc.clone();
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `rc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Rc::unwrap_or_clone(rc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
     #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Rc::try_unwrap(this).unwrap_or_else(|rc| (*rc).clone())
+    }
+}
+
+impl<A: Allocator + Clone> Rc<dyn Any, A> {
     /// Attempt to downcast the `Rc<dyn Any>` to a concrete type.
     ///
     /// # Examples
@@ -1197,15 +1724,57 @@ impl Rc<dyn Any> {
     /// print_if_string(Rc::new(my_string));
     /// print_if_string(Rc::new(0i8));
     /// ```
-    pub fn downcast<T: Any>(self) -> Result<Rc<T>, Rc<dyn Any>> {
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T: Any>(self) -> Result<Rc<T, A>, Self> {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<RcBox<T>>();
-            forget(self);
-            Ok(Rc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<RcBox<T>>();
+                let alloc = self.alloc.clone();
+                forget(self);
+                Ok(Rc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Rc<dyn Any>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::rc::Rc;
+    ///
+    /// let x: Rc<dyn Any> = Rc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T: Any>(self) -> Rc<T, A> {
+        unsafe {
+            let ptr = self.ptr.cast::<RcBox<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Rc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T: ?Sized> Rc<T> {
@@ -1263,28 +1832,30 @@ impl<T: ?Sized> Rc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Allocates an `RcBox<T>` with sufficient space for an unsized inner value
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut RcBox<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut RcBox<T> {
         // Allocate for the `RcBox<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Rc::<T>::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut RcBox<T>).set_ptr_value(mem),
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut RcBox<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Rc<T> {
+    fn from_box_in(v: Box<T, A>) -> Rc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1294,9 +1865,9 @@ impl<T: ?Sized> Rc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1376,6 +1947,20 @@ impl<T> Rc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Allocates an `RcBox<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut RcBox<[T]> {
+        unsafe {
+            Rc::<[T]>::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut RcBox<[T]>,
+            )
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 trait RcFromSlice<T> {
     fn from_slice(slice: &[T]) -> Self;
@@ -1398,7 +1983,7 @@ impl<T: Copy> RcFromSlice<T> for Rc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Rc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Rc<T, A> {
     type Target = T;
 
     #[inline(always)]
@@ -1411,7 +1996,7 @@ impl<T: ?Sized> Deref for Rc<T> {
 impl<T: ?Sized> Receiver for Rc<T> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Rc<T, A> {
     /// Drops the `Rc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1449,7 +2034,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
                 self.inner().dec_weak();
 
                 if self.inner().weak() == 0 {
-                    Global.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
+                    self.alloc.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
                 }
             }
         }
@@ -1457,7 +2042,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Rc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Rc<T, A> {
     /// Makes a clone of the `Rc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1473,9 +2058,11 @@ impl<T: ?Sized> Clone for Rc<T> {
     /// let _ = Rc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Rc<T> {
-        self.inner().inc_strong();
-        Self::from_inner(self.ptr)
+    fn clone(&self) -> Self {
+        unsafe {
+            self.inner().inc_strong();
+            Self::from_inner_in(self.ptr, self.alloc.clone())
+        }
     }
 }
 
@@ -1499,20 +2086,20 @@ impl<T: Default> Default for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait RcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Rc<T>) -> bool;
-    fn ne(&self, other: &Rc<T>) -> bool;
+trait RcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Rc<T, A>) -> bool;
+    fn ne(&self, other: &Rc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Rc<T>) -> bool {
+    default fn eq(&self, other: &Rc<T, A>) -> bool {
         **self == **other
     }
 
     #[inline]
-    default fn ne(&self, other: &Rc<T>) -> bool {
+    default fn ne(&self, other: &Rc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -1531,20 +2118,20 @@ impl<T: Eq> MarkerEq for T {}
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + MarkerEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + MarkerEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         Rc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         !Rc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Rc<T, A> {
     /// Equality for two `Rc`s.
     ///
     /// Two `Rc`s are equal if their inner values are equal, even if they are
@@ -1564,7 +2151,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five == Rc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::eq(self, other)
     }
 
@@ -1586,16 +2173,16 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five != Rc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Rc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Rc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Rc<T, A> {
     /// Partial comparison for two `Rc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -1611,7 +2198,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Rc::new(6)));
     /// ```
     #[inline(always)]
-    fn partial_cmp(&self, other: &Rc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Rc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -1629,7 +2216,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five < Rc::new(6));
     /// ```
     #[inline(always)]
-    fn lt(&self, other: &Rc<T>) -> bool {
+    fn lt(&self, other: &Rc<T, A>) -> bool {
         **self < **other
     }
 
@@ -1647,7 +2234,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five <= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn le(&self, other: &Rc<T>) -> bool {
+    fn le(&self, other: &Rc<T, A>) -> bool {
         **self <= **other
     }
 
@@ -1665,7 +2252,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five > Rc::new(4));
     /// ```
     #[inline(always)]
-    fn gt(&self, other: &Rc<T>) -> bool {
+    fn gt(&self, other: &Rc<T, A>) -> bool {
         **self > **other
     }
 
@@ -1683,13 +2270,13 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five >= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn ge(&self, other: &Rc<T>) -> bool {
+    fn ge(&self, other: &Rc<T, A>) -> bool {
         **self >= **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Rc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Rc<T, A> {
     /// Comparison for two `Rc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -1705,34 +2292,34 @@ impl<T: ?Sized + Ord> Ord for Rc<T> {
     /// assert_eq!(Ordering::Less, five.cmp(&Rc::new(6)));
     /// ```
     #[inline]
-    fn cmp(&self, other: &Rc<T>) -> Ordering {
+    fn cmp(&self, other: &Rc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Rc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Rc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state);
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Rc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Rc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Rc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -1741,7 +2328,7 @@ impl<T: ?Sized> fmt::Pointer for Rc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Rc<T> {
-    /// Converts a generic type `T` into a `Rc<T>`
+    /// Converts a generic type `T` into an `Rc<T>`
     ///
     /// The conversion allocates on the heap and moves `t`
     /// from the stack into it.
@@ -1818,7 +2405,7 @@ impl From<String> for Rc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Rc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Rc<T, A> {
     /// Move a boxed object to a new, reference counted, allocation.
     ///
     /// # Example
@@ -1830,14 +2417,14 @@ impl<T: ?Sized> From<Box<T>> for Rc<T> {
     /// assert_eq!(1, *shared);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Rc<T> {
-        Rc::from_box(v)
+    fn from(v: Box<T, A>) -> Rc<T, A> {
+        Rc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Rc<[T]> {
+impl<T, A: Allocator> From<Vec<T, A>> for Rc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -1849,15 +2436,9 @@ impl<T> From<Vec<T>> for Rc<[T]> {
     /// assert_eq!(vec![1, 2, 3], *shared);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Rc<[T]> {
-        unsafe {
-            let rc = Rc::copy_from_slice(&v);
-
-            // Allow the Vec to free its memory, but not destroy its contents
-            v.set_len(0);
-
-            rc
-        }
+    fn from(v: Vec<T, A>) -> Rc<[T], A> {
+        let boxed_slice = v.into_boxed_slice();
+        Self::from(boxed_slice)
     }
 }
 
@@ -1888,6 +2469,25 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Rc<str>> for Rc<[u8]> {
+    /// Converts a reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::rc::Rc;
+    /// let string: Rc<str> = Rc::from("eggplant");
+    /// let bytes: Rc<[u8]> = Rc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Rc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Rc::from_raw(Rc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
 impl<T, const N: usize> TryFrom<Rc<[T]>> for Rc<[T; N]> {
     type Error = Rc<[T]>;
@@ -1989,7 +2589,7 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 
 /// `Weak` is a version of [`Rc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Rc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Rc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -2008,7 +2608,10 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "rc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesn’t need
@@ -2016,15 +2619,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Send for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Weak<T, A> {}
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Sync for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
@@ -2043,9 +2647,37 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") }
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Weak;
+    ///
+    /// let empty: Weak<i64> = Weak::new();
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -2062,6 +2694,56 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference, and `ptr` must point to a block of memory allocated by the global allocator.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Rc::downgrade(&strong).into_raw();
+    /// let raw_2 = Rc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Rc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    /// [`new`]: Weak::new
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -2086,7 +2768,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: ptr::null
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut RcBox<T> = NonNull::as_ptr(self.ptr);
@@ -2096,7 +2779,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as RcBox (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).value) }
@@ -2130,6 +2813,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -2137,6 +2821,44 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
+    /// Consumes the `Weak<T>` and turns it into a raw pointer.
+    ///
+    /// This converts the weak pointer into a raw pointer, while still preserving the ownership of
+    /// one weak reference (the weak count is not modified by this operation). It can be turned
+    /// back into the `Weak<T>` with [`from_raw`].
+    ///
+    /// The same restrictions of accessing the target of the pointer as with
+    /// [`as_ptr`] apply.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    /// let weak = Rc::downgrade(&strong);
+    /// let raw = weak.into_raw();
+    ///
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    /// assert_eq!("hello", unsafe { &*raw });
+    ///
+    /// drop(unsafe { Weak::from_raw(raw) });
+    /// assert_eq!(0, Rc::weak_count(&strong));
+    /// ```
+    ///
+    /// [`from_raw`]: Weak::from_raw
+    /// [`as_ptr`]: Weak::as_ptr
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn into_raw_and_alloc(self) -> (*const T, A)
+    where
+        A: Clone,
+    {
+        let result = self.as_ptr();
+        let alloc = self.alloc.clone();
+        mem::forget(self);
+        (result, alloc)
+    }
+
     /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
@@ -2148,7 +2870,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and `ptr` must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -2179,8 +2901,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
     /// [`new`]: Weak::new
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -2196,7 +2918,7 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 
     /// Attempts to upgrade the `Weak` pointer to an [`Rc`], delaying
@@ -2222,20 +2944,29 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Rc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Rc<T>> {
+    pub fn upgrade(&self) -> Option<Rc<T, A>>
+    where
+        A: Clone,
+    {
         let inner = self.inner()?;
+
         if inner.strong() == 0 {
             None
         } else {
-            inner.inc_strong();
-            Some(Rc::from_inner(self.ptr))
+            unsafe {
+                inner.inc_strong();
+                Some(Rc::from_inner_in(self.ptr, self.alloc.clone()))
+            }
         }
     }
 
     /// Gets the number of strong (`Rc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
         if let Some(inner) = self.inner() { inner.strong() } else { 0 }
@@ -2244,6 +2975,7 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of `Weak` pointers pointing to this allocation.
     ///
     /// If no strong pointers remain, this will return zero.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
@@ -2313,9 +3045,8 @@ impl<T: ?Sized> Weak<T> {
     /// let third = Rc::downgrade(&third_rc);
     /// assert!(!first.ptr_eq(&third));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -2323,7 +3054,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2356,14 +3087,14 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
         // the strong pointers have disappeared.
         if inner.weak() == 0 {
             unsafe {
-                Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
             }
         }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2376,16 +3107,16 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         if let Some(inner) = self.inner() {
             inner.inc_weak()
         }
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Weak<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Weak<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         write!(f, "(Weak)")
     }
@@ -2396,7 +3127,6 @@ impl<T> Default for Weak<T> {
     /// Constructs a new `Weak<T>`, without allocating any memory.
     /// Calling [`upgrade`] on the return value always gives [`None`].
     ///
-    /// [`None`]: Option
     /// [`upgrade`]: Weak::upgrade
     ///
     /// # Examples
@@ -2435,14 +3165,23 @@ trait RcInnerPtr {
     fn inc_strong(&self) {
         let strong = self.strong();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(strong != 0);
+        }
+
+        let strong = strong.wrapping_add(1);
+        self.strong_ref().set(strong);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if strong == 0 || strong == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(strong == 0) {
             abort();
         }
-        self.strong_ref().set(strong + 1);
     }
 
     #[inline]
@@ -2459,14 +3198,23 @@ trait RcInnerPtr {
     fn inc_weak(&self) {
         let weak = self.weak();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(weak != 0);
+        }
+
+        let weak = weak.wrapping_add(1);
+        self.weak_ref().set(weak);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if weak == 0 || weak == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(weak == 0) {
             abort();
         }
-        self.weak_ref().set(weak + 1);
     }
 
     #[inline]
@@ -2500,21 +3248,21 @@ impl<'a> RcInnerPtr for WeakInner<'a> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Rc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Rc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Rc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Rc<T, A> {}
 
 /// Get the offset within an `RcBox` for the payload behind a pointer.
 ///
@@ -2528,7 +3276,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
@@ -2536,4 +3284,4 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
 fn data_offset_align(align: usize) -> isize {
     let layout = Layout::new::<RcBox<()>>();
     (layout.size() + layout.padding_needed_for(align)) as isize
-}
+}
\ No newline at end of file
diff --git a/rust/alloc/string.rs b/rust/alloc/string.rs
index 55293c304..c4b72c009 100644
--- a/rust/alloc/string.rs
+++ b/rust/alloc/string.rs
@@ -46,11 +46,13 @@
 
 #[cfg(not(no_global_oom_handling))]
 use core::char::{decode_utf16, REPLACEMENT_CHARACTER};
+use core::cmp::Ordering;
+// use core::error::Error;
 use core::fmt;
 use core::hash;
+use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::iter::{from_fn, FromIterator};
-use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::ops::Add;
 #[cfg(not(no_global_oom_handling))]
@@ -60,10 +62,11 @@ use core::ops::Bound::{Excluded, Included, Unbounded};
 use core::ops::{self, Index, IndexMut, Range, RangeBounds};
 use core::ptr;
 use core::slice;
-#[cfg(not(no_global_oom_handling))]
-use core::str::lossy;
 use core::str::pattern::Pattern;
+#[cfg(not(no_global_oom_handling))]
+use core::str::Utf8Chunks;
 
+use crate::alloc::{Allocator, Global};
 #[cfg(not(no_global_oom_handling))]
 use crate::borrow::{Cow, ToOwned};
 use crate::boxed::Box;
@@ -81,7 +84,7 @@ use crate::vec::Vec;
 ///
 /// # Examples
 ///
-/// You can create a `String` from [a literal string][`str`] with [`String::from`]:
+/// You can create a `String` from [a literal string][`&str`] with [`String::from`]:
 ///
 /// [`String::from`]: From::from
 ///
@@ -119,31 +122,103 @@ use crate::vec::Vec;
 ///
 /// # UTF-8
 ///
-/// `String`s are always valid UTF-8. This has a few implications, the first of
-/// which is that if you need a non-UTF-8 string, consider [`OsString`]. It is
-/// similar, but without the UTF-8 constraint. The second implication is that
-/// you cannot index into a `String`:
+/// `String`s are always valid UTF-8. If you need a non-UTF-8 string, consider
+/// [`OsString`]. It is similar, but without the UTF-8 constraint. Because UTF-8
+/// is a variable width encoding, `String`s are typically smaller than an array of
+/// the same `chars`:
+///
+/// ```
+/// use std::mem;
+///
+/// // `s` is ASCII which represents each `char` as one byte
+/// let s = "hello";
+/// assert_eq!(s.len(), 5);
+///
+/// // A `char` array with the same contents would be longer because
+/// // every `char` is four bytes
+/// let s = ['h', 'e', 'l', 'l', 'o'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+///
+/// // However, for non-ASCII strings, the difference will be smaller
+/// // and sometimes they are the same
+/// let s = "💖💖💖💖💖";
+/// assert_eq!(s.len(), 20);
+///
+/// let s = ['💖', '💖', '💖', '💖', '💖'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+/// ```
+///
+/// This raises interesting questions as to how `s[i]` should work.
+/// What should `i` be here? Several options include byte indices and
+/// `char` indices but, because of UTF-8 encoding, only byte indices
+/// would provide constant time indexing. Getting the `i`th `char`, for
+/// example, is available using [`chars`]:
+///
+/// ```
+/// let s = "hello";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('l'));
+///
+/// let s = "💖💖💖💖💖";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('💖'));
+/// ```
+///
+/// Next, what should `s[i]` return? Because indexing returns a reference
+/// to underlying data it could be `&u8`, `&[u8]`, or something else similar.
+/// Since we're only providing one index, `&u8` makes the most sense but that
+/// might not be what the user expects and can be explicitly achieved with
+/// [`as_bytes()`]:
+///
+/// ```
+/// // The first byte is 104 - the byte value of `'h'`
+/// let s = "hello";
+/// assert_eq!(s.as_bytes()[0], 104);
+/// // or
+/// assert_eq!(s.as_bytes()[0], b'h');
+///
+/// // The first byte is 240 which isn't obviously useful
+/// let s = "💖💖💖💖💖";
+/// assert_eq!(s.as_bytes()[0], 240);
+/// ```
+///
+/// Due to these ambiguities/restrictions, indexing with a `usize` is simply
+/// forbidden:
 ///
 /// ```compile_fail,E0277
 /// let s = "hello";
 ///
-/// println!("The first letter of s is {}", s[0]); // ERROR!!!
+/// // The following will not compile!
+/// println!("The first letter of s is {}", s[0]);
 /// ```
 ///
-/// [`OsString`]: ../../std/ffi/struct.OsString.html
+/// It is more clear, however, how `&s[i..j]` should work (that is,
+/// indexing with a range). It should accept byte indices (to be constant-time)
+/// and return a `&str` which is UTF-8 encoded. This is also called "string slicing".
+/// Note this will panic if the byte indices provided are not character
+/// boundaries - see [`is_char_boundary`] for more details. See the implementations
+/// for [`SliceIndex<str>`] for more details on string slicing. For a non-panicking
+/// version of string slicing, see [`get`].
+///
+/// [`OsString`]: ../../std/ffi/struct.OsString.html "ffi::OsString"
+/// [`SliceIndex<str>`]: core::slice::SliceIndex
+/// [`as_bytes()`]: str::as_bytes
+/// [`get`]: str::get
+/// [`is_char_boundary`]: str::is_char_boundary
 ///
-/// Indexing is intended to be a constant-time operation, but UTF-8 encoding
-/// does not allow us to do this. Furthermore, it's not clear what sort of
-/// thing the index should return: a byte, a codepoint, or a grapheme cluster.
-/// The [`bytes`] and [`chars`] methods return iterators over the first
-/// two, respectively.
+/// The [`bytes`] and [`chars`] methods return iterators over the bytes and
+/// codepoints of the string, respectively. To iterate over codepoints along
+/// with byte indices, use [`char_indices`].
 ///
 /// [`bytes`]: str::bytes
 /// [`chars`]: str::chars
+/// [`char_indices`]: str::char_indices
 ///
 /// # Deref
 ///
-/// `String`s implement [`Deref`]`<Target=str>`, and so inherit all of [`str`]'s
+/// `String` implements <code>[Deref]<Target = [str]></code>, and so inherits all of [`str`]'s
 /// methods. In addition, this means that you can pass a `String` to a
 /// function which takes a [`&str`] by using an ampersand (`&`):
 ///
@@ -184,7 +259,7 @@ use crate::vec::Vec;
 /// to explicitly extract the string slice containing the string. The second
 /// way changes `example_func(&example_string);` to
 /// `example_func(&*example_string);`. In this case we are dereferencing a
-/// `String` to a [`str`][`&str`], then referencing the [`str`][`&str`] back to
+/// `String` to a [`str`], then referencing the [`str`] back to
 /// [`&str`]. The second way is more idiomatic, however both work to do the
 /// conversion explicitly rather than relying on the implicit conversion.
 ///
@@ -247,11 +322,11 @@ use crate::vec::Vec;
 ///
 /// ```text
 /// 0
-/// 5
-/// 10
-/// 20
-/// 20
-/// 40
+/// 8
+/// 16
+/// 16
+/// 32
+/// 32
 /// ```
 ///
 /// At first, we have no memory allocated at all, but as we append to the
@@ -284,15 +359,16 @@ use crate::vec::Vec;
 ///
 /// Here, there's no need to allocate more memory inside the loop.
 ///
-/// [`str`]: prim@str
-/// [`&str`]: prim@str
-/// [`Deref`]: core::ops::Deref
+/// [str]: prim@str "str"
+/// [`str`]: prim@str "str"
+/// [`&str`]: prim@str "&str"
+/// [Deref]: core::ops::Deref "ops::Deref"
+/// [`Deref`]: core::ops::Deref "ops::Deref"
 /// [`as_str()`]: String::as_str
-#[derive(PartialOrd, Eq, Ord)]
-#[cfg_attr(not(test), rustc_diagnostic_item = "string_type")]
+#[cfg_attr(not(test), rustc_diagnostic_item = "String")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct String {
-    vec: Vec<u8>,
+pub struct String<#[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global> {
+    vec: Vec<u8, A>,
 }
 
 /// A possible error value when converting a `String` from a UTF-8 byte vector.
@@ -310,10 +386,10 @@ pub struct String {
 /// an analogue to `FromUtf8Error`, and you can get one from a `FromUtf8Error`
 /// through the [`utf8_error`] method.
 ///
-/// [`Utf8Error`]: core::str::Utf8Error
-/// [`std::str`]: core::str
-/// [`&str`]: prim@str
-/// [`utf8_error`]: Self::utf8_error
+/// [`Utf8Error`]: str::Utf8Error "std::str::Utf8Error"
+/// [`std::str`]: core::str "std::str"
+/// [`&str`]: prim@str "&str"
+/// [`utf8_error`]: FromUtf8Error::utf8_error
 ///
 /// # Examples
 ///
@@ -378,17 +454,18 @@ impl String {
     #[inline]
     #[rustc_const_stable(feature = "const_string_new", since = "1.39.0")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub const fn new() -> String {
         String { vec: Vec::new() }
     }
 
-    /// Creates a new empty `String` with a particular capacity.
+    /// Creates a new empty `String` with at least the specified capacity.
     ///
     /// `String`s have an internal buffer to hold their data. The capacity is
     /// the length of that buffer, and can be queried with the [`capacity`]
     /// method. This method creates an empty `String`, but one with an initial
-    /// buffer that can hold `capacity` bytes. This is useful when you may be
-    /// appending a bunch of data to the `String`, reducing the number of
+    /// buffer that can hold at least `capacity` bytes. This is useful when you
+    /// may be appending a bunch of data to the `String`, reducing the number of
     /// reallocations it needs to do.
     ///
     /// [`capacity`]: String::capacity
@@ -421,9 +498,8 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[doc(alias = "alloc")]
-    #[doc(alias = "malloc")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub fn with_capacity(capacity: usize) -> String {
         String { vec: Vec::with_capacity(capacity) }
     }
@@ -491,8 +567,8 @@ impl String {
     /// with this error.
     ///
     /// [`from_utf8_unchecked`]: String::from_utf8_unchecked
-    /// [`Vec<u8>`]: crate::vec::Vec
-    /// [`&str`]: prim@str
+    /// [`Vec<u8>`]: crate::vec::Vec "Vec"
+    /// [`&str`]: prim@str "&str"
     /// [`into_bytes`]: String::into_bytes
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
@@ -528,7 +604,7 @@ impl String {
     /// it's already valid UTF-8, we don't need a new allocation. This return
     /// type allows us to handle both cases.
     ///
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     ///
     /// # Examples
     ///
@@ -552,18 +628,19 @@ impl String {
     ///
     /// assert_eq!("Hello �World", output);
     /// ```
+    #[must_use]
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf8_lossy(v: &[u8]) -> Cow<'_, str> {
-        let mut iter = lossy::Utf8Lossy::from_bytes(v).chunks();
+        let mut iter = Utf8Chunks::new(v);
 
-        let (first_valid, first_broken) = if let Some(chunk) = iter.next() {
-            let lossy::Utf8LossyChunk { valid, broken } = chunk;
-            if valid.len() == v.len() {
-                debug_assert!(broken.is_empty());
+        let first_valid = if let Some(chunk) = iter.next() {
+            let valid = chunk.valid();
+            if chunk.invalid().is_empty() {
+                debug_assert_eq!(valid.len(), v.len());
                 return Cow::Borrowed(valid);
             }
-            (valid, broken)
+            valid
         } else {
             return Cow::Borrowed("");
         };
@@ -572,13 +649,11 @@ impl String {
 
         let mut res = String::with_capacity(v.len());
         res.push_str(first_valid);
-        if !first_broken.is_empty() {
-            res.push_str(REPLACEMENT);
-        }
+        res.push_str(REPLACEMENT);
 
-        for lossy::Utf8LossyChunk { valid, broken } in iter {
-            res.push_str(valid);
-            if !broken.is_empty() {
+        for chunk in iter {
+            res.push_str(chunk.valid());
+            if !chunk.invalid().is_empty() {
                 res.push_str(REPLACEMENT);
             }
         }
@@ -629,7 +704,7 @@ impl String {
     /// conversion requires a memory allocation.
     ///
     /// [`from_utf8_lossy`]: String::from_utf8_lossy
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     /// [U+FFFD]: core::char::REPLACEMENT_CHARACTER
     ///
     /// # Examples
@@ -646,6 +721,7 @@ impl String {
     ///            String::from_utf16_lossy(v));
     /// ```
     #[cfg(not(no_global_oom_handling))]
+    #[must_use]
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf16_lossy(v: &[u16]) -> String {
@@ -678,6 +754,7 @@ impl String {
     /// let rebuilt = unsafe { String::from_raw_parts(ptr, len, cap) };
     /// assert_eq!(rebuilt, "hello");
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[unstable(feature = "vec_into_raw_parts", reason = "new API", issue = "65816")]
     pub fn into_raw_parts(self) -> (*mut u8, usize, usize) {
         self.vec.into_raw_parts()
@@ -697,7 +774,10 @@ impl String {
     /// * The first `length` bytes at `buf` need to be valid UTF-8.
     ///
     /// Violating these may cause problems like corrupting the allocator's
-    /// internal data structures.
+    /// internal data structures. For example, it is normally **not** safe to
+    /// build a `String` from a pointer to a C `char` array containing UTF-8
+    /// _unless_ you are certain that array was originally allocated by the
+    /// Rust standard library's allocator.
     ///
     /// The ownership of `buf` is effectively transferred to the
     /// `String` which may then deallocate, reallocate or change the
@@ -763,6 +843,7 @@ impl String {
     /// assert_eq!("💖", sparkle_heart);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub unsafe fn from_utf8_unchecked(bytes: Vec<u8>) -> String {
         String { vec: bytes }
@@ -783,6 +864,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111][..], &bytes[..]);
     /// ```
     #[inline]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.vec
@@ -800,6 +882,7 @@ impl String {
     /// assert_eq!("foo", s.as_str());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_str(&self) -> &str {
         self
@@ -820,6 +903,7 @@ impl String {
     /// assert_eq!("FOOBAR", s_mut_str);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_mut_str(&mut self) -> &mut str {
         self
@@ -893,26 +977,22 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn capacity(&self) -> usize {
         self.vec.capacity()
     }
 
-    /// Ensures that this `String`'s capacity is at least `additional` bytes
-    /// larger than its length.
-    ///
-    /// The capacity may be increased by more than `additional` bytes if it
-    /// chooses, to prevent frequent reallocations.
-    ///
-    /// If you do not want this "at least" behavior, see the [`reserve_exact`]
-    /// method.
+    /// Reserves capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `reserve`,
+    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Does nothing if capacity is already sufficient.
     ///
     /// # Panics
     ///
     /// Panics if the new capacity overflows [`usize`].
     ///
-    /// [`reserve_exact`]: String::reserve_exact
-    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -925,22 +1005,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -949,17 +1030,18 @@ impl String {
         self.vec.reserve(additional)
     }
 
-    /// Ensures that this `String`'s capacity is `additional` bytes
-    /// larger than its length.
-    ///
-    /// Consider using the [`reserve`] method unless you absolutely know
-    /// better than the allocator.
+    /// Reserves the minimum capacity for at least `additional` bytes more than
+    /// the current length. Unlike [`reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `reserve_exact`, capacity will be greater than or equal to
+    /// `self.len() + additional`. Does nothing if the capacity is already
+    /// sufficient.
     ///
     /// [`reserve`]: String::reserve
     ///
     /// # Panics
     ///
-    /// Panics if the new capacity overflows `usize`.
+    /// Panics if the new capacity overflows [`usize`].
     ///
     /// # Examples
     ///
@@ -973,22 +1055,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve_exact(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -997,11 +1080,12 @@ impl String {
         self.vec.reserve_exact(additional)
     }
 
-    /// Tries to reserve capacity for at least `additional` more elements to be inserted
-    /// in the given `String`. The collection may reserve more space to avoid
-    /// frequent reallocations. After calling `reserve`, capacity will be
-    /// greater than or equal to `self.len() + additional`. Does nothing if
-    /// capacity is already sufficient.
+    /// Tries to reserve capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `try_reserve`, capacity will be
+    /// greater than or equal to `self.len() + additional` if it returns
+    /// `Ok(())`. Does nothing if capacity is already sufficient. This method
+    /// preserves the contents even if an error occurs.
     ///
     /// # Errors
     ///
@@ -1011,7 +1095,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
@@ -1032,14 +1115,18 @@ impl String {
         self.vec.try_reserve(additional)
     }
 
-    /// Tries to reserve the minimum capacity for exactly `additional` more elements to
-    /// be inserted in the given `String`. After calling `reserve_exact`,
-    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Tries to reserve the minimum capacity for at least `additional` bytes
+    /// more than the current length. Unlike [`try_reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `try_reserve_exact`, capacity will be greater than or
+    /// equal to `self.len() + additional` if it returns `Ok(())`.
     /// Does nothing if the capacity is already sufficient.
     ///
     /// Note that the allocator may give the collection more space than it
     /// requests. Therefore, capacity can not be relied upon to be precisely
-    /// minimal. Prefer `reserve` if future insertions are expected.
+    /// minimal. Prefer [`try_reserve`] if future insertions are expected.
+    ///
+    /// [`try_reserve`]: String::try_reserve
     ///
     /// # Errors
     ///
@@ -1049,14 +1136,13 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
     ///     let mut output = String::new();
     ///
     ///     // Pre-reserve the memory, exiting if we can't
-    ///     output.try_reserve(data.len())?;
+    ///     output.try_reserve_exact(data.len())?;
     ///
     ///     // Now we know this can't OOM in the middle of our complex work
     ///     output.push_str(data);
@@ -1102,7 +1188,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(shrink_to)]
     /// let mut s = String::from("foo");
     ///
     /// s.reserve(100);
@@ -1115,7 +1200,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "shrink_to", reason = "new API", issue = "56431")]
+    #[stable(feature = "shrink_to", since = "1.56.0")]
     pub fn shrink_to(&mut self, min_capacity: usize) {
         self.vec.shrink_to(min_capacity)
     }
@@ -1161,6 +1246,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111], s.as_bytes());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.vec
@@ -1354,13 +1440,14 @@ impl String {
     /// assert_eq!(s, "foobar");
     /// ```
     ///
-    /// The exact order may be useful for tracking external state, like an index.
+    /// Because the elements are visited exactly once in the original order,
+    /// external state may be used to decide which elements to keep.
     ///
     /// ```
     /// let mut s = String::from("abcde");
     /// let keep = [false, true, true, false, true];
-    /// let mut i = 0;
-    /// s.retain(|_| (keep[i], i += 1).0);
+    /// let mut iter = keep.iter();
+    /// s.retain(|_| *iter.next().unwrap());
     /// assert_eq!(s, "bce");
     /// ```
     #[inline]
@@ -1387,19 +1474,28 @@ impl String {
         let mut guard = SetLenOnDrop { s: self, idx: 0, del_bytes: 0 };
 
         while guard.idx < len {
-            let ch = unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap() };
+            let ch =
+                // SAFETY: `guard.idx` is positive-or-zero and less that len so the `get_unchecked`
+                // is in bound. `self` is valid UTF-8 like string and the returned slice starts at
+                // a unicode code point so the `Chars` always return one character.
+                unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap_unchecked() };
             let ch_len = ch.len_utf8();
 
             if !f(ch) {
                 guard.del_bytes += ch_len;
             } else if guard.del_bytes > 0 {
-                unsafe {
-                    ptr::copy(
-                        guard.s.vec.as_ptr().add(guard.idx),
-                        guard.s.vec.as_mut_ptr().add(guard.idx - guard.del_bytes),
-                        ch_len,
-                    );
-                }
+                // SAFETY: `guard.idx` is in bound and `guard.del_bytes` represent the number of
+                // bytes that are erased from the string so the resulting `guard.idx -
+                // guard.del_bytes` always represent a valid unicode code point.
+                //
+                // `guard.del_bytes` >= `ch.len_utf8()`, so taking a slice with `ch.len_utf8()` len
+                // is safe.
+                ch.encode_utf8(unsafe {
+                    crate::slice::from_raw_parts_mut(
+                        guard.s.as_mut_ptr().add(guard.idx - guard.del_bytes),
+                        ch.len_utf8(),
+                    )
+                });
             }
 
             // Point idx to the next char
@@ -1453,7 +1549,7 @@ impl String {
 
         unsafe {
             ptr::copy(self.vec.as_ptr().add(idx), self.vec.as_mut_ptr().add(idx + amt), len - idx);
-            ptr::copy(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
+            ptr::copy_nonoverlapping(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
             self.vec.set_len(len + amt);
         }
     }
@@ -1494,10 +1590,11 @@ impl String {
     ///
     /// # Safety
     ///
-    /// This function is unsafe because it does not check that the bytes passed
-    /// to it are valid UTF-8. If this constraint is violated, it may cause
-    /// memory unsafety issues with future users of the `String`, as the rest of
-    /// the standard library assumes that `String`s are valid UTF-8.
+    /// This function is unsafe because the returned `&mut Vec` allows writing
+    /// bytes which are not valid UTF-8. If this constraint is violated, using
+    /// the original `String` after dropping the `&mut Vec` may violate memory
+    /// safety, as the rest of the standard library assumes that `String`s are
+    /// valid UTF-8.
     ///
     /// # Examples
     ///
@@ -1521,7 +1618,7 @@ impl String {
     }
 
     /// Returns the length of this `String`, in bytes, not [`char`]s or
-    /// graphemes. In other words, it may not be what a human considers the
+    /// graphemes. In other words, it might not be what a human considers the
     /// length of the string.
     ///
     /// # Examples
@@ -1536,8 +1633,8 @@ impl String {
     /// assert_eq!(fancy_f.len(), 4);
     /// assert_eq!(fancy_f.chars().count(), 3);
     /// ```
-    #[doc(alias = "length")]
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn len(&self) -> usize {
         self.vec.len()
@@ -1557,6 +1654,7 @@ impl String {
     /// assert!(!v.is_empty());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn is_empty(&self) -> bool {
         self.len() == 0
@@ -1619,17 +1717,24 @@ impl String {
         self.vec.clear()
     }
 
-    /// Creates a draining iterator that removes the specified range in the `String`
-    /// and yields the removed `chars`.
+    /// Removes the specified range from the string in bulk, returning all
+    /// removed characters as an iterator.
     ///
-    /// Note: The element range is removed even if the iterator is not
-    /// consumed until the end.
+    /// The returned iterator keeps a mutable borrow on the string to optimize
+    /// its implementation.
     ///
     /// # Panics
     ///
     /// Panics if the starting point or end point do not lie on a [`char`]
     /// boundary, or if they're out of bounds.
     ///
+    /// # Leaking
+    ///
+    /// If the returned iterator goes out of scope without being dropped (due to
+    /// [`core::mem::forget`], for example), the string may still contain a copy
+    /// of any drained characters, or may have lost characters arbitrarily,
+    /// including characters outside the range.
+    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -1643,7 +1748,7 @@ impl String {
     /// assert_eq!(t, "α is alpha, ");
     /// assert_eq!(s, "β is beta");
     ///
-    /// // A full range clears the string
+    /// // A full range clears the string, like `clear()` does
     /// s.drain(..);
     /// assert_eq!(s, "");
     /// ```
@@ -1724,11 +1829,11 @@ impl String {
         unsafe { self.as_mut_vec() }.splice((start, end), replace_with.bytes());
     }
 
-    /// Converts this `String` into a [`Box`]`<`[`str`]`>`.
+    /// Converts this `String` into a <code>[Box]<[str]></code>.
     ///
     /// This will drop any excess capacity.
     ///
-    /// [`str`]: prim@str
+    /// [str]: prim@str "str"
     ///
     /// # Examples
     ///
@@ -1741,6 +1846,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "box_str", since = "1.4.0")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
     pub fn into_boxed_str(self) -> Box<str> {
         let slice = self.vec.into_boxed_slice();
@@ -1763,6 +1869,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(&[0, 159], value.unwrap_err().as_bytes());
     /// ```
+    #[must_use]
     #[stable(feature = "from_utf8_error_as_bytes", since = "1.26.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.bytes[..]
@@ -1786,6 +1893,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(vec![0, 159], value.unwrap_err().into_bytes());
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.bytes
@@ -1798,8 +1906,8 @@ impl FromUtf8Error {
     /// an analogue to `FromUtf8Error`. See its documentation for more details
     /// on using it.
     ///
-    /// [`std::str`]: core::str
-    /// [`&str`]: prim@str
+    /// [`std::str`]: core::str "std::str"
+    /// [`&str`]: prim@str "&str"
     ///
     /// # Examples
     ///
@@ -1814,6 +1922,7 @@ impl FromUtf8Error {
     /// // the first byte is invalid here
     /// assert_eq!(1, error.valid_up_to());
     /// ```
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn utf8_error(&self) -> Utf8Error {
         self.error
@@ -1834,6 +1943,22 @@ impl fmt::Display for FromUtf16Error {
     }
 }
 
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf8Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-8"
+//     }
+// }
+
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf16Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-16"
+//     }
+// }
+
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "rust1", since = "1.0.0")]
 impl Clone for String {
@@ -2057,17 +2182,38 @@ impl<'a, 'b> Pattern<'a> for &'b String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl PartialEq for String {
+impl<A: Allocator, B: Allocator> PartialEq<String<B>> for String<A> {
     #[inline]
-    fn eq(&self, other: &String) -> bool {
+    fn eq(&self, other: &String<B>) -> bool {
         PartialEq::eq(&self[..], &other[..])
     }
     #[inline]
-    fn ne(&self, other: &String) -> bool {
+    fn ne(&self, other: &String<B>) -> bool {
         PartialEq::ne(&self[..], &other[..])
     }
 }
 
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Eq for String<A> {}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator, B: Allocator>  PartialOrd<String<B>> for String<A> {
+    #[inline]
+    fn partial_cmp(&self, other: &String<B>) -> Option<Ordering> {
+        PartialOrd::partial_cmp(&self[..], &other[..])
+    }
+}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Ord for String<A> {
+    #[inline]
+    fn cmp(&self, other: &String<A>) -> Ordering {
+        Ord::cmp(&self[..], &other[..])
+    }
+}
+
+
+
 macro_rules! impl_eq {
     ($lhs:ty, $rhs: ty) => {
         #[stable(feature = "rust1", since = "1.0.0")]
@@ -2202,7 +2348,7 @@ impl AddAssign<&str> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::Range<usize>> for String {
+impl<A: Allocator> ops::Index<ops::Range<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2211,7 +2357,7 @@ impl ops::Index<ops::Range<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeTo<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2220,7 +2366,7 @@ impl ops::Index<ops::RangeTo<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeFrom<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2229,7 +2375,7 @@ impl ops::Index<ops::RangeFrom<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFull> for String {
+impl<A: Allocator> ops::Index<ops::RangeFull> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2238,7 +2384,7 @@ impl ops::Index<ops::RangeFull> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2247,7 +2393,7 @@ impl ops::Index<ops::RangeInclusive<usize>> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeToInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2257,42 +2403,42 @@ impl ops::Index<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::Range<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::Range<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::Range<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeTo<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeTo<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFrom<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeFrom<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFull> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFull> for String<A> {
     #[inline]
     fn index_mut(&mut self, _index: ops::RangeFull) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeToInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeToInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
@@ -2300,7 +2446,7 @@ impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Deref for String {
+impl<A: Allocator> ops::Deref for String<A> {
     type Target = str;
 
     #[inline]
@@ -2310,7 +2456,7 @@ impl ops::Deref for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::DerefMut for String {
+impl<A: Allocator> ops::DerefMut for String<A> {
     #[inline]
     fn deref_mut(&mut self) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
@@ -2321,7 +2467,7 @@ impl ops::DerefMut for String {
 ///
 /// This alias exists for backwards compatibility, and may be eventually deprecated.
 ///
-/// [`Infallible`]: core::convert::Infallible
+/// [`Infallible`]: core::convert::Infallible "convert::Infallible"
 #[stable(feature = "str_parse_error", since = "1.5.0")]
 pub type ParseError = core::convert::Infallible;
 
@@ -2608,7 +2754,7 @@ impl<'a> From<&'a str> for Cow<'a, str> {
     /// assert_eq!(Cow::from("eggplant"), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a str) -> Cow<'a, str> {
         Cow::Borrowed(s)
@@ -2631,7 +2777,7 @@ impl<'a> From<String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(s), Cow::<'static, str>::Owned(s2));
     /// ```
     ///
-    /// [`Owned`]: crate::borrow::Cow::Owned
+    /// [`Owned`]: crate::borrow::Cow::Owned "borrow::Cow::Owned"
     #[inline]
     fn from(s: String) -> Cow<'a, str> {
         Cow::Owned(s)
@@ -2653,7 +2799,7 @@ impl<'a> From<&'a String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(&s), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a String) -> Cow<'a, str> {
         Cow::Borrowed(s.as_str())
@@ -2697,7 +2843,7 @@ impl From<String> for Vec<u8> {
     /// let v1 = Vec::from(s1);
     ///
     /// for b in v1 {
-    ///     println!("{}", b);
+    ///     println!("{b}");
     /// }
     /// ```
     fn from(string: String) -> Vec<u8> {
@@ -2771,14 +2917,14 @@ impl<'a> Drain<'a> {
     /// # Examples
     ///
     /// ```
-    /// #![feature(string_drain_as_str)]
     /// let mut s = String::from("abc");
     /// let mut drain = s.drain(..);
     /// assert_eq!(drain.as_str(), "abc");
     /// let _ = drain.next().unwrap();
     /// assert_eq!(drain.as_str(), "bc");
     /// ```
-    #[unstable(feature = "string_drain_as_str", issue = "76905")] // Note: uncomment AsRef impls below when stabilizing.
+    #[must_use]
+    #[stable(feature = "string_drain_as_str", since = "1.55.0")]
     pub fn as_str(&self) -> &str {
         self.iter.as_str()
     }
diff --git a/rust/alloc/sync.rs b/rust/alloc/sync.rs
index 1f4e44680..382f849d7 100644
--- a/rust/alloc/sync.rs
+++ b/rust/alloc/sync.rs
@@ -21,13 +21,13 @@ use core::marker::{PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
-#[cfg(not(no_global_oom_handling))]
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
 #[cfg(not(no_global_oom_handling))]
 use core::slice::from_raw_parts_mut;
 use core::sync::atomic;
-use core::sync::atomic::Ordering::{Acquire, Relaxed, Release, SeqCst};
+use core::sync::atomic::Ordering::{Acquire, Relaxed, Release};
 
 #[cfg(not(no_global_oom_handling))]
 use crate::alloc::handle_alloc_error;
@@ -101,8 +101,8 @@ macro_rules! acquire {
 /// first: after all, isn't the point of `Arc<T>` thread safety? The key is
 /// this: `Arc<T>` makes it thread safe to have multiple ownership of the same
 /// data, but it  doesn't add thread safety to its data. Consider
-/// `Arc<`[`RefCell<T>`]`>`. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
-/// [`Send`], `Arc<`[`RefCell<T>`]`>` would be as well. But then we'd have a problem:
+/// <code>Arc<[RefCell\<T>]></code>. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
+/// [`Send`], <code>Arc<[RefCell\<T>]></code> would be as well. But then we'd have a problem:
 /// [`RefCell<T>`] is not thread safe; it keeps track of the borrowing count using
 /// non-atomic operations.
 ///
@@ -148,7 +148,7 @@ macro_rules! acquire {
 /// use std::sync::Arc;
 ///
 /// let my_arc = Arc::new(());
-/// Arc::downgrade(&my_arc);
+/// let my_weak = Arc::downgrade(&my_arc);
 /// ```
 ///
 /// `Arc<T>`'s implementations of traits like `Clone` may also be called using
@@ -178,6 +178,7 @@ macro_rules! acquire {
 /// [deref]: core::ops::Deref
 /// [downgrade]: Arc::downgrade
 /// [upgrade]: Weak::upgrade
+/// [RefCell\<T>]: core::cell::RefCell
 /// [`RefCell<T>`]: core::cell::RefCell
 /// [`std::sync`]: ../../std/sync/index.html
 /// [`Arc::clone(&from)`]: Arc::clone
@@ -201,14 +202,14 @@ macro_rules! acquire {
 ///     let five = Arc::clone(&five);
 ///
 ///     thread::spawn(move || {
-///         println!("{:?}", five);
+///         println!("{five:?}");
 ///     });
 /// }
 /// ```
 ///
 /// Sharing a mutable [`AtomicUsize`]:
 ///
-/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize
+/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize "sync::atomic::AtomicUsize"
 ///
 /// ```no_run
 /// use std::sync::Arc;
@@ -222,7 +223,7 @@ macro_rules! acquire {
 ///
 ///     thread::spawn(move || {
 ///         let v = val.fetch_add(1, Ordering::SeqCst);
-///         println!("{:?}", v);
+///         println!("{v:?}");
 ///     });
 /// }
 /// ```
@@ -233,35 +234,52 @@ macro_rules! acquire {
 /// [rc_examples]: crate::rc#examples
 #[cfg_attr(not(test), rustc_diagnostic_item = "Arc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Arc<T: ?Sized> {
+pub struct Arc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<ArcInner<T>>,
     phantom: PhantomData<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Arc<T, A> {}
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Arc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Arc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Arc<U>> for Arc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Arc<U, A>> for Arc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Arc<U>> for Arc<T> {}
 
 impl<T: ?Sized> Arc<T> {
-    fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
     }
 
     unsafe fn from_ptr(ptr: *mut ArcInner<T>) -> Self {
-        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+        unsafe { Self::from_ptr_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
+    unsafe fn from_inner_in(ptr: NonNull<ArcInner<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
+    }
+
+    unsafe fn from_ptr_in(ptr: *mut ArcInner<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
 /// `Weak` is a version of [`Arc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Arc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Arc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -280,7 +298,10 @@ impl<T: ?Sized> Arc<T> {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "arc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesn’t need
@@ -288,15 +309,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Weak<T, A> {}
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
 
@@ -341,49 +363,83 @@ impl<T> Arc<T> {
     pub fn new(data: T) -> Arc<T> {
         // Start the weak pointer count as 1 which is the weak pointer that's
         // held by all the strong pointers (kinda), see std/rc.rs for more info
-        let x: Box<_> = box ArcInner {
+        let x: Box<_> = Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(1),
             weak: atomic::AtomicUsize::new(1),
             data,
-        };
-        Self::from_inner(Box::leak(x).into())
+        });
+        unsafe { Self::from_inner(Box::leak(x).into()) }
     }
 
-    /// Constructs a new `Arc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Arc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
     ///
-    /// # Examples
-    /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Arc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Arc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Arc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Arc<T>` is not fully-constructed until `Arc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
     ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # #![allow(dead_code)]
     /// use std::sync::{Arc, Weak};
     ///
-    /// struct Foo {
-    ///     me: Weak<Foo>,
+    /// struct Gadget {
+    ///     me: Weak<Gadget>,
     /// }
     ///
-    /// let foo = Arc::new_cyclic(|me| Foo {
-    ///     me: me.clone(),
-    /// });
+    /// impl Gadget {
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Arc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Arc` we're constructing.
+    ///         Arc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
+    ///         })
+    ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Arc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
+    /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Arc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Arc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box ArcInner {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(0),
             weak: atomic::AtomicUsize::new(1),
             data: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
         let init_ptr: NonNull<ArcInner<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -395,7 +451,7 @@ impl<T> Arc<T> {
 
         // Now we can properly initialize the inner value and turn our weak
         // reference into a strong reference.
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).data), data);
 
@@ -413,9 +469,9 @@ impl<T> Arc<T> {
             // possible with safe code alone.
             let prev_value = (*inner).strong.fetch_add(1, Release);
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
-        }
 
-        let strong = Arc::from_inner(init_ptr);
+            Arc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -435,17 +491,16 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -475,9 +530,10 @@ impl<T> Arc<T> {
     /// assert_eq!(*zero, 0)
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -492,10 +548,18 @@ impl<T> Arc<T> {
     /// `data` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(data: T) -> Pin<Arc<T>> {
         unsafe { Pin::new_unchecked(Arc::new(data)) }
     }
 
+    /// Constructs a new `Pin<Arc<T>>`, return an error if allocation fails.
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin(data: T) -> Result<Pin<Arc<T>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new(data)?)) }
+    }
+
     /// Constructs a new `Arc<T>`, returning an error if allocation fails.
     ///
     /// # Examples
@@ -517,7 +581,7 @@ impl<T> Arc<T> {
             weak: atomic::AtomicUsize::new(1),
             data,
         })?;
-        Ok(Self::from_inner(Box::leak(x).into()))
+        unsafe { Ok(Self::from_inner(Box::leak(x).into())) }
     }
 
     /// Constructs a new `Arc` with uninitialized contents, returning an error
@@ -533,12 +597,10 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -587,6 +649,246 @@ impl<T> Arc<T> {
             )?))
         }
     }
+}
+
+impl<T, A: Allocator> Arc<T, A> {
+    /// Constructs a new `Arc<T>` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(data: T, alloc: A) -> Arc<T, A> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        );
+        let (ptr, alloc) = Box::into_unique(x);
+        unsafe { Self::from_inner_in(ptr.into(), alloc) }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator. If `T` does not implement `Unpin`,
+    /// then `data` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(data: T, alloc: A) -> Pin<Arc<T, A>> {
+        unsafe { Pin::new_unchecked(Arc::new_in(data, alloc)) }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator, return an error if allocation
+    /// fails.
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin_in(data: T, alloc: A) -> Result<Pin<Arc<T, A>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new_in(data, alloc)?)) }
+    }
+
+    /// Constructs a new `Arc<T, A>` in the provided allocator, returning an error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::try_new_in(5, System)?;
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(data: T, alloc: A) -> Result<Arc<T, A>, AllocError> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::try_new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        )?;
+        let (ptr, alloc) = Box::into_unique(x);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, in the provided allocator, returning an
+    /// error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if allocation
+    /// fails.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
     /// Returns the inner value, if the `Arc` has exactly one strong reference.
     ///
     /// Otherwise, an [`Err`] is returned with the same `Arc` that was
@@ -617,9 +919,10 @@ impl<T> Arc<T> {
 
         unsafe {
             let elem = ptr::read(&this.ptr.as_ref().data);
+            let alloc = ptr::read(&this.alloc); // copy the allocator
 
             // Make a weak pointer to clean up the implicit strong-weak reference
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc };
             mem::forget(this);
 
             Ok(elem)
@@ -640,19 +943,19 @@ impl<T> Arc<[T]> {
     ///
     /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe { Arc::from_ptr(Arc::allocate_for_slice(len)) }
     }
@@ -676,9 +979,10 @@ impl<T> Arc<[T]> {
     /// assert_eq!(*values, [0, 0, 0])
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -693,7 +997,83 @@ impl<T> Arc<[T]> {
     }
 }
 
-impl<T> Arc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Constructs a new atomically reference-counted slice with uninitialized contents in the
+    /// provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Arc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe { Arc::from_ptr_in(Arc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new atomically reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Arc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut ArcInner<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Arc<mem::MaybeUninit<T>, A> {
     /// Converts to `Arc<T>`.
     ///
     /// # Safety
@@ -704,7 +1084,7 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     /// Calling this when the content is not yet fully initialized
     /// causes immediate undefined behavior.
     ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
     ///
     /// # Examples
     ///
@@ -716,64 +1096,184 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<T> {
-        Arc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Arc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
     }
 }
 
-impl<T> Arc<[mem::MaybeUninit<T>]> {
+impl<T, A: Allocator> Arc<[mem::MaybeUninit<T>], A> {
     /// Converts to `Arc<[T]>`.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    ///
+    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Arc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Arc<T> {
+    /// Constructs an `Arc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Arc<T>` is never accessed.
+    ///
+    /// [into_raw]: Arc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let x = Arc::new("hello".to_owned());
+    /// let x_ptr = Arc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Arc` to prevent leak.
+    ///     let x = Arc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Arc::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let five = Arc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
+    ///
+    ///     // This assertion is deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Arc::increment_strong_count_in(ptr, Global) }
+    }
+
+    /// Decrements the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method. This method can be used to release the final
+    /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
+    /// released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::sync::Arc;
     ///
-    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Arc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     // Those assertions are deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    ///     Arc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Arc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<[T]> {
-        unsafe { Arc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Arc::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Consumes the `Arc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Arc` using
@@ -788,6 +1288,7 @@ impl<T: ?Sized> Arc<T> {
     /// let x_ptr = Arc::into_raw(x);
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use = "losing the pointer will leak memory"]
     #[stable(feature = "rc_raw", since = "1.17.0")]
     pub fn into_raw(this: Self) -> *const T {
         let ptr = Self::as_ptr(&this);
@@ -811,6 +1312,7 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(x_ptr, Arc::as_ptr(&y));
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(this: &Self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(this.ptr);
@@ -821,10 +1323,10 @@ impl<T: ?Sized> Arc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).data) }
     }
 
-    /// Constructs an `Arc<T>` from a raw pointer.
+    /// Constructs an `Arc<T, A>` from a raw pointer.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// [`Arc<U, A>::into_raw`][into_raw] where `U` must have the same size and
     /// alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
@@ -835,7 +1337,8 @@ impl<T: ?Sized> Arc<T> {
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Arc<T>` is never accessed.
+    /// even if the returned `Arc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Arc::into_raw
     /// [transmute]: core::mem::transmute
@@ -843,14 +1346,17 @@ impl<T: ?Sized> Arc<T> {
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let x = Arc::new("hello".to_owned());
+    /// let x = Arc::new_in("hello".to_owned(), System);
     /// let x_ptr = Arc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Arc` to prevent leak.
-    ///     let x = Arc::from_raw(x_ptr);
+    ///     let x = Arc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
@@ -858,15 +1364,15 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         unsafe {
             let offset = data_offset(ptr);
 
             // Reverse the offset to find the original ArcInner.
             let arc_ptr = (ptr as *mut ArcInner<T>).set_ptr_value((ptr as *mut u8).offset(-offset));
 
-            Self::from_ptr(arc_ptr)
+            Self::from_ptr_in(arc_ptr, alloc)
         }
     }
 
@@ -881,8 +1387,13 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// let weak_five = Arc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Arc`"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         // This Relaxed is OK because we're checking the value in the CAS
         // below.
         let mut cur = this.inner().weak.load(Relaxed);
@@ -906,7 +1417,7 @@ impl<T: ?Sized> Arc<T> {
                 Ok(_) => {
                     // Make sure we do not create a dangling Weak
                     debug_assert!(!is_dangling(this.ptr.as_ptr()));
-                    return Weak { ptr: this.ptr };
+                    return Weak { ptr: this.ptr, alloc: this.alloc.clone() };
                 }
                 Err(old) => cur = old,
             }
@@ -934,9 +1445,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(1, Arc::weak_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn weak_count(this: &Self) -> usize {
-        let cnt = this.inner().weak.load(SeqCst);
+        let cnt = this.inner().weak.load(Acquire);
         // If the weak count is currently locked, the value of the
         // count was 0 just before taking the lock.
         if cnt == usize::MAX { 0 } else { cnt - 1 }
@@ -963,9 +1475,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(2, Arc::strong_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn strong_count(this: &Self) -> usize {
-        this.inner().strong.load(SeqCst)
+        this.inner().strong.load(Acquire)
     }
 
     /// Increments the strong reference count on the `Arc<T>` associated with the
@@ -975,30 +1488,37 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// The pointer must have been obtained through `Arc::into_raw`, and the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method,, and `ptr` must point to a block of memory
+    /// allocated by `alloc`.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // This assertion is deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Arc, but don't touch refcount by wrapping in ManuallyDrop
-        let arc = unsafe { mem::ManuallyDrop::new(Arc::<T>::from_raw(ptr)) };
+        let arc = unsafe { mem::ManuallyDrop::new(Arc::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _arc_clone: mem::ManuallyDrop<_> = arc.clone();
     }
@@ -1008,35 +1528,39 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// The pointer must have been obtained through `Arc::into_raw`,  the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release the final
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final
     /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
     /// released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // Those assertions are deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
-    ///     Arc::decrement_strong_count(ptr);
+    ///     Arc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Arc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Arc::from_raw_in(ptr, alloc)) };
     }
 
     #[inline]
@@ -1052,16 +1576,17 @@ impl<T: ?Sized> Arc<T> {
     // Non-inlined part of `drop`.
     #[inline(never)]
     unsafe fn drop_slow(&mut self) {
-        // Destroy the data at this time, even though we may not free the box
-        // allocation itself (there may still be weak pointers lying around).
+        // Destroy the data at this time, even though we must not free the box
+        // allocation itself (there might still be weak pointers lying around).
         unsafe { ptr::drop_in_place(Self::get_mut_unchecked(self)) };
 
         // Drop the weak ref collectively held by all strong references
-        drop(Weak { ptr: self.ptr });
+        // Take a reference to `self.alloc` instead of cloning because 1. it'll
+        // last long enough, and 2. you should be able to drop `Arc`s with
+        // unclonable allocators
+        drop(Weak { ptr: self.ptr, alloc: &self.alloc });
     }
 
-    #[inline]
-    #[stable(feature = "ptr_eq", since = "1.17.0")]
     /// Returns `true` if the two `Arc`s point to the same allocation
     /// (in a vein similar to [`ptr::eq`]).
     ///
@@ -1078,7 +1603,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert!(!Arc::ptr_eq(&five, &other_five));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
+    #[inline]
+    #[must_use]
+    #[stable(feature = "ptr_eq", since = "1.17.0")]
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
@@ -1137,28 +1665,30 @@ impl<T: ?Sized> Arc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Allocates an `ArcInner<T>` with sufficient space for an unsized inner value.
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut ArcInner<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut ArcInner<T> {
         // Allocate for the `ArcInner<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Arc::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut ArcInner<T>).set_ptr_value(mem) as *mut ArcInner<T>,
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut ArcInner<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Arc<T> {
+    fn from_box_in(v: Box<T, A>) -> Arc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1168,9 +1698,9 @@ impl<T: ?Sized> Arc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1277,6 +1807,34 @@ impl<T> Arc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Allocates an `ArcInner<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut ArcInner<[T]> {
+        unsafe {
+            Arc::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut ArcInner<[T]>,
+            )
+        }
+    }
+
+    /// Copy elements from slice into newly allocated Arc<\[T\]>
+    ///
+    /// Unsafe because the caller must either take ownership or bind `T: Copy`.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn copy_from_slice_in(v: &[T], alloc: A) -> Arc<[T], A> {
+        unsafe {
+            let ptr = Self::allocate_for_slice_in(v.len(), &alloc);
+
+            ptr::copy_nonoverlapping(v.as_ptr(), &mut (*ptr).data as *mut [T] as *mut T, v.len());
+
+            Self::from_ptr_in(ptr, alloc)
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 #[cfg(not(no_global_oom_handling))]
 trait ArcFromSlice<T> {
@@ -1300,7 +1858,7 @@ impl<T: Copy> ArcFromSlice<T> for Arc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Arc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Arc<T, A> {
     /// Makes a clone of the `Arc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1316,7 +1874,7 @@ impl<T: ?Sized> Clone for Arc<T> {
     /// let _ = Arc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Arc<T> {
+    fn clone(&self) -> Arc<T, A> {
         // Using a relaxed ordering is alright here, as knowledge of the
         // original reference prevents other threads from erroneously deleting
         // the object.
@@ -1330,25 +1888,26 @@ impl<T: ?Sized> Clone for Arc<T> {
         // [1]: (www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html)
         let old_size = self.inner().strong.fetch_add(1, Relaxed);
 
-        // However we need to guard against massive refcounts in case someone
-        // is `mem::forget`ing Arcs. If we don't do this the count can overflow
-        // and users will use-after free. We racily saturate to `isize::MAX` on
-        // the assumption that there aren't ~2 billion threads incrementing
-        // the reference count at once. This branch will never be taken in
-        // any realistic program.
+        // However we need to guard against massive refcounts in case someone is `mem::forget`ing
+        // Arcs. If we don't do this the count can overflow and users will use-after free. This
+        // branch will never be taken in any realistic program. We abort because such a program is
+        // incredibly degenerate, and we don't care to support it.
         //
-        // We abort because such a program is incredibly degenerate, and we
-        // don't care to support it.
+        // This check is not 100% water-proof: we error when the refcount grows beyond `isize::MAX`.
+        // But we do that check *after* having done the increment, so there is a chance here that
+        // the worst already happened and we actually do overflow the `usize` counter. However, that
+        // requires the counter to grow from `isize::MAX` to `usize::MAX` between the increment
+        // above and the `abort` below, which seems exceedingly unlikely.
         if old_size > MAX_REFCOUNT {
             abort();
         }
 
-        Self::from_inner(self.ptr)
+        unsafe { Self::from_inner_in(self.ptr, self.alloc.clone()) }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Arc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Arc<T, A> {
     type Target = T;
 
     #[inline]
@@ -1360,21 +1919,22 @@ impl<T: ?Sized> Deref for Arc<T> {
 #[unstable(feature = "receiver_trait", issue = "none")]
 impl<T: ?Sized> Receiver for Arc<T> {}
 
-impl<T: Clone> Arc<T> {
+impl<T: Clone, A: Allocator + Clone> Arc<T, A> {
     /// Makes a mutable reference into the given `Arc`.
     ///
-    /// If there are other `Arc` or [`Weak`] pointers to the same allocation,
-    /// then `make_mut` will create a new allocation and invoke [`clone`][clone] on the inner value
-    /// to ensure unique ownership. This is also referred to as clone-on-write.
+    /// If there are other `Arc` pointers to the same allocation, then `make_mut` will
+    /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
+    /// referred to as clone-on-write.
     ///
-    /// Note that this differs from the behavior of [`Rc::make_mut`] which disassociates
-    /// any remaining `Weak` pointers.
+    /// However, if there are no other `Arc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be dissociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`][get_mut], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or dissociating [`Weak`] pointers.
     ///
-    /// [clone]: Clone::clone
-    /// [get_mut]: Arc::get_mut
-    /// [`Rc::make_mut`]: super::rc::Rc::make_mut
+    /// [`clone`]: Clone::clone
+    /// [`get_mut`]: Arc::get_mut
     ///
     /// # Examples
     ///
@@ -1393,6 +1953,23 @@ impl<T: Clone> Arc<T> {
     /// assert_eq!(*data, 8);
     /// assert_eq!(*other_data, 12);
     /// ```
+    ///
+    /// [`Weak`] pointers will be dissociated:
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let mut data = Arc::new(75);
+    /// let weak = Arc::downgrade(&data);
+    ///
+    /// assert!(75 == *data);
+    /// assert!(75 == *weak.upgrade().unwrap());
+    ///
+    /// *Arc::make_mut(&mut data) += 1;
+    ///
+    /// assert!(76 == *data);
+    /// assert!(weak.upgrade().is_none());
+    /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
     #[stable(feature = "arc_unique", since = "1.4.0")]
@@ -1408,7 +1985,7 @@ impl<T: Clone> Arc<T> {
         if this.inner().strong.compare_exchange(1, 0, Acquire, Relaxed).is_err() {
             // Another strong pointer exists, so we must clone.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1429,10 +2006,10 @@ impl<T: Clone> Arc<T> {
 
             // Materialize our own implicit weak pointer, so that it can clean
             // up the ArcInner as needed.
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc: this.alloc.clone() };
 
             // Can just steal the data, all that's left is Weaks
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1448,9 +2025,44 @@ impl<T: Clone> Arc<T> {
         // either unique to begin with, or became one upon cloning the contents.
         unsafe { Self::get_mut_unchecked(this) }
     }
+
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `arc_t` is of type `Arc<T>`, this function is functionally equivalent to
+    /// `(*arc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, sync::Arc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let arc = Arc::new(inner);
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let arc = Arc::new(inner);
+    /// let arc2 = arc.clone();
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `arc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Arc::unwrap_or_clone(arc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
+    #[inline]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Arc::try_unwrap(this).unwrap_or_else(|arc| (*arc).clone())
+    }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Returns a mutable reference into the given `Arc`, if there are
     /// no other `Arc` or [`Weak`] pointers to the same allocation.
     ///
@@ -1458,7 +2070,7 @@ impl<T: ?Sized> Arc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Arc` pointers.
     ///
     /// [make_mut]: Arc::make_mut
     /// [clone]: Clone::clone
@@ -1555,7 +2167,7 @@ impl<T: ?Sized> Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Arc<T, A> {
     /// Drops the `Arc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1626,9 +2238,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
     }
 }
 
-impl Arc<dyn Any + Send + Sync> {
-    #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+impl<A: Allocator + Clone> Arc<dyn Any + Send + Sync, A> {
     /// Attempt to downcast the `Arc<dyn Any + Send + Sync>` to a concrete type.
     ///
     /// # Examples
@@ -1647,18 +2257,63 @@ impl Arc<dyn Any + Send + Sync> {
     /// print_if_string(Arc::new(my_string));
     /// print_if_string(Arc::new(0i8));
     /// ```
-    pub fn downcast<T>(self) -> Result<Arc<T>, Self>
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T>(self) -> Result<Arc<T, A>, Self>
     where
-        T: Any + Send + Sync + 'static,
+        T: Any + Send + Sync,
     {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<ArcInner<T>>();
-            mem::forget(self);
-            Ok(Arc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<ArcInner<T>>();
+                let alloc = self.alloc.clone();
+                mem::forget(self);
+                Ok(Arc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Arc<dyn Any + Send + Sync>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::sync::Arc;
+    ///
+    /// let x: Arc<dyn Any + Send + Sync> = Arc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T>(self) -> Arc<T, A>
+    where
+        T: Any + Send + Sync,
+    {
+        unsafe {
+            let ptr = self.ptr.cast::<ArcInner<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Arc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T> Weak<T> {
@@ -1675,9 +2330,40 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") }
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T, A>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Weak;
+    /// use std::alloc::System;
+    ///
+    /// let empty: Weak<i64, _> = Weak::new_in(System);
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -1689,6 +2375,55 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::{Arc, Weak};
+    ///
+    /// let strong = Arc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Arc::downgrade(&strong).into_raw();
+    /// let raw_2 = Arc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Arc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Arc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`new`]: Weak::new
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Weak::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -1713,7 +2448,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: core::ptr::null "ptr::null"
+    #[must_use]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(self.ptr);
@@ -1723,7 +2459,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as ArcInner (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).data) }
@@ -1757,6 +2493,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -1764,7 +2501,8 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
-    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>` in the provided
+    /// allocator.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
     /// later) or to deallocate the weak count by dropping the `Weak<T>`.
@@ -1775,7 +2513,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -1805,9 +2543,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`new`]: Weak::new
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
-    /// [`forget`]: std::mem::forget
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -1823,11 +2560,11 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 }
 
-impl<T: ?Sized> Weak<T> {
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Attempts to upgrade the `Weak` pointer to an [`Arc`], delaying
     /// dropping of the inner value if successful.
     ///
@@ -1851,8 +2588,13 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Arc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Arc<T>> {
+    pub fn upgrade(&self) -> Option<Arc<T, A>>
+    where
+        A: Clone,
+    {
         // We use a CAS loop to increment the strong count instead of a
         // fetch_add as this function should never take the reference count
         // from zero to one.
@@ -1879,7 +2621,7 @@ impl<T: ?Sized> Weak<T> {
             // value can be initialized after `Weak` references have already been created. In that case, we
             // expect to observe the fully initialized value.
             match inner.strong.compare_exchange_weak(n, n + 1, Acquire, Relaxed) {
-                Ok(_) => return Some(Arc::from_inner(self.ptr)), // null checked above
+                Ok(_) => return Some(unsafe { Arc::from_inner_in(self.ptr, self.alloc.clone()) }), // null checked above
                 Err(old) => n = old,
             }
         }
@@ -1888,9 +2630,10 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of strong (`Arc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
-        if let Some(inner) = self.inner() { inner.strong.load(SeqCst) } else { 0 }
+        if let Some(inner) = self.inner() { inner.strong.load(Acquire) } else { 0 }
     }
 
     /// Gets an approximation of the number of `Weak` pointers pointing to this
@@ -1904,12 +2647,13 @@ impl<T: ?Sized> Weak<T> {
     /// Due to implementation details, the returned value can be off by 1 in
     /// either direction when other threads are manipulating any `Arc`s or
     /// `Weak`s pointing to the same allocation.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
             .map(|inner| {
-                let weak = inner.weak.load(SeqCst);
-                let strong = inner.strong.load(SeqCst);
+                let weak = inner.weak.load(Acquire);
+                let strong = inner.strong.load(Acquire);
                 if strong == 0 {
                     0
                 } else {
@@ -1981,8 +2725,9 @@ impl<T: ?Sized> Weak<T> {
     /// assert!(!first.ptr_eq(&third));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -1990,7 +2735,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2003,11 +2748,11 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         let inner = if let Some(inner) = self.inner() {
             inner
         } else {
-            return Weak { ptr: self.ptr };
+            return Weak { ptr: self.ptr, alloc: self.alloc.clone() };
         };
         // See comments in Arc::clone() for why this is relaxed.  This can use a
         // fetch_add (ignoring the lock) because the weak count is only locked
@@ -2020,7 +2765,7 @@ impl<T: ?Sized> Clone for Weak<T> {
             abort();
         }
 
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
@@ -2046,7 +2791,7 @@ impl<T> Default for Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2084,25 +2829,27 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
 
         if inner.weak.fetch_sub(1, Release) == 1 {
             acquire!(inner.weak);
-            unsafe { Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr())) }
+            unsafe {
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()))
+            }
         }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait ArcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Arc<T>) -> bool;
-    fn ne(&self, other: &Arc<T>) -> bool;
+trait ArcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Arc<T, A>) -> bool;
+    fn ne(&self, other: &Arc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Arc<T>) -> bool {
+    default fn eq(&self, other: &Arc<T, A>) -> bool {
         **self == **other
     }
     #[inline]
-    default fn ne(&self, other: &Arc<T>) -> bool {
+    default fn ne(&self, other: &Arc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -2115,20 +2862,20 @@ impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + crate::rc::MarkerEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + crate::rc::MarkerEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         Arc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         !Arc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Arc<T, A> {
     /// Equality for two `Arc`s.
     ///
     /// Two `Arc`s are equal if their inner values are equal, even if they are
@@ -2147,7 +2894,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five == Arc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::eq(self, other)
     }
 
@@ -2168,13 +2915,13 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five != Arc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Arc<T, A> {
     /// Partial comparison for two `Arc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -2189,7 +2936,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Arc::new(6)));
     /// ```
-    fn partial_cmp(&self, other: &Arc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Arc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -2206,7 +2953,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five < Arc::new(6));
     /// ```
-    fn lt(&self, other: &Arc<T>) -> bool {
+    fn lt(&self, other: &Arc<T, A>) -> bool {
         *(*self) < *(*other)
     }
 
@@ -2223,7 +2970,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five <= Arc::new(5));
     /// ```
-    fn le(&self, other: &Arc<T>) -> bool {
+    fn le(&self, other: &Arc<T, A>) -> bool {
         *(*self) <= *(*other)
     }
 
@@ -2240,7 +2987,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five > Arc::new(4));
     /// ```
-    fn gt(&self, other: &Arc<T>) -> bool {
+    fn gt(&self, other: &Arc<T, A>) -> bool {
         *(*self) > *(*other)
     }
 
@@ -2257,12 +3004,12 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five >= Arc::new(5));
     /// ```
-    fn ge(&self, other: &Arc<T>) -> bool {
+    fn ge(&self, other: &Arc<T, A>) -> bool {
         *(*self) >= *(*other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Arc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Arc<T, A> {
     /// Comparison for two `Arc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -2277,29 +3024,29 @@ impl<T: ?Sized + Ord> Ord for Arc<T> {
     ///
     /// assert_eq!(Ordering::Less, five.cmp(&Arc::new(6)));
     /// ```
-    fn cmp(&self, other: &Arc<T>) -> Ordering {
+    fn cmp(&self, other: &Arc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Arc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Arc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Arc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Arc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Arc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -2324,7 +3071,7 @@ impl<T: Default> Default for Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Arc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Arc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state)
     }
@@ -2333,6 +3080,20 @@ impl<T: ?Sized + Hash> Hash for Arc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Arc<T> {
+    /// Converts a `T` into an `Arc<T>`
+    ///
+    /// The conversion moves the value into a
+    /// newly allocated `Arc`. It is equivalent to
+    /// calling `Arc::new(t)`.
+    ///
+    /// # Example
+    /// ```rust
+    /// # use std::sync::Arc;
+    /// let x = 5;
+    /// let arc = Arc::new(5);
+    ///
+    /// assert_eq!(Arc::from(x), arc);
+    /// ```
     fn from(t: T) -> Self {
         Arc::new(t)
     }
@@ -2397,7 +3158,7 @@ impl From<String> for Arc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Arc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Arc<T, A> {
     /// Move a boxed object to a new, reference-counted allocation.
     ///
     /// # Example
@@ -2409,14 +3170,14 @@ impl<T: ?Sized> From<Box<T>> for Arc<T> {
     /// assert_eq!("eggplant", &shared[..]);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Arc<T> {
-        Arc::from_box(v)
+    fn from(v: Box<T, A>) -> Arc<T, A> {
+        Arc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Arc<[T]> {
+impl<T, A: Allocator + Clone> From<Vec<T, A>> for Arc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -2428,9 +3189,9 @@ impl<T> From<Vec<T>> for Arc<[T]> {
     /// assert_eq!(&[1, 2, 3], &shared[..]);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Arc<[T]> {
+    fn from(mut v: Vec<T, A>) -> Arc<[T], A> {
         unsafe {
-            let arc = Arc::copy_from_slice(&v);
+            let arc = Arc::copy_from_slice_in(&v, v.allocator().clone());
 
             // Allow the Vec to free its memory, but not destroy its contents
             v.set_len(0);
@@ -2493,13 +3254,33 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Arc<str>> for Arc<[u8]> {
+    /// Converts an atomically reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::sync::Arc;
+    /// let string: Arc<str> = Arc::from("eggplant");
+    /// let bytes: Arc<[u8]> = Arc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Arc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Arc::from_raw(Arc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
-impl<T, const N: usize> TryFrom<Arc<[T]>> for Arc<[T; N]> {
-    type Error = Arc<[T]>;
+impl<T, A: Allocator + Clone, const N: usize> TryFrom<Arc<[T], A>> for Arc<[T; N], A> {
+    type Error = Arc<[T], A>;
 
-    fn try_from(boxed_slice: Arc<[T]>) -> Result<Self, Self::Error> {
+    fn try_from(boxed_slice: Arc<[T], A>) -> Result<Self, Self::Error> {
         if boxed_slice.len() == N {
-            Ok(unsafe { Arc::from_raw(Arc::into_raw(boxed_slice) as *mut [T; N]) })
+            let alloc = boxed_slice.alloc.clone();
+            Ok(unsafe { Arc::from_raw_in(Arc::into_raw(boxed_slice) as *mut [T; N], alloc) })
         } else {
             Err(boxed_slice)
         }
@@ -2592,21 +3373,21 @@ impl<T, I: iter::TrustedLen<Item = T>> ToArcSlice<T> for I {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Arc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Arc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Arc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Arc<T, A> {}
 
 /// Get the offset within an `ArcInner` for the payload behind a pointer.
 ///
@@ -2620,7 +3401,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
diff --git a/rust/helpers.c b/rust/helpers.c
index 3416351e0..4967c2932 100644
--- a/rust/helpers.c
+++ b/rust/helpers.c
@@ -44,7 +44,28 @@
 #include <linux/irqstage.h>
 #include <linux/dovetail.h>
 #include <linux/spinlock_pipeline.h>
-
+#include <linux/preempt.h>
+#include <linux/signal_types.h>
+#include <linux/spinlock_types.h>
+#include <linux/log2.h>
+#include <linux/capability.h>
+#include <linux/spinlock.h>
+#include <linux/spinlock_types.h>
+#include <linux/capability.h>
+#include <asm/uaccess.h>
+#include <linux/dovetail.h>
+#include <linux/spinlock_pipeline.h>
+#include <linux/log2.h>
+#include <linux/capability.h>
+#include <linux/spinlock.h>
+#include <linux/spinlock_types.h>
+
+#include <net/sock.h>
+#include <linux/jhash.h>
+#include <linux/bottom_half.h>
+#include <linux/if_vlan.h>
+#include <linux/skbuff.h>
+#include <linux/hashtable.h>
 void rust_helper_BUG(void)
 {
 	BUG();
@@ -270,6 +291,12 @@ int rust_helper_page_aligned(unsigned long size)
 }
 EXPORT_SYMBOL_GPL(rust_helper_page_aligned); 
 
+size_t rust_helper_align(size_t x, unsigned long a)
+{
+	return ALIGN(x,a);
+}
+EXPORT_SYMBOL_GPL(rust_helper_align); 
+
 bool rust_helper_running_inband(void)
 {
 	return running_inband();
@@ -343,6 +370,12 @@ void rust_helper_list_del(struct list_head *list)
 }
 EXPORT_SYMBOL_GPL(rust_helper_list_del);
 
+void rust_helper_list_del_init(struct list_head *list)
+{
+	list_del_init(list);
+}
+EXPORT_SYMBOL_GPL(rust_helper_list_del_init);
+
 const struct cpumask* rust_helper_cpumask_of(int cpu)
 {
 	return cpumask_of(cpu);
@@ -369,12 +402,17 @@ int rust_helper_cpumask_first(const struct cpumask *srcp)
 }
 EXPORT_SYMBOL_GPL(rust_helper_cpumask_first);
 
+void rust_helper_list_add(struct list_head *new,struct list_head *head)
+{
+	list_add(new,head);
+}
+EXPORT_SYMBOL_GPL(rust_helper_list_add);
+
 void rust_helper_list_add_tail(struct list_head *new,struct list_head *head)
 {
 	list_add_tail(new,head);
 }
 EXPORT_SYMBOL_GPL(rust_helper_list_add_tail);
-
 void rust_helper_rcu_read_lock(void)
 {
 	rcu_read_lock();
@@ -426,6 +464,42 @@ void rust_helper_atomic_set(atomic_t *v, int i)
 }
 EXPORT_SYMBOL_GPL(rust_helper_atomic_set);
 
+void rust_helper_atomic_inc(atomic_t *v)
+{
+	atomic_inc(v);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_inc);
+
+bool rust_helper_atomic_dec_and_test(atomic_t *v)
+{
+	return atomic_dec_and_test(v);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_dec_and_test);
+
+int rust_helper_atomic_dec_return(atomic_t *v)
+{
+	return atomic_dec_return(v);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_dec_return);
+
+int rust_helper_atomic_cmpxchg(atomic_t *v, int old, int new)
+{
+	return atomic_cmpxchg(v, old, new);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_cmpxchg);
+
+int rust_helper_atomic_read(atomic_t *v)
+{
+	return atomic_read(v);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_read);
+
+void rust_helper_atomic_add(unsigned int i,atomic_t *v)
+{
+	atomic_add(i,v);
+}
+EXPORT_SYMBOL_GPL(rust_helper_atomic_add);
+
 void rust_helper_init_irq_work(struct irq_work *work, void (*func)(struct irq_work *))
 {
 	init_irq_work(work, func);
@@ -515,11 +589,21 @@ ktime_t rust_helper_ktime_add(ktime_t kt, ktime_t nsval) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_ktime_add);
 
+ktime_t rust_helper_ktime_compare(ktime_t cmp1, ktime_t cmp2) {
+	return ktime_compare(cmp1,cmp2);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ktime_compare);
+
 ktime_t rust_helper_ktime_set(const s64 secs, const unsigned long nsecs) {
 	return ktime_set(secs,nsecs);
 }
 EXPORT_SYMBOL_GPL(rust_helper_ktime_set);
 
+s64 rust_helper_timespec64_to_ktime(struct timespec64 ts){
+	return ktime_set(ts.tv_sec, ts.tv_nsec);
+}
+EXPORT_SYMBOL_GPL(rust_helper_timespec64_to_ktime);
+
 s64 rust_helper_ktime_divns(const ktime_t kt, s64 div) {
 	return ktime_divns(kt,div);
 }
@@ -607,21 +691,6 @@ void rust_helper_stall_oob(void) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_stall_oob);
 
-void rust_helper_cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp) {
-	cpumask_set_cpu(cpu,dstp);
-}
-EXPORT_SYMBOL_GPL(rust_helper_cpumask_set_cpu);
-
-void rust_helper_smp_wmb(void) {
-	smp_wmb();
-}
-EXPORT_SYMBOL_GPL(rust_helper_smp_wmb);
-
-void rust_helper_preempt_enable(void) {
-	preempt_enable();
-}
-EXPORT_SYMBOL_GPL(rust_helper_preempt_enable);
-
 void rust_helper_preempt_disable(void) {
 	preempt_disable();
 }
@@ -632,6 +701,7 @@ void rust_helper_dovetail_leave_oob(void) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_dovetail_leave_oob);
 
+
 void rust_helper_hard_spin_lock(struct raw_spinlock *rlock) {
 	hard_spin_lock(rlock);
 }
@@ -642,6 +712,365 @@ void rust_helper_hard_spin_unlock(struct raw_spinlock *rlock) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_hard_spin_unlock);
 
+void rust_helper_raw_spin_lock(hard_spinlock_t *lock){
+	raw_spin_lock(lock);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_spin_lock);
+
+void rust_helper_raw_spin_unlock(hard_spinlock_t *lock){
+	raw_spin_unlock(lock);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_spin_unlock);
+
+inline int rust_helper_ilog2(size_t size) {
+	return ilog2(size);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ilog2);
+
+int rust_helper_ffs(unsigned long x) {
+	return ffs(x);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ffs);
+
+
+kernel_cap_t rust_helper_current_cap(void) {
+	return current_cap();
+}
+EXPORT_SYMBOL_GPL(rust_helper_current_cap);
+
+int rust_helper_cap_raised(kernel_cap_t c, int flag) {
+	return cap_raised(c, flag);
+}
+EXPORT_SYMBOL_GPL(rust_helper_cap_raised);
+
+struct timespec64 rust_helper_ktime_to_timespec64(ktime_t kt) {
+	return ktime_to_timespec64(kt);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ktime_to_timespec64);
+
+int rust_helper_cpumask_test_cpu(int cpu, const struct cpumask *cpumask) {
+	return cpumask_test_cpu(cpu, cpumask);
+}
+EXPORT_SYMBOL_GPL(rust_helper_cpumask_test_cpu);
+
+void rust_helper_raw_spin_lock_irqsave(hard_spinlock_t *lock, unsigned long *flags) {
+	raw_spin_lock_irqsave(lock, *flags);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_spin_lock_irqsave);
+
+void rust_helper_cap_raise(kernel_cap_t *c, int flag) {
+	cap_raise(*c, flag);
+}
+EXPORT_SYMBOL_GPL(rust_helper_cap_raise);
+
+struct oob_mm_state* rust_helper_doveail_mm_state(void) {
+	return dovetail_mm_state();
+}
+EXPORT_SYMBOL_GPL(rust_helper_doveail_mm_state);
+
+int rust_helper_put_user(int i, int *addr) {
+	return put_user(i, addr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_put_user);
+
+void rust_helper_smp_wmb(void) {
+	smp_wmb();
+}
+EXPORT_SYMBOL_GPL(rust_helper_smp_wmb);
+
+void rust_helper_raw_spin_unlock_irqrestore(hard_spinlock_t *lock, unsigned long flags) {
+	raw_spin_unlock_irqrestore(lock, flags);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_spin_unlock_irqrestore);
+
+void rust_helper_cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp) {
+	cpumask_set_cpu(cpu,dstp);
+}
+EXPORT_SYMBOL_GPL(rust_helper_cpumask_set_cpu);
+
+void rust_helper_preempt_enable(void) {
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(rust_helper_preempt_enable);
+
+struct net * rust_helper_sock_net(const struct sock *sk) {
+	return sock_net(sk);
+}
+EXPORT_SYMBOL_GPL(rust_helper_sock_net);
+
+u32 rust_helper_jhash(const void *key, u32 length, u32 initval){
+	return jhash(key,length,initval);
+}
+EXPORT_SYMBOL_GPL(rust_helper_jhash);
+
+u32 rust_helper_jhash2(const void *key, u32 length, u32 initval){
+	return jhash2(key,length,initval);
+}
+EXPORT_SYMBOL_GPL(rust_helper_jhash2);
+
+
+void rust_helper_local_bh_disable (void) {
+	local_bh_disable();
+}
+EXPORT_SYMBOL_GPL(rust_helper_local_bh_disable);
+
+void rust_helper_local_bh_enable (void) {
+	local_bh_enable();
+}
+EXPORT_SYMBOL_GPL(rust_helper_local_bh_enable);
+
+void rust_helper_skb_list_del_init(struct sk_buff *skb){
+	skb_list_del_init(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_list_del_init);
+
+void rust_helper_list_splice_init(struct list_head *list,struct list_head *head){
+	list_splice_init(list,head);
+}
+EXPORT_SYMBOL_GPL(rust_helper_list_splice_init);
+
+bool rust_helper_list_empty(struct list_head *head){
+	return list_empty(head);
+}
+EXPORT_SYMBOL_GPL(rust_helper_list_empty);
+
+u_int64_t rust_helper_BITS_TO_LONGS(int nr) {
+	return BITS_TO_LONGS(nr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_BITS_TO_LONGS);
+
+void* rust_helper_this_cpu_ptr(void* ptr){
+	return this_cpu_ptr(ptr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_this_cpu_ptr);
+
+__be16 rust_helper_vlan_dev_vlan_proto(const struct net_device *dev){
+	return vlan_dev_vlan_proto(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_vlan_dev_vlan_proto);
+__u16 rust_helper_vlan_dev_vlan_id(const struct net_device *dev){
+	return vlan_dev_vlan_id(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_vlan_dev_vlan_id);
+
+struct net_device* rust_helper_vlan_dev_real_dev(const struct net_device *dev){
+	return vlan_dev_real_dev(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_vlan_dev_real_dev);
+
+__u16 rust_helper_vlan_dev_get_egress_qos_mask(struct net_device *dev,u32 skprio){
+	return vlan_dev_get_egress_qos_mask(dev,skprio);
+}
+EXPORT_SYMBOL_GPL(rust_helper_vlan_dev_get_egress_qos_mask);
+
+bool rust_helper_is_vlan_dev(const struct net_device *dev){
+	return is_vlan_dev(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_is_vlan_dev);
+
+// void rust_helper_hash_del(struct hlist_node *n){
+// 	hash_del_init(n);
+// }
+// EXPORT_SYMBOL_GPL(rust_helper_hash_del);
+
+void rust_helper_hash_add(struct hlist_head *hashtable,size_t length,struct hlist_node *node,u32 key)
+{
+	hlist_add_head(node, &hashtable[hash_min(key, ilog2(length))]);
+}
+EXPORT_SYMBOL_GPL(rust_helper_hash_add);
+
+void rust_helper_hash_del(struct hlist_node* node){
+	hlist_del_init(node);
+}
+EXPORT_SYMBOL_GPL(rust_helper_hash_del);
+
+
+// 这个函数是自己加的
+struct hlist_head* rust_helper_get_hlist_head(struct hlist_head *hashtable,size_t length,u32 key){
+	return &hashtable[hash_min(key, ilog2(length))];
+}
+EXPORT_SYMBOL_GPL(rust_helper_get_hlist_head);
+
+__be16 rust_helper_htons(__u16 x){
+	return htons(x);
+}
+EXPORT_SYMBOL_GPL(rust_helper_htons);
+
+__u16 rust_helper_ntohs(__be16 x){
+	return ntohs(x);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ntohs);
+
+void rust_helper_raw_spin_lock_init(raw_spinlock_t *lock){
+	raw_spin_lock_init(lock);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_spin_lock_init);
+
+bool rust_helper_hard_irqs_disabled(void){
+	return native_irqs_disabled();
+}
+EXPORT_SYMBOL_GPL(rust_helper_hard_irqs_disabled);
+
+bool rust_helper_rros_enable_preempt_top_part(void){
+	return(--dovetail_current_state()->preempt_count == 0);
+}
+EXPORT_SYMBOL_GPL(rust_helper_rros_enable_preempt_top_part);
+
+void rust_helper_rros_disable_preempt(void){
+	dovetail_current_state()->preempt_count++;
+}
+EXPORT_SYMBOL_GPL(rust_helper_rros_disable_preempt);
+
+unsigned int rust_helper_raw_get_user(unsigned int *x,unsigned int* ptr){
+	unsigned int tmp;
+	unsigned ret = __get_user(tmp,ptr);
+	*x = tmp;
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_get_user);
+
+unsigned int rust_helper_raw_get_user_64(unsigned long *x,unsigned long* ptr){
+	unsigned int tmp;
+	unsigned ret = __get_user(tmp,ptr);
+	*x = tmp;
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_get_user_64);
+
+int rust_helper_raw_put_user(unsigned int x,unsigned int* ptr){
+	return __put_user(x,ptr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_put_user);
+
+unsigned long rust_helper_raw_copy_from_user(void* dst,const void*src,unsigned long size){
+	return raw_copy_to_user(dst,src,size);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_copy_from_user);
+
+unsigned long rust_helper_raw_copy_to_user(void* dst,const void*src,unsigned long size){
+	return raw_copy_to_user(dst,src,size);
+}
+EXPORT_SYMBOL_GPL(rust_helper_raw_copy_to_user);
+
+void rust_helper_bitmap_copy(unsigned long *dst, const unsigned long *src,unsigned int nbits){
+	bitmap_copy(dst,src,nbits);
+}
+EXPORT_SYMBOL_GPL(rust_helper_bitmap_copy);
+
+void rust_helper_init_work(struct work_struct*work,void (*rust_helper_work_func)(struct work_struct *work)){
+	INIT_WORK(work,rust_helper_work_func);
+}
+EXPORT_SYMBOL_GPL(rust_helper_init_work);
+
+#ifdef CONFIG_NET_OOB
+void rust_helper__vlan_hwaccel_put_tag(struct sk_buff *skb, __be16 vlan_proto, __u16 vlan_tci){
+	__vlan_hwaccel_put_tag(skb,vlan_proto,vlan_tci);
+}
+EXPORT_SYMBOL_GPL(rust_helper__vlan_hwaccel_put_tag);
+
+int rust_helper__vlan_hwaccel_get_tag(struct sk_buff *skb, __u16* vlan_tci){
+	return __vlan_hwaccel_get_tag(skb,vlan_tci);
+}
+EXPORT_SYMBOL_GPL(rust_helper__vlan_hwaccel_get_tag);
+
+void rust_helper_netdev_is_oob_capable(struct net_device *dev){
+	netdev_is_oob_capable(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_netdev_is_oob_capable);
+
+struct sk_buff* rust_helper_netdev_alloc_oob_skb(struct net_device *dev,dma_addr_t *dma_addr){
+	return netdev_alloc_oob_skb(dev,dma_addr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_netdev_alloc_oob_skb);
+
+struct sk_buff* rust_helper_netif_oob_diversion(const struct net_device *dev){
+	return netif_oob_diversion(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_netif_oob_diversion);
+
+void rust_helper_set_bit(int nr, volatile unsigned long *addr){
+	set_bit(nr,addr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_set_bit);
+
+void rust_helper_clear_bit(int nr, volatile unsigned long *addr){
+	clear_bit(nr,addr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_clear_bit);
+
+struct net_device *
+rust_helper_netdev_notifier_info_to_dev(const struct netdev_notifier_info *info){
+	return info->dev;
+}
+EXPORT_SYMBOL_GPL(rust_helper_netdev_notifier_info_to_dev);
+
+struct net*
+rust_helper_dev_net(const struct net_device *dev){
+	return dev_net(dev);
+}
+EXPORT_SYMBOL_GPL(rust_helper_dev_net);
+
+void rust_helper_dev_kfree_skb(struct sk_buff *skb){
+	dev_kfree_skb(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_dev_kfree_skb);
+
+__u16 rust_helper_skb_vlan_tag_get_id(const struct sk_buff *skb){
+	return skb_vlan_tag_get_id(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_vlan_tag_get_id);
+
+void rust_helper_skb_morph_oob_skb(struct sk_buff *n, struct sk_buff *skb){
+	skb_morph_oob_skb(n,skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_morph_oob_skb);
+
+unsigned char * rust_helper_skb_mac_header(struct sk_buff *skb){
+	return skb_mac_header(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_mac_header);
+
+#include<linux/netdevice.h>
+int rust_helper_dev_parse_header(const struct sk_buff *skb,unsigned char *haddr){
+	return dev_parse_header(skb,haddr);
+}
+EXPORT_SYMBOL_GPL(rust_helper_dev_parse_header);
+
+bool rust_helper_eth_type_vlan(__be16 ethertype){
+	return eth_type_vlan(ethertype);
+}
+EXPORT_SYMBOL_GPL(rust_helper_eth_type_vlan);
+
+void rust_helper_skb_reset_mac_header(struct sk_buff *skb){
+	skb_reset_mac_header(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_reset_mac_header);
+
+int rust_helper_skb_tailroom(const struct sk_buff *skb){
+	return skb_tailroom(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_tailroom);
+
+bool rust_helper_dev_validate_header(const struct net_device* dev,char* ll_header,int len){
+	return dev_validate_header(dev,ll_header,len);
+}
+EXPORT_SYMBOL_GPL(rust_helper_dev_validate_header);
+
+__be16 rust_helper_dev_parse_header_protocol(const struct sk_buff *skb){
+	return dev_parse_header_protocol(skb);
+}
+EXPORT_SYMBOL_GPL(rust_helper_dev_parse_header_protocol);
+
+void rust_helper_skb_set_network_header(struct sk_buff *skb,int offset){
+	skb_set_network_header(skb,offset);
+}
+EXPORT_SYMBOL_GPL(rust_helper_skb_set_network_header);
+
+void rust_helper___vlan_hwaccel_put_tag(struct sk_buff *skb, __be16 vlan_proto, __u16 vlan_tci){
+	__vlan_hwaccel_put_tag(skb,vlan_proto,vlan_tci);
+}
+EXPORT_SYMBOL_GPL(rust_helper___vlan_hwaccel_put_tag);
+#endif 
 /* We use bindgen's --size_t-is-usize option to bind the C size_t type
  * as the Rust usize type, so we can use it in contexts where Rust
  * expects a usize like slice (array) indices. usize is defined to be
diff --git a/rust/kernel/allocator.rs b/rust/kernel/allocator.rs
index 759cec47d..7aaba73be 100644
--- a/rust/kernel/allocator.rs
+++ b/rust/kernel/allocator.rs
@@ -4,10 +4,10 @@
 
 use core::alloc::{GlobalAlloc, Layout};
 use core::ptr;
-
+use crate::timekeeping::*;
 use crate::bindings;
 use crate::c_types;
-
+use crate::pr_info;
 pub struct KernelAllocator;
 
 unsafe impl GlobalAlloc for KernelAllocator {
@@ -32,7 +32,10 @@ static ALLOCATOR: KernelAllocator = KernelAllocator;
 // let's generate them ourselves instead.
 #[no_mangle]
 pub fn __rust_alloc(size: usize, _align: usize) -> *mut u8 {
-    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 }
+    // pr_info!("__rust_alloc: time1 is {} size is {}",ktime_get_real_fast_ns(),size);
+    let x = unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 };
+    // pr_info!("__rust_alloc: time2 is {}",ktime_get_real_fast_ns());
+    return x;
 }
 
 #[no_mangle]
diff --git a/rust/kernel/bindings_helper.h b/rust/kernel/bindings_helper.h
index 6da0fa3fe..2a5a8f89a 100644
--- a/rust/kernel/bindings_helper.h
+++ b/rust/kernel/bindings_helper.h
@@ -27,7 +27,14 @@
 #include <uapi/linux/sched/types.h>
 #include <linux/irq_pipeline.h>
 #include <net/net_namespace.h>
+#include <asm-generic/evl/netdevice.h>
 #include <linux/netdevice.h>
+#include <uapi/linux/unistd.h>
+#include <net/sock.h>
+#include <linux/net.h>
+#include <linux/bottom_half.h>
+#include <uapi/linux/unistd.h>
+#include <linux/capability.h>
 
 // `bindgen` gets confused at certain things
 const gfp_t BINDINGS_GFP_KERNEL = GFP_KERNEL;
diff --git a/rust/kernel/bitmap.rs b/rust/kernel/bitmap.rs
index efcbe2e83..20bea1f0e 100644
--- a/rust/kernel/bitmap.rs
+++ b/rust/kernel/bitmap.rs
@@ -7,21 +7,29 @@
 use crate::{bindings, c_types};
 
 extern "C" {
-    fn rust_helper_test_and_set_bit(bit: c_types::c_uint, p: *mut c_types::c_ulong) -> c_types::c_int;
+    fn rust_helper_test_and_set_bit(
+        bit: c_types::c_uint,
+        p: *mut c_types::c_ulong,
+    ) -> c_types::c_int;
 }
 
 pub fn bitmap_zalloc(nbits: c_types::c_uint, flags: bindings::gfp_t) -> u64 {
-    unsafe{bindings::bitmap_zalloc(nbits, flags) as u64}
+    unsafe { bindings::bitmap_zalloc(nbits, flags) as u64 }
 }
 
-pub fn find_first_zero_bit(addr: *const c_types::c_ulong, size: c_types::c_ulong) -> c_types::c_ulong {
-    unsafe{bindings::_find_first_zero_bit(addr, size)}
+pub fn find_first_zero_bit(
+    addr: *const c_types::c_ulong,
+    size: c_types::c_ulong,
+) -> c_types::c_ulong {
+    unsafe { bindings::_find_first_zero_bit(addr, size) }
 }
 
 pub fn test_and_set_bit(bit: c_types::c_ulong, p: *mut c_types::c_ulong) -> bool {
     let res;
-    unsafe{ res = rust_helper_test_and_set_bit(bit as c_types::c_uint, p);}
-    if res == 1{
+    unsafe {
+        res = rust_helper_test_and_set_bit(bit as c_types::c_uint, p);
+    }
+    if res == 1 {
         return true;
     } else {
         return false;
diff --git a/rust/kernel/chrdev.rs b/rust/kernel/chrdev.rs
index 294cfa558..62abb1b39 100644
--- a/rust/kernel/chrdev.rs
+++ b/rust/kernel/chrdev.rs
@@ -134,7 +134,7 @@ impl<const N: usize> Registration<{ N }> {
     /// Registers a character device.
     ///
     /// You may call this once per device type, up to `N` times.
-    pub fn register<T: file_operations::FileOpener<()>>(self: Pin<&mut Self>) -> Result {
+    pub fn register<T: file_operations::FileOpener<u8>>(self: Pin<&mut Self>) -> Result {
         // SAFETY: We must ensure that we never move out of `this`.
         let this = unsafe { self.get_unchecked_mut() };
         if this.inner.is_none() {
@@ -177,16 +177,19 @@ impl<const N: usize> Registration<{ N }> {
     }
 }
 
+// FIXME: update the kernel-doc comment once the API is finalised as below.
 impl<const N: usize> file_operations::FileOpenAdapter for Registration<{ N }> {
-    type Arg = ();
+    type Arg = u8;
 
     unsafe fn convert(
-        _inode: *mut bindings::inode,
-        _file: *mut bindings::file,
+        inode: *mut bindings::inode,
+        file: *mut bindings::file,
     ) -> *const Self::Arg {
+        // FIXME: this is a temporary hack
+        unsafe{bindings::stream_open(inode, file);}
         // TODO: Update the SAFETY comment on the call to `FileOperationsVTable::build` above once
         // this is updated to retrieve state.
-        &()
+        inode as *const u8
     }
 }
 
diff --git a/rust/kernel/class.rs b/rust/kernel/class.rs
index a104bf5cd..a2fce1ab0 100644
--- a/rust/kernel/class.rs
+++ b/rust/kernel/class.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
@@ -6,7 +6,7 @@
 
 use core::u32;
 
-use crate::{bindings, c_types, error::Error, Result,};
+use crate::{bindings, c_types, error::Error, Result};
 
 extern "C" {
     // #[allow(improper_ctypes)]
diff --git a/rust/kernel/clockchips.rs b/rust/kernel/clockchips.rs
index b1bde7545..d0466b675 100644
--- a/rust/kernel/clockchips.rs
+++ b/rust/kernel/clockchips.rs
@@ -3,118 +3,135 @@
 //! clockchips
 //!
 //! C header: [`include/linux/clockchips.h`](../../../../include/linux/clockchips.h)
-use crate::{
-    bindings,
-    c_types,
-    prelude::*,
-};
+use crate::{bindings, c_types, prelude::*};
 
 type ktime_t = i64;
-pub struct Clock_Proxy_Device{
-    pub ptr:*mut bindings::clock_proxy_device,
+pub struct Clock_Proxy_Device {
+    pub ptr: *mut bindings::clock_proxy_device,
 }
 
-impl Clock_Proxy_Device{
-    pub fn new(cpd:*mut bindings::clock_proxy_device) ->Result<Self>{
+impl Clock_Proxy_Device {
+    pub fn new(cpd: *mut bindings::clock_proxy_device) -> Result<Self> {
         let ptr = cpd;
         if ptr.is_null() {
             pr_warn!("init clock_proxy_device error!");
             return Err(Error::EINVAL);
         }
-        Ok(Self{ptr})
+        Ok(Self { ptr })
     }
-    
-    pub fn get_ptr(&self) -> *mut bindings::clock_proxy_device{
-        return unsafe{&mut *self.ptr as *mut bindings::clock_proxy_device};
+
+    pub fn get_ptr(&self) -> *mut bindings::clock_proxy_device {
+        return unsafe { &mut *self.ptr as *mut bindings::clock_proxy_device };
     }
-    
-    pub fn get_proxy_device(&self) -> *mut bindings::clock_event_device{
-        unsafe{(&mut (*self.ptr).proxy_device) as *mut bindings::clock_event_device}
+
+    pub fn get_proxy_device(&self) -> *mut bindings::clock_event_device {
+        unsafe { (&mut (*self.ptr).proxy_device) as *mut bindings::clock_event_device }
     }
-    
-    pub fn get_real_device(&self) -> *mut bindings::clock_event_device{
-        unsafe{((*self.ptr).real_device) as *mut bindings::clock_event_device}
+
+    pub fn get_real_device(&self) -> *mut bindings::clock_event_device {
+        unsafe { ((*self.ptr).real_device) as *mut bindings::clock_event_device }
     }
 
-    pub fn set_handle_oob_event(&self, func: unsafe extern "C" fn(dev: *mut bindings::clock_event_device)) {
-        unsafe{(*self.ptr).handle_oob_event = Some(func)};
+    pub fn set_handle_oob_event(
+        &self,
+        func: unsafe extern "C" fn(dev: *mut bindings::clock_event_device),
+    ) {
+        unsafe { (*self.ptr).handle_oob_event = Some(func) };
     }
 }
 
-pub struct Clock_Event_Device{
-    pub ptr:*mut bindings::clock_event_device,
+pub struct Clock_Event_Device {
+    pub ptr: *mut bindings::clock_event_device,
 }
-impl Clock_Event_Device{
-    pub fn from_proxy_device(ced:*mut bindings::clock_event_device) ->Result<Self>{
+impl Clock_Event_Device {
+    pub fn from_proxy_device(ced: *mut bindings::clock_event_device) -> Result<Self> {
         let ptr = ced;
         if ptr.is_null() {
             pr_warn!("get proxy_device error!");
             return Err(Error::EINVAL);
         }
-        Ok(Self{ptr})
+        Ok(Self { ptr })
     }
 
-    pub fn get_ptr(&self) -> *mut bindings::clock_event_device{
-        return unsafe{&mut *self.ptr as *mut bindings::clock_event_device};
+    pub fn get_ptr(&self) -> *mut bindings::clock_event_device {
+        return unsafe { &mut *self.ptr as *mut bindings::clock_event_device };
     }
-    
-    pub fn get_features(&self) -> c_types::c_uint{
-        unsafe{(*self.ptr).features}
+
+    pub fn get_features(&self) -> c_types::c_uint {
+        unsafe { (*self.ptr).features }
     }
 
-    pub fn set_features(&self,num:c_types::c_uint){
-        unsafe{(*self.ptr).features = num};
+    pub fn set_features(&self, num: c_types::c_uint) {
+        unsafe { (*self.ptr).features = num };
     }
 
-    pub fn set_set_next_ktime(&self,func:unsafe extern "C" fn(expires: ktime_t, arg1: *mut bindings::clock_event_device) -> c_types::c_int){
-        unsafe{(*self.ptr).set_next_ktime = Some(func)};
+    pub fn set_set_next_ktime(
+        &self,
+        func: unsafe extern "C" fn(
+            expires: ktime_t,
+            arg1: *mut bindings::clock_event_device,
+        ) -> c_types::c_int,
+    ) {
+        unsafe { (*self.ptr).set_next_ktime = Some(func) };
     }
 
-    pub fn set_next_ktime(&self, evt: ktime_t, arg1: *mut bindings::clock_event_device) -> c_types::c_int {
-        unsafe{
+    pub fn set_next_ktime(
+        &self,
+        evt: ktime_t,
+        arg1: *mut bindings::clock_event_device,
+    ) -> c_types::c_int {
+        unsafe {
             if (*self.ptr).set_next_ktime.is_some() {
                 return (*self.ptr).set_next_ktime.unwrap()(evt, arg1);
-            }   
+            }
             return 1;
         }
     }
 
-    pub fn get_set_state_oneshot_stopped(&self) -> Option<
-        unsafe extern "C" fn(arg1: *mut bindings::clock_event_device) -> c_types::c_int,
-    >{
-        unsafe{(*self.ptr).set_state_oneshot_stopped}
+    pub fn get_set_state_oneshot_stopped(
+        &self,
+    ) -> Option<unsafe extern "C" fn(arg1: *mut bindings::clock_event_device) -> c_types::c_int>
+    {
+        unsafe { (*self.ptr).set_state_oneshot_stopped }
     }
 
-    pub fn set_set_state_oneshot_stopped(&self, func:unsafe extern "C" fn(arg1: *mut bindings::clock_event_device) -> c_types::c_int){
-        unsafe{(*self.ptr).set_state_oneshot_stopped = Some(func)};
+    pub fn set_set_state_oneshot_stopped(
+        &self,
+        func: unsafe extern "C" fn(arg1: *mut bindings::clock_event_device) -> c_types::c_int,
+    ) {
+        unsafe { (*self.ptr).set_state_oneshot_stopped = Some(func) };
     }
 
     pub fn get_max_delta_ns(&self) -> u64 {
-        unsafe{(*self.ptr).max_delta_ns as u64}
+        unsafe { (*self.ptr).max_delta_ns as u64 }
     }
 
     pub fn get_min_delta_ns(&self) -> u64 {
-        unsafe{(*self.ptr).min_delta_ns as u64}
+        unsafe { (*self.ptr).min_delta_ns as u64 }
     }
 
     pub fn get_mult(&self) -> u32 {
-        unsafe{(*self.ptr).mult as u32}
+        unsafe { (*self.ptr).mult as u32 }
     }
 
     pub fn get_shift(&self) -> u32 {
-        unsafe{(*self.ptr).shift as u32}
+        unsafe { (*self.ptr).shift as u32 }
     }
 
     pub fn get_min_delta_ticks(&self) -> u64 {
-        unsafe{(*self.ptr).min_delta_ticks as u64}
+        unsafe { (*self.ptr).min_delta_ticks as u64 }
     }
 
-    pub fn set_next_event(&self, evt: c_types::c_ulong, arg1: *mut bindings::clock_event_device) -> c_types::c_int {
-        unsafe{
+    pub fn set_next_event(
+        &self,
+        evt: c_types::c_ulong,
+        arg1: *mut bindings::clock_event_device,
+    ) -> c_types::c_int {
+        unsafe {
             if (*self.ptr).set_next_event.is_some() {
                 return (*self.ptr).set_next_event.unwrap()(evt, arg1);
-            }   
+            }
             return 1;
         }
     }
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/completion.rs b/rust/kernel/completion.rs
index c7e4b1282..8c3f02081 100644
--- a/rust/kernel/completion.rs
+++ b/rust/kernel/completion.rs
@@ -6,28 +6,27 @@
 
 use crate::bindings;
 
-extern "C"{
+extern "C" {
     fn rust_helper_init_completion(x: *mut bindings::completion);
 }
 
 pub struct Completion(bindings::completion);
 
 impl Completion {
-    pub fn new() -> Self{
+    pub fn new() -> Self {
         let completion = bindings::completion::default();
         Self(completion)
     }
-    
+
     pub fn init_completion(&mut self) {
-        unsafe{rust_helper_init_completion(&mut self.0 as *mut bindings::completion)}
+        unsafe { rust_helper_init_completion(&mut self.0 as *mut bindings::completion) }
     }
 
     pub fn complete(&mut self) {
-        unsafe{bindings::complete(&mut self.0 as *mut bindings::completion)}
+        unsafe { bindings::complete(&mut self.0 as *mut bindings::completion) }
     }
 
     pub fn wait_for_completion(&mut self) {
-        unsafe{bindings::wait_for_completion(&mut self.0 as *mut bindings::completion)}
+        unsafe { bindings::wait_for_completion(&mut self.0 as *mut bindings::completion) }
     }
 }
-
diff --git a/rust/kernel/cpumask.rs b/rust/kernel/cpumask.rs
index e122df288..77710a595 100644
--- a/rust/kernel/cpumask.rs
+++ b/rust/kernel/cpumask.rs
@@ -9,7 +9,7 @@ use crate::{
     error::{Error, Result},
     prelude::*,
 };
-extern "C"{
+extern "C" {
     fn rust_helper_num_possible_cpus() -> u32;
 }
 use core::iter::Iterator;
@@ -153,8 +153,11 @@ extern "C" {
         dstp: *mut bindings::cpumask,
     ) -> c_types::c_int;
     fn rust_helper_cpumask_copy(dstp: *mut bindings::cpumask, srcp: *const bindings::cpumask);
-    fn rust_helper_cpumask_and(dstp: *mut bindings::cpumask, srcp1: *const bindings::cpumask,
-                                srcp2: *const bindings::cpumask);
+    fn rust_helper_cpumask_and(
+        dstp: *mut bindings::cpumask,
+        srcp1: *const bindings::cpumask,
+        srcp2: *const bindings::cpumask,
+    );
     fn rust_helper_cpumask_empty(srcp: *const bindings::cpumask) -> c_types::c_int;
     fn rust_helper_cpumask_first(srcp: *const bindings::cpumask);
     fn rust_helper_cpumask_set_cpu(cpu: u32, dstp: *mut bindings::cpumask);
@@ -168,7 +171,7 @@ pub struct CpumaskVarT(bindings::cpumask_var_t);
 
 impl CpumaskT {
     pub const fn from_int(c: u64) -> Self {
-        Self(bindings::cpumask_t { bits: [c, 0 , 0, 0] })
+        Self(bindings::cpumask_t { bits: [c, 0, 0, 0] })
         // Self(bindings::cpumask_t { bits: [c] })
     }
 
@@ -176,7 +179,7 @@ impl CpumaskT {
         &mut self.0 as *mut bindings::cpumask_t
     }
 
-    pub fn cpu_mask_all() -> Self{
+    pub fn cpu_mask_all() -> Self {
         let c: u64 = u64::MAX;
         Self(bindings::cpumask_t { bits: [c, c, c, c] })
         // Self(bindings::cpumask_t { bits: [c] })
@@ -186,20 +189,19 @@ impl CpumaskT {
 #[cfg(not(CONFIG_CPUMASK_OFFSTACK))]
 impl CpumaskVarT {
     pub const fn from_int(c: u64) -> Self {
-        Self([bindings::cpumask_t { bits: [c, 0 , 0, 0] }])
+        Self([bindings::cpumask_t { bits: [c, 0, 0, 0] }])
         // Self([bindings::cpumask_t { bits: [c] }])
     }
 
-    pub fn alloc_cpumask_var(mask: &mut CpumaskVarT) -> Result<usize>{
+    pub fn alloc_cpumask_var(mask: &mut CpumaskVarT) -> Result<usize> {
         Ok(0)
     }
 
-    pub fn free_cpumask_var(mask: &mut CpumaskVarT) -> Result<usize>{
+    pub fn free_cpumask_var(mask: &mut CpumaskVarT) -> Result<usize> {
         Ok(0)
     }
 }
 
-
 #[cfg(CONFIG_CPUMASK_OFFSTACK)]
 impl CpumaskVarT {
     // todo: implement for x86_64/x86
@@ -221,13 +223,15 @@ pub fn cpumask_copy(dstp: *mut bindings::cpumask, srcp: *const bindings::cpumask
     unsafe { rust_helper_cpumask_copy(dstp, srcp) }
 }
 
-pub fn cpumask_and(dstp: *mut bindings::cpumask, srcp1: *const bindings::cpumask,
-                                srcp2: *const bindings::cpumask) {
+pub fn cpumask_and(
+    dstp: *mut bindings::cpumask,
+    srcp1: *const bindings::cpumask,
+    srcp2: *const bindings::cpumask,
+) {
     unsafe { rust_helper_cpumask_and(dstp, srcp1, srcp2) }
 }
 
-
-pub fn cpumask_empty(srcp: *const bindings::cpumask, ) -> Result<usize> {
+pub fn cpumask_empty(srcp: *const bindings::cpumask) -> Result<usize> {
     let res = unsafe { rust_helper_cpumask_empty(srcp) };
     if res == 1 {
         return Ok(0);
@@ -235,14 +239,14 @@ pub fn cpumask_empty(srcp: *const bindings::cpumask, ) -> Result<usize> {
     Err(Error::EINVAL)
 }
 
-pub fn cpumask_first(srcp: *const bindings::cpumask, ) {
+pub fn cpumask_first(srcp: *const bindings::cpumask) {
     unsafe { rust_helper_cpumask_first(srcp) }
 }
 
 pub fn num_possible_cpus() -> u32 {
-    unsafe{rust_helper_num_possible_cpus()}
+    unsafe { rust_helper_num_possible_cpus() }
 }
 
 pub fn cpumask_set_cpu(cpu: u32, dstp: *mut bindings::cpumask) {
-    unsafe {rust_helper_cpumask_set_cpu(cpu as c_types::c_uint, dstp) }
-}
\ No newline at end of file
+    unsafe { rust_helper_cpumask_set_cpu(cpu as c_types::c_uint, dstp) }
+}
diff --git a/rust/kernel/device.rs b/rust/kernel/device.rs
index 943e683bf..13dc12a47 100644
--- a/rust/kernel/device.rs
+++ b/rust/kernel/device.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
diff --git a/rust/kernel/double_linked_list.rs b/rust/kernel/double_linked_list.rs
index a7d3e0221..0f3cad394 100644
--- a/rust/kernel/double_linked_list.rs
+++ b/rust/kernel/double_linked_list.rs
@@ -3,199 +3,213 @@
 //! Double Linked lists.
 //!
 //! TODO: This module is a work in progress.
-use core::ptr::NonNull;
-use alloc::boxed::Box;
 use crate::prelude::*;
+use alloc::boxed::Box;
+use core::ptr::NonNull;
+use alloc::alloc_rros::*;
 
-#[derive(Debug,Clone,Copy)]
-pub struct Node<T>{
-   pub next: Option<NonNull<Node<T>>>,
-   pub prev: Option<NonNull<Node<T>>>,
-   pub value: T,
+#[derive(Debug, Clone, Copy)]
+pub struct Node<T> {
+    pub next: Option<NonNull<Node<T>>>,
+    pub prev: Option<NonNull<Node<T>>>,
+    pub value: T,
 }
 
 impl<T> Node<T> {
-   pub fn new(v: T) -> Self {
-      Node{
-         next: None,
-         prev: None,
-         value: v,
-      }
-   }
-
-   //self---n---next
-   pub fn add(&mut self, next: *mut Node<T>, n: T) {
-      let mut node = Box::try_new(Node::new(n)).unwrap();
-      node.next = NonNull::new(next);
-      node.prev = NonNull::new(self as *mut Node<T>);
-      let node = NonNull::new(Box::into_raw(node));
-
-      self.next = node;
-      unsafe{(*next).prev =  node;}
-   }
-
-   pub fn remove(&mut self) {
-      if self.next.is_some() && self.prev.is_some() {
-         unsafe{
-            let next = self.next.unwrap().as_ptr();
-            let prev = self.prev.unwrap().as_ptr();
-            if next == prev {//处理list中只有一个元素的情况
-               (*next).prev = None;
-               (*next).next = None;
-            } else {
-               (*next).prev = self.prev;
-               (*prev).next = self.next;
+    pub fn new(v: T) -> Self {
+        Node {
+            next: None,
+            prev: None,
+            value: v,
+        }
+    }
+
+    //self---n---next
+    pub fn add(&mut self, next: *mut Node<T>, n: T) {
+        // Box::try_new_in(123, RrosMem);
+        // let mut node = Box::try_new_in(Node::new(n), RrosMem).unwrap();
+        let mut node = Box::try_new(Node::new(n)).unwrap();
+        node.next = NonNull::new(next);
+        node.prev = NonNull::new(self as *mut Node<T>);
+        let node = NonNull::new(Box::into_raw(node));
+
+        self.next = node;
+        unsafe {
+            (*next).prev = node;
+        }
+    }
+
+    pub fn remove(&mut self) {
+        let free_ptr = unsafe{(*self.prev.unwrap().as_ptr()).next.unwrap().as_ptr()};
+        if self.next.is_some() && self.prev.is_some() {
+            unsafe {
+                let next = self.next.unwrap().as_ptr();
+                let prev = self.prev.unwrap().as_ptr();
+                if next == prev {
+                    //处理list中只有一个元素的情况
+                    (*next).prev = None;
+                    (*next).next = None;
+                } else {
+                    (*next).prev = self.prev;
+                    (*prev).next = self.next;
+                }
             }
-         }
-      }
-   }
+        }
+        unsafe{Box::from_raw(free_ptr as *mut Node<T>);}
+    }
 
-   pub fn into_val(self: Box<Self>) -> T {
-      self.value
-   }
+    pub fn into_val(self: Box<Self>) -> T {
+        self.value
+    }
 }
 
-#[derive(Clone,Copy)]
-pub struct List<T> { 
-   pub head: Node<T>
+#[derive(Clone, Copy)]
+pub struct List<T> {
+    pub head: Node<T>,
 }
 
-impl<T> List<T>{
-
-   pub fn new(v :T) -> Self{
-      List{ 
-         head: Node::new(v)//头节点
-      }
-   }
-
-   // 从队头入队
-   pub fn add_head(&mut self, v: T) {
-      if self.is_empty() {
-         let x = &mut self.head as *mut Node<T>;
-         unsafe{self.head.add(x, v);}
-      } else {
-         unsafe{self.head.add(self.head.next.unwrap().as_ptr(), v);}
-      }
-   }
-
-   //入队尾
-   pub fn add_tail(&mut self, v: T) {
-      if self.is_empty() {
-         let x = &mut self.head as *mut Node<T>;
-         unsafe{self.head.add(x, v);}
-      } else {
-         unsafe{
-            let prev = self.head.prev.unwrap().as_mut();
-            prev.add(&mut self.head as *mut Node<T>, v);
-         }
-      }
-      pr_info!("after add tail, the length is {}", self.len());
-   }
-
-   //得到队头
-   pub fn get_head<'a>(&self) -> Option<&'a mut Node<T>> {
-      if self.is_empty() {
-         return None;
-      } else {
-         Some(unsafe{self.head.next.unwrap().as_mut()})
-      }
-   }
-
-   //得到队尾
-   pub fn get_tail<'a>(&self) -> Option<&'a mut Node<T>> {
-      if self.is_empty() {
-         return None;
-      } else {
-         Some(unsafe{self.head.prev.unwrap().as_mut()})
-      }
-   }
-
-   //按index取node
-   pub fn get_by_index<'a>(&mut self, index: u32) -> Option<&'a mut Node<T>> {
-      if index <= self.len() {
-         let mut p = self.head.next;
-         for _ in 1..index {
-            p = unsafe{p.unwrap().as_ref().next};
-         }
-         return Some(unsafe{p.unwrap().as_mut()});
-      }else{
-         return None
-      }
-   }
-
-   //入到index之后 0表示队头
-   pub fn enqueue_by_index(&mut self, index: u32, v: T) {//测试通过
-      if index >= 0 && index <= self.len() {
-         if index == 0{
-            self.add_head(v);
-         }else if index == self.len() {
-            self.add_tail(v);
-         }else {
-            let x = self.get_by_index(index).unwrap();
-            let next = self.get_by_index(index + 1).unwrap();
-            x.add(next as *mut Node<T>, v);
-         }
-      }
-   }
-
-   //按index出队
-   pub fn dequeue(&mut self, index:u32) {
-      if self.len() == 1 && index == 1 {
-         self.head.next = None;
-         self.head.prev = None;
-      }else if index <= self.len() {
-         self.get_by_index(index).unwrap().remove();
-      }
-   }
-
-
-   //出队头
-   pub fn de_head(&mut self) {
-      self.dequeue(1);
-   }
-
-   //出队尾
-   pub fn de_tail(&mut self) {
-      self.dequeue(self.len());
-   }
-
-   // 计算链表长度
-   pub fn len(&self) -> u32 {
-      let mut ans = 0;
-      if !self.is_empty() {
-         ans = 1;
-         let mut p = self.head.next;
-         while p.unwrap().as_ptr() != self.head.prev.unwrap().as_ptr() {
-            ans = ans + 1;
-            unsafe{
-               p = p.unwrap().as_ref().next;
+impl<T> List<T> {
+    pub fn new(v: T) -> Self {
+        List {
+            head: Node::new(v), //头节点
+        }
+    }
+
+    // 从队头入队
+    pub fn add_head(&mut self, v: T) {
+        if self.is_empty() {
+            let x = &mut self.head as *mut Node<T>;
+            unsafe {
+                self.head.add(x, v);
+            }
+        } else {
+            unsafe {
+                self.head.add(self.head.next.unwrap().as_ptr(), v);
+            }
+        }
+    }
+
+    //入队尾
+    pub fn add_tail(&mut self, v: T) {
+        if self.is_empty() {
+            let x = &mut self.head as *mut Node<T>;
+            unsafe {
+                self.head.add(x, v);
+            }
+        } else {
+            unsafe {
+                let prev = self.head.prev.unwrap().as_mut();
+                prev.add(&mut self.head as *mut Node<T>, v);
             }
-          }
-      }
-      ans
-   }
+        }
+        // pr_info!("after add tail, the length is {}", self.len());
+    }
 
-   //判空
-   pub fn is_empty(&self) -> bool {
-      self.head.next.is_none() && self.head.prev.is_none()
-   }
-   
+    //得到队头
+    pub fn get_head<'a>(&self) -> Option<&'a mut Node<T>> {
+        if self.is_empty() {
+            return None;
+        } else {
+            Some(unsafe { self.head.next.unwrap().as_mut() })
+        }
+    }
 
+    //得到队尾
+    pub fn get_tail<'a>(&self) -> Option<&'a mut Node<T>> {
+        if self.is_empty() {
+            return None;
+        } else {
+            Some(unsafe { self.head.prev.unwrap().as_mut() })
+        }
+    }
+
+    //按index取node
+    pub fn get_by_index<'a>(&mut self, index: u32) -> Option<&'a mut Node<T>> {
+        if index <= self.len() {
+            let mut p = self.head.next;
+            for _ in 1..index {
+                p = unsafe { p.unwrap().as_ref().next };
+            }
+            return Some(unsafe { p.unwrap().as_mut() });
+        } else {
+            return None;
+        }
+    }
+
+    //入到index之后 0表示队头
+    pub fn enqueue_by_index(&mut self, index: u32, v: T) {
+        //测试通过
+        if index >= 0 && index <= self.len() {
+            if index == 0 {
+                self.add_head(v);
+            } else if index == self.len() {
+                self.add_tail(v);
+            } else {
+                let x = self.get_by_index(index).unwrap();
+                let next = self.get_by_index(index + 1).unwrap();
+                x.add(next as *mut Node<T>, v);
+            }
+        }
+    }
+
+    //按index出队
+    pub fn dequeue(&mut self, index: u32) {
+        if self.len() == 1 && index == 1 {
+            unsafe{Box::from_raw(self.head.next.as_mut().unwrap().as_ptr() as *mut Node<T>);}
+            self.head.next = None;
+            self.head.prev = None;
+        } else if index <= self.len() {
+            self.get_by_index(index).unwrap().remove();
+        }
+    }
+
+    //出队头
+    pub fn de_head(&mut self) {
+        self.dequeue(1);
+    }
+
+    //出队尾
+    pub fn de_tail(&mut self) {
+        self.dequeue(self.len());
+    }
+
+    // 计算链表长度
+    pub fn len(&self) -> u32 {
+        let mut ans = 0;
+        if !self.is_empty() {
+            ans = 1;
+            let mut p = self.head.next;
+            while p.unwrap().as_ptr() != self.head.prev.unwrap().as_ptr() {
+                ans = ans + 1;
+                unsafe {
+                    p = p.unwrap().as_ref().next;
+                }
+            }
+        }
+        ans
+    }
+
+    //判空
+    pub fn is_empty(&self) -> bool {
+        self.head.next.is_none() && self.head.prev.is_none()
+    }
 }
 
 //用于测试
 impl<i32: core::fmt::Display> List<i32> {
-   pub fn traverse(&self) {
-       if self.is_empty() {
-          return ;
-       }
-       let mut p = self.head.next;
-       unsafe{pr_info!("x is {}", p.unwrap().as_ref().value);}
-       while p.unwrap().as_ptr() != self.head.prev.unwrap().as_ptr() {
-         unsafe{
-            p = p.unwrap().as_ref().next;
+    pub fn traverse(&self) {
+        if self.is_empty() {
+            return;
+        }
+        let mut p = self.head.next;
+        unsafe {
             pr_info!("x is {}", p.unwrap().as_ref().value);
-         }
-       }
+        }
+        while p.unwrap().as_ptr() != self.head.prev.unwrap().as_ptr() {
+            unsafe {
+                p = p.unwrap().as_ref().next;
+                pr_info!("x is {}", p.unwrap().as_ref().value);
+            }
+        }
     }
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/double_linked_list2.rs b/rust/kernel/double_linked_list2.rs
index 685ffbeec..32bad3959 100644
--- a/rust/kernel/double_linked_list2.rs
+++ b/rust/kernel/double_linked_list2.rs
@@ -24,7 +24,11 @@ struct Node<T> {
 
 impl<T> Node<T> {
     fn new(element: T) -> Self {
-        Node { next: None, prev: None, element }
+        Node {
+            next: None,
+            prev: None,
+            element,
+        }
     }
 
     fn into_element(self: Box<Self>) -> T {
diff --git a/rust/kernel/double_linked_list_test.rs b/rust/kernel/double_linked_list_test.rs
index e69de29bb..8b1378917 100644
--- a/rust/kernel/double_linked_list_test.rs
+++ b/rust/kernel/double_linked_list_test.rs
@@ -0,0 +1 @@
+
diff --git a/rust/kernel/dovetail.rs b/rust/kernel/dovetail.rs
index 63b1268cf..71c69461a 100644
--- a/rust/kernel/dovetail.rs
+++ b/rust/kernel/dovetail.rs
@@ -1,9 +1,9 @@
-﻿use crate::{prelude::*, bindings};
+use crate::{bindings, prelude::*};
 
 pub fn dovetail_start() -> Result<usize> {
-    let res = unsafe{bindings::dovetail_start()};
+    let res = unsafe { bindings::dovetail_start() };
     if res == 0 {
         return Ok(0);
     }
     Err(Error::EINVAL)
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/error.rs b/rust/kernel/error.rs
index 1910b62fa..ba4187416 100644
--- a/rust/kernel/error.rs
+++ b/rust/kernel/error.rs
@@ -63,6 +63,24 @@ impl Error {
     /// Bad file number.
     pub const EBADF: Self = Error(-(bindings::EBADF as i32));
 
+    /// Resource deadlock would occur
+    pub const EDEADLK: Self = Error(-(bindings::EDEADLK as i32));
+
+    /// Connection timed out
+    pub const ETIMEDOUT: Self = Error(-(bindings::ETIMEDOUT as i32));
+
+    /// Owner died
+    pub const EOWNERDEAD: Self = Error(-(bindings::EOWNERDEAD as i32));
+
+    /// Identifier removed
+    pub const EIDRM: Self = Error(-(bindings::EIDRM as i32));
+
+    ///	Stale file handle
+    pub const ESTALE: Self = Error(-(bindings::ESTALE as i32));
+
+    /// Not a typewriter
+    pub const ENOTTY: Self = Error(-(bindings::ENOTTY as i32));
+
     /// Creates an [`Error`] from a kernel error code.
     ///
     /// It is a bug to pass an out-of-range `errno`. `EINVAL` would
diff --git a/rust/kernel/file.rs b/rust/kernel/file.rs
index 091b3a430..24a3cb93c 100644
--- a/rust/kernel/file.rs
+++ b/rust/kernel/file.rs
@@ -5,7 +5,7 @@
 //! C headers: [`include/linux/fs.h`](../../../../include/linux/fs.h) and
 //! [`include/linux/file.h`](../../../../include/linux/file.h)
 
-use crate::{bindings, error::Error, Result};
+use crate::{bindings, error::Error, Result, c_types};
 use core::{mem::ManuallyDrop, ops::Deref};
 
 /// Wraps the kernel's `struct file`.
@@ -44,6 +44,10 @@ impl File {
         // SAFETY: `File::ptr` is guaranteed to be valid by the type invariants.
         unsafe { (*self.ptr).f_flags & bindings::O_NONBLOCK == 0 }
     }
+
+    pub fn set_private_data(&self, data: *mut c_types::c_void) {
+        unsafe { (*self.ptr).private_data = data as _ };
+    }
 }
 
 impl Drop for File {
diff --git a/rust/kernel/file_operations.rs b/rust/kernel/file_operations.rs
index 483d73af3..2611b57e8 100644
--- a/rust/kernel/file_operations.rs
+++ b/rust/kernel/file_operations.rs
@@ -498,8 +498,8 @@ pub trait IoctlHandler: Sync {
 /// It can use the components of an ioctl command to dispatch ioctls using
 /// [`IoctlCommand::dispatch`].
 pub struct IoctlCommand {
-    cmd: u32,
-    arg: usize,
+    pub cmd: u32,
+    pub arg: usize,
     user_slice: Option<UserSlicePtr>,
 }
 
@@ -585,6 +585,12 @@ impl<T: FileOperations<Wrapper = Box<T>> + Default> FileOpener<()> for T {
     }
 }
 
+impl<T: FileOperations<Wrapper = Box<T>> + Default> FileOpener<u8> for T {
+    fn open(_: &u8) -> Result<Self::Wrapper> {
+        Ok(Box::try_new(T::default())?)
+    }
+}
+
 /// Corresponds to the kernel's `struct file_operations`.
 ///
 /// You implement this trait whenever you would create a `struct file_operations`.
diff --git a/rust/kernel/fs.rs b/rust/kernel/fs.rs
index 500ca3fc1..bac675f18 100644
--- a/rust/kernel/fs.rs
+++ b/rust/kernel/fs.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
@@ -6,11 +6,10 @@
 //!
 
 use crate::{
-    bindings,
+    bindings, c_types,
     error::{Error, Result},
     prelude::*,
     str::CStr,
-    c_types
 };
 
 pub struct Filename(*mut bindings::filename);
@@ -18,15 +17,17 @@ pub struct Filename(*mut bindings::filename);
 impl Filename {
     pub fn getname_kernel(arg1: &'static CStr) -> Result<Self> {
         let res;
-        unsafe { res =  bindings::getname_kernel(arg1.as_char_ptr()); }
-        if res==core::ptr::null_mut() {
+        unsafe {
+            res = bindings::getname_kernel(arg1.as_char_ptr());
+        }
+        if res == core::ptr::null_mut() {
             return Err(Error::EINVAL);
         }
         Ok(Self(res))
     }
 
-    pub fn get_name(&mut self) -> *const c_types::c_char {
-        unsafe{(*self.0).name}
+    pub fn get_name(& self) -> *const c_types::c_char {
+        unsafe { (*self.0).name }
     }
 }
 
@@ -37,8 +38,8 @@ impl Drop for Filename {
 }
 
 pub fn hashlen_string(salt: *const c_types::c_char, filename: *mut Filename) -> u64 {
-    unsafe{
+    unsafe {
         let name = (*filename).get_name();
         bindings::hashlen_string(salt as *const c_types::c_void, name)
     }
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/irq_pipeline.rs b/rust/kernel/irq_pipeline.rs
index e16631e3d..e34e0fd9d 100644
--- a/rust/kernel/irq_pipeline.rs
+++ b/rust/kernel/irq_pipeline.rs
@@ -4,7 +4,7 @@
 //!
 //! C header: [`include/linux/irq_pipeline.h`](../../../../include/linux/irq_pipeline.h)
 
-extern "C"{
+extern "C" {
     fn rust_helper_irq_send_oob_ipi(ipi: usize, cpumask: *const cpumask::CpumaskT);
     fn rust_helper_irq_get_TIMER_OOB_IPI() -> usize;
 }
@@ -12,9 +12,9 @@ extern "C"{
 use crate::cpumask;
 
 pub fn irq_send_oob_ipi(ipi: usize, cpumask: *const cpumask::CpumaskT) {
-    unsafe{rust_helper_irq_send_oob_ipi(ipi, cpumask)};
+    unsafe { rust_helper_irq_send_oob_ipi(ipi, cpumask) };
 }
 
 pub fn irq_get_TIMER_OOB_IPI() -> usize {
-    unsafe{rust_helper_irq_get_TIMER_OOB_IPI()}
-}
\ No newline at end of file
+    unsafe { rust_helper_irq_get_TIMER_OOB_IPI() }
+}
diff --git a/rust/kernel/irq_work.rs b/rust/kernel/irq_work.rs
index 07f4ce094..6132988a4 100644
--- a/rust/kernel/irq_work.rs
+++ b/rust/kernel/irq_work.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
@@ -10,11 +10,13 @@ use crate::{
 };
 
 extern "C" {
-    fn rust_helper_init_irq_work(work:*mut bindings::irq_work, 
-        func: unsafe extern "C" fn(work:*mut bindings::irq_work));
+    fn rust_helper_init_irq_work(
+        work: *mut bindings::irq_work,
+        func: unsafe extern "C" fn(work: *mut bindings::irq_work),
+    );
 }
 
-pub struct IrqWork(bindings::irq_work);
+pub struct IrqWork(pub bindings::irq_work);
 
 impl IrqWork {
     pub fn new() -> Self {
@@ -22,13 +24,18 @@ impl IrqWork {
         Self(irq_work)
     }
 
-    pub fn init_irq_work(&mut self, func: unsafe extern "C" fn(work:*mut bindings::irq_work)) -> Result<usize> {
-        unsafe{rust_helper_init_irq_work(&mut self.0 as *mut bindings::irq_work, func);}
+    pub fn init_irq_work(
+        &mut self,
+        func: unsafe extern "C" fn(work: *mut bindings::irq_work),
+    ) -> Result<usize> {
+        unsafe {
+            rust_helper_init_irq_work(&mut self.0 as *mut bindings::irq_work, func);
+        }
         Ok(0)
     }
 
     pub fn irq_work_queue(&mut self) -> Result<usize> {
-        let res = unsafe{bindings::irq_work_queue(&mut self.0 as *mut bindings::irq_work)};
+        let res = unsafe { bindings::irq_work_queue(&mut self.0 as *mut bindings::irq_work) };
         if res == true {
             Ok(0)
         } else {
@@ -37,7 +44,6 @@ impl IrqWork {
     }
 
     pub fn get_ptr(&mut self) -> *mut bindings::irq_work {
-        unsafe{&mut self.0 as *mut bindings::irq_work}
+        unsafe { &mut self.0 as *mut bindings::irq_work }
     }
 }
-
diff --git a/rust/kernel/kthread.rs b/rust/kernel/kthread.rs
index 14117b784..704341e8a 100644
--- a/rust/kernel/kthread.rs
+++ b/rust/kernel/kthread.rs
@@ -1,5 +1,4 @@
-﻿use alloc::rc::Rc;
-
-use crate::{prelude::* ,bindings, c_types};
-use core::{fmt, cell::RefCell};
+use alloc::rc::Rc;
 
+use crate::{bindings, c_types, prelude::*};
+use core::{cell::RefCell, fmt};
diff --git a/rust/kernel/ktime.rs b/rust/kernel/ktime.rs
index e3e840144..a0e6bb8d3 100644
--- a/rust/kernel/ktime.rs
+++ b/rust/kernel/ktime.rs
@@ -6,26 +6,27 @@
 use crate::bindings;
 pub type KtimeT = i64;
 
-extern "C"{
-    fn rust_helper_ktime_sub(lhs: KtimeT,rhs: KtimeT) -> KtimeT;
+extern "C" {
+    fn rust_helper_ktime_sub(lhs: KtimeT, rhs: KtimeT) -> KtimeT;
     fn rust_helper_ktime_add_ns(kt: KtimeT, nsval: u64) -> KtimeT;
     fn rust_helper_ktime_add(kt: KtimeT, nsval: KtimeT) -> KtimeT;
     fn rust_helper_ktime_set(secs: i64, nsecs: usize) -> KtimeT;
     fn rust_helper_ktime_divns(kt: KtimeT, div: i64) -> i64;
+    fn rust_helper_ktime_compare(cmp1: KtimeT, cmp2: KtimeT) -> KtimeT;
 }
 pub fn ktime_get() -> KtimeT {
-    unsafe{bindings::ktime_get() as KtimeT}
+    unsafe { bindings::ktime_get() as KtimeT }
 }
-pub fn ktime_sub(lhs: KtimeT,rhs: KtimeT) -> KtimeT {
-    unsafe{rust_helper_ktime_sub(lhs, rhs)}
+pub fn ktime_sub(lhs: KtimeT, rhs: KtimeT) -> KtimeT {
+    unsafe { rust_helper_ktime_sub(lhs, rhs) }
 }
 
 pub fn ktime_add_ns(kt: KtimeT, nsval: u64) -> KtimeT {
-    unsafe{rust_helper_ktime_add_ns(kt, nsval)}
+    unsafe { rust_helper_ktime_add_ns(kt, nsval) }
 }
 
 pub fn ktime_add(kt: KtimeT, nsval: KtimeT) -> KtimeT {
-    unsafe{rust_helper_ktime_add(kt, nsval)}
+    unsafe { rust_helper_ktime_add(kt, nsval) }
 }
 
 pub fn ktime_to_ns(kt: KtimeT) -> i64 {
@@ -33,9 +34,13 @@ pub fn ktime_to_ns(kt: KtimeT) -> i64 {
 }
 
 pub fn ktime_set(secs: i64, nsecs: usize) -> KtimeT {
-    unsafe{rust_helper_ktime_set(secs, nsecs)}
+    unsafe { rust_helper_ktime_set(secs, nsecs) }
 }
 
 pub fn ktime_divns(kt: KtimeT, div: i64) -> i64 {
-    unsafe{rust_helper_ktime_divns(kt, div)}
-}
\ No newline at end of file
+    unsafe { rust_helper_ktime_divns(kt, div) }
+}
+
+pub fn ktime_compare(cmp1: KtimeT, cmp2: KtimeT) -> KtimeT {
+    unsafe { rust_helper_ktime_compare(cmp1, cmp2) }
+}
diff --git a/rust/kernel/lib.rs b/rust/kernel/lib.rs
index e7b8c318a..df497579d 100644
--- a/rust/kernel/lib.rs
+++ b/rust/kernel/lib.rs
@@ -77,17 +77,23 @@ pub mod user_ptr;
 
 pub mod bitmap;
 pub mod class;
-pub mod cpumask;
 pub mod clockchips;
+pub mod completion;
+pub mod cpumask;
+pub mod device;
 pub mod double_linked_list;
 pub mod double_linked_list2;
-pub mod device;
+pub mod dovetail;
 pub mod endian;
 pub mod fs;
+pub mod irq_pipeline;
 pub mod irq_work;
 pub mod irqstage;
-pub mod irq_pipeline;
+pub mod kthread;
+pub mod ktime;
 pub mod mm;
+#[cfg(CONFIG_NET)]
+pub mod net;
 pub mod percpu;
 pub mod percpu_defs;
 pub mod premmpt;
@@ -96,18 +102,15 @@ pub mod timekeeping;
 pub mod uidgid;
 pub mod vmalloc;
 pub mod workqueue;
-pub mod kthread;
-pub mod ktime;
-pub mod dovetail;
-pub mod completion;
-#[cfg(CONFIG_NET)]
-pub mod net;
+pub mod memory_rros;
+pub mod memory_rros_test;
+pub mod uapi;
 
 #[doc(hidden)]
 pub use build_error::build_error;
 
 pub use crate::error::{Error, Result};
-pub use crate::types::{Mode, Opaque, ScopeGuard,ARef,AlwaysRefCounted};
+pub use crate::types::{ARef, AlwaysRefCounted, Mode, Opaque, ScopeGuard};
 
 /// Page size defined in terms of the `PAGE_SHIFT` macro from C.
 ///
diff --git a/rust/kernel/linked_list.rs b/rust/kernel/linked_list.rs
index d57bf1b88..2c381b459 100644
--- a/rust/kernel/linked_list.rs
+++ b/rust/kernel/linked_list.rs
@@ -113,7 +113,7 @@ pub struct List<G: GetLinksWrapped> {
 
 impl<G: GetLinksWrapped> List<G> {
     /// Constructs a new empty linked list.
-    pub fn new() -> Self {
+    pub const fn new() -> Self {
         Self {
             list: RawList::new(),
         }
@@ -139,6 +139,21 @@ impl<G: GetLinksWrapped> List<G> {
         }
     }
 
+    /// Adds the given object to the start (front) of the list.
+    ///
+    /// It is dropped if it's already on this (or another) list; this can happen for
+    /// reference-counted objects, so dropping means decrementing the reference count.
+    pub fn push_front(&mut self, data: G::Wrapped) {
+        let ptr = data.into_pointer();
+
+        // SAFETY: We took ownership of the entry, so it is safe to insert it.
+        if !unsafe { self.list.push_front(ptr.as_ref()) } {
+            // If insertion failed, rebuild object so that it can be freed.
+            // SAFETY: We just called `into_pointer` above.
+            unsafe { G::Wrapped::from_pointer(ptr) };
+        }
+    }
+
     /// Inserts the given object after `existing`.
     ///
     /// It is dropped if it's already on this (or another) list; this can happen for
@@ -189,6 +204,11 @@ impl<G: GetLinksWrapped> List<G> {
     pub fn cursor_front_mut(&mut self) -> CursorMut<'_, G> {
         CursorMut::new(self.list.cursor_front_mut())
     }
+
+    /// Returns a mutable cursor starting on the last (back) element of the list.
+    pub fn cursor_back_mut(&mut self) -> CursorMut<'_, G> {
+        CursorMut::new(self.list.cursor_back_mut())
+    }
 }
 
 impl<G: GetLinksWrapped> Default for List<G> {
@@ -242,4 +262,9 @@ impl<'a, G: GetLinksWrapped> CursorMut<'a, G> {
     pub fn move_next(&mut self) {
         self.cursor.move_next();
     }
+
+    /// Moves the cursor to the next element.
+    pub fn move_prev(&mut self) {
+        self.cursor.move_prev();
+    }
 }
diff --git a/rust/kernel/memory_rros.rs b/rust/kernel/memory_rros.rs
new file mode 100644
index 000000000..e69de29bb
diff --git a/rust/kernel/memory_rros_test.rs b/rust/kernel/memory_rros_test.rs
new file mode 100644
index 000000000..e69de29bb
diff --git a/rust/kernel/net.rs b/rust/kernel/net.rs
index 58e559b68..fb052ecbe 100644
--- a/rust/kernel/net.rs
+++ b/rust/kernel/net.rs
@@ -1,14 +1,13 @@
-use crate::{bindings, str::CStr, ARef,AlwaysRefCounted};
+use crate::{bindings, str::CStr, ARef, AlwaysRefCounted};
 use core::{cell::UnsafeCell, ptr::NonNull};
 
 extern "C" {
-    fn rust_helper_dev_hold(dev:*mut bindings::net_device) -> ();
-    fn rust_helper_dev_put(dev:*mut bindings::net_device) -> ();
-    fn rust_helper_get_net(net:*mut bindings::net) -> *mut bindings::net;
-    fn rust_helper_put_net(net:*mut bindings::net) -> ();
+    fn rust_helper_dev_hold(dev: *mut bindings::net_device) -> ();
+    fn rust_helper_dev_put(dev: *mut bindings::net_device) -> ();
+    fn rust_helper_get_net(net: *mut bindings::net) -> *mut bindings::net;
+    fn rust_helper_put_net(net: *mut bindings::net) -> ();
 }
 
-
 /// Wraps the kernel's `struct net_device`.
 #[repr(transparent)]
 pub struct Device(UnsafeCell<bindings::net_device>);
@@ -56,4 +55,4 @@ unsafe impl AlwaysRefCounted for Namespace {
 /// Returns the network namespace for the `init` process.
 pub fn init_ns() -> &'static Namespace {
     unsafe { &*core::ptr::addr_of!(bindings::init_net).cast() }
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/percpu_defs.rs b/rust/kernel/percpu_defs.rs
index 6e82c4e6f..77bfccc1d 100644
--- a/rust/kernel/percpu_defs.rs
+++ b/rust/kernel/percpu_defs.rs
@@ -1,8 +1,6 @@
 use core::i32;
 
-use crate::{
-    c_types,
-};
+use crate::c_types;
 
 extern "C" {
     fn rust_helper_per_cpu_ptr(
@@ -10,9 +8,7 @@ extern "C" {
         cpu: c_types::c_int,
     ) -> *mut c_types::c_void;
 
-    fn rust_helper_raw_cpu_ptr(
-        var: *mut c_types::c_void,
-    ) -> *mut c_types::c_void;
+    fn rust_helper_raw_cpu_ptr(var: *mut c_types::c_void) -> *mut c_types::c_void;
 
     fn rust_helper_smp_processor_id() -> c_types::c_int;
 }
@@ -27,21 +23,20 @@ pub fn per_cpu_ptr(var: *mut u8, cpu: i32) -> *mut u8 {
     }
 }
 
-// We can use generic to implement part of the ability of function per_cpu. But due to the absence of the 
+// We can use generic to implement part of the ability of function per_cpu. But due to the absence of the
 // macro define_percpu, this function has little chance to be used.
-pub fn per_cpu<T> (var: *mut T, cpu:i32) -> *mut T{
-    unsafe{
-        return per_cpu_ptr(var as *mut u8,cpu) as *mut T;
+pub fn per_cpu<T>(var: *mut T, cpu: i32) -> *mut T {
+    unsafe {
+        return per_cpu_ptr(var as *mut u8, cpu) as *mut T;
     }
 }
 
 pub fn raw_cpu_ptr(var: *mut u8) -> *mut u8 {
     unsafe {
-        return rust_helper_raw_cpu_ptr(var as *mut c_types::c_void)
-            as *mut u8;
+        return rust_helper_raw_cpu_ptr(var as *mut c_types::c_void) as *mut u8;
     }
 }
 
-pub fn smp_processor_id() -> c_types::c_int{
-    unsafe{rust_helper_smp_processor_id()}
-}
\ No newline at end of file
+pub fn smp_processor_id() -> c_types::c_int {
+    unsafe { rust_helper_smp_processor_id() }
+}
diff --git a/rust/kernel/raw_list.rs b/rust/kernel/raw_list.rs
index 4bc4f4a24..f155e7d65 100644
--- a/rust/kernel/raw_list.rs
+++ b/rust/kernel/raw_list.rs
@@ -85,7 +85,7 @@ pub(crate) struct RawList<G: GetLinks> {
 }
 
 impl<G: GetLinks> RawList<G> {
-    pub(crate) fn new() -> Self {
+    pub(crate) const fn new() -> Self {
         Self { head: None }
     }
 
@@ -163,6 +163,37 @@ impl<G: GetLinks> RawList<G> {
         self.push_back_internal(new)
     }
 
+    fn push_front_internal(&mut self, new: &G::EntryType) -> bool {
+        let links = G::get_links(new);
+        if !links.acquire_for_insertion() {
+            // Nothing to do if already inserted.
+            return false;
+        }
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let new_entry = unsafe { &mut *links.entry.get() };
+        let new_ptr = Some(NonNull::from(new));
+        match self.back() {
+            // SAFETY: `back` is valid as the list cannot change.
+            Some(back) => {
+                self.insert_after_priv(unsafe { back.as_ref() }, new_entry, new_ptr);
+                self.head = self.back(); // move head so that the back become the front
+            },
+            None => {
+                self.head = new_ptr;
+                new_entry.next = new_ptr;
+                new_entry.prev = new_ptr;
+            }
+        }
+        true
+
+    }
+
+    pub(crate) unsafe fn push_front(&mut self, new: &G::EntryType) -> bool {
+        self.push_front_internal(new)
+    }
+
+
     fn remove_internal(&mut self, data: &G::EntryType) -> bool {
         let links = G::get_links(data);
 
@@ -238,6 +269,10 @@ impl<G: GetLinks> RawList<G> {
     pub(crate) fn cursor_front_mut(&mut self) -> CursorMut<'_, G> {
         CursorMut::new(self, self.front())
     }
+
+    pub(crate) fn cursor_back_mut(&mut self) -> CursorMut<'_, G> {
+        CursorMut::new(self, self.back())
+    }
 }
 
 struct CommonCursor<G: GetLinks> {
@@ -299,6 +334,13 @@ impl<'a, G: GetLinks> Cursor<'a, G> {
         }
     }
 
+    /// Returns the element the cursor is currently positioned on.
+    pub fn current_mut(&self) -> Option<&'a mut G::EntryType> {
+        let cur = self.cursor.cur?;
+        // SAFETY: Objects must be kept alive while on the list.
+        Some(unsafe { &mut *cur.as_ptr() })
+    }
+
     /// Returns the element the cursor is currently positioned on.
     pub fn current(&self) -> Option<&'a G::EntryType> {
         let cur = self.cursor.cur?;
@@ -358,4 +400,9 @@ impl<'a, G: GetLinks> CursorMut<'a, G> {
     pub(crate) fn move_next(&mut self) {
         self.cursor.move_next(self.list);
     }
+
+    /// Moves the cursor to the prev element.
+    pub(crate) fn move_prev(&mut self) {
+        self.cursor.move_prev(self.list);
+    }
 }
diff --git a/rust/kernel/sync/mutex.rs b/rust/kernel/sync/mutex.rs
index 92d01ba45..e581f015a 100644
--- a/rust/kernel/sync/mutex.rs
+++ b/rust/kernel/sync/mutex.rs
@@ -5,9 +5,9 @@
 //! This module allows Rust code to use the kernel's [`struct mutex`].
 
 use super::{Guard, Lock, NeedsLockClass};
+use crate::pr_info;
 use crate::{bindings, str::CStr, Opaque};
 use core::{cell::UnsafeCell, marker::PhantomPinned, pin::Pin};
-use crate::pr_info;
 
 /// Safely initialises a [`Mutex`] with the given name, generating a new lock class.
 #[macro_export]
diff --git a/rust/kernel/sync/spinlock.rs b/rust/kernel/sync/spinlock.rs
index 0746e51ae..2a687111b 100644
--- a/rust/kernel/sync/spinlock.rs
+++ b/rust/kernel/sync/spinlock.rs
@@ -22,6 +22,8 @@ extern "C" {
     fn rust_helper_spin_unlock(lock: *mut bindings::spinlock);
     fn rust_helper_hard_spin_lock(lock: *mut bindings::raw_spinlock);
     fn rust_helper_hard_spin_unlock(lock: *mut bindings::raw_spinlock);
+    fn rust_helper_raw_spin_lock_irqsave(lock: *mut bindings::hard_spinlock_t, flags: *mut u32);
+    fn rust_helper_raw_spin_unlock_irqrestore(lock: *mut bindings::hard_spinlock_t, flags: u32);
 }
 
 /// Safely initialises a [`SpinLock`] with the given name, generating a new lock class.
@@ -85,6 +87,23 @@ impl<T: ?Sized> SpinLock<T> {
         // SAFETY: The spinlock was just acquired.
         unsafe { Guard::new(self) }
     }
+
+    pub fn irq_lock(&self) -> Guard<'_, Self> {
+        self.lock_noguard();
+
+        // SAFETY: The spinlock was just acquired.
+        unsafe { Guard::new(self) }
+    }
+
+    // FIXME: use this to enable the smp function
+    pub fn irq_lock_noguard(&self, flags: *mut u32) {
+        unsafe{rust_helper_raw_spin_lock_irqsave(self.spin_lock.get() as *mut bindings::hard_spinlock_t, flags);}
+    }
+
+    // FIXME: use this to enable the smp function
+    pub fn irq_unlock_noguard(&self, flags: u32) {
+        unsafe{rust_helper_raw_spin_unlock_irqrestore(self.spin_lock.get() as *mut bindings::hard_spinlock_t, flags);}
+    }
 }
 
 impl<T: ?Sized> NeedsLockClass for SpinLock<T> {
@@ -100,15 +119,17 @@ impl<T: ?Sized> Lock for SpinLock<T> {
         // SAFETY: `spin_lock` points to valid memory.
         // unsafe { rust_helper_spin_lock(self.spin_lock.get()) };
         unsafe { rust_helper_hard_spin_lock(self.spin_lock.get() as *mut bindings::raw_spinlock) };
-        // unsafe { rust_helper_hard_spin_lock((*self.spin_lock.get()).rlock() 
-            // as *mut bindings::raw_spinlock) };
+        // unsafe { rust_helper_hard_spin_lock((*self.spin_lock.get()).rlock()
+        // as *mut bindings::raw_spinlock) };
     }
 
     unsafe fn unlock(&self) {
         // unsafe { rust_helper_spin_unlock(self.spin_lock.get()) };
-        unsafe { rust_helper_hard_spin_unlock(self.spin_lock.get() as *mut bindings::raw_spinlock) };
-        // unsafe { rust_helper_hard_spin_unlock((*self.spin_lock.get()).rlock() 
-            // as *mut bindings::raw_spinlock) };
+        unsafe {
+            rust_helper_hard_spin_unlock(self.spin_lock.get() as *mut bindings::raw_spinlock)
+        };
+        // unsafe { rust_helper_hard_spin_unlock((*self.spin_lock.get()).rlock()
+        // as *mut bindings::raw_spinlock) };
     }
 
     fn locked_data(&self) -> &UnsafeCell<T> {
diff --git a/rust/kernel/sysfs.rs b/rust/kernel/sysfs.rs
index 587dcf781..874e4de54 100644
--- a/rust/kernel/sysfs.rs
+++ b/rust/kernel/sysfs.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! System control.
 //!
diff --git a/rust/kernel/task.rs b/rust/kernel/task.rs
index 056d6a1a0..c10a4a863 100644
--- a/rust/kernel/task.rs
+++ b/rust/kernel/task.rs
@@ -119,7 +119,11 @@ impl Task {
     }
 
     pub fn kernel(&self) -> bool {
-        unsafe { (*self.ptr).mm == core::ptr::null_mut()}
+        unsafe { (*self.ptr).mm == core::ptr::null_mut() }
+    }
+
+    pub fn state(&self) -> u32 {
+        unsafe { (*self.ptr).state as u32 }
     }
 
     /// Determines whether the given task has pending signals.
diff --git a/rust/kernel/types.rs b/rust/kernel/types.rs
index ca0c3c62c..f5df6f51b 100644
--- a/rust/kernel/types.rs
+++ b/rust/kernel/types.rs
@@ -9,7 +9,9 @@ use crate::{
     sync::{Ref, RefBorrow},
 };
 use alloc::{boxed::Box, sync::Arc};
-use core::{cell::UnsafeCell, mem::MaybeUninit, ops::Deref, pin::Pin, ptr::NonNull,marker::PhantomData};
+use core::{
+    cell::UnsafeCell, marker::PhantomData, mem::MaybeUninit, ops::Deref, pin::Pin, ptr::NonNull,
+};
 
 /// Permissions.
 ///
@@ -290,6 +292,14 @@ impl HlistNode {
     pub fn new() -> Self {
         Self(bindings::hlist_node::default())
     }
+    // pub fn hash_del(&mut self){
+    //     extern "C"{
+    //         fn rust_helper_hash_del(node: *mut bindings::hlist_node);
+    //     }
+    //     unsafe{
+    //         rust_helper_hash_del(&mut self.0 as *mut bindings::hlist_node);
+    //     }
+    // }
 }
 
 #[derive(Clone, Copy)]
@@ -304,10 +314,13 @@ impl HlistHead {
         &mut self.0 as *mut bindings::hlist_head
     }
 }
+
+
 pub fn hash_init(ht: *mut bindings::hlist_head, size: u32) {
     unsafe { rust_helper_hash_init(ht, size) };
 }
 
+
 /// Types that are _always_ reference counted.
 ///
 /// It allows such types to define their own custom ref increment and decrement functions.
@@ -410,4 +423,4 @@ impl<T: AlwaysRefCounted> Drop for ARef<T> {
         // decrement.
         unsafe { T::dec_ref(self.ptr) };
     }
-}
\ No newline at end of file
+}
diff --git a/rust/kernel/uapi/mod.rs b/rust/kernel/uapi/mod.rs
new file mode 100644
index 000000000..a275418c5
--- /dev/null
+++ b/rust/kernel/uapi/mod.rs
@@ -0,0 +1 @@
+pub mod time_types;
\ No newline at end of file
diff --git a/rust/kernel/uapi/time_types.rs b/rust/kernel/uapi/time_types.rs
new file mode 100644
index 000000000..0cea94a0c
--- /dev/null
+++ b/rust/kernel/uapi/time_types.rs
@@ -0,0 +1,25 @@
+use crate::bindings;
+
+pub struct KernelOldTimespec {
+    pub spec: bindings::__kernel_old_timespec,
+}
+
+impl KernelOldTimespec {
+    pub fn new() -> Self {
+        Self { spec: bindings::__kernel_old_timespec::default() }
+    }
+}
+
+pub struct KernelTimespec {
+    pub spec: bindings::__kernel_timespec,
+}
+
+impl KernelTimespec {
+    pub fn new() -> Self {
+        Self { spec: bindings::__kernel_timespec::default() }
+    }
+    
+    // fn set_tv_sec(tv_sec: bindings::__kernel_old_time_t) {
+    //     self.spec.tv_sec = tv_sec;
+    // }
+}
\ No newline at end of file
diff --git a/rust/kernel/uidgid.rs b/rust/kernel/uidgid.rs
index 392a9a228..6ca526601 100644
--- a/rust/kernel/uidgid.rs
+++ b/rust/kernel/uidgid.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
@@ -6,5 +6,5 @@
 
 use crate::bindings;
 
-pub struct KuidT(bindings::kuid_t);
-pub struct KgidT(bindings::kgid_t);
+pub struct KuidT(pub bindings::kuid_t);
+pub struct KgidT(pub bindings::kgid_t);
diff --git a/rust/kernel/user_ptr.rs b/rust/kernel/user_ptr.rs
index 72f31db6e..102b8c5dd 100644
--- a/rust/kernel/user_ptr.rs
+++ b/rust/kernel/user_ptr.rs
@@ -25,7 +25,10 @@ extern "C" {
         n: c_types::c_ulong,
     ) -> c_types::c_ulong;
 
-    pub fn rust_helper_clear_user(to: *mut c_types::c_void, n: c_types::c_ulong) -> c_types::c_ulong;
+    pub fn rust_helper_clear_user(
+        to: *mut c_types::c_void,
+        n: c_types::c_ulong,
+    ) -> c_types::c_ulong;
 }
 
 /// A reference to an area in userspace memory, which can be either
diff --git a/rust/kernel/workqueue.rs b/rust/kernel/workqueue.rs
index ae6db3259..fa497f6d2 100644
--- a/rust/kernel/workqueue.rs
+++ b/rust/kernel/workqueue.rs
@@ -1,4 +1,4 @@
-﻿// SPDX-License-Identifier: GPL-2.0
+// SPDX-License-Identifier: GPL-2.0
 
 //! cpumask
 //!
@@ -28,4 +28,4 @@ impl Work {
     pub unsafe fn new() -> Self {
         Self(Opaque::uninit())
     }
-}
\ No newline at end of file
+}
diff --git a/scripts/Makefile.build b/scripts/Makefile.build
index bb22acf84..b404b393a 100644
--- a/scripts/Makefile.build
+++ b/scripts/Makefile.build
@@ -300,7 +300,7 @@ quiet_cmd_rustc_o_rs = $(RUSTC_OR_CLIPPY_QUIET) $(quiet_modtag) $@
       cmd_rustc_o_rs = \
 	RUST_MODFILE=$(modfile) \
 	$(RUSTC_OR_CLIPPY) $(rust_flags) $(rust_cross_flags) \
-		-Zallow-features=allocator_api,bench_black_box,concat_idents,global_asm,try_reserve \
+		-Zallow-features=allocator_api,const_fn_transmute,bench_black_box,concat_idents,global_asm,try_reserve,maybe_uninit_extra,new_uninit \
 		--extern alloc --extern kernel \
 		--crate-type rlib --out-dir $(obj) -L $(objtree)/rust/ \
 		--crate-name $(patsubst %.o,%,$(notdir $@)) $<; \
-- 
2.17.1

