From 07102682da8eac4382f45d124aa629a9cffc6da6 Mon Sep 17 00:00:00 2001
From: Qiu Qichen <cheemsFries@gmail.com>
Date: Wed, 14 Dec 2022 07:16:16 +0800
Subject: [PATCH] lab4

---
 .gitignore                                |    1 +
 kernel/dovetail.c                         |    1 -
 kernel/rros/init.rs                       |   35 +-
 kernel/rros/lab_mem_test/mod.rs           |  132 +++
 kernel/rros/lab_mem_test/rros_mem_test.rs |  559 +++++++++
 kernel/rros/lab_mem_test/tlsf_test.rs     |  657 +++++++++++
 kernel/rros/list_head.rs                  |    0
 kernel/rros/memory_test.rs                |  273 +++++
 kernel/rros/thread.rs                     |    3 +-
 kernel/rros/thread_test.rs                |   47 +-
 kernel/rros/tlsf.rs                       |  752 ++++++++++++
 kernel/rros/tlsf_raw_list.rs              |  220 ++++
 lab4.md                                   |  455 ++++++++
 rust/alloc/alloc_rros.rs                  |  210 ++++
 rust/alloc/lib.rs                         |    2 +-
 rust/alloc/rc.rs                          | 1226 ++++++++++++++++----
 rust/alloc/string.rs                      |  438 ++++---
 rust/alloc/sync.rs                        | 1257 +++++++++++++++++----
 rust/helpers.c                            |   19 +
 rust/kernel/allocator.rs                  |    7 +-
 rust/kernel/double_linked_list3.rs        |  366 ++++++
 rust/kernel/lib.rs                        |    7 +-
 rust/kernel/memory_rros.rs                |  644 +++++++++++
 scripts/Makefile.build                    |    2 +-
 test1.py                                  |    4 +-
 25 files changed, 6646 insertions(+), 671 deletions(-)
 create mode 100644 kernel/rros/lab_mem_test/mod.rs
 create mode 100644 kernel/rros/lab_mem_test/rros_mem_test.rs
 create mode 100644 kernel/rros/lab_mem_test/tlsf_test.rs
 create mode 100644 kernel/rros/list_head.rs
 create mode 100644 kernel/rros/memory_test.rs
 create mode 100644 kernel/rros/tlsf.rs
 create mode 100644 kernel/rros/tlsf_raw_list.rs
 create mode 100644 lab4.md
 create mode 100644 rust/alloc/alloc_rros.rs
 create mode 100644 rust/kernel/double_linked_list3.rs
 create mode 100644 rust/kernel/memory_rros.rs

diff --git a/.gitignore b/.gitignore
index aab78d7c3..279cf70f2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -50,6 +50,7 @@
 *.zst
 *.exp
 *.text
+*.py
 Module.symvers
 modules.order
 
diff --git a/kernel/dovetail.c b/kernel/dovetail.c
index 0bcb0f705..09aaacd78 100644
--- a/kernel/dovetail.c
+++ b/kernel/dovetail.c
@@ -278,7 +278,6 @@ extern void rust_handle_inband_event(enum inband_event_type event, void *data);
 
 void __weak handle_inband_event(enum inband_event_type event, void *data)
 {
-	pr_info("rust_handle_inband_event in");
 	rust_handle_inband_event(event,data);
 }
 
diff --git a/kernel/rros/init.rs b/kernel/rros/init.rs
index baeeec936..e9daa5f3f 100644
--- a/kernel/rros/init.rs
+++ b/kernel/rros/init.rs
@@ -1,5 +1,8 @@
 ï»¿#![no_std]
 #![feature(allocator_api, global_asm)]
+#![feature(new_uninit)]
+#![feature(array_map)]
+#![feature(destructuring_assignment)]
 use alloc::vec;
 use kernel::{cpumask, irqstage, prelude::*, str::CStr,
      ThisModule, c_str, chrdev, file_operations::FileOperations,
@@ -30,6 +33,7 @@ mod list;
 mod list_test;
 mod lock;
 mod memory;
+mod memory_test;
 mod monitor;
 mod timer;
 mod timer_test;
@@ -43,9 +47,14 @@ mod factory;
 use factory::rros_early_init_factories;
 
 use crate::sched::this_rros_rq;
+use kernel::memory_rros::evl_init_memory;
 mod file;
 mod crossing;
 mod wait;
+mod tlsf;
+// mod lab_mem_test;
+mod lab_mem_test;
+mod tlsf_raw_list;
 #[cfg(CONFIG_NET)]
 mod net;
 module! {
@@ -135,7 +144,6 @@ fn set_rros_state(state: RrosRunStates) {
 fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>> {
     let res =
         irqstage::enable_oob_stage(CStr::from_bytes_with_nul("rros\0".as_bytes())?.as_char_ptr());
-    pr_info!("hello");
     match res {
         Ok(_o) => (),
         Err(_e) => {
@@ -143,15 +151,14 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
             return Err(kernel::Error::EINVAL);
         }
     }
-    pr_info!("hella");
-    // let res = init_memory(*sysheap_size_arg.read());
-    // match res {
-    //     Ok(_o) => (),
-    //     Err(_e) => {
-    //         pr_warn!("memory init wrong");
-    //         return Err(_e);
-    //     }
-    // }
+    let res = evl_init_memory();
+    match res {
+        Ok(_o) => (),
+        Err(_e) => {
+            pr_warn!("memory init wrong");
+            return Err(_e);
+        }
+    }
     let res = rros_early_init_factories(&THIS_MODULE);
     let fac_reg;
     match res {
@@ -246,6 +253,11 @@ fn test_sched() {
 fn test_fifo() {
     fifo_test::test___rros_enqueue_fifo_thread();
 }
+fn test_mem() {
+    lab_mem_test::tlsf_test::test_tlsf();
+    lab_mem_test::rros_mem_test::test_evl_heap();
+}
+
 
 impl KernelModule for Rros {
     fn init() -> Result<Self> {
@@ -296,7 +308,8 @@ impl KernelModule for Rros {
         // test_clock();
         // test_sched();
         // test_fifo();
-        test_thread();
+        // test_thread();
+        test_mem();
         //test_double_linked_list();
         match res {
             Ok(_o) => {
diff --git a/kernel/rros/lab_mem_test/mod.rs b/kernel/rros/lab_mem_test/mod.rs
new file mode 100644
index 000000000..6489fa650
--- /dev/null
+++ b/kernel/rros/lab_mem_test/mod.rs
@@ -0,0 +1,132 @@
+use core::fmt::{Display,Formatter};
+use kernel::{bindings};
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use core::fmt;
+
+
+#[derive(Debug, Clone)]
+pub struct TestFailed{
+    file: &'static str,
+    line: u32,
+    col: u32,
+    msg: &'static str,
+}
+
+impl TestFailed {
+    fn new_without_msg(file: &'static str, line: u32, col: u32) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg: "",
+        }
+    }
+    fn new(file: &'static str, line: u32, col: u32, msg: &'static str) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg,
+        }
+    }
+}
+
+impl fmt::Display for TestFailed {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        if self.msg.is_empty(){
+            write!(f, "Test failed on {}:{}:{}",self.file, self.line, self.col)
+        }else{
+            write!(f, "Test failed on {}:{}:{}  :  {}",self.file, self.line, self.col, self.msg)
+        }
+    }
+}
+macro_rules! test_failed_here {
+    () => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""});
+    };
+    ($msg:expr) => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg});
+    };
+}
+
+macro_rules! test {
+    ($left:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $msg:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+
+
+macro_rules! test_eq {
+    ($left:expr, $right:expr $(,)?) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $right:expr, $msg:expr) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+type Result<T> = core::result::Result<T, TestFailed>;
+#[inline]
+fn handle_and_print_result(test_name:&'static str, r: Result<()>){
+    match r{
+        Ok(())=>{pr_crit!("[RAND]Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+}
+// macro_rules! pr_color_crit {
+//     ($color:expr, $($arg:tt)*) => {{
+//         pr_crit!("\x1b[{}m",$color);
+//         pr_crit!($($arg)*);
+//         pr_crit!("\x1b[0m");
+//     }};
+// }
+// macro_rules! pr_green_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(32,$($arg)*);
+//     }};
+// }
+// macro_rules! pr_red_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(31,$($arg)*);
+//     }};
+// }
+
+
+
+
+pub mod rros_mem_test;
+pub mod tlsf_test;
\ No newline at end of file
diff --git a/kernel/rros/lab_mem_test/rros_mem_test.rs b/kernel/rros/lab_mem_test/rros_mem_test.rs
new file mode 100644
index 000000000..e98f95148
--- /dev/null
+++ b/kernel/rros/lab_mem_test/rros_mem_test.rs
@@ -0,0 +1,559 @@
+
+use alloc::alloc_rros::RrosMem;
+use alloc::format;
+use kernel::memory_rros::{evl_heap, evl_heap_range, addr_add_size};
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use alloc::boxed::Box;
+use alloc::alloc::Global;
+use core::ptr::{null, null_mut};
+use core::{ptr, slice};
+use alloc::alloc::Layout;
+use alloc::alloc::alloc;
+use core::fmt::{Display,Formatter};
+use crate::{bindings};
+use super::{TestFailed,handle_and_print_result};
+type Result<T> = core::result::Result<T, TestFailed>;
+
+#[inline]
+fn handle_alloc_ptr_result_default(test_name:&'static str, test_fn:fn(*mut c_types::c_void,usize) -> Result<()>){
+    let size = mm::page_align(1024).unwrap();
+    let ptr = vmalloc::c_vmalloc(size as c_types::c_ulong).unwrap();
+    let r = test_fn(ptr,size);
+    match r{
+        Ok(())=>{pr_crit!("[RAND]Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+    vmalloc::c_vfree(ptr as *const c_types::c_void);
+}
+
+pub fn test_evl_heap(){
+    pr_crit!("--------------test rros dynamic memory allocator start------------------");
+
+    handle_alloc_ptr_result_default("insert_by_size",test_insert_bysize);
+    handle_alloc_ptr_result_default("insert_by_addr",test_insert_byaddr);
+    handle_alloc_ptr_result_default("search_right_mergeable",test_search_right_mergeable);
+    handle_alloc_ptr_result_default("release_page_range",test_release_page_range);
+    handle_alloc_ptr_result_default("search_size_ge",test_search_size_ge);
+    handle_alloc_ptr_result_default("reserve_page_range",test_reserve_page_range);
+    handle_alloc_ptr_result_default("move_page_back",test_move_page_back);
+    handle_alloc_ptr_result_default("heap_alloc",test_heap_alloc);
+    handle_alloc_ptr_result_default("heap_alloc_small",test_heap_alloc_small);
+    handle_alloc_ptr_result_default("heap_write_then_free", test_heap_alloc_write_then_free);
+    handle_alloc_ptr_result_default("heap_free", test_heap_free);
+
+    handle_and_print_result("heap_box",test_rrosmem_box());
+
+    
+    // handle_alloc_ptr_result_default("test_init",test_search_right_mergeable);
+    // åé¢å¯ä»¥ä½¿ç¨init
+    pr_crit!("--------------test rros dynamic memory allocator end------------------");
+
+}
+
+pub fn get_uninit_heap(ptr:*mut c_types::c_void,size:usize)->evl_heap{
+    // unsafe
+    let mut evl_heap: evl_heap = evl_heap {
+        membase: 0 as *mut u8,
+        addr_tree: None,
+        size_tree: None,
+        pagemap: None,
+        usable_size: 0,
+        used_size: 0,
+        buckets: [0; 5 as usize],
+        lock: None,
+    };
+    
+    evl_heap.membase = unsafe{ ptr as *mut u8};
+    evl_heap.usable_size = size;
+    evl_heap.used_size = 0;
+    evl_heap.size_tree = Some(bindings::rb_root::default());
+    evl_heap.addr_tree = Some(bindings::rb_root::default());
+    evl_heap
+}
+pub fn clear_heap(heap:&mut evl_heap){
+    heap.used_size = 0;
+    heap.size_tree = Some(bindings::rb_root::default());
+    heap.addr_tree = Some(bindings::rb_root::default());
+}
+pub fn get_evl_heap(ptr:*mut c_types::c_void,size:usize)->Result<evl_heap>{
+    let mut evl_heap: evl_heap = evl_heap {
+        membase: 0 as *mut u8,
+        addr_tree: None,
+        size_tree: None,
+        pagemap: None,
+        usable_size: 0,
+        used_size: 0,
+        buckets: [0; 5 as usize],
+        lock: None,
+    };
+    let ret = unsafe{evl_heap.init(ptr as *mut u8, size as usize)};
+    match ret {
+        Err(_) => {
+            test_failed_here!()
+        },
+        Ok(_) => Ok(evl_heap)
+    }
+    
+}
+const EVL_HEAP_PAGE_SIZE:usize = 512;
+
+
+pub fn check_if_size_in_tree(root:&*mut bindings::rb_node,size:u64) -> bool {
+    unsafe {
+        let mut node = root;
+        let mut parent = core::ptr::null_mut();
+        if ((*container_of!(*node, evl_heap_range, size_node)).size as u64==size){ return true};
+        while !node.is_null() {
+            let p = container_of!(*node, evl_heap_range, size_node);
+            parent = *node;
+            if size == ((*p).size as u64) {
+                return true;
+            } else if size < ((*p).size as u64) {
+                node = &mut (*parent).rb_left;
+            } else {            
+                node = &mut (*parent).rb_right;
+            }
+        }
+        return false;
+
+    }
+}
+
+pub fn check_if_addr_in_tree(root:&*mut bindings::rb_node,addr:u64) -> bool {
+    unsafe {
+        let mut node = root;
+        let mut parent = core::ptr::null_mut();
+        if (container_of!(*node, evl_heap_range, addr_node) as u64==addr){ return true};
+        while !node.is_null() {
+            let p = container_of!(*node, evl_heap_range, addr_node);
+            parent = *node;
+            if addr == (p as u64) {
+                return true;
+            } else if addr < (p as u64) {
+                node = &mut (*parent).rb_left;
+            } else {            
+                node = &mut (*parent).rb_right;
+            }
+        }
+        return false;
+
+    }
+}
+pub fn next_addr(node: *const bindings::rb_node) -> Option<*const evl_heap_range>{
+    unsafe{
+        let next:*const bindings::rb_node = bindings::rb_next(node) as *const bindings::rb_node;
+        if next.is_null(){
+            None
+        }else{
+            Some(container_of!(next, evl_heap_range, addr_node))
+        }
+    }
+}
+
+pub fn next_size(node: *const bindings::rb_node) -> Option<*const evl_heap_range>{
+    unsafe{
+        let next:*const bindings::rb_node = bindings::rb_next(node) as *const bindings::rb_node;
+        if next.is_null(){
+            None
+        }else{
+            Some(container_of!(next, evl_heap_range, size_node))
+        }
+    }
+}
+
+// pub fn walk_size_tree(root: *const bindings::rb_root,func:fn(*const evl_heap_range)){
+//     let node = unsafe{&((*root).rb_node) as *const bindings::rb_node};
+//     loop{
+//         bindings::rb_next(node) as *const bindings::rb_node
+//     }
+// }
+pub fn print_size_tree(heap:&evl_heap){
+    let root = heap.size_tree.as_ref().unwrap();
+    let mut node = unsafe{(*root).rb_node as *const bindings::rb_node};
+    loop{
+        let p = unsafe{container_of!(&*node, evl_heap_range, size_node)};
+        pr_info!("size:{}",(*p).size);
+        node = unsafe{bindings::rb_next(&*node) as *const bindings::rb_node};
+        if node.is_null(){
+            break;
+        }
+    }
+}
+pub fn test_insert_bysize(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,3,4,5].map(|s|s*EVL_HEAP_PAGE_SIZE); // SAFETY: éè¦ä¿è¯size>åç§»
+    if size_array[0] != 1024{
+        panic!();
+    }
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let mut ptr = ptr as *mut evl_heap_range;
+        for i in size_array{
+            (*ptr).size = i;
+            heap.insert_range_bysize(ptr);
+            let size_root = heap.size_tree.as_ref().unwrap();
+            test!(check_if_size_in_tree(&size_root.rb_node, i as u64),"insert failed")?;
+            ptr = ((ptr as *mut u8).offset(i as isize) as * mut evl_heap_range);
+        }
+        for i in [512,4096,8192]{
+            let size_root = heap.size_tree.clone().unwrap();
+            test!(!check_if_size_in_tree(&size_root.rb_node, i as u64),"insert failed")?;
+        }
+    }
+    Ok(())
+}
+
+pub fn test_insert_byaddr(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,3,4,5].map(|s|s*EVL_HEAP_PAGE_SIZE); // SAFETY: éè¦ä¿è¯size>åç§»
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let mut ptr = ptr as *mut evl_heap_range;
+        for i in size_array{
+            (*ptr).size = i;
+            heap.insert_range_byaddr(ptr);
+            let addr_root = heap.addr_tree.as_ref().unwrap();
+            test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64),"addr insert failed")?;
+            ptr = ((ptr as *mut u8).offset(i as isize) as * mut evl_heap_range);
+        }
+    }
+    Ok(())
+}
+
+pub fn test_search_right_mergeable(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let ptr = ptr as *mut evl_heap_range;
+        (*ptr).size += size;
+        test!(heap.search_right_mergeable(ptr).is_none(),"test_search_right_mergeable failed 1");
+        
+        // ç¬¬ä¸çæ¯4Pageï¼ç¬¬äºçæ¯2Page
+        let size1 = EVL_HEAP_PAGE_SIZE * 4;
+        let size2 = EVL_HEAP_PAGE_SIZE * 2;
+        let block1 = ptr;
+        (*block1).size = size1;
+        let block2 = unsafe{ addr_add_size(ptr as *mut u8, size1) as *mut evl_heap_range };
+        (*block2).size = size2;
+        heap.addr_tree.as_mut().unwrap();
+        heap.insert_range_bysize(block1);
+        heap.insert_range_byaddr(block1);
+        heap.insert_range_bysize(block2);
+        heap.insert_range_byaddr(block2);
+        let result = heap.search_right_mergeable(block1);
+        test!(result.is_some(),"merge 1");
+        test_eq!(result.unwrap(),block2,"merge 1 failed");
+
+        // ç°å¨åæä¸¤çï¼ä½æ¯ä¸æ¯é»è¿çã
+        clear_heap(&mut heap);
+        let size1 = EVL_HEAP_PAGE_SIZE * 4;
+        let size2 = EVL_HEAP_PAGE_SIZE * 2;
+
+        let block1 = ptr;
+        (*block1).size = size1;
+        let block2 = unsafe{ addr_add_size(ptr as *mut u8, size1+EVL_HEAP_PAGE_SIZE) as *mut evl_heap_range };
+        (*block2).size = size2;
+        heap.addr_tree.as_mut().unwrap();
+        heap.insert_range_bysize(block1);
+        heap.insert_range_byaddr(block1);
+        heap.insert_range_bysize(block2);
+        heap.insert_range_byaddr(block2);
+        test!(result.is_none());
+    }
+    Ok(())
+}
+
+pub fn test_release_page_range(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,2,2].map(|s|s*EVL_HEAP_PAGE_SIZE);
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let ptr = ptr as *mut evl_heap_range;
+        // è¿éåªæµè¯3ç§æåµ
+        // æ²¡æå¯åå¹¶ç
+        (*ptr).size = size;
+        heap.release_page_range(ptr as *mut u8,size); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64));
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, size as u64));
+
+        // leftå¯åå¹¶
+        clear_heap(&mut heap);
+        (*ptr).size = size_array[0];
+        heap.insert_range_bysize(ptr);// é¢åæå¥å·¦è¾¹
+        heap.insert_range_byaddr(ptr);// é¢åæå¥å·¦è¾¹
+        let block2 = ((ptr as *mut u8).offset(size_array[0] as isize) as * mut evl_heap_range);
+        (*block2).size = size_array[1];
+        heap.release_page_range(block2 as *mut u8, (*block2).size); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block2 as u64));
+
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size_array[0] + size_array[1]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[0] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[1] as u64));
+
+
+        // leftï¼righté½å¯ä»¥åå¹¶
+        clear_heap(&mut heap);
+        let block1 = ptr;
+        (*block1).size = size_array[0];
+        heap.insert_range_bysize(block1);// é¢åæå¥å·¦è¾¹
+        heap.insert_range_byaddr(block1);// é¢åæå¥å·¦è¾¹
+        let block2 = ((ptr as *mut u8).offset(size_array[0] as isize) as * mut evl_heap_range);
+        (*block2).size = size_array[1]; 
+        let block3 = ((block2 as *mut u8).offset(size_array[1] as isize) as * mut evl_heap_range);
+        (*block3).size = size_array[2]; 
+        heap.insert_range_bysize(block3);// é¢åæå¥å³è¾¹
+        heap.insert_range_byaddr(block3);// é¢åæå¥å³è¾¹
+
+        heap.release_page_range(block2 as *mut u8,size_array[1]); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, block1 as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block2 as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block3 as u64));
+
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size_array[0]+size_array[1]+size_array[2]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, (size_array[0]+size_array[1]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, (size_array[1]+size_array[2]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[0] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[1] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[2] as u64));
+    }
+    Ok(())
+}
+
+pub fn test_search_size_ge(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let split_area = EVL_HEAP_PAGE_SIZE;
+    unsafe{
+        let mut heap = get_uninit_heap(ptr, size);
+        let block_array = [1,3,5,7].map(|s|s*EVL_HEAP_PAGE_SIZE);
+        let mut block = ptr as *mut evl_heap_range;
+        for each_size in block_array{
+            (*block).size = each_size;
+            heap.insert_range_byaddr(block);
+            heap.insert_range_bysize(block);
+            block = (block as *mut u8).offset(each_size as isize) as * mut evl_heap_range;
+        };
+        // should not use block[7]
+
+        // test equal
+        let result = heap.search_size_ge(block_array[0]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[0]);
+
+        let result = heap.search_size_ge(block_array[1]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[1]);
+        
+        let result = heap.search_size_ge(block_array[2]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[2]);
+
+        let result = heap.search_size_ge(block_array[3]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[3]);
+
+        let result = heap.search_size_ge(block_array[3]+10);
+        test!(result.is_none());
+
+        // let result = heap.search_size_ge(4*EVL_HEAP_PAGE_SIZE);
+        // test!(result.is_some());
+        // test_but_allow_failing!((*result.unwrap()).size != 5*EVL_HEAP_PAGE_SIZE,"not best fit");
+
+        // let result = heap.search_size_ge(5*EVL_HEAP_PAGE_SIZE);
+        // test!(result.is_some());
+        // test_but_allow_failing!((*result.unwrap()).size != 5*EVL_HEAP_PAGE_SIZE,"not best fit");
+    }
+    Ok(())
+}
+pub fn test_reserve_page_range(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size);
+        test!(heap.is_ok());
+        let mut heap = heap.unwrap();
+        let alloc1 = EVL_HEAP_PAGE_SIZE * 2;
+        let a = heap.reserve_page_range(alloc1);
+        test!(a.is_some());
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size - alloc1) as u64));
+        
+        let alloc2 = EVL_HEAP_PAGE_SIZE * 4;
+        let a = heap.reserve_page_range(alloc2);
+        test!(a.is_some());
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size - alloc1 -alloc2) as u64));
+
+    }
+    Ok(())
+}
+pub fn test_move_page_back(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size);
+        test!(heap.is_ok());
+        let mut heap =heap.unwrap();
+        heap.add_page_front(16,4);
+        heap.add_page_front(17,4);
+        heap.add_page_front(18,4);
+        heap.add_page_front(19,4);
+        heap.add_page_front(20,4);
+        // let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        // 20 -> 19 -> 18 -> 17 -> 16
+
+        heap.move_page_back(20, 4);// 19 -> 18 -> 17 -> 16 -> 20
+        let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        let start = 19;
+        let mut counter = 0;
+        let mut large_array = [0;20];
+        large_array[counter] = start;
+        let mut node = heap.get_pagemap(start as i32);
+        while  counter != 5{
+            counter+=1;
+            large_array[counter] = (*node).next;
+            node = heap.get_pagemap((*node).next as i32);
+        }
+        test_eq!(large_array[counter],20);
+
+        // 19  -> 17 -> 16 -> 20 -> 18
+        heap.move_page_back(18, 4);
+        let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        let start = 19;
+        let mut counter = 0;
+        let mut large_array = [0;20];
+        large_array[counter] = start;
+        let mut node = heap.get_pagemap(start as i32);
+        while  counter != 5{
+            counter+=1;
+            large_array[counter] = (*node).next;
+            node = heap.get_pagemap((*node).next as i32);
+        }
+        test_eq!(large_array[counter],18);
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        for i in 0..5{
+            let r = heap.evl_alloc_chunk(1000); // alloc 1024
+            test!(r.is_some());
+        }
+        for i in 0..5{
+            let r = heap.evl_alloc_chunk(256); // alloc 1024
+            test!(r.is_some());
+        }
+    }
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        // test alloc big
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(1000); // alloc 1024
+            test!(r.is_some());
+        }
+        let r = heap.evl_alloc_chunk(1000); // alloc 1024
+        test!(r.is_none());
+    }
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size).unwrap();
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(256); 
+            test!(r.is_some());
+        }
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(128); 
+            test!(r.is_some());
+        }
+        for i in 0..1024{
+            let r = heap.evl_alloc_chunk(64); 
+            test!(r.is_some());
+        }
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc_small(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    // æµè¯ä»½åéå°å
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        for small_block_size in [16,32,64,128,256]{
+            let num = EVL_HEAP_PAGE_SIZE / small_block_size;
+            let mut page = 0;
+            for i in 0..num{
+                let r = heap.evl_alloc_chunk(small_block_size);
+                test!(r.is_some());
+                let r = unsafe{(r.unwrap() as usize - heap.membase as usize) >> 9};
+                if i == 0{
+                    page = r;
+                }else{
+                    test_eq!(r,page);
+                }
+            }
+        }
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc_write_then_free(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    // æµè¯åéååå¥ï¼ç¶åéæ¾
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        let block_sizes = [2,3,4,5,6,7,8,9,10,11,12,13].map(|s|s*EVL_HEAP_PAGE_SIZE);
+        let mut ptrs = [null_mut();20];
+        let magic = 0x2A;
+        for i in 0..block_sizes.len(){
+            let block_size = block_sizes[i];
+            let mut a = heap.evl_alloc_chunk(block_size);
+            test!(a.is_some());
+            let mut a = a.unwrap();
+            ptrs[i] = a;
+            unsafe{
+                let buf: &mut [u8] = core::slice::from_raw_parts_mut(a, block_size as usize);
+                buf.fill(magic);
+            }
+        }
+        for i in 0..block_sizes.len(){
+            heap.evl_free_chunk(ptrs[i]);
+        }
+    }
+    Ok(())
+}
+
+
+pub fn test_rrosmem_box()->Result<()>{
+    for i in 0..10{
+        let mut a = Box::try_new_in([0;256],RrosMem).unwrap();
+        a[0] = 2;
+        a[1] = 3;
+    }
+    Ok(())
+}
+
+pub fn test_heap_free(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        let alloc_big = [1230,2340,4560,7890,1098,2356,2330,1140,5140,1270];
+        let mut alloc_big_ptrs = [null_mut()  ;20];
+        let alloc_small = [16,32,64,114,51,4,16,32,64,114];
+        let mut page =[0;10];
+        let mut alloc_small_ptrs= [null_mut() ;20];
+        for i in 0..10{
+            for j in 0..10{
+                let p = heap.evl_alloc_chunk(alloc_small[j]);
+                test!(p.is_some());
+                alloc_small_ptrs[j] = p.unwrap();
+                page[j] = (p.unwrap() as usize - heap.membase as usize) >> 9;
+            }
+            let p = heap.evl_alloc_chunk(alloc_big[i]);
+            test!(p.is_some());
+            alloc_big_ptrs[i] = p.unwrap();
+
+            for j in 0..10{
+                heap.evl_free_chunk(alloc_small_ptrs[j] );
+            }
+        }
+        for i in 0..alloc_big.len(){
+            heap.evl_free_chunk(alloc_big_ptrs[i]);
+        }
+    }
+    Ok(())
+}
diff --git a/kernel/rros/lab_mem_test/tlsf_test.rs b/kernel/rros/lab_mem_test/tlsf_test.rs
new file mode 100644
index 000000000..66425d3f4
--- /dev/null
+++ b/kernel/rros/lab_mem_test/tlsf_test.rs
@@ -0,0 +1,657 @@
+
+use alloc::format;
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use crate::{tlsf::*, tlsf_raw_list};
+use alloc::boxed::Box;
+use alloc::alloc::{Global, dealloc};
+use core::convert::AsMut;
+use core::mem::size_of;
+use core::ptr::{null, null_mut};
+use core::{ptr, slice};
+use alloc::alloc::Layout;
+use alloc::alloc::alloc;
+use kernel::bindings;
+use super::{TestFailed,handle_and_print_result};
+
+type Result<T> = core::result::Result<T, TestFailed>;
+#[repr(C)]
+#[repr(C)]
+pub struct MockBlockHeader {
+    pub prev_phys_block: *mut MockBlockHeader,
+    magic : u32,
+    size: u32, 
+}
+
+impl MockBlockHeader {
+    pub fn get_size(&self) -> usize {
+        self.size as usize & !(BLOCK_HEADER_FREE_BIT | BLOCK_HEADER_PREV_FREE_BIT)
+    }
+    pub fn set_size(&mut self, size: usize) {
+        let old_size = self.size;
+        self.size = size as u32 | (old_size & (BLOCK_HEADER_FREE_BIT as u32 | BLOCK_HEADER_PREV_FREE_BIT as u32));
+        self.magic = BLOCK_HEADER_MAGIC as u32;
+    }
+    pub fn set_prev_free(&mut self) {
+        self.size |= BLOCK_HEADER_PREV_FREE_BIT as u32;
+    }
+    pub fn set_prev_used(&mut self) {
+        self.size  &= !(BLOCK_HEADER_PREV_FREE_BIT) as u32;
+    }
+    pub fn is_prev_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_PREV_FREE_BIT) != 0
+    }
+    pub fn set_free(&mut self) {
+        self.size |= BLOCK_HEADER_FREE_BIT as u32; 
+    }
+    pub fn set_used(&mut self) {
+        self.size &= !(BLOCK_HEADER_FREE_BIT) as u32;
+    }
+    pub fn is_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_FREE_BIT) != 0
+    }
+    pub fn data_ptr<T>(&self) -> *mut T {
+        unsafe { (self as *const _ as *mut u8).offset(size_of::<Self>() as isize) as *mut T }
+    }
+    pub fn link_next(&mut self) -> *mut MockBlockHeader {
+        unsafe {
+            let next = ptr::NonNull::new(
+                (self.data_ptr() as *mut u8)
+                    .offset((self.get_size() - BLOCK_HEADER_OVERHEAD) as isize)
+                    as *mut MockBlockHeader)
+            .unwrap()
+            .as_mut();
+            next.prev_phys_block = self as *const _ as *mut MockBlockHeader;
+            return (next as *mut MockBlockHeader)
+        }
+    }
+}
+
+macro_rules! as_blockheader_mut {
+    ($mock:expr) => {
+        unsafe{
+            &mut *($mock as *const MockBlockHeader as *mut FreeBlockHeader)
+        }
+    };
+}
+/// éåä¸ä¸ªåå­æ± 
+fn walk_pool(mut ptr: *mut MockBlockHeader, mut walk: impl FnMut(MockBlockHeader)) {
+    unsafe {
+        loop {
+            let block = ptr::read(ptr as *mut MockBlockHeader);
+            let size = block.get_size();
+            walk(block);
+            if size == 0 {
+                return;
+            }
+            ptr = (ptr as *mut u8)
+                .offset((size + (size_of::<MockBlockHeader>() - BLOCK_HEADER_OVERHEAD)) as isize)
+                as *mut MockBlockHeader;
+        }
+    }
+}
+
+/// è°è¯ä½¿ç¨ãæå°åºæ¯ä¸ªblockçä¿¡æ¯
+fn walk_debug(mut ptr: *mut MockBlockHeader) {
+    let mut counter = 0;
+    walk_pool(ptr, |block| {
+        counter += 1;
+        let size = block.get_size();
+        let is_prev_free = block.is_prev_free();
+        let is_free = block.is_free();
+        if (size != 0) {
+            pr_info!(
+                "block#{},size={},is_prev_free={},is_free={}",
+                counter, size, is_prev_free, is_free
+            );
+        } else {
+            pr_info!("sentinel,is_prev_free={},is_free={}", is_prev_free, is_free);
+        }
+    })
+}
+
+pub fn test_tlsf(){
+    pr_info!("--------------test tlsf begin-----------------");
+    handle_and_print_result("tlsf c-style list example",test_c_style_list()); // 5
+    handle_and_print_result("tlsf blockHeader(split)",test_blockHeader_split()); // 10
+    handle_and_print_result("tlsf blockHeader(absorb)",test_blockHeader_absorb());// 5
+    handle_and_print_result("tlsf mapping",test_mapping_insert()); // 5
+    handle_and_print_result("tlsf heap init(add pool)",test_init()); // 5
+
+    handle_and_print_result("tlsf malloc",test_malloc());// 15
+    handle_and_print_result("tlsf free",test_free()); // 15
+    handle_and_print_result("tlsf multiple allocation",test_multiple_alloc()); // 10
+    handle_and_print_result("tlsf torture", test_torture()); // 10
+    pr_info!("tlsf test ok!");
+    init_tlsfheap();
+    pr_info!("initialized tlsf heap done");
+    handle_and_print_result("tlsf allocator",tlsf_allocator()); // 5
+    // a report 15
+    // pr_crit!("pass all tests of tlsf");
+    pr_info!("--------------test tlsf end------------------");
+}
+fn get_block_vector(ptr: *mut MockBlockHeader) -> (u32,Box<[Option<MockBlockHeader>;20]>){
+    let mut result : Box<[Option<MockBlockHeader>;20]>= unsafe{Box::try_new_uninit_in(Global).unwrap().assume_init()};
+    let mut counter = 0;
+    walk_pool(ptr,|block|{
+        result[counter] = Some(block);
+        counter+=1;
+    });
+    (counter as u32,result)
+}
+
+pub fn write_buffer(ptr: *mut u8, size:usize){
+    /// generate random number
+    /// we don't use rand generator because we have to wait it to be initialized
+    // kernel::random::getrandom(dst); // not check
+    let dst = unsafe{core::slice::from_raw_parts_mut(ptr,size)};
+    let b = &[b'B', b'U', b'P', b'T'];
+    let mut i = 0;
+    while i < dst.len() {
+        for &byte in b {
+            if i >= dst.len() {
+                break;
+            }
+            dst[i] = byte;
+            i += 1;
+        }
+    }
+}
+
+pub fn test_c_style_list() -> Result<()>{
+    extern "C"{
+        fn rust_helper_INIT_LIST_HEAD(list : *mut bindings::list_head);
+    }
+    unsafe{
+        let mut l1 = bindings::list_head::default();
+        let mut l2 = bindings::list_head::default();
+        rust_helper_INIT_LIST_HEAD(&mut l1 as *mut bindings::list_head);
+        rust_helper_INIT_LIST_HEAD(&mut l2 as *mut bindings::list_head);
+
+        let mut ex1 = Example{
+            val1 : 1,
+            val2 : 2,
+            list1 : bindings::list_head::default(),
+            list2 : bindings::list_head::default(),
+        };
+        let mut ex2 = Example{
+            val1 : 0,
+            val2 : 4,
+            list1 : bindings::list_head::default(),
+            list2 : bindings::list_head::default(),
+        };
+        list_example1_sort_decending_respectively(&mut l1, &mut l2, &mut ex1, &mut ex2);
+        let mut next = list_entry!(&l1,Example,list1);
+        test!(core::ptr::eq(&ex1 as *const Example,next));
+        next = list_entry!(next,Example,list1);
+        test!(core::ptr::eq(&ex2 as *const Example,next));
+
+        let mut next = list_entry!(&l2,Example,list2);
+        test!(core::ptr::eq(&ex2 as *const Example,next));
+        next = list_entry!(next,Example,list2);
+        test!(core::ptr::eq(&ex1 as *const Example,next));
+    }
+
+    Ok(())
+}
+
+pub fn test_blockHeader_split() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap()) as *mut MockBlockHeader;
+        let block = ptr::NonNull::new(ptr)
+        .unwrap()
+        .as_mut();
+        block.set_size(32);
+        block.set_free();
+        block.set_prev_used();
+       
+        let mut next = (&mut *block.link_next());
+        next.set_size(0);
+        next.set_used();
+        next.set_prev_free();
+
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),32)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+        as_blockheader_mut!(block).split(16);
+
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,3)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),16)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),8)?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0)?;
+
+        as_blockheader_mut!(block).split(8);// split failed
+
+        let (len,_result) = get_block_vector(ptr);
+        test_eq!(len,3)?;
+
+        dealloc(ptr as *mut u8, Layout::from_size_align(size, 8).unwrap());
+    }
+
+    // pr_info!("end of test MockBlockHeader");
+    Ok(())
+}
+
+pub fn test_blockHeader_absorb() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap()) as *mut MockBlockHeader;
+        let mut block = ptr::NonNull::new(ptr)
+        .unwrap()
+        .as_mut();
+
+        let mut blocks : [*mut MockBlockHeader;5] = [core::ptr::null_mut();5];
+        for i in 0..5{
+            block.set_size(64);
+            block.set_free();
+            if i==0{
+                block.set_prev_used();
+            }else{
+                block.set_prev_free();
+            }
+            blocks[i] = block as *mut MockBlockHeader;
+            block = (&mut *block.link_next());
+        }
+        block.set_size(0);
+        block.set_used();
+        block.set_prev_free();
+
+        // init done;
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,6)?;
+        (*(blocks[0] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,5)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),128+BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true)?;
+        test!(core::ptr::eq(result[1].as_ref().unwrap().prev_phys_block, blocks[0]))?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),64)?;
+
+        // 0 - 128+8
+        // 1(ä¸å­å¨)
+        // 2 - 64
+        // 3 - 64
+        // 4 - 64
+        // 5 - 0
+
+        (*(blocks[2] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,4)?;
+        // 0 - 128+8
+        // 1
+        // 2 - 128+8
+        // 3 
+        // 4 - 64
+        // 5 - 0
+        test_eq!(result[1].as_ref().unwrap().get_size(),128+BLOCK_HEADER_OVERHEAD)?;
+        test!(core::ptr::eq(result[2].as_ref().unwrap().prev_phys_block, blocks[2]))?;
+
+        (*(blocks[4] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,4)?;
+        // 0 - 128+8
+        // 1
+        // 2 - 128+8
+        // 3 
+        // 4 - 64
+        // 5 - 0
+        test_eq!(result[2].as_ref().unwrap().get_size(),64)?;
+
+        dealloc(ptr as *mut u8, Layout::from_size_align(size, 8).unwrap());
+    }
+    // pr_info!("end of test MockBlockHeader");
+    Ok(())
+}
+
+pub fn test_init() -> Result<()>{
+    unsafe{
+        let size1 = 1024;
+        let layout = Layout::from_size_align(size1, 8);
+        let ptr1 = alloc(layout.unwrap()) as *mut u8;
+
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr1 as  *mut u8, size1 as usize);
+        let ptr1 = ptr1.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+        let (len,result) = get_block_vector(ptr1 as *mut MockBlockHeader);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size1 as usize-2*BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free (),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+
+        let size2 = 1024*4;
+        let layout = Layout::from_size_align(size2, 8);
+        let ptr2 = alloc(layout.unwrap()) as *mut u8;
+
+        control.add_pool(ptr2 as  *mut u8, size2 as usize);
+        let ptr2 = ptr2.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+        let (len,result) = get_block_vector(ptr2 as *mut MockBlockHeader);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size2 as usize-2*BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+        
+    }
+    // pr_info!("test tlsf init ok!");
+    Ok(())
+}
+
+pub fn test_mapping_insert() -> Result<()>{
+    let (a,b) = mapping_insert(234);
+    test_eq!(a,0,"mapping_insert(234) failed")?;
+    test_eq!(b,29,"mapping_insert(234) failed")?;
+    let (a,b) = mapping_insert(1234);
+    test_eq!(a,3,"mapping_insert(1234) failed")?;
+    test_eq!(b,6,"mapping_insert(1234) failed")?;
+    let (a,b) = mapping_insert(560);
+    test_eq!(a,2,"mapping_insert(560) failed")?;
+    test_eq!(b,3,"mapping_insert(560) failed")?;
+    let (a,b) = mapping_insert(1024);
+    test_eq!(a,3,"mapping_insert(1024) failed")?;
+    test_eq!(b,0,"mapping_insert(1024) failed")?;
+    let (a,b) = mapping_insert(2345);
+    test_eq!(a,4,"mapping_insert(2345) failed")?;
+    test_eq!(b,4,"mapping_insert(2345) failed")?;
+    let (a,b) = mapping_insert(12345);
+    test_eq!(a,6,"mapping_insert(12345) failed")?;
+    test_eq!(b,16,"mapping_insert(12345) failed")?;
+    // pr_info!("test mapping_insert ok!");
+    Ok(())
+}
+
+pub fn test_malloc()->Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        // test begin
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"init failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size-2*BLOCK_HEADER_OVERHEAD,"init failed")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"init failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"init failed:missing sentinel")?;
+
+        // test malloc 1
+        let user_ptr = control.malloc::<u8>(256);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,3,"malloc 1 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 1 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),744,"malloc 1 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true,"malloc 1 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 1 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0,"malloc 1 failed:missing sentinel")?;
+        write_buffer(user_ptr,256);
+
+
+        // test malloc 2
+        let user_ptr = control.malloc::<u8>(256);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 2 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 2 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),256,"malloc 2 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),480,"malloc 2 failed:remain size error")?;
+        test_eq!(result[2].as_ref().unwrap().is_free(),true,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().is_prev_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[3].as_ref().unwrap().get_size(),0,"malloc 2 failed:missing sentinel")?;
+        write_buffer(user_ptr,256);
+
+        // test malloc 3
+        let user_ptr = control.malloc::<u8>(512);
+        test!(user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 3 failed: wrong length")?;
+
+        // test malloc 4
+        let user_ptr = control.malloc::<u8>(472);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 4 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 4 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),256,"malloc 4 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),480,"malloc 4 failed:remain size error")?;
+        test_eq!(result[2].as_ref().unwrap().is_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().is_prev_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[3].as_ref().unwrap().get_size(),0,"malloc 2 failed:missing sentinel")?;
+        write_buffer(user_ptr,472);
+
+    }
+    // pr_info!("test_malloc ok");
+    Ok(())
+}
+
+pub fn test_free() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"init failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size-2*BLOCK_HEADER_OVERHEAD,"init failed")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"init failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"init failed:missing sentinel")?;
+
+        // test free 1
+        let user_ptr1 = control.malloc::<u8>(512);
+        test!(!user_ptr1.is_null(),"malloc 1 failed")?;
+        write_buffer(user_ptr1,512);
+
+        let user_ptr2 = control.malloc::<u8>(240);
+        test!(!user_ptr2.is_null(),"malloc 2 failed")?;
+        write_buffer(user_ptr2,240);
+
+        let user_ptr3 = control.malloc::<u8>(248); //  should failed
+        test!(user_ptr3.is_null(),"malloc 3 failed")?;
+
+        control.free(user_ptr2 as *mut u8);
+        drop(user_ptr2);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,3,"free 1 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),512,"free 1 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"free 1 failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),488,"free 1 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true,"free 1 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"free 1 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0,"free 1 failed:missing sentinel")?;
+
+        control.free(user_ptr1 as *mut u8);
+        drop(user_ptr1);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"free 2 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),1008,"free 2 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"free 2 failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"free 2 failed:missing sentinel")?;
+    }
+    // pr_info!("test_free ok");
+    Ok(())
+}
+
+pub fn test_multiple_alloc() -> Result<()>{
+    unsafe{
+        let size = 2048;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+
+
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        let user_ptr1 = control.malloc::<u8>(512);
+        test!(!user_ptr1.is_null(),"malloc 1 failed")?;
+        write_buffer(user_ptr1,512);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+
+        let user_ptr2 = control.malloc::<u8>(512);
+        test!(!user_ptr2.is_null(),"malloc 2 failed")?;
+        write_buffer(user_ptr2,512);
+
+        let user_ptr3 = control.malloc::<u8>(512);
+        test!(!user_ptr3.is_null(),"malloc 3 failed")?;
+        write_buffer(user_ptr3,512);
+
+        let null = control.malloc::<u8>(600);
+        test!(null.is_null(),"malloc 4 failed")?;
+        
+        control.free(user_ptr2 as *mut u8);
+        // walk_debug(ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize)) as *mut MockBlockHeader);
+        // ç®ååºè¯¥ä¸ºï¼
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=512,is_prev_free=false,is_free=true
+        // block#3,size=512,is_prev_free=true,is_free=false
+        // block#4,size=472,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+
+        drop(user_ptr2);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+
+        let null = control.malloc::<u8>(600); 
+        test!(null.is_null(),"malloc 5 failed"); // should faile?d
+
+        control.free(user_ptr3 as *mut u8);
+        drop(user_ptr3);
+        // æ­¤æ¶åºè¯¥ä¸ºï¼
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1512,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+
+        let user_ptr4 = control.malloc::<u8>(1024);
+        test!(!user_ptr4.is_null(),"malloc 6 failed"); // should successs
+        // write_buffer(user_ptr4,1024);
+
+        // æ­¤æ¶åºè¯¥ä¸º
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1024,is_prev_free=false,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+        control.free(user_ptr1 as *mut u8);
+        let null = control.malloc::<u8>(600);
+        test!(null.is_null(),"malloc 7 failed")?;
+
+
+        let user_ptr5= control.malloc::<u8>(472);
+        test!(!user_ptr5.is_null(),"malloc 8 failed")?;
+        // write_buffer(user_ptr5,472);
+
+        // æ­¤æ¶åºè¯¥ä¸º
+        // block#1,size=512,is_prev_free=false,is_free=true
+        // block#2,size=1024,is_prev_free=true,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=false
+        // sentinel,is_prev_free=true,is_free=false
+
+        let user_ptr6 = control.malloc::<u8>(510);
+        test!(!user_ptr6.is_null(),"malloc 9 failed")?;
+        // æ­¤æ¶åºè¯¥ä¸º
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1024,is_prev_free=false,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=false
+        // sentinel,is_prev_free=true,is_free=false
+        
+        let null = control.malloc::<u8>(8);
+        test!(null.is_null(),"malloc 10 failed")?;
+    }
+    Ok(())
+    // pr_info!("test_multiple_alloc ok");
+}
+pub fn tlsf_allocator() -> Result<()>{
+    unsafe{
+        for i in 0..100{
+            let a = Box::try_new_in("hello world from our tlsf allocator", TLSFMem).unwrap();
+            let b = Box::try_new_in(123456789, TLSFMem).unwrap();
+            let c = Box::try_new_in(1.23456789, TLSFMem).unwrap();
+            let d = Box::try_new_in([1,2,3,4,5,6,7,8,9,0], TLSFMem).unwrap();
+            let e = Box::try_new_in((1,2,3,4,5), TLSFMem).unwrap();
+        }
+        let a = Box::try_new_in("hello world from our tlsf allocator", TLSFMem).unwrap();
+        pr_info!("{}",a);
+        // pr_info!("tlsf_allocator ok");
+    }
+    Ok(())
+}
+
+#[inline]
+fn handle_alloc_ptr_result_default(test_name:&'static str, test_fn:fn(*mut c_types::c_void,usize) -> Result<()>){
+    let size = mm::page_align(1024).unwrap();
+    let ptr = vmalloc::c_vmalloc(size as c_types::c_ulong).unwrap();
+    let r = test_fn(ptr,size);
+    match r{
+        Ok(())=>{pr_crit!("Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+    vmalloc::c_vfree(ptr as *const c_types::c_void);
+}
+
+fn test_torture()->Result<()>{
+    unsafe{
+        let size = 2048;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let mut array = Box::<[*mut u8;64]>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(array.as_mut() as *const _ as *mut u8, 0,size_of::<[*mut u8;64]>());
+        let mut array = array.assume_init();
+        for block_size in [32,64,128,256,512,42,33,123,114,514]{
+            let mut counter = 0;
+            loop{
+                let ptr = control.malloc::<u8>(block_size as usize);
+                if ptr.is_null(){
+                    break;
+                }
+                array[counter] = ptr;
+                counter+=1;
+            }
+
+            for i in 0..counter{
+                control.free(array[i]);
+            }
+        }
+    }
+    // ä¸æµè¿ç»­åéï¼éæºéæ¾
+    Ok(())
+}
\ No newline at end of file
diff --git a/kernel/rros/list_head.rs b/kernel/rros/list_head.rs
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/memory_test.rs b/kernel/rros/memory_test.rs
new file mode 100644
index 000000000..29eabfac9
--- /dev/null
+++ b/kernel/rros/memory_test.rs
@@ -0,0 +1,273 @@
+use core::sync::atomic::AtomicUsize;
+use core::{borrow::BorrowMut, mem::size_of, mem::zeroed};
+
+use kernel::{
+    init_static_sync, mm,
+    prelude::*,
+    premmpt, spinlock_init,
+    sync::{self, Mutex, SpinLock},
+    vmalloc, memory_rros::*,
+    rbtree::{RBTree, RBTreeNode},
+    memory_rros::*,container_of,
+};
+
+use alloc::alloc::*;
+use alloc::alloc_rros::*;
+use alloc::boxed::*;
+
+use crate::{list, monitor};
+
+pub fn mem_test() {
+    // mem_test1();
+    // mem_test2();
+    // test_rbtree();
+    // test_init_system_heap();
+    // test_insert_system_heap();
+    // test_alloc_chunk();
+    test_box_allocator();
+    test_chunk();
+    // test_arc();
+    // test_buckets();
+}
+
+fn test_buckets() {
+    test_213();
+    unsafe {
+        pr_info!("test_buckets: xxx is {}",evl_system_heap.buckets[1]);
+    }
+}
+
+fn test_213() {
+    unsafe {
+        evl_system_heap.buckets[1] = 22;
+    }
+}
+
+fn test_arc() {
+    pr_info!("test_arc: begin");
+    let b = 5;
+    let x = Arc::try_new_in(8881, RrosMem);
+    match x {
+        Ok(a) => {
+            test_fn(a.clone());
+            pr_info!("x is {}",a);
+        },
+        Err(_) => {
+            pr_info!("test_arc: arc alloc err");
+        },
+    }
+    pr_info!("test_arc: end");
+}
+
+fn test_fn(x: Arc<i32,RrosMem>) {
+    pr_info!("test_fn x is {}",x);
+}
+
+fn mem_test1() {
+    let b = 5;
+    let x = Box::try_new_in(123, Global);
+    match x {
+        Err(_) => {
+            pr_info!("alloc error");
+        },
+        Ok(y) => {
+            let z =y;
+            pr_info!("z is {}",z);
+        },
+    }
+    pr_info!("alloc success");
+}
+struct mem_testxy {
+    x: i32,
+    y: i32,
+    z: i32,
+}
+//æµè¯ç³è¯·å°çåå­ç´æ¥è½¬æ¢ä¸ºç»æä½æéï¼ç»è®ºæ¯å¯ä»¥ç´æ¥ä½¿ç¨
+pub fn mem_test2() -> Result<usize>{
+    let vmalloc_res = vmalloc::c_vmalloc(1024 as u64);
+    let memptr;
+    match vmalloc_res {
+        Some(ptr) => memptr = ptr,
+        None => return Err(kernel::Error::ENOMEM),
+    }
+    let xxx = memptr as *mut mem_testxy;
+    unsafe {
+        (*xxx).x = 11;
+        (*xxx).y = 22;
+        (*xxx).z = 33;
+        pr_info!("mem_test2: z is {}",(*xxx).z);
+        pr_info!("mem_test2: x addr is {:p}",&mut (*xxx).x as *mut i32);
+        pr_info!("mem_test2: y addr is {:p}",&mut (*xxx).y as *mut i32);
+        pr_info!("mem_test2: z addr is {:p}",&mut (*xxx).z as *mut i32);
+    }
+    Ok(0)
+}
+
+struct pageinfo {
+    membase:u32,
+    size:u32,
+}
+
+//æµè¯å®æï¼
+fn test_rbtree() -> Result<usize>{
+    pr_info!("~~~test_rbtree begin~~~");
+    let mut root: RBTree<u32, pageinfo> = RBTree::new();
+
+    let mut x1 = pageinfo{
+        membase:100,
+        size:200,
+    };
+    let mut x2 = pageinfo{
+        membase:101,
+        size:200,
+    };
+    let mut x3 = pageinfo{
+        membase:102,
+        size:200,
+    };
+    
+    let mut node1 = RBTree::try_allocate_node(100,x1)?;
+    // let mut node1: = RBTree::try_allocate_node(300,x2)?;
+    let mut node2 = RBTree::try_allocate_node(101,x2)?;
+    let mut node3 = RBTree::try_allocate_node(102,x3)?;
+    root.insert(node1);
+    root.insert(node2);
+    root.insert(node3);
+    //éåçº¢é»æ æ¹å¼ï¼
+    for item in root.iter() {
+        pr_info!("item.0 is {}",item.0);
+        pr_info!("item.1.size is {}",item.1.size);
+    }
+    pr_info!("~~~test_rbtree end~~~");
+    Ok(0)
+}
+
+//æµè¯åå§åç³»ç»å 
+fn test_init_system_heap() {
+    init_system_heap();
+}
+
+//æµè¯ç³»ç»å æå¥èç¹ââæµè¯éè¿
+fn test_insert_system_heap() -> Result<usize> {
+    pr_info!("~~~test_insert_system_heap begin~~~");
+    init_system_heap();
+    
+    unsafe{
+        let membase = evl_system_heap.membase;
+        pr_info!("test_insert_system_heap: membase is {:p}",membase);
+        let mut x1 = new_evl_heap_range(membase, 1024);
+        let mut x2 = new_evl_heap_range(addr_add_size(membase,1024), 2048);
+        let mut x3 = new_evl_heap_range(addr_add_size(membase,2048), 4096);
+
+        pr_info!("test_insert_system_heap: 1");
+        evl_system_heap.insert_range_byaddr(x1);
+        evl_system_heap.insert_range_byaddr(x2);
+        evl_system_heap.insert_range_byaddr(x3);
+        pr_info!("test_insert_system_heap: 2");
+        let mut rb_node = evl_system_heap.addr_tree.clone().unwrap().rb_node;
+        if rb_node.is_null() {
+            pr_info!("test_insert_system_heap: root is null");
+        } else {
+            let p = container_of!(rb_node, evl_heap_range, addr_node);
+            pr_info!("test_insert_system_heap root size is {}",(*p).size);
+        }
+        pr_info!("test_insert_system_heap: 3");
+    }
+    Ok(0)
+}
+
+//æµè¯å°åå­çåéä¸åæ¶
+fn test_small_chunk() {
+
+}
+use kernel::timekeeping::ktime_get_real_fast_ns;
+//å¤æ¬¡åéåæ¶
+fn test_chunk() {
+    pr_info!("~~~test_chunk: begin~~~");
+    let t1 = ktime_get_real_fast_ns();
+    for i in 0..100{
+        let mut a = Box::try_new_in([0;128],RrosMem).unwrap();
+    }
+    let t2 = ktime_get_real_fast_ns();
+    pr_info!("rros alloc time is {}",t2-t1);
+    let t3 = ktime_get_real_fast_ns();
+    for i in 0..100
+    {
+        let mut a = Box::try_new_in([0;64],Global).unwrap();
+    }
+    let t4 = ktime_get_real_fast_ns();
+    pr_info!("rust alloc time is {}",t4-t3);
+    pr_info!("~~~test_chunk: 1~~~");
+    let y = __rros_sys_heap_alloc(4,0);
+    pr_info!("~~~test_chunk: end~~~");
+}
+
+//æµè¯åéchunk
+fn test_alloc_chunk() {
+    pr_info!("~~~test_alloc_chunk begin~~~");
+    unsafe {
+        //æ¥çå½åevl_system_heap sizeæ çæ ¹
+        let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+        let mut p = container_of!(rb_node, evl_heap_range, size_node);
+        let raw_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", raw_size);
+        let membase = evl_system_heap.membase;
+        pr_info!("test_alloc_chunk: membase is {}",membase as u32);
+        let res = evl_system_heap.evl_alloc_chunk(1024);
+        let mut x:u32 = 0;
+        let mut addr = 0 as *mut u8;
+        match res {
+            Some(a) => {
+                addr = a;
+                x = a as u32;
+                pr_info!("test_alloc_chunk: alloc addr is {}",x as u32);
+            },
+            None => {
+                pr_info!("test_alloc_chunk: alloc err");
+            }
+        }
+        pr_info!("test_alloc_chunk: membase - alloc = {}",x as u32 - membase as u32);
+        p = container_of!(rb_node, evl_heap_range, size_node);
+        let mut new_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", new_size);
+        pr_info!("test_alloc_chunk: raw_size - new_size = {}", raw_size - new_size);
+        //æµè¯åæ¶
+        pr_info!("~~~test_alloc_chunk: test free begin~~~");
+        evl_system_heap.evl_free_chunk(addr);
+        p = container_of!(rb_node, evl_heap_range, size_node);
+        new_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", new_size);
+        pr_info!("~~~test_alloc_chunk: test free end~~~");
+    }
+    pr_info!("~~~test_alloc_chunk end~~~");
+
+}
+
+//æµè¯boxçèªå®ä¹åéå¨
+fn test_box_allocator() {
+    pr_info!("test_box_allocator: begin");
+    let b = 5;
+    let x = Box::try_new_in(123, RrosMem);
+    match x {
+        Err(_) => {
+            pr_info!("test_box_allocator: alloc error");
+            return ;
+        },
+        Ok(_x) => {
+            unsafe {
+                let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+                let mut p = container_of!(rb_node, evl_heap_range, size_node);
+                let raw_size = (*p).size;
+                pr_info!("test_box_allocator: root size is {}", raw_size);
+            }
+            pr_info!("test_box_allocator: x is {}",_x);
+        },
+    }
+    unsafe {
+        let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+        let mut p = container_of!(rb_node, evl_heap_range, size_node);
+        let raw_size = (*p).size;
+        pr_info!("test_box_allocator: root size is {}", raw_size);
+    }
+    pr_info!("test_box_allocator: alloc success");
+}
\ No newline at end of file
diff --git a/kernel/rros/thread.rs b/kernel/rros/thread.rs
index 43c956c47..0baf851b8 100644
--- a/kernel/rros/thread.rs
+++ b/kernel/rros/thread.rs
@@ -273,7 +273,8 @@ fn evl_wakeup_thread_locked(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, i
         // let mut oldstate = thread.lock();
         thread.lock().state &= !mask;
 
-		if (mask & (T_DELAY|T_PEND)) != 0x0 {
+
+		if (mask & (sched::T_DELAY|sched::T_PEND)) != 0x0 {
             let rtimer = thread.lock().rtimer.clone();
             timer::rros_stop_timer(rtimer.unwrap());
         }
diff --git a/kernel/rros/thread_test.rs b/kernel/rros/thread_test.rs
index 7036692cb..e3bcfaafc 100644
--- a/kernel/rros/thread_test.rs
+++ b/kernel/rros/thread_test.rs
@@ -3,7 +3,8 @@ use crate::{
     sched::{self, this_rros_rq}
 };
 use kernel::{bindings, prelude::*, c_str, spinlock_init, sync::{SpinLock, Lock, Guard}, c_types, };
-use alloc::rc::Rc;
+use alloc::alloc_rros::*;
+use alloc::alloc::*;
 use core::cell::RefCell;
 
 struct KthreadRunner {
@@ -57,7 +58,7 @@ pub fn test_thread_context_switch() {
         kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
         kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
         
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
+        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
         // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
         // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
 
@@ -90,7 +91,7 @@ pub fn test_thread_context_switch() {
         kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
         kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
 
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
+        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
         // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
         // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
         
@@ -164,32 +165,16 @@ pub fn kfn_1() {
 
         thread::rros_sleep(1000000000);//sleepæå¤§é®é¢ ææ¶ä¸ç¨äº
         
+        unsafe { pr_info!("kfn1: time begin is {}", clock::RROS_REALTIME_CLOCK.read())};
         
-        // unsafe{
-        //     let mut tmb = timer::rros_this_cpu_timers(&clock::RROS_MONO_CLOCK);
-        //     if (*tmb).q.is_empty() == true {
-        //         // tick
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        // //         // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
-        //     }
-        // //     // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
+        // for i in 1..3 {
+        let a = Arc::try_new_in(1000, RrosMem);
+        unsafe { pr_info!("kfn1: time1 is {}", clock::RROS_REALTIME_CLOCK.read())};
+        let b = Arc::try_new_in(1000, RrosMem);
+        unsafe { pr_info!("kfn1: time2 is {}", clock::RROS_REALTIME_CLOCK.read())};
         // }
-        // // unsafe{
-        //     let this_rq = this_rros_rq();
-        //     tick::rros_notify_proxy_tick(this_rq);
-        // }
-        // pr_info!("hello! from rros~~~~~~~~~~~~");
+        unsafe { pr_info!("kfn1: time end is {}", clock::RROS_REALTIME_CLOCK.read())};
+        // pr_info!("kfn1: waste time is {}",y-x);
         pr_emerg!("hello! from rros~~~~~~~~~~~~");
         
     // }
@@ -198,6 +183,14 @@ pub fn kfn_1() {
 pub fn kfn_2() {
     // while 1==1 {
         thread::rros_sleep(1000000000);
+        let x = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        pr_info!("kfn2: x is {}",x);
+        // for i in 1..100 {
+            let a = Arc::try_new(1000);
+        // }
+        let y = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        pr_info!("kfn2: y is {}",y);
+        pr_info!("kfn_2: waste time is {}",y-x);
         pr_info!("world! from rros~~~~~~~~~~~~");
         
     // }
diff --git a/kernel/rros/tlsf.rs b/kernel/rros/tlsf.rs
new file mode 100644
index 000000000..075bdd579
--- /dev/null
+++ b/kernel/rros/tlsf.rs
@@ -0,0 +1,752 @@
+use core::alloc::{Layout};
+use core::{ptr, slice};
+use core::ptr::NonNull;
+use core::mem::{size_of,MaybeUninit};
+use core::ops::{Deref, DerefMut};
+use alloc::alloc::Global;
+use alloc::boxed::Box;
+use alloc::vec::Vec;
+use crate::tlsf_raw_list::{Links,RawList,GetLinks};
+
+
+// FIXME: You can still try to use your own list in lab1 instead!
+// use kernel::double_linked_list3::LinkedList;
+
+/// the last `ALIGN_SIZE_LOG2` bit of the size field is used to store some flags
+pub const ALIGN_SIZE_LOG2: usize = 3; 
+
+/// memory alignment size
+pub const ALIGN_SIZE: usize = 1 << ALIGN_SIZE_LOG2; 
+
+ /// the second level index table has 2^SL_INDEX_COUNT_LOG2 entries
+pub const SL_INDEX_COUNT_LOG2: usize = 5;
+
+/// the maximum size of a block is 2^FL_INDEX_MAX
+pub const FL_INDEX_MAX: usize = 32; 
+
+/// The number of shift bits of the first level index
+pub const FL_INDEX_SHIFT: usize = (SL_INDEX_COUNT_LOG2 + ALIGN_SIZE_LOG2); 
+
+/// Size of first level index table
+pub const FL_INDEX_COUNT: usize = (FL_INDEX_MAX - FL_INDEX_SHIFT + 1);
+
+/// Size of second level index table
+pub const SL_INDEX_COUNT: usize = 1 << SL_INDEX_COUNT_LOG2;
+
+/// The maximum size of a block stores in blocks[0]
+pub const SMALL_BLOCK_SIZE: usize = 1 << FL_INDEX_SHIFT; // ç¬¬0å±å¯è½å­æ¾çæå¤§åå¤§å°
+
+/// is_free flag mask
+pub const BLOCK_HEADER_FREE_BIT: usize = 1 << 0; 
+
+/// is_prev_free flag mask
+pub const BLOCK_HEADER_PREV_FREE_BIT: usize = 1 << 1; 
+
+/// overhead of a block. The overhead is the size of the BlockHeader struct.
+pub const BLOCK_HEADER_OVERHEAD: usize = core::mem::size_of::<usize>();
+
+/// overhead of a pool. Both the first block and sentinel block need a size field.
+pub const ALLOC_POOL_OVERHEAD: usize = 2*BLOCK_HEADER_OVERHEAD; 
+
+pub const BLOCK_HEADER_MAGIC : usize = 0xdeadbeef;
+
+
+trait ValidityCheck{
+    fn check_validity(&self)->bool;
+}
+
+/// A wrapper for a non-null pointer to a BlockHeader
+/// We have implemented Deref and DerefMut for this type,
+/// So you can visit the fields and methods of BlockHeader directly
+/// Just use it as if it is a BlockHeader.
+///
+/// For example:
+/// ```
+/// let hdr = AutoDerefPointer::from_raw_pointer(ptr);
+/// hdr.set_size(100);
+/// ```
+#[derive(Clone, Copy)]
+pub struct AutoDerefPointer<T>(NonNull<T>);
+
+impl<T> Deref for AutoDerefPointer<T>{
+    type Target = T;
+    fn deref(&self) -> &Self::Target{
+        unsafe{
+            self.0.as_ref()
+        }
+    }
+}
+
+impl<T> DerefMut for AutoDerefPointer<T>{
+    fn deref_mut(&mut self) -> &mut Self::Target{
+        unsafe{
+            self.0.as_mut()
+        }
+    }
+}
+impl<T:ValidityCheck> AutoDerefPointer<T>{
+    // convert a raw pointer to AutoDerefPointer
+    fn from_raw_pointer(ptr: *mut T) -> Self {
+        let p =AutoDerefPointer{
+            0 : NonNull::new(ptr as *mut T)
+                .unwrap(),
+        };
+        if p.check_validity(){
+            p
+        }else{
+            panic!("invalid block header");
+        }
+    }
+    unsafe fn from_raw_pointer_unchecked(ptr: *mut T) -> Self {
+        AutoDerefPointer{
+            0 : NonNull::new(ptr as *mut T).unwrap(),
+        }
+    }
+}
+
+
+pub type BlockHeaderPointer = AutoDerefPointer<BlockHeader>;
+pub type FreeBlockHeaderPointer = AutoDerefPointer<FreeBlockHeader>;
+
+
+
+/// The header of a memory block. <br>
+/// The header is stored before the data part of the block, see the documentation for detail.<br>
+/// You should **not** allocate or create Block Header manually.
+/// Because the new block is either on stack or other heap memory, not our memory pool.<br>
+/// Typically, you should use BlockHeaderPointer to access the header of a block.
+/// There are mainly two ways we get the header of a block:
+/// * Through the pointer stored in the TLSFControl.blocks
+/// * Use the pointer passing by user in `free`function 
+#[repr(C)]
+pub struct BlockHeader {
+    /// we can get the next header easily through `size` field.
+    /// but unless we have a pointer to previous header,
+    /// we don't know the exact location of last block start.
+    prev_phys_block: *mut BlockHeader,
+
+    /// magic number to check the validity of the block
+    magic : u32,
+
+    /// the last 3 **bit** is used to store some flags
+    size: u32, 
+}
+
+
+/// The header of a free memory block. <br>
+/// * The `links` is used to link the free blocks in the same size class.
+/// * When the block is allocated, the links will be covered by the user data.
+/// * In this lab, you are **not** required to use `links`. You can use your own list instead 
+/// as long as you can pass the tests.
+pub struct FreeBlockHeader{
+    header : BlockHeader,
+    links : Links<FreeBlockHeader>,
+}
+impl GetLinks for FreeBlockHeader{
+    type EntryType = FreeBlockHeader;
+    fn get_links(data:&Self::EntryType) -> &Links<Self::EntryType> {
+        &data.links
+    }
+}
+
+
+/// The control block of the TLSF memory allocator.<br>
+/// If you are not familiar with bit operation, replace the bitmap with a bool array.
+pub struct TLSFControl {
+    /// first level bitmap
+    fl_bitmap: usize, 
+
+    /// second level bitmap
+    sl_bitmap: [usize; FL_INDEX_COUNT],
+
+    /// a linked list of free blocks for each size class
+    blocks: [[RawList<FreeBlockHeader>; SL_INDEX_COUNT]; FL_INDEX_COUNT],
+}
+
+
+
+impl BlockHeader {
+    /// å°æåæ°æ®é¨åçè£¸æéè½¬æ¢ææååå¤´é¨çå¼ç¨  
+    /// è¿åå¼ï¼         --->| prev_phys_block |   
+    ///                     | size            |   
+    /// ä¼ å¥çåæ°ï¼ ptr --> | data            |  
+    /// 
+    /// convert a pointer passed by user to BlockHeaderPointer <br>
+    /// Some evil users would pass a pointer not allocated by our allocator.
+    /// You can check the safety of pointer but it is not compulsory in our tests.
+    /// # Argument
+    /// * `ptr` - the pointer passed by user
+    /// # Return
+    /// block header 
+    /// 
+    /// # Safety
+    /// The caller should guarantee that the pointer is valid.
+    /// 
+    pub fn from_user_pointer<T>(ptr: *mut T) -> BlockHeaderPointer {
+        unsafe{
+            BlockHeaderPointer::from_raw_pointer(ptr.offset(-(size_of::<Self>() as isize)) as *mut BlockHeader)
+        }
+    }
+
+    /// è¿åæ°æ®åç®¡ççåå­å¤§å° <br>
+    /// Get the size of the memory block managed by this header.   <br>
+    /// Note that the size is aligned to ALIGN_SIZE, 
+    /// and the last few bit of `size` is used to store flags.
+    pub fn get_size(&self) -> usize {
+        self.size as usize & !(BLOCK_HEADER_FREE_BIT | BLOCK_HEADER_PREV_FREE_BIT)
+    }
+
+    /// Check if the magic number is correct.
+    pub fn check_magic(&self) -> bool {
+        self.magic == BLOCK_HEADER_MAGIC as u32
+    }
+
+    /// è®¾ç½®æ å¿ä½is_prev_freeä¸ºfalse <br>
+    /// Set the flag is_prev_free to false.
+    pub fn set_prev_used(&mut self) {
+        self.size  &= !(BLOCK_HEADER_PREV_FREE_BIT) as u32;
+    }
+
+    /// è¿åæ å¿ä½is_prev_freeçå¼ï¼æ¥çåä¸ä¸ªåæ¯å¦ç©ºé² <br>
+    /// Return the value of the flag is_prev_free: whether the previous block is free.
+    pub fn is_prev_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_PREV_FREE_BIT) != 0
+    }
+
+    /// è¿åå½ååæ¯å¦ç©ºé² <br>
+    /// Return whether the current block is free.
+    pub fn is_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_FREE_BIT) != 0
+    }
+
+    /// è·åå½ååçæ°æ®æé <br>
+    /// Return the pointer to the data of the current block.
+    /// # Safety
+    /// If the `self` is valid, the returned pointer is valid and it can write `size` bytes at most.
+    pub fn data_ptr<T>(&self) -> *mut T {
+        unsafe { (self as *const _ as *mut u8).offset(size_of::<Self>() as isize) as *mut T }
+    }
+
+    /// è®¾ç½®å½ååç©ºé²ï¼å¹¶æä¸ä¸ä¸ªåçis_prev_freeæ å¿ä½ç½®ä¸ºtrue <br>
+    /// Set the current block to free, and set the is_prev_free flag of the next block to true.
+    pub fn mark_as_free(&mut self) -> FreeBlockHeaderPointer {
+        self.set_free();
+        self.link_next();
+        FreeBlockHeaderPointer::from_raw_pointer(self as *const _ as *mut FreeBlockHeader)
+    }
+
+    /// è·åä¸ä¸ä¸ªåãå¦æä¸ä¸ä¸ªåä¸ºå¨åµï¼è¿åNone <br>
+    /// Return the pointer to the next block. 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel
+    pub fn next(&self) -> Option<BlockHeaderPointer> {
+        let next_block = unsafe{&mut *self.next_uncheked()};
+        if (next_block.get_size() == 0) {
+            None
+        } else {
+            Some(
+                BlockHeaderPointer::from_raw_pointer(next_block as *mut BlockHeader)
+            )
+        }
+    }
+
+    /// Return the pointer to the next block.
+    /// # Return 
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel or is not free block.
+    pub fn next_free(&self) -> Option<FreeBlockHeaderPointer>{
+        match self.next(){
+            Some(next) => {
+                if next.is_free(){
+                    Some(FreeBlockHeaderPointer::from_raw_pointer(next.0.as_ptr() as *const _ as *mut FreeBlockHeader))
+                }else{
+                    None
+                }
+            },
+            None => None
+        }
+    }
+
+    /// è·ååä¸ä¸ªåãå¦æåä¸ä¸ªåå·²è¢«åéï¼è¿åNone <br>
+    /// Return the pointer to the previous block.
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the previous block
+    /// * `None` - the previous block is allocated or just not exists.
+    pub fn prev(&self) -> Option<BlockHeaderPointer> {
+        if self.prev_phys_block.is_null() {
+            None
+        } else {
+            Some(BlockHeaderPointer::from_raw_pointer(self.prev_phys_block))
+        }
+    }
+
+    /// Return the pointer to the previous block.
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the previous block
+    /// * `None` - the previous block is allocated or just not exists.
+    /// 
+    /// # Safety
+    /// You may encouter some memory page fault here. Make sure the `prev_phys_block` is valid
+    /// otherwise it will panic here.
+    pub fn prev_free(&self) -> Option<FreeBlockHeaderPointer>{
+        match self.prev(){
+            Some(prev) => {
+                if prev.is_free(){
+                    Some(FreeBlockHeaderPointer::from_raw_pointer(prev.0.as_ptr() as *const _ as *mut FreeBlockHeader))
+                }else{
+                    None
+                }
+            },
+            None => None
+        }
+    }
+
+
+    /// è®¾ç½®æ°æ®åç®¡ççåå­å¤§å° <br>
+    /// Set the size of the memory block header. 
+    fn set_size(&mut self, size: usize) {
+        let old_size = self.size;
+        self.size = size as u32 | (old_size & (BLOCK_HEADER_FREE_BIT as u32 | BLOCK_HEADER_PREV_FREE_BIT as u32));
+        self.magic = BLOCK_HEADER_MAGIC as u32;
+    }
+
+    /// è®¾ç½®æ å¿ä½is_prev_freeä¸ºtrue <br>
+    /// Set the flag is_prev_free to true.
+    fn set_prev_free(&mut self) {
+        self.size |= BLOCK_HEADER_PREV_FREE_BIT as u32;
+    }
+
+    /// è®¾ç½®å½ååç©ºé² <br>
+    /// Set the current block to free.
+    fn set_free(&mut self) {
+        self.size |= BLOCK_HEADER_FREE_BIT as u32; 
+    }
+
+    /// è®¾ç½®å½ååå·²ä½¿ç¨ <br>
+    /// Set the current block to used.
+    fn set_used(&mut self) {
+        self.size &= !(BLOCK_HEADER_FREE_BIT) as u32;
+    }
+
+    /// è·åä¸ä¸ä¸ªåçæé <br>
+    /// Return the pointer to the next block. 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel
+    #[inline]
+    unsafe fn next_uncheked(&self) -> *mut BlockHeader {
+        unsafe { &mut *((self.data_ptr() as *mut u8).offset((self.get_size() - BLOCK_HEADER_OVERHEAD) as isize) as *mut BlockHeader)}
+    }
+   
+    fn link_next(&mut self){
+        let next = self.next(); // we don't cate if the next block is free or not
+        if let Some(mut next) = next {
+            next.prev_phys_block = self as *const _ as *mut BlockHeader;
+            next.set_prev_free();
+        }
+    }
+}
+
+impl ValidityCheck for BlockHeader {
+    fn check_validity(&self) -> bool {
+        self.magic == BLOCK_HEADER_MAGIC as u32
+    }
+}
+
+impl ValidityCheck for FreeBlockHeader {
+    fn check_validity(&self) -> bool {
+        self.header.magic == BLOCK_HEADER_MAGIC as u32
+    }
+}
+
+impl FreeBlockHeader{
+    /// åå§åä¸ä¸ªåå­æ±  <br>
+    /// Initialize a new memory pool as a block.
+    /// # Input
+    /// * `ptr` - the pointer to the memory pool
+    /// * `size` - the real size of the memory pool(which means that you should minus the overhead)
+    /// # Safety
+    /// user must ensure that the pointer point to a memory that owned by us.
+    /// 
+    /// # Hints
+    /// * Don't forget to add a sentinel block at the end of the memory pool. The sentinel block's size is 0
+    /// and it's `set_used`.
+    /// * The first block's `previous_used` can be set to true.
+    /// * use `link_next_init` to generate the next block's pointer.**Don't** use `link_next` here because it
+    /// will do some checkings.
+    pub unsafe fn init_block(ptr : *mut FreeBlockHeader,size:usize) -> FreeBlockHeaderPointer{
+        let block = unsafe{&mut (*ptr)};
+        // TODO: YOUR CODE HERE
+
+        FreeBlockHeaderPointer::from_raw_pointer(ptr)
+    }
+
+    /// è®¾ç½®å½ååå·²ä½¿ç¨ï¼å¹¶æä¸ä¸ä¸ªåçis_prev_freeæ å¿ä½ç½®ä¸ºfalse <br>
+    /// Set the current block to used, and set the is_prev_free flag of the next block to false.
+    pub fn mark_as_used(&mut self) -> BlockHeaderPointer {
+        self.header.set_used();
+        let next = self.header.next();
+        if let Some(mut next) = next {
+            next.set_prev_used();
+        }
+        BlockHeaderPointer::from_raw_pointer(self as *mut FreeBlockHeader as *mut BlockHeader)
+    }
+
+     /// å¤æ­å½ååæ¯å¦å¯ä»¥åå²
+    /// Return whether the current block can be split.
+    pub fn can_split(&self, size: usize) -> bool {
+        self.header.get_size() >= size_of::<BlockHeader>() + size
+    }
+
+    /// æå½åååä¸ºä¸¤ä¸ªåï¼åä¸ä¸ªåå¤§å°ä¸ºsizeï¼åä¸ä¸ªåå¤§å°ä¸ºself.size - size - BLOCK_HEADER_OVERHEAD <br>
+    /// Split the current block into two blocks, the size of the first block is `size`,
+    /// and the size of the second block is `self.size - size - BLOCK_HEADER_OVERHEAD`.
+    /// 
+    /// # Arguments
+    /// * `size` - the size of the first block(`self` block) after `split`.
+    /// 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the second block, convert it to `BlockHeaderPointer`
+    /// * `None` - the block cannot be split
+    /// 
+    /// # Tips:
+    /// * When compute the pointer to the next block, cast it to `*mut u8` first.
+    /// * What is the position of the second block? Think carefully before you starting to write the code.
+    /// * The second block should be marked as free.
+    /// * The size of the second block should consider the overhead of the block header.
+    /// * Some useful functions: `self.header.set_size`,`can_split`,`self.header.get_size,`mark_as_free`
+    pub fn split(&mut self, size: usize) -> Option<FreeBlockHeaderPointer> {
+        // TODO: YOUR CODE HERE
+        None
+    }
+
+    /// åå¹¶ä¸¤ä¸ªå <br>
+    /// Merge the next block into the current block.
+    /// The next block should exist and free
+    /// # Safety
+    /// Don't use the pointer to the next block after calling this function.
+    /// 
+    /// # Hint
+    /// * Update the size of the current block.
+    /// * Remember to update the prev_phys_block of the next block.(You may use `link_next`)
+    pub fn absorb(&mut self) {
+        if self.header.get_size() == 0 {
+            return;
+        }
+        // TODO: YOUR CODE HERE
+    }
+    
+    /// * Let next block point to self <br>
+    /// * You may only use it in `FreeBlockHeader::init_block`
+    fn link_next_init(&mut self) -> BlockHeaderPointer {
+        let next = unsafe{&mut *self.header.next_uncheked()};
+        next.prev_phys_block = self as *const _ as *mut BlockHeader;
+        unsafe{BlockHeaderPointer::from_raw_pointer_unchecked(next as *mut BlockHeader)}
+    }
+}
+
+impl TLSFControl {
+    /// å¨å ä¸åå§åæ§å¶å <br>
+    /// Initialize the control block on the heap.(Kernel global heap)
+    /// # Arguments
+    /// * `tmp` - the temporary control block
+    /// 
+    /// # Return
+    /// * `Box<Self,Global>` - initialize the value of `tmp` and return it.
+    pub fn init_on_heap(mut tmp : Box<TLSFControl,Global>) -> Box<Self,Global>{
+        // TODO: YOUR CODE HERE
+        tmp.fl_bitmap = 0;
+        tmp.sl_bitmap = [0; FL_INDEX_COUNT];
+        for i in 0..FL_INDEX_COUNT {
+            for j in 0..SL_INDEX_COUNT {
+                tmp.blocks[i][j] = RawList::new();
+            }
+        }
+        tmp
+        // END OF YOUR CODE
+    }
+
+    /// å°ä¸ä¸ªåå­å°åå å¥ç®¡ç <br>
+    /// Add a contiguous memory block to the control block. 
+    /// 
+    /// # Arguments
+    /// * `mem_base` - the base address of the memory block
+    /// * `size` - the size of the memory block
+    /// 
+    /// # Hints
+    /// * Fill in the blank in `init_block` first
+    /// * Use `self.insert_block` to insert the block into the free list
+    pub fn add_pool(&mut self,mut mem_base: *mut u8, size: usize){
+        let real_size = align_down(size - ALLOC_POOL_OVERHEAD, ALIGN_SIZE);
+        assert_eq!(
+            mem_base.align_offset(ALIGN_SIZE),
+            0,
+            "mem_base is not align"
+        );
+        // TODO: YOUR CODE HERE
+        let mut block = unsafe{FreeBlockHeader::init_block(mem_base.offset(-(BLOCK_HEADER_OVERHEAD as isize)) as *mut FreeBlockHeader, real_size)};
+        // END OF YOUR CODE
+    }
+
+    /// åéä¸ä¸ªå¤§å°è³å°ä¸ºsizeçåå­åãå¦æåéå¤±è´¥ï¼è¿åç©ºæéã <br>
+    /// Allocate a memory block whose size is at least `size`.
+    /// # Arguments
+    /// * `size` - the size of the memory block user needs
+    /// 
+    /// # Return
+    /// * `*mut T` - the pointer to the allocated memory block
+    /// * `nullptr` - if the allocation fails
+    /// 
+    /// # Hints
+    /// * Align the size first
+    /// * You may need a helper method to find a free block
+    /// * Split the free block to perfectly fit the size we need, and put the rest into the free pool
+    /// * Use `self.insert_block` to insert the rest block into the free list
+    /// * Don't forget to mark the block as used
+    /// * You may use the following functions: `split`,`mark_as_used`,`set_prev_used`,`insert_block`
+    pub fn malloc<T>(&mut self, size: usize) -> *mut T {
+        let adjust = if size > 0 {
+            align_up(size, ALIGN_SIZE)
+        } else {
+            panic!("size must be greater than 0");
+        };
+        // TODO: YOUR CODE HERE
+        ptr::null_mut()
+        // END OF YOUR CODE
+    }
+
+    /// éæ¾ä¸ä¸ªæéæåçåå­ <br>
+    /// Free a memory block.
+    /// 
+    /// # Arguments
+    /// * `ptr` - the pointer to the memory block
+    /// 
+    /// # Hints
+    /// * You don't have to check the validity of the pointer
+    /// * Use `BlockHeader::from_user_pointer` to get the block header
+    /// * After free the block, try to merge the neighbor free blocks.
+    ///   You may need a helper method to deal with the merging. As a reference,
+    ///   We use `merge_prev_block` and `merge_next_block` to merge the previous 
+    ///   and next blocks respectively.
+    /// * Don't forget to put the merged block into the free list
+    /// 
+    pub fn free<T>(&mut self, ptr: *mut T) {
+        if ptr.is_null() {
+            panic!("try to free null pointer");
+        }
+        // TODO: YOUR CODE HERE
+    }
+
+    /// æä¸ä¸ªåå­åæå¥ç©ºé²é¾è¡¨ <br>
+    /// Insert a free block into the free list.
+    /// 
+    /// # Hints
+    /// * Use `mapping_insert` to get the index of the free list
+    /// * Fill in the blank in `mapping_insert` first
+    /// * Use raw_list or your own implementation to insert the block into the free list
+    /// * Use `self.set_bitmap` to set the bitmap
+    fn insert_block(&mut self, mut block: FreeBlockHeaderPointer) {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+    }
+
+
+    /// æ¥æ¾ä¸ä¸ªä¸å°äºè¯·æ±sizeçç©ºé²åå­å  <br>
+    /// find a free block whose size is not less than `size`
+    /// # Return 
+    /// * `Option<BlockHeaderPointer>` - the pointer to the free block
+    /// * `None` - if no such block exists
+    /// 
+    /// # Hints
+    /// * You need to know the exact `fl`, `sl` index of the free list. Use `mapping_search` to get them.
+    /// * If there's no such block, you need to find a larger block and split it. If you are using bit operations
+    ///   with `fl_bitmap`  and `sl_bitmap`, you may find `ffs` and `fls` useful.
+    /// * pop a block from the free list and return it
+    /// * Remember to unset the bitmap after you find a block
+    fn block_locate_free(&mut self, size: usize) -> Option<FreeBlockHeaderPointer> {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        None
+    }
+
+    /// set the bitmap of control block
+    fn set_bitmap(&mut self, fl: u32, sl: u32) {
+        self.fl_bitmap |= 1 << fl;
+        self.sl_bitmap[fl as usize] |= 1 << sl;
+    }
+
+    /// unset the bitmap of control block
+    fn unset_bitmap(&mut self, fl: u32, sl: u32) {
+        if self.blocks[fl as usize][sl as usize].is_empty() {
+            self.sl_bitmap[fl as usize] &= !(1 << sl);
+            if self.sl_bitmap[fl as usize] == 0 {
+                self.fl_bitmap &= !(1 << fl);
+            }
+        }
+    }
+
+    /// remove a block from the free list  
+    /// * The function search the free list to find the block and remove it.
+    /// 
+    /// # Arguments
+    /// * `block` - the block to be removed
+    fn remove_block(&mut self, block: &mut FreeBlockHeaderPointer) {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+    }
+
+    /// å°è¯ååä¸ä¸ªååå¹¶ <br>
+    /// Try to merge the previous block
+    /// 
+    /// # Return 
+    /// * `BlockHeaderPointer` - the pointer to the merged block
+    /// * If merge is successful, the pointer to the merged block is returned.
+    /// * Otherwise the pointer to the original block.
+    /// 
+    /// # Hints
+    /// * If the previous block is not free, just return current block.
+    /// * Try to use `absorb` to merge the block
+    /// * Use `prev_free` instead of `prev`
+    /// * Don't forget to remove the previous block from the free list
+    fn merge_prev_block(&mut self, block: FreeBlockHeaderPointer) -> FreeBlockHeaderPointer {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        unimplemented!()
+
+    }
+
+    /// å°è¯åå¹¶ä¸ä¸ä¸ªå <br>
+    /// Try to merge the next block
+    /// 
+    /// # Return 
+    /// * `BlockHeaderPointer` - the pointer to the merged block
+    /// * If merge is successful, the pointer to the merged block is returned.
+    /// * Otherwise the pointer to the original block.
+    fn merge_next_block(&mut self, mut block: FreeBlockHeaderPointer) -> FreeBlockHeaderPointer {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        unimplemented!()
+    }
+}
+
+
+/// å°xåä¸å¯¹é½å°alignçåæ° <br>
+/// Align x up to a multiple of align
+#[inline]
+fn align_up(x: usize, align: usize) -> usize {
+    (x + align - 1) & !(align - 1)
+}
+
+/// å°xåä¸å¯¹é½å°alignçåæ° <br>
+/// Align x down to a multiple of align
+#[inline]
+fn align_down(x: usize, align: usize) -> usize {
+    x - (x & (align - 1))
+}
+
+/// è®¡ç®xçæé«ä½1çä½ç½® <br>
+/// get the position of the highest bit 1 of x, also known as find last set
+#[inline]
+fn fls(word: usize) -> u32 {
+    (32 - (word as u32).leading_zeros() - 1) as u32
+}
+
+/// è®¡ç®xçæä½ä½1çä½ç½® <br>
+/// get the position of the lowest bit 1 of x,also known as find first set
+#[inline]
+fn ffs(word: usize) -> u32 {
+    (word as u32).trailing_zeros() as u32
+}
+
+/// æTLSFæè¯´çæ¹æ³å°sizeæ å°å°flåsl
+/// Mapping the size to fl and sl according to the method in TLSF
+/// 
+/// # Arguments
+/// * `size` - the size of the block
+/// 
+/// # Return
+/// - (fl, sl) - the fl and sl of the block
+/// 
+/// # Hints
+/// * You can handle the situation when size is smaller than SMALL_BLOCK_SIZE specially.
+/// * The methods above like ffs,fls may be helpful.
+#[inline]
+pub fn mapping_insert(size: usize) -> (u32, u32) {
+    // TODO: YOUR CODE HERE
+    (0,0)
+    // END OF YOUR CODE
+}
+
+/// åéæ¶ï¼æ ¹æ®sizeæ¾å°åéçflåslãè¿éä¼æsizeåä¸å ä¸ä¸ªroundï¼é²æ­¢åçåå­æ¯å®éå° <br>
+/// roundæçæ¯ä¸ä¸ªsecond_level rangeçå¤§å°ï¼ä¾å¦ï¼å½SLI=5, f=10ï¼æ¯ä¸ªround=2^10/2^5=32å­è
+#[inline]
+fn mapping_search(mut size: usize) -> (u32, u32) {
+    if size >= SMALL_BLOCK_SIZE {
+        // ä¸ºé²æ­¢åçåå­æ¯å®éå°ï¼åä¸åæ´
+        let round = (1 << (fls(size) as usize - SL_INDEX_COUNT_LOG2)) - 1;
+        size += round as usize;
+    }
+    mapping_insert(size)
+}
+
+pub static mut TLSFHeap : Option<Box<TLSFControl>> = None;
+
+use core::alloc::{Allocator,AllocError};
+pub struct TLSFMem;
+unsafe impl Allocator for TLSFMem{
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError>{
+        let size = layout.size();
+        unsafe{
+            let ptr = TLSFHeap.as_mut().unwrap().malloc::<u8>(size);
+            if ptr.is_null(){
+                Err(AllocError)
+            }else{
+                let r= slice::from_raw_parts(ptr, size);
+                Ok(NonNull::from(r))
+            }
+        }
+    }
+
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, _layout: Layout){
+        unsafe{
+            TLSFHeap.as_mut().unwrap().free(ptr.as_ptr() as *mut u8);
+        }
+    }
+}
+use kernel::{vmalloc, pr_info};
+pub fn init_tlsfheap(){
+    unsafe{
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        TLSFHeap = Some(
+            TLSFControl::init_on_heap(tmp)
+        );
+        let size = 4096*4;
+        let ptr = vmalloc::c_vmalloc(size).unwrap();
+        TLSFHeap.as_mut().unwrap().add_pool(ptr as *mut u8,size as usize);
+    }
+}
+
+
+use kernel::bindings;
+pub struct Example{
+    pub val1 : u32,
+    pub list1 : bindings::list_head,
+
+    pub val2 : u32,
+    pub list2 : bindings::list_head
+}
+
+
+/// ä»å¤§å°å°æåº
+/// sort the ex1 and ex2 in descending order in two lists respectively
+/// # Hints
+/// * You can use the rust_helper_list_add_tail function.
+/// * Just compare the val1 of ex1 and ex2 and the add the ex1 and ex2 to the list respectively.
+pub fn list_example1_sort_decending_respectively(val_list1:&mut bindings::list_head,
+                    val_list2:&mut bindings::list_head,
+                    ex1: &mut Example,
+                    ex2: &mut Example){
+    extern "C"{
+        fn rust_helper_list_add_tail(new : *mut bindings::list_head, list : *mut bindings::list_head);
+    }
+    // TODO: YOUR CODE HERE
+}
+
diff --git a/kernel/rros/tlsf_raw_list.rs b/kernel/rros/tlsf_raw_list.rs
new file mode 100644
index 000000000..48b486045
--- /dev/null
+++ b/kernel/rros/tlsf_raw_list.rs
@@ -0,0 +1,220 @@
+// SPDX-License-Identifier: GPL-2.0
+
+//! A fork of raw lists.
+//!
+
+use core::{
+    cell::UnsafeCell,
+    ptr,
+    ptr::NonNull,
+    sync::atomic::{AtomicBool, Ordering},
+};
+
+/// A descriptor of list elements.
+///
+/// It describes the type of list elements and provides a function to determine how to get the
+/// links to be used on a list.
+///
+/// A type that may be in multiple lists simultaneously needs to implement one of these for each
+/// simultaneous list.
+pub trait GetLinks {
+    /// The type of the entries in the list.
+    type EntryType: ?Sized;
+
+    /// Returns the links to be used when linking an entry within a list.
+    fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType>;
+}
+
+/// The links used to link an object on a linked list.
+///
+/// Instances of this type are usually embedded in structures and returned in calls to
+/// [`GetLinks::get_links`].
+pub struct Links<T: ?Sized> {
+    entry: UnsafeCell<ListEntry<T>>,
+}
+
+impl<T: ?Sized> Links<T> {
+    /// Constructs a new [`Links`] instance that isn't inserted on any lists yet.
+    pub fn new() -> Self {
+        Self {
+            entry: UnsafeCell::new(ListEntry::new()),
+        }
+    }
+}
+
+impl<T: ?Sized> Default for Links<T> {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+struct ListEntry<T: ?Sized> {
+    next: Option<NonNull<T>>,
+    prev: Option<NonNull<T>>,
+}
+
+impl<T: ?Sized> ListEntry<T> {
+    fn new() -> Self {
+        Self {
+            next: None,
+            prev: None,
+        }
+    }
+}
+
+/// A linked list.
+///
+/// # Invariants
+///
+/// The links of objects added to a list are owned by the list.
+pub struct RawList<G: GetLinks> {
+    head: Option<NonNull<G::EntryType>>,
+}
+
+impl<G: GetLinks> RawList<G> {
+    pub fn new() -> Self {
+        Self { head: None }
+    }
+
+    pub fn is_empty(&self) -> bool {
+        self.head.is_none()
+    }
+
+    fn insert_after_priv(
+        &mut self,
+        existing: &G::EntryType,
+        new_entry: &mut ListEntry<G::EntryType>,
+        new_ptr: Option<NonNull<G::EntryType>>,
+    ) {
+        {
+            // SAFETY: It's safe to get the previous entry of `existing` because the list cannot
+            // change.
+            let existing_links = unsafe { &mut *G::get_links(existing).entry.get() };
+            new_entry.next = existing_links.next;
+            existing_links.next = new_ptr;
+        }
+
+        new_entry.prev = Some(NonNull::from(existing));
+
+        // SAFETY: It's safe to get the next entry of `existing` because the list cannot change.
+        let next_links =
+            unsafe { &mut *G::get_links(new_entry.next.unwrap().as_ref()).entry.get() };
+        next_links.prev = new_ptr;
+    }
+
+    /// Inserts the given object after `existing`.
+    ///
+    /// # Safety
+    ///
+    /// Callers must ensure that `existing` points to a valid entry that is on the list.
+    pub unsafe fn insert_after(
+        &mut self,
+        existing: &G::EntryType,
+        new: &G::EntryType,
+    ) -> bool {
+        let links = G::get_links(new);
+        // if !links.acquire_for_insertion() {
+        //     // Nothing to do if already inserted.
+        //     return false;
+        // }
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let new_entry = unsafe { &mut *links.entry.get() };
+        self.insert_after_priv(existing, new_entry, Some(NonNull::from(new)));
+        true
+    }
+
+    fn push_back_internal(&mut self, new: &G::EntryType) -> bool {
+        let links = G::get_links(new);
+        // if !links.acquire_for_insertion() {
+        //     // Nothing to do if already inserted.
+        //     return false;
+        // }
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let new_entry = unsafe { &mut *links.entry.get() };
+        let new_ptr = Some(NonNull::from(new));
+        match self.back() {
+            // SAFETY: `back` is valid as the list cannot change.
+            Some(back) => self.insert_after_priv(unsafe { back.as_ref() }, new_entry, new_ptr),
+            None => {
+                self.head = new_ptr;
+                new_entry.next = new_ptr;
+                new_entry.prev = new_ptr;
+            }
+        }
+        true
+    }
+
+    pub unsafe fn push_back(&mut self, new: &G::EntryType) -> bool {
+        self.push_back_internal(new)
+    }
+
+    fn remove_internal(&mut self, data: &G::EntryType) -> bool {
+        let links = G::get_links(data);
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let entry = unsafe { &mut *links.entry.get() };
+        let next = if let Some(next) = entry.next {
+            next
+        } else {
+            // Nothing to do if the entry is not on the list.
+            return false;
+        };
+
+        if ptr::eq(data, next.as_ptr()) {
+            // We're removing the only element.
+            self.head = None
+        } else {
+            // Update the head if we're removing it.
+            if let Some(raw_head) = self.head {
+                if ptr::eq(data, raw_head.as_ptr()) {
+                    self.head = Some(next);
+                }
+            }
+
+            // SAFETY: It's safe to get the previous entry because the list cannot change.
+            unsafe { &mut *G::get_links(entry.prev.unwrap().as_ref()).entry.get() }.next =
+                entry.next;
+
+            // SAFETY: It's safe to get the next entry because the list cannot change.
+            unsafe { &mut *G::get_links(next.as_ref()).entry.get() }.prev = entry.prev;
+        }
+
+        // Reset the links of the element we're removing so that we know it's not on any list.
+        entry.next = None;
+        entry.prev = None;
+        // links.release_after_removal();
+        true
+    }
+
+    /// Removes the given entry.
+    ///
+    /// # Safety
+    ///
+    /// Callers must ensure that `data` is either on this list or in no list. It being on another
+    /// list leads to memory unsafety.
+    pub unsafe fn remove(&mut self, data: &G::EntryType) -> bool {
+        self.remove_internal(data)
+    }
+
+    fn pop_front_internal(&mut self) -> Option<NonNull<G::EntryType>> {
+        let head = self.head?;
+        // SAFETY: The head is on the list as we just got it from there and it cannot change.
+        unsafe { self.remove(head.as_ref()) };
+        Some(head)
+    }
+
+    pub fn pop_front(&mut self) -> Option<NonNull<G::EntryType>> {
+        self.pop_front_internal()
+    }
+
+    pub fn front(&self) -> Option<NonNull<G::EntryType>> {
+        self.head
+    }
+
+    pub fn back(&self) -> Option<NonNull<G::EntryType>> {
+        // SAFETY: The links of head are owned by the list, so it is safe to get a reference.
+        unsafe { &*G::get_links(self.head?.as_ref()).entry.get() }.prev
+    }
+}
\ No newline at end of file
diff --git a/lab4.md b/lab4.md
new file mode 100644
index 000000000..ec7ffb162
--- /dev/null
+++ b/lab4.md
@@ -0,0 +1,455 @@
+# OS Lab 4  mem
+
+ å¨rrosçåºç¡ä¸å¡«ç©ºå®ç°ä¸ä¸ªåå­åéç®æ³
+
+å®ç°ä¸ä¸ªç¸å¯¹å®æ¶çåå­åéç®æ³ï¼ä¸»è¦æ¯rflçåå­åéç®æ³å¦ä½ålinuxéæé¨åï¼ï¼å¯¹ä»£ç æç©ºï¼æåå­åéç®æ³é¨åææ
+
+åä¸ä¸ªå®éªæåReference
+
+* å®æ¶åå­åéç®æ³è®ºæ1
+
+* å®æ¶åå­åéç®æ³è®ºæ2
+
+## Intro
+
+
+
+## æäº¤æ ¼å¼
+
+## Quick Start
+
+### ç¯å¢æ­å»º
+
+ï¼è¿éåºè¯¥åä¸ä¸ä¸ªå®éªä¸è´ï¼å æ­¤çä¸ä¸ä¸ªå®éªæä¹æ­å»ºäº.ï¼
+
+ä¸»è¦è¦ä¿®æ¹çæä»¶ï¼
+
+* `rust/kernel/memory_rros.rs`
+
+
+
+### Rust for linuxç¯å¢ä»ç»
+
+
+
+## åå­åéå¨çç¸å³ç¥è¯
+
+### å¸¸è§åå­åéç®æ³
+
+
+
+### å¸¸ç¨æºå¶
+
+å¤§å¤æ°å¨æåå­åé(DSA)ç®æ³é½ä½¿ç¨äºä¸é¢çä¸ä¸ªæèå¤ä¸ªæºå¶çç»åãè¿éå°å®ä»¬ååºï¼æä»¬ä¸»è¦åèäº[Dynamic Storage Allocation: A Survey and Critical Review](https://users.cs.northwestern.edu/~pdinda/ics-s05/doc/dsa.pdf) ï¼è¯»èæå´è¶£å¯ä»¥æ·±å¥äºè§£ã
+
+* **header**
+
+  å¤§å¤æ°åéå¨åéçåä¸é¢é½ä¼å¸¦çä¸ä¸ªç¹æ®çheaderï¼å¶ä¸­åæ¬äºåçéè¦ä¿¡æ¯ï¼ä¾å¦åçé¿åº¦ãä¸è¬æ¥è¯´ï¼headerå­æ®µçé¿åº¦é½æ¯ä¸ä¸ªå­é¿ãç±äºåéçåé¿åº¦ä¸è¬æ¯å¯¹é½çï¼å°¾é¨ä¼æå ä¸ªåä½çbitãè¿å ä¸ªbitä¸è¬ä½ä¸ºç¶æä½ã
+
+* **boundary tags**
+
+  é¤äºheaderï¼æçåè¿ä¼æfooterå­æ®µãfooteråæ ·ä¿å­åé¿åº¦ååæ¯å¦è¢«ä½¿ç¨ãå¨åéè¦åå¹¶çæ¶åï¼å¯ä»¥æ¹ä¾¿å°æ£æ¥ä¸é¢çåæ¯å¦å¯ä»¥è¢«åå¹¶ï¼åªéè¦çå®çfooterå­æ®µå³å¯ï¼ã
+
+  åæ¶ä½¿ç¨headeråfooteræµªè´¹äºå¾å¤ç©ºé´ãä½æ¯å®éä¸ï¼å ä¸ºæä»¬åªå¨åå¹¶åçæ¶åç¨å°footerï¼å æ­¤footerå­æ®µå¨åè¢«ä½¿ç¨çæ¶åæ¯æ æçï¼å¯ä»¥åªå¨åä¸è¢«ç¨çæ¶åä½¿ç¨footerï¼è¿æ ·footeråå°±æ²¡æï¼ä½¿ç¨æ¶ï¼ç©ºé´å¼éäºã
+
+  TODO: å ä¸ªå¾
+
+* **link field**
+
+  é¾è¡¨åæ ç»æç»å¸¸è¢«ç¨äºç®¡çååå­ãåæ ·ï¼ä¸è¬åªæç©ºé²åéè¦è¢«ç®¡çï¼å æ­¤é¾è¡¨/æ èç¹ç´æ¥è¢«æ¾å¨ç©ºé²ååï¼å½ç¶ï¼éè¦éå¶æå°åçå¤§å°ï¼ä»¥ä¾¿è½æ¾ä¸èç¹ï¼ï¼å½åè¢«åéçæ¶åï¼åçæææè½¬ç§»ç»ç¨æ·ï¼å¹¶è¢«ä»é¾è¡¨/æ ä¸é¢å é¤ã
+
+* **lookup table**
+
+  ä¸äºåéå¨ä¸ä¼æç»å®å¤§å°åéåï¼èæ¯åä¸åæ´å°ä¸ä¸ªå¤§å°ååéç»ç¨æ·ãèåéå¨åé¨ä¼æä¸å®è§åé¢åæ¾å¥½ä¸äºåºå®å¤§å°çåï¼åéæ¶ç´æ¥æ¥æ¾å¯¹åºå¤§å°çåé¾è¡¨/æ å³å¯ãéå¸¸ä¼æ2çå¹å»åéï¼ä¹æä½¿ç¨ææ³¢é£å¥å½æ°ç­çãä½æ¯ï¼lookup tableå¦æåå¾å¤ªç»å¯è½å ç¨å¾å¤§ç©ºé´ã
+
+* **éå¯¹å°å¯¹è±¡çä¼å**
+
+  å¯¹äºå¤§å¤æ°ç³»ç»ï¼åéå°å¯¹è±¡çæ¬¡æ°ä¼è¿å¤äºå¤§å¯¹è±¡ãå æ­¤ä¸äºåéå¨ä¼ç¹æ®å¤çå°å¯¹è±¡çåéãä¸äºå¸¸è§çç»åæï¼å¯¹å°å¯¹è±¡ä½¿ç¨å¿«éåéç®æ³ï¼å¯¹å¤§å¯¹è±¡ä½¿ç¨èçç©ºé´çæå·§ï¼å¯¹å°å¯¹è±¡ä½¿ç¨lookup tableï¼å¯¹å¤§å¯¹è±¡ä½¿ç¨æ¯è¾å¤æçè®¡ç®ï¼æ¶é´æ¢ç©ºé´ï¼ã
+
+## TLSF(Two-Level Segregated Fit) å®æ¶åå­åéç®æ³
+
+å¨è¿ä¸é¨åï¼æä»¬å°ä»ç»ä¸ä¸ªæ¯è¾ç®åçå®æ¶åå­åéç®æ³ââTLSFãå®å¨[TLSF: A new dynamic memory allocator for real-time systems(Masmano,et al.)](http://www.gii.upv.es/tlsf/files/ecrts04_tlsf.pdf)è¿ç¯è®ºæä¸­æåºã
+
+TLSFæ¯éå¯¹å®æ¶æä½ç³»ç»çå¨æåå­åéç®æ³ï¼è½å¤å¨`O(1)`çæ¶é´åè¿ååéçåå­ãTLSFçåééåº¦å¿«ï¼ä½æ¯ç¸å¯¹åå­å©ç¨çä½ï¼å®¹æåºç°åé¨ç¢çï¼æ¯è¾å¸¸ç¨äºåµå¥å¼åºæ¯ã
+
+### ç®æ³ä»ç»
+
+è¿ä¸é¨åï¼æä»¬å°ä»ç»TLSFç®æ³ãæä»¬éç¨çå®ç°æ¯åè[mattconte/tlsf: Two-Level Segregated Fit memory allocator implementation. (github.com)](https://github.com/mattconte/tlsf)ï¼è¿ä¸åççTLSFç®æ³ç¥æä¸åã
+
+#### ç©ºé²åå­ç®¡ç
+
+TLSFå°ç®¡ççç©ºé²åå­åä¸ºæ¾å¨ä¸ä¸ªä¸¤ç»´é¾è¡¨æ°ç»éï¼æ°ç»çæ¯ä¸ªåç´ è¡¨ç¤ºè¯¥åå­åºé´çfree listï¼ç©ºé²åå­é¾è¡¨ï¼ã
+
+
+
+![tlsf1](assets\tlsf1.png)
+
+å·ä½æ¥è¯´ï¼å¯¹äºä¸ä¸ªsizeçç©ºé²åï¼å®çæ å°å¬å¼å¦ä¸ï¼
+
+![tlsf1](assets\tlsf2.png)
+
+å¶ä¸­fè¡¨ç¤ºç¬¬ä¸å±ï¼sè¡¨ç¤ºç¬¬äºå±ï¼SLI(second level index)è¡¨ç¤ºç¬¬äºå±ç®¡ççæ¯ç¹æ°ãç¬¬ä¸å±æ¯æ2çå¹è¿è¡ååï¼ä¹å°±æ¯è¯´ï¼æé«bitæ¯ç¬¬å ä½ï¼ç¬¬ä¸å±få°±æ¯å¤å°ãè¿æ ·åå­å°±è¢«ååä¸ºäº[2^4,2^5-1], [2^5,2^6-1]....ãç¬¬äºå±è¿ä¸æ­¥ææ¯ä¸ªåºé´åä¸º2^{SLI}ä»½ï¼åæå ä»½é½å¯ä»¥ï¼ä½æ¯åä¸º2^{SLI}ä»½æä½è¿ç®æ´å¥½ç®ï¼ãä¸è¬SLI=4æè5ã
+
+ä¾å¦ï¼å½SLI=4, size=460æ¶
+$$
+f=log2(460) = 8ï¼s = (16/256)*(460-256)=12
+$$
+![tlsf1](assets\tlsf3.png)
+
+åæ¯å¦ï¼å½SLI=5, size=1234æ¶
+$$
+f=log2(1234) = 10ï¼s = (32/1024)*(1234-1024)=6
+$$
+ä¸é¢fçæå°å¼ä¸è¬ä¸ä¸º0ãä¸æ¹é¢ï¼åéåä¸è¬ä¼ææå°åçéå¶ï¼ä¾å¦16å­èæè32å­èãå¦ä¸æ¹é¢ï¼åå­åå¤§å°éå¸¸ä¼è§å®ä¸º4å­èæè8å­èçæ´æ°åï¼è¿æ ·åå¤§å°çä½2,3ä¸ªbitæ°¸è¿ä¸º0ï¼å¯ä»¥ä½ä¸ºæ å¿ä½ç®¡çåå­åã
+
+æä»¬åé¢çé»è®¤å®ç°éç¨çæ¯SLI=5, åéé¿åº¦æ8å­èçæ´æ°åã
+
+#### åå­åç»æ
+
+ç©ºé²ååéç©ºé²åçåå­åæ°æ®ç»æå¦ä¸ï¼
+
+![tlsf_block](assets\tlsf_block.jpg)
+
+å¯ä»¥çå°ï¼ç©ºé²åæ¯éç©ºé²åå¤äºä¸¤ä¸ªå­æ®µï¼`next_free`,`prev_free`ãè¿ä¸¤ä¸ªå­æ®µæ¯ä¸ä¸ä¸ªå°èç©ºé²åç®¡çä¸­çé¾è¡¨ç¨å°çã`size`è¡¨ç¤ºçæ¯åéåå­çå¤§å°ï¼ä¸åæ¬åæ°æ®å¼éï¼ï¼ä¸å¾çsizeæ¯8çæ´æ°åï¼å æ­¤æå3bitæ¯ç©ºé²çï¼è¿éåªç¨äºä¸¤ä¸ªbitï¼åå«è¡¨ç¤ºå½ååæ¯å¦ç©ºé²ï¼ä»¥ååä¸ä¸ªåæ¯å¦ç©ºé²ã`prev_phys_block` æ¯ä¸ä¸ªæååä¸ä¸ªåçæéãè¿éåªææååä¸ä¸ªåçæéï¼èæ²¡ææååä¸ä¸ªçãè¿æ¯å ä¸ºæ¯ä¸ªåé½æ¯ç´§æ¨ççï¼æä»¬åªéè¦å°æéååç§»å¨`sizeof(prev_phys_block) + sizeof(size) + size` å°±å¯ä»¥è·³è½¬å°ä¸ä¸ä¸ªåãèåä¸ä¸ªåï¼åæ æ³éè¿è¿ç§æ¹å¼å¾å°ä½ç½®ã
+
+è¿éå¯ä»¥åä¸ä¸ªä¼åï¼è®©`prev_phys_block`å¨éç©ºé²åä¸­ä¸å ç¨ä½ç½®ãå¨ç®æ³ä¸­ï¼`prev_phys_block`æ¯ç¨æ¥åç©ºé²åé´åå¹¶çã`prev_phys_block`ï¼æååä¸ä¸ªåçæéï¼åªæå¨`is_prev_free`ï¼åä¸ä¸ªåæ¯ç©ºé²ï¼æ¶æä¼è¢«ä½¿ç¨ï¼ å æ­¤å¯ä»¥è®©`prev_phys_block`âä¾µå âåä¸ä¸ªåçä½ç½®ãå¦æåä¸ä¸ªåææï¼é£ä¹æä»¬ç`prev_phys_block`å¯è½ä¼è¢«ç¨æ·æ°æ®è¦çäºï¼ä½æ¯å®æ¬èº«ä¹ç¨ä¸å°ï¼è¦çäºä¹æ²¡äºï¼å¦æåä¸ä¸ªåæ¯ç©ºé²çï¼`prev_phys_block`åä¸ä¼è¢«è¦çæãå¯ä»¥åè§ä¸ä¸å°èä¸­çç¤ºæå¾ï¼å¶ä¸­èçº¿ç`prev_phys_block`è¡¨ç¤ºä¸è½ä½¿ç¨çæéï¼å®çº¿è¡¨ç¤ºå¯ä»¥ä½¿ç¨ã
+
+åçå®æ´è§å¾å¦ä¸ï¼å¶ä¸­blockæéè¡¨ç¤ºæä½åæ¶ç¨çæéï¼èptræ¯è¿åç»ç¨æ·çæéãå½è¦éæ¾åå­æ¶ï¼åå°çæéä¹æ¯è¿ä¸ªæéãå°å®åä¸ç§»å¨ä¸¤ä¸ªå­é¿å°±è½å¾å°æä½åçæé
+
+![used_block](assets\used_block.jpg)
+
+#### æ§å¶åçç»æä½
+
+æ§å¶åé¤äºä¸é¢æå°äºäºç»´æ°ç»é¾è¡¨ï¼è¿æä¸¤ä¸ªå­æ®µ`fl_bitmap`,`sl_bitmap[]` ä½¿ç¨bitä½è¡¨ç¤ºå¯¹åºåºé´çåå­æ¯å¦è¿æç©ºé²ãå¨ç®¡çåå­çæåé¢æä¸ä¸ªå¨åµåï¼å®çé¿åº¦ä¸º0ãåæ¶ï¼ç¬¬ä¸ä¸ªåç`is_prev_free`ä¼ç½®ä¸º0ï¼é²æ­¢è®¿é®å°åéåå­å¤çå°åã
+
+![tlsf1](assets\overview.jpg)
+
+#### åå§å
+
+åå§åæ¶ï¼å°fl_bitmapåsl_bitmapç½®ä¸º0ï¼è¡¨ç¤ºæ²¡æç©ºé²åãç¶ååå§åblocksæ°ç»éé¢çé¾è¡¨å¤´ã
+
+å½åæ§å¶åæ·»å ä¸åç®¡ççåå­æ¶ï¼æè¿ç»­çåå­å½åä¸ä¸ªåæå¥å°å¯¹åºäºç»´æ°ç»çé¾è¡¨ä¸­ï¼è®¾ç½®bitmapãæ³¨æè¿ä¸ªåç`size`éè¦é¢çä½ç½®ç»å¨åµåï¼åæ¶è¿ææ£é¤èªèº«åæ°æ®`size`çå¤§å°ï¼8å­èï¼ï¼ä¸å±æ¯16å­èãè®¾ç½®è¿ä¸ªå`is_prev_free`ä¸ºfalseï¼è·³è½¬å°è¿ä¸ªåæåé¢ï¼æå¥ä¸ä¸ªå¨åµåã
+
+ä¾å¦ï¼å¨SLI=5çæ¶åï¼åå§åçæ¶åæå¥çåå­åæ¯2048å­èï¼å¶ä¸­16å­èè¢«ç¨äºååæ°æ®ï¼å¯ç¨é¿åº¦ä¸º2032å­èãé£ä¹åå§ååæä¸¤ä¸ªåï¼ä¸ä¸ªé¿åº¦ä¸º2032å­èï¼ä¸ä¸ªé¿åº¦ä¸º0å­èã2032å­èçåä¼è¢«æå¥å°f=10,s=31çé¾è¡¨éï¼ç¸åºçæ å¿ä½ä¹ä¼è¢«ç½®ä½ã
+
+å¨åé¢çå®ç°ä¸­ï¼å ä¸ºæä»¬æ8å­èå¯¹é½ï¼å¹¶ä¸SLI=5ï¼å æ­¤æå°çå¤§ååºä¸º256å­èèä¸æ¯0å­èãæä»¬å¯ä»¥æfåå»(5+3-1)=7ä»¥èçæ°ç»ç©ºé´ã**åé¢ä½¿ç¨fl æä»£åç§»åçf**ï¼å½fl=0æ¶ï¼åºé´è¡¨ç¤º[0,256)å­èï¼å½fl=1æ¶ï¼åºé´ä¸º[256,512)å­èï¼å¯¹åºåæ¥çf=8ï¼ä»¥æ­¤ç±»æ¨ã
+
+å æ­¤ï¼æå¥2048å­èï¼å®éå¯ç¨2032å­èï¼æ¶ï¼fl=3,sl=31ã`fl_bitmap |= 1 << 3`ï¼`sl_bitmap[3] |= 1<<31`
+
+#### è·åä¸ååå­
+
+å½è¯·æ±ä¸ååå­æ¶ï¼é¦åè¦æå¤§å°æ8å­èåä¸å¯¹é½ãç¶åæ¥æ¾å¯¹åºå¤§å°åºé´ä¸­çç©ºé²é¾è¡¨ï¼çæ¯å¦æå¯ç¨çåå­ãä¹åç»´æ¤çbitmapç¨æ¥å¯»æ¾ç¬¦åè¦æ±çæå°åå­åã
+
+ä¾å¦ï¼SLI=5æ¶ï¼åéä¸åå¤§å°ä¸º460çåå­ï¼å¯¹é½åæ¯464ï¼é£ä¹ï¼
+$$
+f=log2(464)=8,fl = 8-7=1ï¼s = (32/256)*(464-256)=26
+$$
+æ¥è¯¢bitmapï¼åç°`sl_bitmap[1]`ç­äº0ï¼æ²¡æå¯¹åºçåï¼åæ¥è¯¢`fl_bitmap`ï¼åç°æä½ç½®ä½(`lowbit`)ä¸ºç¬¬4ä¸ªbitï¼`sl_bitmap[3]`çç¬¬32ä¸ªbitä¸º1ï¼å æ­¤ä»`blocks[3][31]`çé¾è¡¨éå¤´æèéå°¾ååºä¸ä¸ªç©ºé²åãå¦æè¿æ¶é¾è¡¨ä¸ºç©ºï¼éè¦å¯¹bitmapæ¸ç©ºã
+
+æ¾å°ä¸ä¸ªååï¼è¿è¦å¯¹åè¿è¡åå²ï¼ååºéè¦åéçå­èæ°ï¼å©ä¸çåå­å¦æå¯ä»¥å»ºåï¼é£ä¹æ°å»ºä¸ä¸ªåï¼ç¶åå°å®å å¥å°ç©ºé²é¾è¡¨ä¸­ãä»¥ä¸é¢çæåµä¸ºä¾ï¼å¾å°çæ¯2032å­èçåï¼ååº464å­èåè¿å©1568å­èï¼å¯ä»¥æ°å»ºä¸ä¸ª1560é¿åº¦çåï¼è¿æ8å­èç¨äºå­æ¾é¿åº¦ï¼ã
+
+æåï¼å°å¾å°çåè½¬æ¢æä¸ä¸ªæéï¼è¿åç»ç¨æ·ã
+
+![used_block](assets\used_block.jpg)
+
+#### éæ¾ä¸ååå­
+
+éæ¾åå­çæ¶åï¼é¦åè¦æç¨æ·çæéè½¬æ¢æåçæéãå°è¿ä¸ªåBæ è®°ä¸ºç©ºé²åï¼ä¸ä¸ä¸ªåCç`is_prev_free`æ è®°ç½®ä¸ºtrueã
+
+![used_block](assets\release1.jpg)
+
+ç¶åï¼å°è¯å°è¿ä¸ªåä¸å®çé»å±ï¼ä¸ä¸çåï¼åå¹¶ãå¯¹äºä¸é¢çåAï¼å¦æå½ååB`is_prev_free=true`,é£ä¹åå¹¶Aï¼Bä¸¤ä¸ªåï¼å¾å°æ°çåBãåçï¼å¦æä¸é¢çåCä¹æ¯ç©ºé²çï¼é£ä¹å°å®åå¹¶ãæåï¼å°å½ååBæå¥å°ç©ºé²é¾è¡¨ä¸­ã
+
+![used_block](assets\release2.jpg)
+
+#### åè
+
+* [esp-idfçåå­ç®¡çââtlsfç®æ³](https://blog.csdn.net/gzxb1995/article/details/124504705)
+* [TLSF åå­åéç®æ³è¯¦è§£](https://blog.csdn.net/pwl999/article/details/118253758)
+
+### ä»»å¡
+
+å¨è¿ä¸é¨åï¼ä½ éè¦å®ç°ä¸ä¸ªTLSFåéå¨ï¼å¹¶éè¿ç¸åºæµè¯ã
+
+ä½ ä¸éè¦ç¬ç«å®æå¨é¨ä»£ç ï¼æä»¬å·²ç»æä¾äºBlockHeaderçé¨åä»£ç åä¸ä¸ªç®åçæ¡æ¶ãä½ éè¦å®æçä»£ç ä¸»è¦å¨`rust/kernel/tlsf.rs`ï¼æµè¯çä»£ç å¨`test/rros/test_tlsf.rs`ã
+
+ä¸ºäºåå°å®ç°çé¾åº¦ï¼ä½ å¯ä»¥ä½¿ç¨ä½ å¨Lab1éé¢å®ç°çé¾è¡¨æ¥ç®¡çç©ºé²åå­ï¼èä¸æ¯ç´æ¥ä½¿ç¨ååæéãä¸é¢ç»åºäºä¸ä¸ªç¤ºä¾çç»æä½å®ä¹ï¼
+
+```rust
+const FL_INDEX_COUNT:usize = 25;
+const SL_INDEX_COUNT:usize = 32;
+pub struct TLSFControl<'a> {
+    fl_bitmap: usize,
+    sl_bitmap: [usize; FL_INDEX_COUNT],
+    blocks: [[LinkedList<&'a mut BlockHeader>; SL_INDEX_COUNT]; FL_INDEX_COUNT],
+}
+```
+
+å¦æä½ ä¸çæä½è¿ç®ï¼ä½ ä¹å¯ä»¥ä½¿ç¨boolæ°ç»æ¥æ¿ä»£bitmapã
+
+è½ç¶ä¸é¢çæ§å¶ååªå ç¨å KBçåå­ï¼ä½æ¯linuxåæ ¸æ ä¸è¬ä¹åªæ4KBæè8KBï¼å¦ææä¸é¢çæ§å¶åæ¾å¨åæ ¸æ ä¸é¢ä¼å¯¼è´åæ ¸æ æº¢åºãå æ­¤æä»¬è¦ææ§å¶åæ´ä½æ¾å°å ä¸ãå¨æ¡æ¶ä»£ç ä¸­ï¼æä»¬ä½¿ç¨äºRustçMaybeUninitãè°ç¨Box::try_new_uninit_inæ²¡æé©¬ä¸è¿è¡åå§åï¼èæ¯è¿åä¸ä¸ª`Box<MaybeUninit<T>>`.ç¶åæä»¬ç´æ¥å¯¹å ä¸çåå­åå§åãè¿æ ·å¯ä»¥é²æ­¢å¨åå§åé¶æ®µå°±åçæ æº¢åºãè¿éæä¾ä¸ä¸ªåå§åçç¤ºä¾ä»£ç ï¼
+
+```rust
+pub fn init_on_heap(tmp : Box<TLSFControl<'a>,Global>) -> Box<Self,Global>{
+    // TODO: YOUR CODE HERE
+    for i in 0..FL_INDEX_COUNT {
+        for j in 0..SL_INDEX_COUNT {
+            tmp.blocks[i][j] = LinkedList::new();
+        }
+    }
+    tmp.fl_bitmap = 0;
+    tmp.sl_bitmap = [0; FL_INDEX_COUNT];
+    tmp
+    // END OF YOUR CODE
+}
+```
+
+
+
+#### å®ç°blockHeader
+
+æä»¬å·²ç»å®ç°äºä¸ä¸ªç®åçBlockHeaderï¼å³ç®¡çåå­åçç»æä½ï¼å¹¶æä¾äºä¸äºå¯è½ä¼ç¨å°çæ¹æ³ï¼ä½ ä¹å¯ä»¥éåä¸ä¸ªBlockHeaderï¼ä½æ¯å®åå­ä¸­çä¿å­æ¹å¼è¦åä¸å¾ç¸åï¼sizeå¯ä»¥ä¸ç¨æ¯8å­èçï¼4å­èçu32ä¹å¯ä»¥ãå©ä¸4å­èæ¾æ å¿ä½ï¼
+
+![used_block](assets\block.jpg)
+
+å¦æä½ ç´æ¥ä½¿ç¨æä»¬æç»çBlockHeaderï¼ä½ è¿éè¦èªå·±å®ç°`split`å`link_next`ä¸¤ä¸ªå½æ°ã
+
+* `BlockHeader::split`ï¼ å°ä¸ä¸ªååä¸ºä¸¤ä¸ªåï¼åä¸ä¸ªåçå¤§å°ä¸ºsizeï¼åä¸ä¸ªåå¤§å°ä¸º`self.size-size-BLOCK_HEADER_OVERHEAD`ãå¦ææ æ³åå²ï¼è¿åNone
+* `BlockHeader::link_next`ï¼ æ¾å°ä¸ä¸ä¸ªåï¼ç¶åæä¸ä¸ä¸ªåç`prev_phys_block`æéæåå½ååçå°åã
+  * è·åå½ååçå°åå¯ä»¥è¿æ ·åï¼`unsafe{next.prev_phys_block = self as *const _ as *mut BlockHeader;}`
+  * ä¸ä¸ä¸ªåçå°åä¸º`å½ååå°å+size-BLOCK_HEADER_OVERHEAD`
+
+å½ä½ çä»£ç æ­£ç¡®çæ¶åï¼ä½ åºè¯¥è½éè¿`test/rros/test_tlsf.rs`ä¸­çæµè¯`test_blockHeader`ã
+
+#### å çåå§å
+
+æ¥ä¸æ¥å®ç°å çåå§åãå®å
+
+* `TLSFControl::init_on_heap`ï¼ åå§åæ§å¶å
+* `TLSFControl::add_pool` : å°ä¸ä¸ªåå­å°åå å¥ç®¡ç
+
+ä¸¤ä¸ªå½æ°ãè¿ä¸é¨ååªéè¦æç§ä¸é¢æè¿°å®ç°å³å¯.
+
+å½ä½ çä»£ç æ­£ç¡®çæ¶åï¼ä½ åºè¯¥è½éè¿`test/rros/test_tlsf.rs`ä¸­çæµè¯`test_init`ã
+
+#### mapping
+
+å®ç°ä¸ä¸ªmappingå½æ°ï¼å°sizeæ å°ä¸ºå¯¹åºçfåsãå®å
+
+* `mapping_insert` å°sizeæ å°å°flåsl
+
+å½æ°ã
+
+åé¢ä½ ä¼ç¨å°mappingå½æ°ãä¸å±æä¸¤ä¸ªmappingå½æ°ï¼`mapping_insert`å`mapping_search`ï¼å·²å®ç°ï¼ã`mapping_insert`å¨æå¥ç©ºé²åæ¶ä½¿ç¨ï¼`mapping_search`ç¨äºåéåå­æ¶è·åæ¯ç»å®sizeç¨å¤§çåã
+
+#### malloc
+
+ä¸é¢å®ç°ä¸ä¸ªmallocçæ¥å£ãè¿æ¯mallocçä¼ªä»£ç ï¼
+
+```
+void* malloc(self,size){
+	var size = adjust_size(size); // å°sizeåä¸å¯¹é½ï¼å¯ä»¥ä½¿ç¨align_upå½æ°
+	var block = self.block_locate_free(size); //æ¾å°ä¸ä¸ªåéçå
+	if !block{
+		return null;// è¿åç©ºæé
+	}
+	var remain_block = block.split(size); // æåæ¥çååæä¸¤é¨å
+	è®¾ç½®remain_blockçæ è®°ä½ï¼å¹¶æremain_blockçä¸ä¸ä¸ªåè®¾ç½®ä¸ºblockçå°å
+	self.insert(remain_block); // å å¥ç©ºé²é¾è¡¨
+	return block.get_ptr(); // è¿åæé
+}
+```
+
+å®å`TLSFControl::malloc`å½æ°ãä½ å¯è½éè¦è±è¾å¤æ¶é´å¨`block_locate_free`å½æ°ã
+
+å½ä½ çä»£ç æ­£ç¡®çæ¶åï¼ä½ åºè¯¥è½éè¿`test/rros/test_tlsf.rs`ä¸­çæµè¯`test_malloc`ã
+
+#### åå­éæ¾
+
+æåå®ç°ä¸ä¸ªfreeçæ¥å£ãè¿æ¯freeçä¼ªä»£ç ï¼
+
+```
+void free(self,ptr){
+	var block = BlockHeader::from_raw_pointer(ptr);
+	å°blockè®¾ç½®ä¸ºfreeï¼åæ¶blockçä¸ä¸ä¸ªåçis_prev_freeè¦è®¾ç½®ä¸ºtrue.
+	å°è¯åå¹¶ä¸ä¸ååå½åå
+	å°è¯åå¹¶å½åååä¸ä¸å
+	self.insert(block); // å°å½ååæå¥ç©ºé²é¾è¡¨
+}
+```
+
+å®å`TLSFControl::free`å½æ°ã
+
+å½ä½ çä»£ç æ­£ç¡®çæ¶åï¼ä½ åºè¯¥è½éè¿`test/rros/test_tlsf.rs`ä¸­çæµè¯`test_free`å`test_multiple_alloc`ã
+
+#### rustçalloc_api
+
+rustæä¾äºä¸ä¸ª`Allocator` traitãå½ä¸ä¸ªç»æä½å®ç°äº`allocate`å`deallocate`ä¸¤ä¸ªå±æ§æ¶ï¼å®å°±è½æä¸ºä¸ä¸ª`Allocator` ãæä»¬å·²ç»ä¸º`TLSFMem`å®ç°äº`Allocator` ã`tlsf.rs`å£°æäºä¸ä¸ªå¨å±éæåé`TLSFHeap`ãæé å¨å¯¹è±¡`TLSFMem`ä¼è°ç¨`TLSFHeap`è¿è¡åå­åéåéæ¯ã`TLSFMem`åå§æ¶æ¯ä¸ä¸ªNoneãå½è°ç¨init_tlsfheapæ¶ï¼ä¼åéç»å è¥å¹²ä¸ªé¡µï¼ç¶ååå§ååå­æ± ã
+
+```rust
+pub fn init_tlsfheap(){
+    unsafe{
+        let tmp = Box::<TLSFControl<'static>>::try_new_uninit_in(Global).unwrap().assume_init();
+        TLSFHeap = Some(
+            TLSFControl::init_on_heap(tmp)
+        );
+        let size = 4096*4;
+        let ptr = vmalloc::c_vmalloc(size).unwrap();
+        TLSFHeap.as_mut().unwrap().add_pool(ptr as *mut u8,size as usize);
+    }
+}
+```
+
+æä»¬ç¼åäºä¸ä¸ªæµè¯`tlsf_allocator`ãè¿éä¼ä½¿ç¨ä½ åçå è¿è¡åå­çåéåéæ¯ï¼
+
+```rust
+pub fn tlsf_allocator(){
+    unsafe{
+        for i in 0..100{
+            let a = Box::try_new_in("hello world from our allocator", TLSFMem).unwrap();
+            let b = Box::try_new_in(123456789, TLSFMem).unwrap();
+            let c = Box::try_new_in(1.23456789, TLSFMem).unwrap();
+            let d = Box::try_new_in([1,2,3,4,5,6,7,8,9,0], TLSFMem).unwrap();
+            let e = Box::try_new_in((1,2,3,4,5), TLSFMem).unwrap();
+        }
+        let a = Box::try_new_in("hello world from our allocator", TLSFMem).unwrap();
+        pr_info!("{}",a);
+        pr_info!("tlsf_allocator ok");
+    }
+}
+
+```
+
+å¦æä¹åæµè¯å¨é¨é¡ºå©éè¿ï¼ä½ åºè¯¥è½å¨è¿è¡æ¶çå°âhello world from our allocatorâ
+
+#### åå­éæ¾
+
+#### æµè¯æªå¾
+
+å½å¨é¨æµè¯æåæ¶ï¼ä¼å¨è¾åºä¸­çå°ï¼
+
+![test succ](assets\test_succ.png)
+
+èå¦ææµè¯ä¸­åºç°äºéè¯¯ï¼ä¼ç´æ¥å¯¼è´panicï¼initè¿ç¨è¢«ææ­»ï¼
+
+![test fail](assets\failed.png)
+
+TIPSï¼
+
+> * å¦æå¨åæ ¸ç¯å¢ä¸é¢å¼ååè°è¯æ¯è¾å°é¾ï¼ä¹å¯ä»¥æ°å»ºcargoé¡¹ç®å¹¶æ·è´TLSFåæµè¯çä»£ç è¿è¡æµè¯åè°è¯ãä½æ¯ï¼ä½ çå®ç°åºè¯¥æ²¡æç¬¬ä¸æ¹çä¾èµï¼å ä¸ºrust for linuxä¸æ¯æcargoãï¼å½ç¶ï¼å¦æä½ å¯ä»¥æä¾èµçä»£ç å¤å¶å°é¡¹ç®éè¿æ¥ï¼å¹¶ä¸è½éè¿åæ ¸çç¼è¯ï¼ä¹æ¯å¯ä»¥çï¼
+> * ä½ å¯ä»¥ä½¿ç¨ä½ èªå·±å®ç°çé¾è¡¨æ¥å®æè¿ä¸ªå®éªï¼ä¹å¯ä»¥ç¨kernelåå®ç°å¥½çä¸ä¸ªé¾è¡¨(`rust/kernel/double_linked_list`)
+> * å¨ä½¿ç¨å åéåå­æ¶ï¼åstdç¯å¢ä¸ä¸åï¼ä½ éè¦ä½¿ç¨`use alloc::boxed::Box;`å¼å¥`Box`ãç±äºrust for linuxä¼ å¥äº`no_global_oom_handling`æ å¿ï¼å æ­¤ä¸è½ä½¿ç¨`Box::new`,`Vec`ç­æ¥å£ãä½ å¯ä»¥ä½¿ç¨`Box::try_new_in<T,Global>.unwrap()`æ¥æ¿ä»£(`use alloc::alloc::Global;`)ã
+
+## EVLä¸­çå®æ¶åå­åéç®æ³
+
+
+
+## æµè¯åè°è¯
+
+### è°è¯
+
+#### gdb
+
+ä½¿ç¨gdbçremote debugå¯ä»¥è°è¯åæ ¸ãé¦åå¨å¯å¨åæ ¸çå½ä»¤åé¢å ä¸`-s -S`ï¼-s è¡¨ç¤ºå¯å¨gdb serverï¼-Sè¡¨ç¤ºä¸è¦ç«å»æ§è¡æä»¤ï¼æ`c`å¯ä»¥å¼å§æ§è¡ï¼ãä¾å¦ï¼
+
+```bash
+qemu-system-aarch64 -nographic  -kernel arch/arm64/boot/Image -initrd /data/rootfs.cpio.gz  -machine type=virt -cpu cortex-a57 -append "rdinit=/linuxrc console=ttyAMA0" -device virtio-scsi-device -smp 1 -m 4096 -s -S
+```
+
+å¶ä¸­`arch/arm64/boot/Image`æ¯åæ ¸çè·¯å¾ï¼`/data/rootfs.cpio.gz`æ¯æä»¶ç³»ç»çè·¯å¾
+
+ç¶åæ°å»ºä¸ä¸ªçªå£ï¼å¯å¨gdb(ä¾å¦`gdb-multiarch`ï¼ãå¨gdbéï¼è¾å¥ä¸é¢çå½ä»¤
+
+```
+file vmlinux
+target remote localhost:1234
+set lang rust
+```
+
+ç¶åï¼å¨æä»¶ä¸é¢æ­ç¹ï¼ä¾å¦ï¼
+
+```
+b rust/kernel/memory_rros.rs:131
+```
+
+è¡¨ç¤ºå¨131è¡æ­ç¹ã
+
+è¾å¥`c`(continue)å¼å§è°è¯ã
+
+ä¸é¢ç»åºäºgdbçä¸äºå¸¸è§å½ä»¤
+
+TODO:å å¾ç
+
+#### vscode
+
+ä¹å¯ä»¥ç»vscodeæ·»å éç½®æä»¶ã
+
+é¦åï¼åæ ·è¿æ¯å¨å½ä»¤è¡å¯å¨è°è¯çqemuï¼
+
+```
+qemu-system-aarch64 -nographic  -kernel arch/arm64/boot/Image -initrd /data/rootfs.cpio.gz  -machine type=virt -cpu cortex-a57 -append "rdinit=/linuxrc console=ttyAMA0" -device virtio-scsi-device -smp 1 -m 4096 -s -S
+```
+
+å¶ä¸­`arch/arm64/boot/Image`æ¯åæ ¸çè·¯å¾ï¼`/data/rootfs.cpio.gz`æ¯æä»¶ç³»ç»çè·¯å¾
+
+
+
+ç¶åï¼å¨é¡¹ç®æ ¹ç®å½ç.vscodeæä»¶å¤¹ä¸­ï¼æå¼launch.json(æ²¡æçè¯æ°å»ºä¸ä¸ª)ï¼æä¸é¢çéç½®ç²è´´è¿å»ï¼
+
+```json
+{
+    // Use IntelliSense to learn about possible attributes.
+    // Hover to view descriptions of existing attributes.
+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
+    "version": "0.2.0",
+    "configurations": [
+        {
+            "name": "linux kernel debug",
+            "type": "cppdbg",
+            "request": "launch",
+            "miDebuggerServerAddress": "127.0.0.1:1234",
+            "program": "${workspaceFolder}/vmlinux",
+            "args": [],
+            "stopAtEntry": false,
+            "cwd": "${workspaceFolder}",
+            "environment": [],
+            "externalConsole": false,
+            "logging": {
+                "engineLogging": false
+            },
+            "MIMode": "gdb",
+        }
+    ]
+}
+```
+
+
+
+## TODO
+
+è®¾è®¡æè·¯ï¼
+
+* ç¨å¾®æä¸ä¸rust for linux
+* è®²è§£linuxçº¢é»æ .
+
+è¦è®²è§£çæ¨¡å
+
+* `rust/kernel/memory_rros.rs`
+* linuxçº¢é»æ é¨å
+* ålinuxçæ¥å£
+
+`string.rs`
+
+### è¦åçæ¨¡å
+
+* `insert_range_bysize`
+
+* `evl_alloc_chunk`
+
+* `reserve_page_range` ,`search_size_ge`
+* `remove_page`
+* `add_page_front`
+
+## Q&A
+
+### å¦æéè¦ä½¿ç¨`#feature[]`
\ No newline at end of file
diff --git a/rust/alloc/alloc_rros.rs b/rust/alloc/alloc_rros.rs
new file mode 100644
index 000000000..a461a3662
--- /dev/null
+++ b/rust/alloc/alloc_rros.rs
@@ -0,0 +1,210 @@
+// SPDX-License-Identifier: Apache-2.0 OR MIT
+
+//! Memory allocation APIs
+
+#![stable(feature = "alloc_module", since = "1.28.0")]
+
+#[cfg(not(test))]
+use core::intrinsics;
+use core::intrinsics::{min_align_of_val, size_of_val};
+
+use core::ptr::Unique;
+#[cfg(not(test))]
+use core::ptr::{self, NonNull};
+
+#[stable(feature = "alloc_module", since = "1.28.0")]
+#[doc(inline)]
+pub use core::alloc::*;
+
+#[cfg(test)]
+mod tests;
+
+extern "Rust" {
+    #[rustc_allocator]
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc(size: usize, align: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_dealloc(ptr: *mut u8, size: usize, align: usize);
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc_zerod(size: usize, align: usize) -> *mut u8;
+}
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[derive(Copy, Clone, Default, Debug)]
+#[cfg(not(test))]
+pub struct RrosMem;
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc(layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_dealloc(ptr: *mut u8, layout: Layout) {
+    unsafe { __rros_sys_heap_dealloc(ptr, layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+    unsafe { __rros_sys_heap_realloc(ptr, layout.size(), layout.align(), new_size) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc_zeroed(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc_zerod(layout.size(), layout.align()) }
+}
+
+#[cfg(not(test))]
+impl RrosMem {
+    #[inline]
+    fn alloc_impl(&self, layout: Layout, zeroed: bool) -> Result<NonNull<[u8]>, AllocError> {
+        match layout.size() {
+            0 => Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0)),
+            size => unsafe {
+                let raw_ptr = if zeroed { rros_alloc_zeroed(layout) } else { rros_alloc(layout) };
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, size))
+            },
+        }
+    }
+
+    // SAFETY: Same as `Allocator::grow`
+    #[inline]
+    unsafe fn grow_impl(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+        zeroed: bool,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() >= old_layout.size(),
+            "`new_layout.size()` must be greater than or equal to `old_layout.size()`"
+        );
+
+        match old_layout.size() {
+            0 => self.alloc_impl(new_layout, zeroed),
+
+            // SAFETY: `new_size` is non-zero as `old_size` is greater than or equal to `new_size`
+            // as required by safety conditions. Other conditions must be upheld by the caller
+            old_size if old_layout.align() == new_layout.align() => unsafe {
+                let new_size = new_layout.size();
+
+                // `realloc` probably checks for `new_size >= old_layout.size()` or something similar.
+                intrinsics::assume(new_size >= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                if zeroed {
+                    raw_ptr.add(old_size).write_bytes(0, new_size - old_size);
+                }
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_layout.size()` must be greater than or equal to `old_size`,
+            // both the old and new memory allocation are valid for reads and writes for `old_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            old_size => unsafe {
+                let new_ptr = self.alloc_impl(new_layout, zeroed)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), old_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
+
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[cfg(not(test))]
+unsafe impl Allocator for RrosMem {
+    #[inline]
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, false)
+    }
+
+    #[inline]
+    fn allocate_zeroed(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, true)
+    }
+
+    #[inline]
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
+        if layout.size() != 0 {
+            // SAFETY: `layout` is non-zero in size,
+            // other conditions must be upheld by the caller
+            unsafe { rros_dealloc(ptr.as_ptr(), layout) }
+        }
+    }
+
+    #[inline]
+    unsafe fn grow(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, false) }
+    }
+
+    #[inline]
+    unsafe fn grow_zeroed(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, true) }
+    }
+
+    #[inline]
+    unsafe fn shrink(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() <= old_layout.size(),
+            "`new_layout.size()` must be smaller than or equal to `old_layout.size()`"
+        );
+
+        match new_layout.size() {
+            // SAFETY: conditions must be upheld by the caller
+            0 => unsafe {
+                self.deallocate(ptr, old_layout);
+                Ok(NonNull::slice_from_raw_parts(new_layout.dangling(), 0))
+            },
+
+            // SAFETY: `new_size` is non-zero. Other conditions must be upheld by the caller
+            new_size if old_layout.align() == new_layout.align() => unsafe {
+                // `realloc` probably checks for `new_size <= old_layout.size()` or something similar.
+                intrinsics::assume(new_size <= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_size` must be smaller than or equal to `old_layout.size()`,
+            // both the old and new memory allocation are valid for reads and writes for `new_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            new_size => unsafe {
+                let new_ptr = self.allocate(new_layout)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), new_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
\ No newline at end of file
diff --git a/rust/alloc/lib.rs b/rust/alloc/lib.rs
index f109e7902..84a37e361 100644
--- a/rust/alloc/lib.rs
+++ b/rust/alloc/lib.rs
@@ -161,7 +161,7 @@ mod macros;
 // Heaps provided for low-level allocation strategies
 
 pub mod alloc;
-
+pub mod alloc_rros;
 // Primitive types using the heaps above
 
 // Need to conditionally define the mod from `boxed.rs` to avoid
diff --git a/rust/alloc/rc.rs b/rust/alloc/rc.rs
index 7344cd9a4..6ac5ecb70 100644
--- a/rust/alloc/rc.rs
+++ b/rust/alloc/rc.rs
@@ -1,5 +1,3 @@
-// SPDX-License-Identifier: Apache-2.0 OR MIT
-
 //! Single-threaded reference-counting pointers. 'Rc' stands for 'Reference
 //! Counted'.
 //!
@@ -43,7 +41,7 @@
 //! use std::rc::Rc;
 //!
 //! let my_rc = Rc::new(());
-//! Rc::downgrade(&my_rc);
+//! let my_weak = Rc::downgrade(&my_rc);
 //! ```
 //!
 //! `Rc<T>`'s implementations of traits like `Clone` may also be called using
@@ -143,7 +141,7 @@
 //! ```
 //!
 //! If our requirements change, and we also need to be able to traverse from
-//! `Owner` toÂ `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
+//! `Owner` to `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
 //! to `Gadget` introduces a cycle. This means that their
 //! reference counts can never reach 0, and the allocation will never be destroyed:
 //! a memory leak. In order to get around this, we can use [`Weak`]
@@ -264,6 +262,7 @@ use core::marker::{self, PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw, forget};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 #[cfg(not(no_global_oom_handling))]
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
@@ -306,23 +305,51 @@ struct RcBox<T: ?Sized> {
 /// [get_mut]: Rc::get_mut
 #[cfg_attr(not(test), rustc_diagnostic_item = "Rc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Rc<T: ?Sized> {
+#[rustc_insignificant_dtor]
+pub struct Rc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<RcBox<T>>,
     phantom: PhantomData<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Send for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Rc<T, A> {}
+
+// Note that this negative impl isn't strictly necessary for correctness,
+// as `Rc` transitively contains a `Cell`, which is itself `!Sync`.
+// However, given how important `Rc`'s `!Sync`-ness is,
+// having an explicit negative impl is nice for documentation purposes
+// and results in nicer error messages.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Sync for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Rc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Rc<T, A> {}
+// #[stable(feature = "rc_ref_unwind_safe", since = "1.58.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> RefUnwindSafe for Rc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Rc<U>> for Rc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Rc<U, A>> for Rc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Rc<U>> for Rc<T> {}
 
 impl<T: ?Sized> Rc<T> {
+    #[inline]
+    unsafe fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
+    }
+
+    #[inline]
+    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
+        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     #[inline(always)]
     fn inner(&self) -> &RcBox<T> {
         // This unsafety is ok because while this Rc is alive we're guaranteed
@@ -330,12 +357,12 @@ impl<T: ?Sized> Rc<T> {
         unsafe { self.ptr.as_ref() }
     }
 
-    fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner_in(ptr: NonNull<RcBox<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
     }
 
-    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
-        Self::from_inner(unsafe { NonNull::new_unchecked(ptr) })
+    unsafe fn from_ptr_in(ptr: *mut RcBox<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
@@ -356,50 +383,83 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Self::from_inner(
-            Box::leak(box RcBox { strong: Cell::new(1), weak: Cell::new(1), value }).into(),
-        )
+        unsafe {
+            Self::from_inner(
+                Box::leak(Box::new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value }))
+                    .into(),
+            )
+        }
     }
 
-    /// Constructs a new `Rc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Rc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
+    ///
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Rc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Rc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Rc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Rc<T>` is not fully-constructed until `Rc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
+    ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// # #![allow(dead_code)]
     /// use std::rc::{Rc, Weak};
     ///
     /// struct Gadget {
-    ///     self_weak: Weak<Self>,
-    ///     // ... more fields
+    ///     me: Weak<Gadget>,
     /// }
+    ///
     /// impl Gadget {
-    ///     pub fn new() -> Rc<Self> {
-    ///         Rc::new_cyclic(|self_weak| {
-    ///             Gadget { self_weak: self_weak.clone(), /* ... */ }
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Rc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Rc` we're constructing.
+    ///         Rc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
     ///         })
     ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Rc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
     /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Rc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Rc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box RcBox {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(RcBox {
             strong: Cell::new(0),
             weak: Cell::new(1),
             value: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
 
         let init_ptr: NonNull<RcBox<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -409,16 +469,16 @@ impl<T> Rc<T> {
         // otherwise.
         let data = data_fn(&weak);
 
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).value), data);
 
             let prev_value = (*inner).strong.get();
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
             (*inner).strong.set(1);
-        }
 
-        let strong = Rc::from_inner(init_ptr);
+            Rc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -438,17 +498,16 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -481,6 +540,7 @@ impl<T> Rc<T> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -508,10 +568,12 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Ok(Self::from_inner(
-            Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
-                .into(),
-        ))
+        unsafe {
+            Ok(Self::from_inner(
+                Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
+                    .into(),
+            ))
+        }
     }
 
     /// Constructs a new `Rc` with uninitialized contents, returning an error if the allocation fails
@@ -526,12 +588,10 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -584,9 +644,229 @@ impl<T> Rc<T> {
     /// `value` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(value: T) -> Pin<Rc<T>> {
         unsafe { Pin::new_unchecked(Rc::new(value)) }
     }
+}
+
+impl<T, A: Allocator> Rc<T, A> {
+    /// Constructs a new `Rc` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(value: T, alloc: A) -> Rc<T, A> {
+        // NOTE: Prefer match over unwrap_or_else since closure sometimes not inlineable.
+        // That would make code size bigger.
+        match Self::try_new_in(value, alloc) {
+            Ok(m) => m,
+            Err(_) => handle_alloc_error(Layout::new::<RcBox<T>>()),
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc<T>` in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::try_new_in(5, System);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(value: T, alloc: A) -> Result<Self, AllocError> {
+        // There is an implicit weak pointer owned by all the strong
+        // pointers, which ensures that the weak destructor never frees
+        // the allocation while the strong destructor is running, even
+        // if the weak pointer is stored inside the strong one.
+        let (ptr, alloc) = Box::into_unique(Box::try_new_in(
+            RcBox { strong: Cell::new(1), weak: Cell::new(1), value },
+            alloc,
+        )?);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, in the provided allocator, returning an
+    /// error if the allocation fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    //#[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Pin<Rc<T>>` in the provided allocator. If `T` does not implement `Unpin`, then
+    /// `value` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(value: T, alloc: A) -> Pin<Self> {
+        unsafe { Pin::new_unchecked(Rc::new_in(value, alloc)) }
+    }
 
     /// Returns the inner value, if the `Rc` has exactly one strong reference.
     ///
@@ -613,13 +893,14 @@ impl<T> Rc<T> {
         if Rc::strong_count(&this) == 1 {
             unsafe {
                 let val = ptr::read(&*this); // copy the contained object
+                let alloc = ptr::read(&this.alloc); // copy the allocator
 
                 // Indicate to Weaks that they can't be promoted by decrementing
                 // the strong count, and then remove the implicit "strong weak"
                 // pointer while also handling drop logic by just crafting a
                 // fake Weak.
                 this.inner().dec_strong();
-                let _weak = Weak { ptr: this.ptr };
+                let _weak = Weak { ptr: this.ptr, alloc };
                 forget(this);
                 Ok(val)
             }
@@ -642,19 +923,19 @@ impl<T> Rc<[T]> {
     ///
     /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe { Rc::from_ptr(Rc::allocate_for_slice(len)) }
     }
@@ -681,6 +962,7 @@ impl<T> Rc<[T]> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -695,7 +977,84 @@ impl<T> Rc<[T]> {
     }
 }
 
-impl<T> Rc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Constructs a new reference-counted slice with uninitialized contents.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Rc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe { Rc::from_ptr_in(Rc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Rc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut RcBox<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Rc<mem::MaybeUninit<T>, A> {
     /// Converts to `Rc<T>`.
     ///
     /// # Safety
@@ -718,70 +1077,185 @@ impl<T> Rc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<T> {
-        Rc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Rc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
+    }
+}
+
+impl<T, A: Allocator> Rc<[mem::MaybeUninit<T>], A> {
+    /// Converts to `Rc<[T]>`.
+    ///
+    /// # Safety
+    ///
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    ///
+    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Rc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Rc<T> {
+    /// Constructs an `Rc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// and alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The raw pointer must point to a block of memory allocated by the global allocator
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Rc<T>` is never accessed.
+    ///
+    /// [into_raw]: Rc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let x = Rc::new("hello".to_owned());
+    /// let x_ptr = Rc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Rc` to prevent leak.
+    ///     let x = Rc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let five = Rc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
+    ///
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Self::increment_strong_count_in(ptr, Global) }
     }
-}
 
-impl<T> Rc<[mem::MaybeUninit<T>]> {
-    /// Converts to `Rc<[T]>`.
+    /// Decrements the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator. This method can be used to release the final `Rc` and
+    /// backing storage, but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::rc::Rc;
     ///
-    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Rc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    ///     Rc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Rc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<[T]> {
-        unsafe { Rc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Self::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Rc<T> {
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Consumes the `Rc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Rc` using
-    /// [`Rc::from_raw`][from_raw].
-    ///
-    /// [from_raw]: Rc::from_raw
+    /// [`Rc::from_raw`].
     ///
     /// # Examples
     ///
@@ -825,36 +1299,41 @@ impl<T: ?Sized> Rc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).value) }
     }
 
-    /// Constructs an `Rc<T>` from a raw pointer.
+    /// Constructs an `Rc<T, A>` from a raw pointer in the provided allocator.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// [`Rc<U, A>::into_raw`][into_raw] where `U` must have the same size
     /// and alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
-    /// [`mem::transmute`][transmute] for more information on what
+    /// [`mem::transmute`] for more information on what
     /// restrictions apply in this case.
     ///
+    /// The raw pointer must point to a block of memory allocated by `alloc`
+    ///
     /// The user of `from_raw` has to make sure a specific value of `T` is only
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Rc<T>` is never accessed.
+    /// even if the returned `Rc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Rc::into_raw
-    /// [transmute]: core::mem::transmute
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let x = Rc::new("hello".to_owned());
+    /// let x = Rc::new_in("hello".to_owned(), System);
     /// let x_ptr = Rc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Rc` to prevent leak.
-    ///     let x = Rc::from_raw(x_ptr);
+    ///     let x = Rc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
@@ -862,15 +1341,15 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         let offset = unsafe { data_offset(ptr) };
 
         // Reverse the offset to find the original RcBox.
         let rc_ptr =
             unsafe { (ptr as *mut RcBox<T>).set_ptr_value((ptr as *mut u8).offset(-offset)) };
 
-        unsafe { Self::from_ptr(rc_ptr) }
+        unsafe { Self::from_ptr_in(rc_ptr, alloc) }
     }
 
     /// Creates a new [`Weak`] pointer to this allocation.
@@ -884,12 +1363,17 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// let weak_five = Rc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Rc`"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         this.inner().inc_weak();
         // Make sure we do not create a dangling Weak
         debug_assert!(!is_dangling(this.ptr.as_ptr()));
-        Weak { ptr: this.ptr }
+        Weak { ptr: this.ptr, alloc: this.alloc.clone() }
     }
 
     /// Gets the number of [`Weak`] pointers to this allocation.
@@ -933,30 +1417,37 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Rc, but don't touch refcount by wrapping in ManuallyDrop
-        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T>::from_raw(ptr)) };
+        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T, A>::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _rc_clone: mem::ManuallyDrop<_> = rc.clone();
     }
@@ -966,33 +1457,36 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release
-    /// the final `Rc` and backing storage, but **should not** be called after
-    /// the final `Rc` has been released.
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final `Rc` and backing storage,
+    /// but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
-    ///     Rc::decrement_strong_count(ptr);
+    ///     Rc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Rc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Rc::from_raw_in(ptr, alloc)) };
     }
 
     /// Returns `true` if there are no other `Rc` or [`Weak`] pointers to
@@ -1009,7 +1503,7 @@ impl<T: ?Sized> Rc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Rc` pointers.
     ///
     /// [make_mut]: Rc::make_mut
     /// [clone]: Clone::clone
@@ -1084,24 +1578,24 @@ impl<T: ?Sized> Rc<T> {
     /// assert!(Rc::ptr_eq(&five, &same_five));
     /// assert!(!Rc::ptr_eq(&five, &other_five));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
 }
 
-impl<T: Clone> Rc<T> {
+impl<T: Clone, A: Allocator + Clone> Rc<T, A> {
     /// Makes a mutable reference into the given `Rc`.
     ///
     /// If there are other `Rc` pointers to the same allocation, then `make_mut` will
     /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
     /// referred to as clone-on-write.
     ///
-    /// If there are no other `Rc` pointers to this allocation, then [`Weak`]
-    /// pointers to this allocation will be disassociated.
+    /// However, if there are no other `Rc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be disassociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or diassociating [`Weak`] pointers.
     ///
     /// [`clone`]: Clone::clone
     /// [`get_mut`]: Rc::get_mut
@@ -1113,11 +1607,11 @@ impl<T: Clone> Rc<T> {
     ///
     /// let mut data = Rc::new(5);
     ///
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// let mut other_data = Rc::clone(&data);    // Won't clone inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Clones inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// *Rc::make_mut(&mut other_data) *= 2;  // Won't clone anything
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// let mut other_data = Rc::clone(&data); // Won't clone inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Clones inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// *Rc::make_mut(&mut other_data) *= 2;   // Won't clone anything
     ///
     /// // Now `data` and `other_data` point to different allocations.
     /// assert_eq!(*data, 8);
@@ -1147,7 +1641,7 @@ impl<T: Clone> Rc<T> {
         if Rc::strong_count(this) != 1 {
             // Gotta clone the data, there are other Rcs.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1155,7 +1649,7 @@ impl<T: Clone> Rc<T> {
             }
         } else if Rc::weak_count(this) != 0 {
             // Can just steal the data, all that's left is Weaks
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1174,11 +1668,44 @@ impl<T: Clone> Rc<T> {
         // reference to the allocation.
         unsafe { &mut this.ptr.as_mut().value }
     }
-}
 
-impl Rc<dyn Any> {
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `rc_t` is of type `Rc<T>`, this function is functionally equivalent to
+    /// `(*rc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, rc::Rc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let rc = Rc::new(inner);
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let rc = Rc::new(inner);
+    /// let rc2 = rc.clone();
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `rc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Rc::unwrap_or_clone(rc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
     #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Rc::try_unwrap(this).unwrap_or_else(|rc| (*rc).clone())
+    }
+}
+
+impl<A: Allocator + Clone> Rc<dyn Any, A> {
     /// Attempt to downcast the `Rc<dyn Any>` to a concrete type.
     ///
     /// # Examples
@@ -1197,15 +1724,57 @@ impl Rc<dyn Any> {
     /// print_if_string(Rc::new(my_string));
     /// print_if_string(Rc::new(0i8));
     /// ```
-    pub fn downcast<T: Any>(self) -> Result<Rc<T>, Rc<dyn Any>> {
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T: Any>(self) -> Result<Rc<T, A>, Self> {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<RcBox<T>>();
-            forget(self);
-            Ok(Rc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<RcBox<T>>();
+                let alloc = self.alloc.clone();
+                forget(self);
+                Ok(Rc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Rc<dyn Any>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::rc::Rc;
+    ///
+    /// let x: Rc<dyn Any> = Rc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T: Any>(self) -> Rc<T, A> {
+        unsafe {
+            let ptr = self.ptr.cast::<RcBox<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Rc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T: ?Sized> Rc<T> {
@@ -1263,28 +1832,30 @@ impl<T: ?Sized> Rc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Allocates an `RcBox<T>` with sufficient space for an unsized inner value
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut RcBox<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut RcBox<T> {
         // Allocate for the `RcBox<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Rc::<T>::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut RcBox<T>).set_ptr_value(mem),
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut RcBox<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Rc<T> {
+    fn from_box_in(v: Box<T, A>) -> Rc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1294,9 +1865,9 @@ impl<T: ?Sized> Rc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1376,6 +1947,20 @@ impl<T> Rc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Allocates an `RcBox<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut RcBox<[T]> {
+        unsafe {
+            Rc::<[T]>::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut RcBox<[T]>,
+            )
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 trait RcFromSlice<T> {
     fn from_slice(slice: &[T]) -> Self;
@@ -1398,7 +1983,7 @@ impl<T: Copy> RcFromSlice<T> for Rc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Rc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Rc<T, A> {
     type Target = T;
 
     #[inline(always)]
@@ -1411,7 +1996,7 @@ impl<T: ?Sized> Deref for Rc<T> {
 impl<T: ?Sized> Receiver for Rc<T> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Rc<T, A> {
     /// Drops the `Rc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1449,7 +2034,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
                 self.inner().dec_weak();
 
                 if self.inner().weak() == 0 {
-                    Global.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
+                    self.alloc.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
                 }
             }
         }
@@ -1457,7 +2042,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Rc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Rc<T, A> {
     /// Makes a clone of the `Rc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1473,9 +2058,11 @@ impl<T: ?Sized> Clone for Rc<T> {
     /// let _ = Rc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Rc<T> {
-        self.inner().inc_strong();
-        Self::from_inner(self.ptr)
+    fn clone(&self) -> Self {
+        unsafe {
+            self.inner().inc_strong();
+            Self::from_inner_in(self.ptr, self.alloc.clone())
+        }
     }
 }
 
@@ -1499,20 +2086,20 @@ impl<T: Default> Default for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait RcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Rc<T>) -> bool;
-    fn ne(&self, other: &Rc<T>) -> bool;
+trait RcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Rc<T, A>) -> bool;
+    fn ne(&self, other: &Rc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Rc<T>) -> bool {
+    default fn eq(&self, other: &Rc<T, A>) -> bool {
         **self == **other
     }
 
     #[inline]
-    default fn ne(&self, other: &Rc<T>) -> bool {
+    default fn ne(&self, other: &Rc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -1531,20 +2118,20 @@ impl<T: Eq> MarkerEq for T {}
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + MarkerEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + MarkerEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         Rc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         !Rc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Rc<T, A> {
     /// Equality for two `Rc`s.
     ///
     /// Two `Rc`s are equal if their inner values are equal, even if they are
@@ -1564,7 +2151,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five == Rc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::eq(self, other)
     }
 
@@ -1586,16 +2173,16 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five != Rc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Rc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Rc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Rc<T, A> {
     /// Partial comparison for two `Rc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -1611,7 +2198,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Rc::new(6)));
     /// ```
     #[inline(always)]
-    fn partial_cmp(&self, other: &Rc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Rc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -1629,7 +2216,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five < Rc::new(6));
     /// ```
     #[inline(always)]
-    fn lt(&self, other: &Rc<T>) -> bool {
+    fn lt(&self, other: &Rc<T, A>) -> bool {
         **self < **other
     }
 
@@ -1647,7 +2234,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five <= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn le(&self, other: &Rc<T>) -> bool {
+    fn le(&self, other: &Rc<T, A>) -> bool {
         **self <= **other
     }
 
@@ -1665,7 +2252,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five > Rc::new(4));
     /// ```
     #[inline(always)]
-    fn gt(&self, other: &Rc<T>) -> bool {
+    fn gt(&self, other: &Rc<T, A>) -> bool {
         **self > **other
     }
 
@@ -1683,13 +2270,13 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five >= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn ge(&self, other: &Rc<T>) -> bool {
+    fn ge(&self, other: &Rc<T, A>) -> bool {
         **self >= **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Rc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Rc<T, A> {
     /// Comparison for two `Rc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -1705,34 +2292,34 @@ impl<T: ?Sized + Ord> Ord for Rc<T> {
     /// assert_eq!(Ordering::Less, five.cmp(&Rc::new(6)));
     /// ```
     #[inline]
-    fn cmp(&self, other: &Rc<T>) -> Ordering {
+    fn cmp(&self, other: &Rc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Rc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Rc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state);
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Rc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Rc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Rc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -1741,7 +2328,7 @@ impl<T: ?Sized> fmt::Pointer for Rc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Rc<T> {
-    /// Converts a generic type `T` into a `Rc<T>`
+    /// Converts a generic type `T` into an `Rc<T>`
     ///
     /// The conversion allocates on the heap and moves `t`
     /// from the stack into it.
@@ -1818,7 +2405,7 @@ impl From<String> for Rc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Rc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Rc<T, A> {
     /// Move a boxed object to a new, reference counted, allocation.
     ///
     /// # Example
@@ -1830,14 +2417,14 @@ impl<T: ?Sized> From<Box<T>> for Rc<T> {
     /// assert_eq!(1, *shared);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Rc<T> {
-        Rc::from_box(v)
+    fn from(v: Box<T, A>) -> Rc<T, A> {
+        Rc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Rc<[T]> {
+impl<T, A: Allocator> From<Vec<T, A>> for Rc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -1849,15 +2436,9 @@ impl<T> From<Vec<T>> for Rc<[T]> {
     /// assert_eq!(vec![1, 2, 3], *shared);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Rc<[T]> {
-        unsafe {
-            let rc = Rc::copy_from_slice(&v);
-
-            // Allow the Vec to free its memory, but not destroy its contents
-            v.set_len(0);
-
-            rc
-        }
+    fn from(v: Vec<T, A>) -> Rc<[T], A> {
+        let boxed_slice = v.into_boxed_slice();
+        Self::from(boxed_slice)
     }
 }
 
@@ -1888,6 +2469,25 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Rc<str>> for Rc<[u8]> {
+    /// Converts a reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::rc::Rc;
+    /// let string: Rc<str> = Rc::from("eggplant");
+    /// let bytes: Rc<[u8]> = Rc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Rc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Rc::from_raw(Rc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
 impl<T, const N: usize> TryFrom<Rc<[T]>> for Rc<[T; N]> {
     type Error = Rc<[T]>;
@@ -1989,7 +2589,7 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 
 /// `Weak` is a version of [`Rc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Rc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Rc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -2008,7 +2608,10 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "rc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesnât need
@@ -2016,15 +2619,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Send for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Weak<T, A> {}
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Sync for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
@@ -2043,9 +2647,37 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") }
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Weak;
+    ///
+    /// let empty: Weak<i64> = Weak::new();
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -2062,6 +2694,56 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference, and `ptr` must point to a block of memory allocated by the global allocator.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Rc::downgrade(&strong).into_raw();
+    /// let raw_2 = Rc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Rc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    /// [`new`]: Weak::new
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -2086,7 +2768,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: ptr::null
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut RcBox<T> = NonNull::as_ptr(self.ptr);
@@ -2096,7 +2779,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as RcBox (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).value) }
@@ -2130,6 +2813,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -2137,6 +2821,44 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
+    /// Consumes the `Weak<T>` and turns it into a raw pointer.
+    ///
+    /// This converts the weak pointer into a raw pointer, while still preserving the ownership of
+    /// one weak reference (the weak count is not modified by this operation). It can be turned
+    /// back into the `Weak<T>` with [`from_raw`].
+    ///
+    /// The same restrictions of accessing the target of the pointer as with
+    /// [`as_ptr`] apply.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    /// let weak = Rc::downgrade(&strong);
+    /// let raw = weak.into_raw();
+    ///
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    /// assert_eq!("hello", unsafe { &*raw });
+    ///
+    /// drop(unsafe { Weak::from_raw(raw) });
+    /// assert_eq!(0, Rc::weak_count(&strong));
+    /// ```
+    ///
+    /// [`from_raw`]: Weak::from_raw
+    /// [`as_ptr`]: Weak::as_ptr
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn into_raw_and_alloc(self) -> (*const T, A)
+    where
+        A: Clone,
+    {
+        let result = self.as_ptr();
+        let alloc = self.alloc.clone();
+        mem::forget(self);
+        (result, alloc)
+    }
+
     /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
@@ -2148,7 +2870,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and `ptr` must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -2179,8 +2901,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
     /// [`new`]: Weak::new
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -2196,7 +2918,7 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 
     /// Attempts to upgrade the `Weak` pointer to an [`Rc`], delaying
@@ -2222,20 +2944,29 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Rc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Rc<T>> {
+    pub fn upgrade(&self) -> Option<Rc<T, A>>
+    where
+        A: Clone,
+    {
         let inner = self.inner()?;
+
         if inner.strong() == 0 {
             None
         } else {
-            inner.inc_strong();
-            Some(Rc::from_inner(self.ptr))
+            unsafe {
+                inner.inc_strong();
+                Some(Rc::from_inner_in(self.ptr, self.alloc.clone()))
+            }
         }
     }
 
     /// Gets the number of strong (`Rc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
         if let Some(inner) = self.inner() { inner.strong() } else { 0 }
@@ -2244,6 +2975,7 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of `Weak` pointers pointing to this allocation.
     ///
     /// If no strong pointers remain, this will return zero.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
@@ -2313,9 +3045,8 @@ impl<T: ?Sized> Weak<T> {
     /// let third = Rc::downgrade(&third_rc);
     /// assert!(!first.ptr_eq(&third));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -2323,7 +3054,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2356,14 +3087,14 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
         // the strong pointers have disappeared.
         if inner.weak() == 0 {
             unsafe {
-                Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
             }
         }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2376,16 +3107,16 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         if let Some(inner) = self.inner() {
             inner.inc_weak()
         }
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Weak<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Weak<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         write!(f, "(Weak)")
     }
@@ -2396,7 +3127,6 @@ impl<T> Default for Weak<T> {
     /// Constructs a new `Weak<T>`, without allocating any memory.
     /// Calling [`upgrade`] on the return value always gives [`None`].
     ///
-    /// [`None`]: Option
     /// [`upgrade`]: Weak::upgrade
     ///
     /// # Examples
@@ -2435,14 +3165,23 @@ trait RcInnerPtr {
     fn inc_strong(&self) {
         let strong = self.strong();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(strong != 0);
+        }
+
+        let strong = strong.wrapping_add(1);
+        self.strong_ref().set(strong);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if strong == 0 || strong == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(strong == 0) {
             abort();
         }
-        self.strong_ref().set(strong + 1);
     }
 
     #[inline]
@@ -2459,14 +3198,23 @@ trait RcInnerPtr {
     fn inc_weak(&self) {
         let weak = self.weak();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(weak != 0);
+        }
+
+        let weak = weak.wrapping_add(1);
+        self.weak_ref().set(weak);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if weak == 0 || weak == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(weak == 0) {
             abort();
         }
-        self.weak_ref().set(weak + 1);
     }
 
     #[inline]
@@ -2500,21 +3248,21 @@ impl<'a> RcInnerPtr for WeakInner<'a> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Rc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Rc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Rc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Rc<T, A> {}
 
 /// Get the offset within an `RcBox` for the payload behind a pointer.
 ///
@@ -2528,7 +3276,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
@@ -2536,4 +3284,4 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
 fn data_offset_align(align: usize) -> isize {
     let layout = Layout::new::<RcBox<()>>();
     (layout.size() + layout.padding_needed_for(align)) as isize
-}
+}
\ No newline at end of file
diff --git a/rust/alloc/string.rs b/rust/alloc/string.rs
index 55293c304..c4b72c009 100644
--- a/rust/alloc/string.rs
+++ b/rust/alloc/string.rs
@@ -46,11 +46,13 @@
 
 #[cfg(not(no_global_oom_handling))]
 use core::char::{decode_utf16, REPLACEMENT_CHARACTER};
+use core::cmp::Ordering;
+// use core::error::Error;
 use core::fmt;
 use core::hash;
+use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::iter::{from_fn, FromIterator};
-use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::ops::Add;
 #[cfg(not(no_global_oom_handling))]
@@ -60,10 +62,11 @@ use core::ops::Bound::{Excluded, Included, Unbounded};
 use core::ops::{self, Index, IndexMut, Range, RangeBounds};
 use core::ptr;
 use core::slice;
-#[cfg(not(no_global_oom_handling))]
-use core::str::lossy;
 use core::str::pattern::Pattern;
+#[cfg(not(no_global_oom_handling))]
+use core::str::Utf8Chunks;
 
+use crate::alloc::{Allocator, Global};
 #[cfg(not(no_global_oom_handling))]
 use crate::borrow::{Cow, ToOwned};
 use crate::boxed::Box;
@@ -81,7 +84,7 @@ use crate::vec::Vec;
 ///
 /// # Examples
 ///
-/// You can create a `String` from [a literal string][`str`] with [`String::from`]:
+/// You can create a `String` from [a literal string][`&str`] with [`String::from`]:
 ///
 /// [`String::from`]: From::from
 ///
@@ -119,31 +122,103 @@ use crate::vec::Vec;
 ///
 /// # UTF-8
 ///
-/// `String`s are always valid UTF-8. This has a few implications, the first of
-/// which is that if you need a non-UTF-8 string, consider [`OsString`]. It is
-/// similar, but without the UTF-8 constraint. The second implication is that
-/// you cannot index into a `String`:
+/// `String`s are always valid UTF-8. If you need a non-UTF-8 string, consider
+/// [`OsString`]. It is similar, but without the UTF-8 constraint. Because UTF-8
+/// is a variable width encoding, `String`s are typically smaller than an array of
+/// the same `chars`:
+///
+/// ```
+/// use std::mem;
+///
+/// // `s` is ASCII which represents each `char` as one byte
+/// let s = "hello";
+/// assert_eq!(s.len(), 5);
+///
+/// // A `char` array with the same contents would be longer because
+/// // every `char` is four bytes
+/// let s = ['h', 'e', 'l', 'l', 'o'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+///
+/// // However, for non-ASCII strings, the difference will be smaller
+/// // and sometimes they are the same
+/// let s = "ððððð";
+/// assert_eq!(s.len(), 20);
+///
+/// let s = ['ð', 'ð', 'ð', 'ð', 'ð'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+/// ```
+///
+/// This raises interesting questions as to how `s[i]` should work.
+/// What should `i` be here? Several options include byte indices and
+/// `char` indices but, because of UTF-8 encoding, only byte indices
+/// would provide constant time indexing. Getting the `i`th `char`, for
+/// example, is available using [`chars`]:
+///
+/// ```
+/// let s = "hello";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('l'));
+///
+/// let s = "ððððð";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('ð'));
+/// ```
+///
+/// Next, what should `s[i]` return? Because indexing returns a reference
+/// to underlying data it could be `&u8`, `&[u8]`, or something else similar.
+/// Since we're only providing one index, `&u8` makes the most sense but that
+/// might not be what the user expects and can be explicitly achieved with
+/// [`as_bytes()`]:
+///
+/// ```
+/// // The first byte is 104 - the byte value of `'h'`
+/// let s = "hello";
+/// assert_eq!(s.as_bytes()[0], 104);
+/// // or
+/// assert_eq!(s.as_bytes()[0], b'h');
+///
+/// // The first byte is 240 which isn't obviously useful
+/// let s = "ððððð";
+/// assert_eq!(s.as_bytes()[0], 240);
+/// ```
+///
+/// Due to these ambiguities/restrictions, indexing with a `usize` is simply
+/// forbidden:
 ///
 /// ```compile_fail,E0277
 /// let s = "hello";
 ///
-/// println!("The first letter of s is {}", s[0]); // ERROR!!!
+/// // The following will not compile!
+/// println!("The first letter of s is {}", s[0]);
 /// ```
 ///
-/// [`OsString`]: ../../std/ffi/struct.OsString.html
+/// It is more clear, however, how `&s[i..j]` should work (that is,
+/// indexing with a range). It should accept byte indices (to be constant-time)
+/// and return a `&str` which is UTF-8 encoded. This is also called "string slicing".
+/// Note this will panic if the byte indices provided are not character
+/// boundaries - see [`is_char_boundary`] for more details. See the implementations
+/// for [`SliceIndex<str>`] for more details on string slicing. For a non-panicking
+/// version of string slicing, see [`get`].
+///
+/// [`OsString`]: ../../std/ffi/struct.OsString.html "ffi::OsString"
+/// [`SliceIndex<str>`]: core::slice::SliceIndex
+/// [`as_bytes()`]: str::as_bytes
+/// [`get`]: str::get
+/// [`is_char_boundary`]: str::is_char_boundary
 ///
-/// Indexing is intended to be a constant-time operation, but UTF-8 encoding
-/// does not allow us to do this. Furthermore, it's not clear what sort of
-/// thing the index should return: a byte, a codepoint, or a grapheme cluster.
-/// The [`bytes`] and [`chars`] methods return iterators over the first
-/// two, respectively.
+/// The [`bytes`] and [`chars`] methods return iterators over the bytes and
+/// codepoints of the string, respectively. To iterate over codepoints along
+/// with byte indices, use [`char_indices`].
 ///
 /// [`bytes`]: str::bytes
 /// [`chars`]: str::chars
+/// [`char_indices`]: str::char_indices
 ///
 /// # Deref
 ///
-/// `String`s implement [`Deref`]`<Target=str>`, and so inherit all of [`str`]'s
+/// `String` implements <code>[Deref]<Target = [str]></code>, and so inherits all of [`str`]'s
 /// methods. In addition, this means that you can pass a `String` to a
 /// function which takes a [`&str`] by using an ampersand (`&`):
 ///
@@ -184,7 +259,7 @@ use crate::vec::Vec;
 /// to explicitly extract the string slice containing the string. The second
 /// way changes `example_func(&example_string);` to
 /// `example_func(&*example_string);`. In this case we are dereferencing a
-/// `String` to a [`str`][`&str`], then referencing the [`str`][`&str`] back to
+/// `String` to a [`str`], then referencing the [`str`] back to
 /// [`&str`]. The second way is more idiomatic, however both work to do the
 /// conversion explicitly rather than relying on the implicit conversion.
 ///
@@ -247,11 +322,11 @@ use crate::vec::Vec;
 ///
 /// ```text
 /// 0
-/// 5
-/// 10
-/// 20
-/// 20
-/// 40
+/// 8
+/// 16
+/// 16
+/// 32
+/// 32
 /// ```
 ///
 /// At first, we have no memory allocated at all, but as we append to the
@@ -284,15 +359,16 @@ use crate::vec::Vec;
 ///
 /// Here, there's no need to allocate more memory inside the loop.
 ///
-/// [`str`]: prim@str
-/// [`&str`]: prim@str
-/// [`Deref`]: core::ops::Deref
+/// [str]: prim@str "str"
+/// [`str`]: prim@str "str"
+/// [`&str`]: prim@str "&str"
+/// [Deref]: core::ops::Deref "ops::Deref"
+/// [`Deref`]: core::ops::Deref "ops::Deref"
 /// [`as_str()`]: String::as_str
-#[derive(PartialOrd, Eq, Ord)]
-#[cfg_attr(not(test), rustc_diagnostic_item = "string_type")]
+#[cfg_attr(not(test), rustc_diagnostic_item = "String")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct String {
-    vec: Vec<u8>,
+pub struct String<#[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global> {
+    vec: Vec<u8, A>,
 }
 
 /// A possible error value when converting a `String` from a UTF-8 byte vector.
@@ -310,10 +386,10 @@ pub struct String {
 /// an analogue to `FromUtf8Error`, and you can get one from a `FromUtf8Error`
 /// through the [`utf8_error`] method.
 ///
-/// [`Utf8Error`]: core::str::Utf8Error
-/// [`std::str`]: core::str
-/// [`&str`]: prim@str
-/// [`utf8_error`]: Self::utf8_error
+/// [`Utf8Error`]: str::Utf8Error "std::str::Utf8Error"
+/// [`std::str`]: core::str "std::str"
+/// [`&str`]: prim@str "&str"
+/// [`utf8_error`]: FromUtf8Error::utf8_error
 ///
 /// # Examples
 ///
@@ -378,17 +454,18 @@ impl String {
     #[inline]
     #[rustc_const_stable(feature = "const_string_new", since = "1.39.0")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub const fn new() -> String {
         String { vec: Vec::new() }
     }
 
-    /// Creates a new empty `String` with a particular capacity.
+    /// Creates a new empty `String` with at least the specified capacity.
     ///
     /// `String`s have an internal buffer to hold their data. The capacity is
     /// the length of that buffer, and can be queried with the [`capacity`]
     /// method. This method creates an empty `String`, but one with an initial
-    /// buffer that can hold `capacity` bytes. This is useful when you may be
-    /// appending a bunch of data to the `String`, reducing the number of
+    /// buffer that can hold at least `capacity` bytes. This is useful when you
+    /// may be appending a bunch of data to the `String`, reducing the number of
     /// reallocations it needs to do.
     ///
     /// [`capacity`]: String::capacity
@@ -421,9 +498,8 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[doc(alias = "alloc")]
-    #[doc(alias = "malloc")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub fn with_capacity(capacity: usize) -> String {
         String { vec: Vec::with_capacity(capacity) }
     }
@@ -491,8 +567,8 @@ impl String {
     /// with this error.
     ///
     /// [`from_utf8_unchecked`]: String::from_utf8_unchecked
-    /// [`Vec<u8>`]: crate::vec::Vec
-    /// [`&str`]: prim@str
+    /// [`Vec<u8>`]: crate::vec::Vec "Vec"
+    /// [`&str`]: prim@str "&str"
     /// [`into_bytes`]: String::into_bytes
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
@@ -528,7 +604,7 @@ impl String {
     /// it's already valid UTF-8, we don't need a new allocation. This return
     /// type allows us to handle both cases.
     ///
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     ///
     /// # Examples
     ///
@@ -552,18 +628,19 @@ impl String {
     ///
     /// assert_eq!("Hello ï¿½World", output);
     /// ```
+    #[must_use]
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf8_lossy(v: &[u8]) -> Cow<'_, str> {
-        let mut iter = lossy::Utf8Lossy::from_bytes(v).chunks();
+        let mut iter = Utf8Chunks::new(v);
 
-        let (first_valid, first_broken) = if let Some(chunk) = iter.next() {
-            let lossy::Utf8LossyChunk { valid, broken } = chunk;
-            if valid.len() == v.len() {
-                debug_assert!(broken.is_empty());
+        let first_valid = if let Some(chunk) = iter.next() {
+            let valid = chunk.valid();
+            if chunk.invalid().is_empty() {
+                debug_assert_eq!(valid.len(), v.len());
                 return Cow::Borrowed(valid);
             }
-            (valid, broken)
+            valid
         } else {
             return Cow::Borrowed("");
         };
@@ -572,13 +649,11 @@ impl String {
 
         let mut res = String::with_capacity(v.len());
         res.push_str(first_valid);
-        if !first_broken.is_empty() {
-            res.push_str(REPLACEMENT);
-        }
+        res.push_str(REPLACEMENT);
 
-        for lossy::Utf8LossyChunk { valid, broken } in iter {
-            res.push_str(valid);
-            if !broken.is_empty() {
+        for chunk in iter {
+            res.push_str(chunk.valid());
+            if !chunk.invalid().is_empty() {
                 res.push_str(REPLACEMENT);
             }
         }
@@ -629,7 +704,7 @@ impl String {
     /// conversion requires a memory allocation.
     ///
     /// [`from_utf8_lossy`]: String::from_utf8_lossy
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     /// [U+FFFD]: core::char::REPLACEMENT_CHARACTER
     ///
     /// # Examples
@@ -646,6 +721,7 @@ impl String {
     ///            String::from_utf16_lossy(v));
     /// ```
     #[cfg(not(no_global_oom_handling))]
+    #[must_use]
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf16_lossy(v: &[u16]) -> String {
@@ -678,6 +754,7 @@ impl String {
     /// let rebuilt = unsafe { String::from_raw_parts(ptr, len, cap) };
     /// assert_eq!(rebuilt, "hello");
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[unstable(feature = "vec_into_raw_parts", reason = "new API", issue = "65816")]
     pub fn into_raw_parts(self) -> (*mut u8, usize, usize) {
         self.vec.into_raw_parts()
@@ -697,7 +774,10 @@ impl String {
     /// * The first `length` bytes at `buf` need to be valid UTF-8.
     ///
     /// Violating these may cause problems like corrupting the allocator's
-    /// internal data structures.
+    /// internal data structures. For example, it is normally **not** safe to
+    /// build a `String` from a pointer to a C `char` array containing UTF-8
+    /// _unless_ you are certain that array was originally allocated by the
+    /// Rust standard library's allocator.
     ///
     /// The ownership of `buf` is effectively transferred to the
     /// `String` which may then deallocate, reallocate or change the
@@ -763,6 +843,7 @@ impl String {
     /// assert_eq!("ð", sparkle_heart);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub unsafe fn from_utf8_unchecked(bytes: Vec<u8>) -> String {
         String { vec: bytes }
@@ -783,6 +864,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111][..], &bytes[..]);
     /// ```
     #[inline]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.vec
@@ -800,6 +882,7 @@ impl String {
     /// assert_eq!("foo", s.as_str());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_str(&self) -> &str {
         self
@@ -820,6 +903,7 @@ impl String {
     /// assert_eq!("FOOBAR", s_mut_str);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_mut_str(&mut self) -> &mut str {
         self
@@ -893,26 +977,22 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn capacity(&self) -> usize {
         self.vec.capacity()
     }
 
-    /// Ensures that this `String`'s capacity is at least `additional` bytes
-    /// larger than its length.
-    ///
-    /// The capacity may be increased by more than `additional` bytes if it
-    /// chooses, to prevent frequent reallocations.
-    ///
-    /// If you do not want this "at least" behavior, see the [`reserve_exact`]
-    /// method.
+    /// Reserves capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `reserve`,
+    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Does nothing if capacity is already sufficient.
     ///
     /// # Panics
     ///
     /// Panics if the new capacity overflows [`usize`].
     ///
-    /// [`reserve_exact`]: String::reserve_exact
-    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -925,22 +1005,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -949,17 +1030,18 @@ impl String {
         self.vec.reserve(additional)
     }
 
-    /// Ensures that this `String`'s capacity is `additional` bytes
-    /// larger than its length.
-    ///
-    /// Consider using the [`reserve`] method unless you absolutely know
-    /// better than the allocator.
+    /// Reserves the minimum capacity for at least `additional` bytes more than
+    /// the current length. Unlike [`reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `reserve_exact`, capacity will be greater than or equal to
+    /// `self.len() + additional`. Does nothing if the capacity is already
+    /// sufficient.
     ///
     /// [`reserve`]: String::reserve
     ///
     /// # Panics
     ///
-    /// Panics if the new capacity overflows `usize`.
+    /// Panics if the new capacity overflows [`usize`].
     ///
     /// # Examples
     ///
@@ -973,22 +1055,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve_exact(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -997,11 +1080,12 @@ impl String {
         self.vec.reserve_exact(additional)
     }
 
-    /// Tries to reserve capacity for at least `additional` more elements to be inserted
-    /// in the given `String`. The collection may reserve more space to avoid
-    /// frequent reallocations. After calling `reserve`, capacity will be
-    /// greater than or equal to `self.len() + additional`. Does nothing if
-    /// capacity is already sufficient.
+    /// Tries to reserve capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `try_reserve`, capacity will be
+    /// greater than or equal to `self.len() + additional` if it returns
+    /// `Ok(())`. Does nothing if capacity is already sufficient. This method
+    /// preserves the contents even if an error occurs.
     ///
     /// # Errors
     ///
@@ -1011,7 +1095,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
@@ -1032,14 +1115,18 @@ impl String {
         self.vec.try_reserve(additional)
     }
 
-    /// Tries to reserve the minimum capacity for exactly `additional` more elements to
-    /// be inserted in the given `String`. After calling `reserve_exact`,
-    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Tries to reserve the minimum capacity for at least `additional` bytes
+    /// more than the current length. Unlike [`try_reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `try_reserve_exact`, capacity will be greater than or
+    /// equal to `self.len() + additional` if it returns `Ok(())`.
     /// Does nothing if the capacity is already sufficient.
     ///
     /// Note that the allocator may give the collection more space than it
     /// requests. Therefore, capacity can not be relied upon to be precisely
-    /// minimal. Prefer `reserve` if future insertions are expected.
+    /// minimal. Prefer [`try_reserve`] if future insertions are expected.
+    ///
+    /// [`try_reserve`]: String::try_reserve
     ///
     /// # Errors
     ///
@@ -1049,14 +1136,13 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
     ///     let mut output = String::new();
     ///
     ///     // Pre-reserve the memory, exiting if we can't
-    ///     output.try_reserve(data.len())?;
+    ///     output.try_reserve_exact(data.len())?;
     ///
     ///     // Now we know this can't OOM in the middle of our complex work
     ///     output.push_str(data);
@@ -1102,7 +1188,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(shrink_to)]
     /// let mut s = String::from("foo");
     ///
     /// s.reserve(100);
@@ -1115,7 +1200,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "shrink_to", reason = "new API", issue = "56431")]
+    #[stable(feature = "shrink_to", since = "1.56.0")]
     pub fn shrink_to(&mut self, min_capacity: usize) {
         self.vec.shrink_to(min_capacity)
     }
@@ -1161,6 +1246,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111], s.as_bytes());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.vec
@@ -1354,13 +1440,14 @@ impl String {
     /// assert_eq!(s, "foobar");
     /// ```
     ///
-    /// The exact order may be useful for tracking external state, like an index.
+    /// Because the elements are visited exactly once in the original order,
+    /// external state may be used to decide which elements to keep.
     ///
     /// ```
     /// let mut s = String::from("abcde");
     /// let keep = [false, true, true, false, true];
-    /// let mut i = 0;
-    /// s.retain(|_| (keep[i], i += 1).0);
+    /// let mut iter = keep.iter();
+    /// s.retain(|_| *iter.next().unwrap());
     /// assert_eq!(s, "bce");
     /// ```
     #[inline]
@@ -1387,19 +1474,28 @@ impl String {
         let mut guard = SetLenOnDrop { s: self, idx: 0, del_bytes: 0 };
 
         while guard.idx < len {
-            let ch = unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap() };
+            let ch =
+                // SAFETY: `guard.idx` is positive-or-zero and less that len so the `get_unchecked`
+                // is in bound. `self` is valid UTF-8 like string and the returned slice starts at
+                // a unicode code point so the `Chars` always return one character.
+                unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap_unchecked() };
             let ch_len = ch.len_utf8();
 
             if !f(ch) {
                 guard.del_bytes += ch_len;
             } else if guard.del_bytes > 0 {
-                unsafe {
-                    ptr::copy(
-                        guard.s.vec.as_ptr().add(guard.idx),
-                        guard.s.vec.as_mut_ptr().add(guard.idx - guard.del_bytes),
-                        ch_len,
-                    );
-                }
+                // SAFETY: `guard.idx` is in bound and `guard.del_bytes` represent the number of
+                // bytes that are erased from the string so the resulting `guard.idx -
+                // guard.del_bytes` always represent a valid unicode code point.
+                //
+                // `guard.del_bytes` >= `ch.len_utf8()`, so taking a slice with `ch.len_utf8()` len
+                // is safe.
+                ch.encode_utf8(unsafe {
+                    crate::slice::from_raw_parts_mut(
+                        guard.s.as_mut_ptr().add(guard.idx - guard.del_bytes),
+                        ch.len_utf8(),
+                    )
+                });
             }
 
             // Point idx to the next char
@@ -1453,7 +1549,7 @@ impl String {
 
         unsafe {
             ptr::copy(self.vec.as_ptr().add(idx), self.vec.as_mut_ptr().add(idx + amt), len - idx);
-            ptr::copy(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
+            ptr::copy_nonoverlapping(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
             self.vec.set_len(len + amt);
         }
     }
@@ -1494,10 +1590,11 @@ impl String {
     ///
     /// # Safety
     ///
-    /// This function is unsafe because it does not check that the bytes passed
-    /// to it are valid UTF-8. If this constraint is violated, it may cause
-    /// memory unsafety issues with future users of the `String`, as the rest of
-    /// the standard library assumes that `String`s are valid UTF-8.
+    /// This function is unsafe because the returned `&mut Vec` allows writing
+    /// bytes which are not valid UTF-8. If this constraint is violated, using
+    /// the original `String` after dropping the `&mut Vec` may violate memory
+    /// safety, as the rest of the standard library assumes that `String`s are
+    /// valid UTF-8.
     ///
     /// # Examples
     ///
@@ -1521,7 +1618,7 @@ impl String {
     }
 
     /// Returns the length of this `String`, in bytes, not [`char`]s or
-    /// graphemes. In other words, it may not be what a human considers the
+    /// graphemes. In other words, it might not be what a human considers the
     /// length of the string.
     ///
     /// # Examples
@@ -1536,8 +1633,8 @@ impl String {
     /// assert_eq!(fancy_f.len(), 4);
     /// assert_eq!(fancy_f.chars().count(), 3);
     /// ```
-    #[doc(alias = "length")]
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn len(&self) -> usize {
         self.vec.len()
@@ -1557,6 +1654,7 @@ impl String {
     /// assert!(!v.is_empty());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn is_empty(&self) -> bool {
         self.len() == 0
@@ -1619,17 +1717,24 @@ impl String {
         self.vec.clear()
     }
 
-    /// Creates a draining iterator that removes the specified range in the `String`
-    /// and yields the removed `chars`.
+    /// Removes the specified range from the string in bulk, returning all
+    /// removed characters as an iterator.
     ///
-    /// Note: The element range is removed even if the iterator is not
-    /// consumed until the end.
+    /// The returned iterator keeps a mutable borrow on the string to optimize
+    /// its implementation.
     ///
     /// # Panics
     ///
     /// Panics if the starting point or end point do not lie on a [`char`]
     /// boundary, or if they're out of bounds.
     ///
+    /// # Leaking
+    ///
+    /// If the returned iterator goes out of scope without being dropped (due to
+    /// [`core::mem::forget`], for example), the string may still contain a copy
+    /// of any drained characters, or may have lost characters arbitrarily,
+    /// including characters outside the range.
+    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -1643,7 +1748,7 @@ impl String {
     /// assert_eq!(t, "Î± is alpha, ");
     /// assert_eq!(s, "Î² is beta");
     ///
-    /// // A full range clears the string
+    /// // A full range clears the string, like `clear()` does
     /// s.drain(..);
     /// assert_eq!(s, "");
     /// ```
@@ -1724,11 +1829,11 @@ impl String {
         unsafe { self.as_mut_vec() }.splice((start, end), replace_with.bytes());
     }
 
-    /// Converts this `String` into a [`Box`]`<`[`str`]`>`.
+    /// Converts this `String` into a <code>[Box]<[str]></code>.
     ///
     /// This will drop any excess capacity.
     ///
-    /// [`str`]: prim@str
+    /// [str]: prim@str "str"
     ///
     /// # Examples
     ///
@@ -1741,6 +1846,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "box_str", since = "1.4.0")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
     pub fn into_boxed_str(self) -> Box<str> {
         let slice = self.vec.into_boxed_slice();
@@ -1763,6 +1869,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(&[0, 159], value.unwrap_err().as_bytes());
     /// ```
+    #[must_use]
     #[stable(feature = "from_utf8_error_as_bytes", since = "1.26.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.bytes[..]
@@ -1786,6 +1893,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(vec![0, 159], value.unwrap_err().into_bytes());
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.bytes
@@ -1798,8 +1906,8 @@ impl FromUtf8Error {
     /// an analogue to `FromUtf8Error`. See its documentation for more details
     /// on using it.
     ///
-    /// [`std::str`]: core::str
-    /// [`&str`]: prim@str
+    /// [`std::str`]: core::str "std::str"
+    /// [`&str`]: prim@str "&str"
     ///
     /// # Examples
     ///
@@ -1814,6 +1922,7 @@ impl FromUtf8Error {
     /// // the first byte is invalid here
     /// assert_eq!(1, error.valid_up_to());
     /// ```
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn utf8_error(&self) -> Utf8Error {
         self.error
@@ -1834,6 +1943,22 @@ impl fmt::Display for FromUtf16Error {
     }
 }
 
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf8Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-8"
+//     }
+// }
+
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf16Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-16"
+//     }
+// }
+
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "rust1", since = "1.0.0")]
 impl Clone for String {
@@ -2057,17 +2182,38 @@ impl<'a, 'b> Pattern<'a> for &'b String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl PartialEq for String {
+impl<A: Allocator, B: Allocator> PartialEq<String<B>> for String<A> {
     #[inline]
-    fn eq(&self, other: &String) -> bool {
+    fn eq(&self, other: &String<B>) -> bool {
         PartialEq::eq(&self[..], &other[..])
     }
     #[inline]
-    fn ne(&self, other: &String) -> bool {
+    fn ne(&self, other: &String<B>) -> bool {
         PartialEq::ne(&self[..], &other[..])
     }
 }
 
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Eq for String<A> {}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator, B: Allocator>  PartialOrd<String<B>> for String<A> {
+    #[inline]
+    fn partial_cmp(&self, other: &String<B>) -> Option<Ordering> {
+        PartialOrd::partial_cmp(&self[..], &other[..])
+    }
+}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Ord for String<A> {
+    #[inline]
+    fn cmp(&self, other: &String<A>) -> Ordering {
+        Ord::cmp(&self[..], &other[..])
+    }
+}
+
+
+
 macro_rules! impl_eq {
     ($lhs:ty, $rhs: ty) => {
         #[stable(feature = "rust1", since = "1.0.0")]
@@ -2202,7 +2348,7 @@ impl AddAssign<&str> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::Range<usize>> for String {
+impl<A: Allocator> ops::Index<ops::Range<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2211,7 +2357,7 @@ impl ops::Index<ops::Range<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeTo<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2220,7 +2366,7 @@ impl ops::Index<ops::RangeTo<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeFrom<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2229,7 +2375,7 @@ impl ops::Index<ops::RangeFrom<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFull> for String {
+impl<A: Allocator> ops::Index<ops::RangeFull> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2238,7 +2384,7 @@ impl ops::Index<ops::RangeFull> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2247,7 +2393,7 @@ impl ops::Index<ops::RangeInclusive<usize>> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeToInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2257,42 +2403,42 @@ impl ops::Index<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::Range<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::Range<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::Range<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeTo<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeTo<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFrom<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeFrom<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFull> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFull> for String<A> {
     #[inline]
     fn index_mut(&mut self, _index: ops::RangeFull) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeToInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeToInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
@@ -2300,7 +2446,7 @@ impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Deref for String {
+impl<A: Allocator> ops::Deref for String<A> {
     type Target = str;
 
     #[inline]
@@ -2310,7 +2456,7 @@ impl ops::Deref for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::DerefMut for String {
+impl<A: Allocator> ops::DerefMut for String<A> {
     #[inline]
     fn deref_mut(&mut self) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
@@ -2321,7 +2467,7 @@ impl ops::DerefMut for String {
 ///
 /// This alias exists for backwards compatibility, and may be eventually deprecated.
 ///
-/// [`Infallible`]: core::convert::Infallible
+/// [`Infallible`]: core::convert::Infallible "convert::Infallible"
 #[stable(feature = "str_parse_error", since = "1.5.0")]
 pub type ParseError = core::convert::Infallible;
 
@@ -2608,7 +2754,7 @@ impl<'a> From<&'a str> for Cow<'a, str> {
     /// assert_eq!(Cow::from("eggplant"), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a str) -> Cow<'a, str> {
         Cow::Borrowed(s)
@@ -2631,7 +2777,7 @@ impl<'a> From<String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(s), Cow::<'static, str>::Owned(s2));
     /// ```
     ///
-    /// [`Owned`]: crate::borrow::Cow::Owned
+    /// [`Owned`]: crate::borrow::Cow::Owned "borrow::Cow::Owned"
     #[inline]
     fn from(s: String) -> Cow<'a, str> {
         Cow::Owned(s)
@@ -2653,7 +2799,7 @@ impl<'a> From<&'a String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(&s), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a String) -> Cow<'a, str> {
         Cow::Borrowed(s.as_str())
@@ -2697,7 +2843,7 @@ impl From<String> for Vec<u8> {
     /// let v1 = Vec::from(s1);
     ///
     /// for b in v1 {
-    ///     println!("{}", b);
+    ///     println!("{b}");
     /// }
     /// ```
     fn from(string: String) -> Vec<u8> {
@@ -2771,14 +2917,14 @@ impl<'a> Drain<'a> {
     /// # Examples
     ///
     /// ```
-    /// #![feature(string_drain_as_str)]
     /// let mut s = String::from("abc");
     /// let mut drain = s.drain(..);
     /// assert_eq!(drain.as_str(), "abc");
     /// let _ = drain.next().unwrap();
     /// assert_eq!(drain.as_str(), "bc");
     /// ```
-    #[unstable(feature = "string_drain_as_str", issue = "76905")] // Note: uncomment AsRef impls below when stabilizing.
+    #[must_use]
+    #[stable(feature = "string_drain_as_str", since = "1.55.0")]
     pub fn as_str(&self) -> &str {
         self.iter.as_str()
     }
diff --git a/rust/alloc/sync.rs b/rust/alloc/sync.rs
index 1f4e44680..382f849d7 100644
--- a/rust/alloc/sync.rs
+++ b/rust/alloc/sync.rs
@@ -21,13 +21,13 @@ use core::marker::{PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
-#[cfg(not(no_global_oom_handling))]
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
 #[cfg(not(no_global_oom_handling))]
 use core::slice::from_raw_parts_mut;
 use core::sync::atomic;
-use core::sync::atomic::Ordering::{Acquire, Relaxed, Release, SeqCst};
+use core::sync::atomic::Ordering::{Acquire, Relaxed, Release};
 
 #[cfg(not(no_global_oom_handling))]
 use crate::alloc::handle_alloc_error;
@@ -101,8 +101,8 @@ macro_rules! acquire {
 /// first: after all, isn't the point of `Arc<T>` thread safety? The key is
 /// this: `Arc<T>` makes it thread safe to have multiple ownership of the same
 /// data, but it  doesn't add thread safety to its data. Consider
-/// `Arc<`[`RefCell<T>`]`>`. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
-/// [`Send`], `Arc<`[`RefCell<T>`]`>` would be as well. But then we'd have a problem:
+/// <code>Arc<[RefCell\<T>]></code>. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
+/// [`Send`], <code>Arc<[RefCell\<T>]></code> would be as well. But then we'd have a problem:
 /// [`RefCell<T>`] is not thread safe; it keeps track of the borrowing count using
 /// non-atomic operations.
 ///
@@ -148,7 +148,7 @@ macro_rules! acquire {
 /// use std::sync::Arc;
 ///
 /// let my_arc = Arc::new(());
-/// Arc::downgrade(&my_arc);
+/// let my_weak = Arc::downgrade(&my_arc);
 /// ```
 ///
 /// `Arc<T>`'s implementations of traits like `Clone` may also be called using
@@ -178,6 +178,7 @@ macro_rules! acquire {
 /// [deref]: core::ops::Deref
 /// [downgrade]: Arc::downgrade
 /// [upgrade]: Weak::upgrade
+/// [RefCell\<T>]: core::cell::RefCell
 /// [`RefCell<T>`]: core::cell::RefCell
 /// [`std::sync`]: ../../std/sync/index.html
 /// [`Arc::clone(&from)`]: Arc::clone
@@ -201,14 +202,14 @@ macro_rules! acquire {
 ///     let five = Arc::clone(&five);
 ///
 ///     thread::spawn(move || {
-///         println!("{:?}", five);
+///         println!("{five:?}");
 ///     });
 /// }
 /// ```
 ///
 /// Sharing a mutable [`AtomicUsize`]:
 ///
-/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize
+/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize "sync::atomic::AtomicUsize"
 ///
 /// ```no_run
 /// use std::sync::Arc;
@@ -222,7 +223,7 @@ macro_rules! acquire {
 ///
 ///     thread::spawn(move || {
 ///         let v = val.fetch_add(1, Ordering::SeqCst);
-///         println!("{:?}", v);
+///         println!("{v:?}");
 ///     });
 /// }
 /// ```
@@ -233,35 +234,52 @@ macro_rules! acquire {
 /// [rc_examples]: crate::rc#examples
 #[cfg_attr(not(test), rustc_diagnostic_item = "Arc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Arc<T: ?Sized> {
+pub struct Arc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<ArcInner<T>>,
     phantom: PhantomData<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Arc<T, A> {}
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Arc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Arc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Arc<U>> for Arc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Arc<U, A>> for Arc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Arc<U>> for Arc<T> {}
 
 impl<T: ?Sized> Arc<T> {
-    fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
     }
 
     unsafe fn from_ptr(ptr: *mut ArcInner<T>) -> Self {
-        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+        unsafe { Self::from_ptr_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
+    unsafe fn from_inner_in(ptr: NonNull<ArcInner<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
+    }
+
+    unsafe fn from_ptr_in(ptr: *mut ArcInner<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
 /// `Weak` is a version of [`Arc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Arc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Arc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -280,7 +298,10 @@ impl<T: ?Sized> Arc<T> {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "arc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesnât need
@@ -288,15 +309,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Weak<T, A> {}
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
 
@@ -341,49 +363,83 @@ impl<T> Arc<T> {
     pub fn new(data: T) -> Arc<T> {
         // Start the weak pointer count as 1 which is the weak pointer that's
         // held by all the strong pointers (kinda), see std/rc.rs for more info
-        let x: Box<_> = box ArcInner {
+        let x: Box<_> = Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(1),
             weak: atomic::AtomicUsize::new(1),
             data,
-        };
-        Self::from_inner(Box::leak(x).into())
+        });
+        unsafe { Self::from_inner(Box::leak(x).into()) }
     }
 
-    /// Constructs a new `Arc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Arc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
     ///
-    /// # Examples
-    /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Arc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Arc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Arc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Arc<T>` is not fully-constructed until `Arc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
     ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # #![allow(dead_code)]
     /// use std::sync::{Arc, Weak};
     ///
-    /// struct Foo {
-    ///     me: Weak<Foo>,
+    /// struct Gadget {
+    ///     me: Weak<Gadget>,
     /// }
     ///
-    /// let foo = Arc::new_cyclic(|me| Foo {
-    ///     me: me.clone(),
-    /// });
+    /// impl Gadget {
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Arc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Arc` we're constructing.
+    ///         Arc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
+    ///         })
+    ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Arc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
+    /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Arc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Arc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box ArcInner {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(0),
             weak: atomic::AtomicUsize::new(1),
             data: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
         let init_ptr: NonNull<ArcInner<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -395,7 +451,7 @@ impl<T> Arc<T> {
 
         // Now we can properly initialize the inner value and turn our weak
         // reference into a strong reference.
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).data), data);
 
@@ -413,9 +469,9 @@ impl<T> Arc<T> {
             // possible with safe code alone.
             let prev_value = (*inner).strong.fetch_add(1, Release);
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
-        }
 
-        let strong = Arc::from_inner(init_ptr);
+            Arc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -435,17 +491,16 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -475,9 +530,10 @@ impl<T> Arc<T> {
     /// assert_eq!(*zero, 0)
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -492,10 +548,18 @@ impl<T> Arc<T> {
     /// `data` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(data: T) -> Pin<Arc<T>> {
         unsafe { Pin::new_unchecked(Arc::new(data)) }
     }
 
+    /// Constructs a new `Pin<Arc<T>>`, return an error if allocation fails.
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin(data: T) -> Result<Pin<Arc<T>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new(data)?)) }
+    }
+
     /// Constructs a new `Arc<T>`, returning an error if allocation fails.
     ///
     /// # Examples
@@ -517,7 +581,7 @@ impl<T> Arc<T> {
             weak: atomic::AtomicUsize::new(1),
             data,
         })?;
-        Ok(Self::from_inner(Box::leak(x).into()))
+        unsafe { Ok(Self::from_inner(Box::leak(x).into())) }
     }
 
     /// Constructs a new `Arc` with uninitialized contents, returning an error
@@ -533,12 +597,10 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -587,6 +649,246 @@ impl<T> Arc<T> {
             )?))
         }
     }
+}
+
+impl<T, A: Allocator> Arc<T, A> {
+    /// Constructs a new `Arc<T>` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(data: T, alloc: A) -> Arc<T, A> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        );
+        let (ptr, alloc) = Box::into_unique(x);
+        unsafe { Self::from_inner_in(ptr.into(), alloc) }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator. If `T` does not implement `Unpin`,
+    /// then `data` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(data: T, alloc: A) -> Pin<Arc<T, A>> {
+        unsafe { Pin::new_unchecked(Arc::new_in(data, alloc)) }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator, return an error if allocation
+    /// fails.
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin_in(data: T, alloc: A) -> Result<Pin<Arc<T, A>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new_in(data, alloc)?)) }
+    }
+
+    /// Constructs a new `Arc<T, A>` in the provided allocator, returning an error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::try_new_in(5, System)?;
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(data: T, alloc: A) -> Result<Arc<T, A>, AllocError> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::try_new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        )?;
+        let (ptr, alloc) = Box::into_unique(x);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, in the provided allocator, returning an
+    /// error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if allocation
+    /// fails.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
     /// Returns the inner value, if the `Arc` has exactly one strong reference.
     ///
     /// Otherwise, an [`Err`] is returned with the same `Arc` that was
@@ -617,9 +919,10 @@ impl<T> Arc<T> {
 
         unsafe {
             let elem = ptr::read(&this.ptr.as_ref().data);
+            let alloc = ptr::read(&this.alloc); // copy the allocator
 
             // Make a weak pointer to clean up the implicit strong-weak reference
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc };
             mem::forget(this);
 
             Ok(elem)
@@ -640,19 +943,19 @@ impl<T> Arc<[T]> {
     ///
     /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe { Arc::from_ptr(Arc::allocate_for_slice(len)) }
     }
@@ -676,9 +979,10 @@ impl<T> Arc<[T]> {
     /// assert_eq!(*values, [0, 0, 0])
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -693,7 +997,83 @@ impl<T> Arc<[T]> {
     }
 }
 
-impl<T> Arc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Constructs a new atomically reference-counted slice with uninitialized contents in the
+    /// provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Arc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe { Arc::from_ptr_in(Arc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new atomically reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Arc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut ArcInner<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Arc<mem::MaybeUninit<T>, A> {
     /// Converts to `Arc<T>`.
     ///
     /// # Safety
@@ -704,7 +1084,7 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     /// Calling this when the content is not yet fully initialized
     /// causes immediate undefined behavior.
     ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
     ///
     /// # Examples
     ///
@@ -716,64 +1096,184 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<T> {
-        Arc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Arc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
     }
 }
 
-impl<T> Arc<[mem::MaybeUninit<T>]> {
+impl<T, A: Allocator> Arc<[mem::MaybeUninit<T>], A> {
     /// Converts to `Arc<[T]>`.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    ///
+    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Arc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Arc<T> {
+    /// Constructs an `Arc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Arc<T>` is never accessed.
+    ///
+    /// [into_raw]: Arc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let x = Arc::new("hello".to_owned());
+    /// let x_ptr = Arc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Arc` to prevent leak.
+    ///     let x = Arc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Arc::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let five = Arc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
+    ///
+    ///     // This assertion is deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Arc::increment_strong_count_in(ptr, Global) }
+    }
+
+    /// Decrements the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method. This method can be used to release the final
+    /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
+    /// released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::sync::Arc;
     ///
-    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Arc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     // Those assertions are deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    ///     Arc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Arc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<[T]> {
-        unsafe { Arc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Arc::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Consumes the `Arc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Arc` using
@@ -788,6 +1288,7 @@ impl<T: ?Sized> Arc<T> {
     /// let x_ptr = Arc::into_raw(x);
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use = "losing the pointer will leak memory"]
     #[stable(feature = "rc_raw", since = "1.17.0")]
     pub fn into_raw(this: Self) -> *const T {
         let ptr = Self::as_ptr(&this);
@@ -811,6 +1312,7 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(x_ptr, Arc::as_ptr(&y));
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(this: &Self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(this.ptr);
@@ -821,10 +1323,10 @@ impl<T: ?Sized> Arc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).data) }
     }
 
-    /// Constructs an `Arc<T>` from a raw pointer.
+    /// Constructs an `Arc<T, A>` from a raw pointer.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// [`Arc<U, A>::into_raw`][into_raw] where `U` must have the same size and
     /// alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
@@ -835,7 +1337,8 @@ impl<T: ?Sized> Arc<T> {
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Arc<T>` is never accessed.
+    /// even if the returned `Arc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Arc::into_raw
     /// [transmute]: core::mem::transmute
@@ -843,14 +1346,17 @@ impl<T: ?Sized> Arc<T> {
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let x = Arc::new("hello".to_owned());
+    /// let x = Arc::new_in("hello".to_owned(), System);
     /// let x_ptr = Arc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Arc` to prevent leak.
-    ///     let x = Arc::from_raw(x_ptr);
+    ///     let x = Arc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
@@ -858,15 +1364,15 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         unsafe {
             let offset = data_offset(ptr);
 
             // Reverse the offset to find the original ArcInner.
             let arc_ptr = (ptr as *mut ArcInner<T>).set_ptr_value((ptr as *mut u8).offset(-offset));
 
-            Self::from_ptr(arc_ptr)
+            Self::from_ptr_in(arc_ptr, alloc)
         }
     }
 
@@ -881,8 +1387,13 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// let weak_five = Arc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Arc`"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         // This Relaxed is OK because we're checking the value in the CAS
         // below.
         let mut cur = this.inner().weak.load(Relaxed);
@@ -906,7 +1417,7 @@ impl<T: ?Sized> Arc<T> {
                 Ok(_) => {
                     // Make sure we do not create a dangling Weak
                     debug_assert!(!is_dangling(this.ptr.as_ptr()));
-                    return Weak { ptr: this.ptr };
+                    return Weak { ptr: this.ptr, alloc: this.alloc.clone() };
                 }
                 Err(old) => cur = old,
             }
@@ -934,9 +1445,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(1, Arc::weak_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn weak_count(this: &Self) -> usize {
-        let cnt = this.inner().weak.load(SeqCst);
+        let cnt = this.inner().weak.load(Acquire);
         // If the weak count is currently locked, the value of the
         // count was 0 just before taking the lock.
         if cnt == usize::MAX { 0 } else { cnt - 1 }
@@ -963,9 +1475,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(2, Arc::strong_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn strong_count(this: &Self) -> usize {
-        this.inner().strong.load(SeqCst)
+        this.inner().strong.load(Acquire)
     }
 
     /// Increments the strong reference count on the `Arc<T>` associated with the
@@ -975,30 +1488,37 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// The pointer must have been obtained through `Arc::into_raw`, and the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method,, and `ptr` must point to a block of memory
+    /// allocated by `alloc`.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // This assertion is deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Arc, but don't touch refcount by wrapping in ManuallyDrop
-        let arc = unsafe { mem::ManuallyDrop::new(Arc::<T>::from_raw(ptr)) };
+        let arc = unsafe { mem::ManuallyDrop::new(Arc::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _arc_clone: mem::ManuallyDrop<_> = arc.clone();
     }
@@ -1008,35 +1528,39 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// The pointer must have been obtained through `Arc::into_raw`,  the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release the final
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final
     /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
     /// released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // Those assertions are deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
-    ///     Arc::decrement_strong_count(ptr);
+    ///     Arc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Arc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Arc::from_raw_in(ptr, alloc)) };
     }
 
     #[inline]
@@ -1052,16 +1576,17 @@ impl<T: ?Sized> Arc<T> {
     // Non-inlined part of `drop`.
     #[inline(never)]
     unsafe fn drop_slow(&mut self) {
-        // Destroy the data at this time, even though we may not free the box
-        // allocation itself (there may still be weak pointers lying around).
+        // Destroy the data at this time, even though we must not free the box
+        // allocation itself (there might still be weak pointers lying around).
         unsafe { ptr::drop_in_place(Self::get_mut_unchecked(self)) };
 
         // Drop the weak ref collectively held by all strong references
-        drop(Weak { ptr: self.ptr });
+        // Take a reference to `self.alloc` instead of cloning because 1. it'll
+        // last long enough, and 2. you should be able to drop `Arc`s with
+        // unclonable allocators
+        drop(Weak { ptr: self.ptr, alloc: &self.alloc });
     }
 
-    #[inline]
-    #[stable(feature = "ptr_eq", since = "1.17.0")]
     /// Returns `true` if the two `Arc`s point to the same allocation
     /// (in a vein similar to [`ptr::eq`]).
     ///
@@ -1078,7 +1603,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert!(!Arc::ptr_eq(&five, &other_five));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
+    #[inline]
+    #[must_use]
+    #[stable(feature = "ptr_eq", since = "1.17.0")]
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
@@ -1137,28 +1665,30 @@ impl<T: ?Sized> Arc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Allocates an `ArcInner<T>` with sufficient space for an unsized inner value.
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut ArcInner<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut ArcInner<T> {
         // Allocate for the `ArcInner<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Arc::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut ArcInner<T>).set_ptr_value(mem) as *mut ArcInner<T>,
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut ArcInner<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Arc<T> {
+    fn from_box_in(v: Box<T, A>) -> Arc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1168,9 +1698,9 @@ impl<T: ?Sized> Arc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1277,6 +1807,34 @@ impl<T> Arc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Allocates an `ArcInner<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut ArcInner<[T]> {
+        unsafe {
+            Arc::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut ArcInner<[T]>,
+            )
+        }
+    }
+
+    /// Copy elements from slice into newly allocated Arc<\[T\]>
+    ///
+    /// Unsafe because the caller must either take ownership or bind `T: Copy`.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn copy_from_slice_in(v: &[T], alloc: A) -> Arc<[T], A> {
+        unsafe {
+            let ptr = Self::allocate_for_slice_in(v.len(), &alloc);
+
+            ptr::copy_nonoverlapping(v.as_ptr(), &mut (*ptr).data as *mut [T] as *mut T, v.len());
+
+            Self::from_ptr_in(ptr, alloc)
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 #[cfg(not(no_global_oom_handling))]
 trait ArcFromSlice<T> {
@@ -1300,7 +1858,7 @@ impl<T: Copy> ArcFromSlice<T> for Arc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Arc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Arc<T, A> {
     /// Makes a clone of the `Arc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1316,7 +1874,7 @@ impl<T: ?Sized> Clone for Arc<T> {
     /// let _ = Arc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Arc<T> {
+    fn clone(&self) -> Arc<T, A> {
         // Using a relaxed ordering is alright here, as knowledge of the
         // original reference prevents other threads from erroneously deleting
         // the object.
@@ -1330,25 +1888,26 @@ impl<T: ?Sized> Clone for Arc<T> {
         // [1]: (www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html)
         let old_size = self.inner().strong.fetch_add(1, Relaxed);
 
-        // However we need to guard against massive refcounts in case someone
-        // is `mem::forget`ing Arcs. If we don't do this the count can overflow
-        // and users will use-after free. We racily saturate to `isize::MAX` on
-        // the assumption that there aren't ~2 billion threads incrementing
-        // the reference count at once. This branch will never be taken in
-        // any realistic program.
+        // However we need to guard against massive refcounts in case someone is `mem::forget`ing
+        // Arcs. If we don't do this the count can overflow and users will use-after free. This
+        // branch will never be taken in any realistic program. We abort because such a program is
+        // incredibly degenerate, and we don't care to support it.
         //
-        // We abort because such a program is incredibly degenerate, and we
-        // don't care to support it.
+        // This check is not 100% water-proof: we error when the refcount grows beyond `isize::MAX`.
+        // But we do that check *after* having done the increment, so there is a chance here that
+        // the worst already happened and we actually do overflow the `usize` counter. However, that
+        // requires the counter to grow from `isize::MAX` to `usize::MAX` between the increment
+        // above and the `abort` below, which seems exceedingly unlikely.
         if old_size > MAX_REFCOUNT {
             abort();
         }
 
-        Self::from_inner(self.ptr)
+        unsafe { Self::from_inner_in(self.ptr, self.alloc.clone()) }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Arc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Arc<T, A> {
     type Target = T;
 
     #[inline]
@@ -1360,21 +1919,22 @@ impl<T: ?Sized> Deref for Arc<T> {
 #[unstable(feature = "receiver_trait", issue = "none")]
 impl<T: ?Sized> Receiver for Arc<T> {}
 
-impl<T: Clone> Arc<T> {
+impl<T: Clone, A: Allocator + Clone> Arc<T, A> {
     /// Makes a mutable reference into the given `Arc`.
     ///
-    /// If there are other `Arc` or [`Weak`] pointers to the same allocation,
-    /// then `make_mut` will create a new allocation and invoke [`clone`][clone] on the inner value
-    /// to ensure unique ownership. This is also referred to as clone-on-write.
+    /// If there are other `Arc` pointers to the same allocation, then `make_mut` will
+    /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
+    /// referred to as clone-on-write.
     ///
-    /// Note that this differs from the behavior of [`Rc::make_mut`] which disassociates
-    /// any remaining `Weak` pointers.
+    /// However, if there are no other `Arc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be dissociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`][get_mut], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or dissociating [`Weak`] pointers.
     ///
-    /// [clone]: Clone::clone
-    /// [get_mut]: Arc::get_mut
-    /// [`Rc::make_mut`]: super::rc::Rc::make_mut
+    /// [`clone`]: Clone::clone
+    /// [`get_mut`]: Arc::get_mut
     ///
     /// # Examples
     ///
@@ -1393,6 +1953,23 @@ impl<T: Clone> Arc<T> {
     /// assert_eq!(*data, 8);
     /// assert_eq!(*other_data, 12);
     /// ```
+    ///
+    /// [`Weak`] pointers will be dissociated:
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let mut data = Arc::new(75);
+    /// let weak = Arc::downgrade(&data);
+    ///
+    /// assert!(75 == *data);
+    /// assert!(75 == *weak.upgrade().unwrap());
+    ///
+    /// *Arc::make_mut(&mut data) += 1;
+    ///
+    /// assert!(76 == *data);
+    /// assert!(weak.upgrade().is_none());
+    /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
     #[stable(feature = "arc_unique", since = "1.4.0")]
@@ -1408,7 +1985,7 @@ impl<T: Clone> Arc<T> {
         if this.inner().strong.compare_exchange(1, 0, Acquire, Relaxed).is_err() {
             // Another strong pointer exists, so we must clone.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1429,10 +2006,10 @@ impl<T: Clone> Arc<T> {
 
             // Materialize our own implicit weak pointer, so that it can clean
             // up the ArcInner as needed.
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc: this.alloc.clone() };
 
             // Can just steal the data, all that's left is Weaks
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1448,9 +2025,44 @@ impl<T: Clone> Arc<T> {
         // either unique to begin with, or became one upon cloning the contents.
         unsafe { Self::get_mut_unchecked(this) }
     }
+
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `arc_t` is of type `Arc<T>`, this function is functionally equivalent to
+    /// `(*arc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, sync::Arc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let arc = Arc::new(inner);
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let arc = Arc::new(inner);
+    /// let arc2 = arc.clone();
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `arc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Arc::unwrap_or_clone(arc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
+    #[inline]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Arc::try_unwrap(this).unwrap_or_else(|arc| (*arc).clone())
+    }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Returns a mutable reference into the given `Arc`, if there are
     /// no other `Arc` or [`Weak`] pointers to the same allocation.
     ///
@@ -1458,7 +2070,7 @@ impl<T: ?Sized> Arc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Arc` pointers.
     ///
     /// [make_mut]: Arc::make_mut
     /// [clone]: Clone::clone
@@ -1555,7 +2167,7 @@ impl<T: ?Sized> Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Arc<T, A> {
     /// Drops the `Arc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1626,9 +2238,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
     }
 }
 
-impl Arc<dyn Any + Send + Sync> {
-    #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+impl<A: Allocator + Clone> Arc<dyn Any + Send + Sync, A> {
     /// Attempt to downcast the `Arc<dyn Any + Send + Sync>` to a concrete type.
     ///
     /// # Examples
@@ -1647,18 +2257,63 @@ impl Arc<dyn Any + Send + Sync> {
     /// print_if_string(Arc::new(my_string));
     /// print_if_string(Arc::new(0i8));
     /// ```
-    pub fn downcast<T>(self) -> Result<Arc<T>, Self>
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T>(self) -> Result<Arc<T, A>, Self>
     where
-        T: Any + Send + Sync + 'static,
+        T: Any + Send + Sync,
     {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<ArcInner<T>>();
-            mem::forget(self);
-            Ok(Arc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<ArcInner<T>>();
+                let alloc = self.alloc.clone();
+                mem::forget(self);
+                Ok(Arc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Arc<dyn Any + Send + Sync>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::sync::Arc;
+    ///
+    /// let x: Arc<dyn Any + Send + Sync> = Arc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T>(self) -> Arc<T, A>
+    where
+        T: Any + Send + Sync,
+    {
+        unsafe {
+            let ptr = self.ptr.cast::<ArcInner<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Arc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T> Weak<T> {
@@ -1675,9 +2330,40 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") }
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T, A>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Weak;
+    /// use std::alloc::System;
+    ///
+    /// let empty: Weak<i64, _> = Weak::new_in(System);
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -1689,6 +2375,55 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::{Arc, Weak};
+    ///
+    /// let strong = Arc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Arc::downgrade(&strong).into_raw();
+    /// let raw_2 = Arc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Arc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Arc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`new`]: Weak::new
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Weak::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -1713,7 +2448,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: core::ptr::null "ptr::null"
+    #[must_use]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(self.ptr);
@@ -1723,7 +2459,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as ArcInner (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).data) }
@@ -1757,6 +2493,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -1764,7 +2501,8 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
-    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>` in the provided
+    /// allocator.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
     /// later) or to deallocate the weak count by dropping the `Weak<T>`.
@@ -1775,7 +2513,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -1805,9 +2543,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`new`]: Weak::new
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
-    /// [`forget`]: std::mem::forget
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -1823,11 +2560,11 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 }
 
-impl<T: ?Sized> Weak<T> {
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Attempts to upgrade the `Weak` pointer to an [`Arc`], delaying
     /// dropping of the inner value if successful.
     ///
@@ -1851,8 +2588,13 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Arc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Arc<T>> {
+    pub fn upgrade(&self) -> Option<Arc<T, A>>
+    where
+        A: Clone,
+    {
         // We use a CAS loop to increment the strong count instead of a
         // fetch_add as this function should never take the reference count
         // from zero to one.
@@ -1879,7 +2621,7 @@ impl<T: ?Sized> Weak<T> {
             // value can be initialized after `Weak` references have already been created. In that case, we
             // expect to observe the fully initialized value.
             match inner.strong.compare_exchange_weak(n, n + 1, Acquire, Relaxed) {
-                Ok(_) => return Some(Arc::from_inner(self.ptr)), // null checked above
+                Ok(_) => return Some(unsafe { Arc::from_inner_in(self.ptr, self.alloc.clone()) }), // null checked above
                 Err(old) => n = old,
             }
         }
@@ -1888,9 +2630,10 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of strong (`Arc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
-        if let Some(inner) = self.inner() { inner.strong.load(SeqCst) } else { 0 }
+        if let Some(inner) = self.inner() { inner.strong.load(Acquire) } else { 0 }
     }
 
     /// Gets an approximation of the number of `Weak` pointers pointing to this
@@ -1904,12 +2647,13 @@ impl<T: ?Sized> Weak<T> {
     /// Due to implementation details, the returned value can be off by 1 in
     /// either direction when other threads are manipulating any `Arc`s or
     /// `Weak`s pointing to the same allocation.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
             .map(|inner| {
-                let weak = inner.weak.load(SeqCst);
-                let strong = inner.strong.load(SeqCst);
+                let weak = inner.weak.load(Acquire);
+                let strong = inner.strong.load(Acquire);
                 if strong == 0 {
                     0
                 } else {
@@ -1981,8 +2725,9 @@ impl<T: ?Sized> Weak<T> {
     /// assert!(!first.ptr_eq(&third));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -1990,7 +2735,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2003,11 +2748,11 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         let inner = if let Some(inner) = self.inner() {
             inner
         } else {
-            return Weak { ptr: self.ptr };
+            return Weak { ptr: self.ptr, alloc: self.alloc.clone() };
         };
         // See comments in Arc::clone() for why this is relaxed.  This can use a
         // fetch_add (ignoring the lock) because the weak count is only locked
@@ -2020,7 +2765,7 @@ impl<T: ?Sized> Clone for Weak<T> {
             abort();
         }
 
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
@@ -2046,7 +2791,7 @@ impl<T> Default for Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2084,25 +2829,27 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
 
         if inner.weak.fetch_sub(1, Release) == 1 {
             acquire!(inner.weak);
-            unsafe { Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr())) }
+            unsafe {
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()))
+            }
         }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait ArcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Arc<T>) -> bool;
-    fn ne(&self, other: &Arc<T>) -> bool;
+trait ArcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Arc<T, A>) -> bool;
+    fn ne(&self, other: &Arc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Arc<T>) -> bool {
+    default fn eq(&self, other: &Arc<T, A>) -> bool {
         **self == **other
     }
     #[inline]
-    default fn ne(&self, other: &Arc<T>) -> bool {
+    default fn ne(&self, other: &Arc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -2115,20 +2862,20 @@ impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + crate::rc::MarkerEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + crate::rc::MarkerEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         Arc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         !Arc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Arc<T, A> {
     /// Equality for two `Arc`s.
     ///
     /// Two `Arc`s are equal if their inner values are equal, even if they are
@@ -2147,7 +2894,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five == Arc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::eq(self, other)
     }
 
@@ -2168,13 +2915,13 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five != Arc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Arc<T, A> {
     /// Partial comparison for two `Arc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -2189,7 +2936,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Arc::new(6)));
     /// ```
-    fn partial_cmp(&self, other: &Arc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Arc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -2206,7 +2953,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five < Arc::new(6));
     /// ```
-    fn lt(&self, other: &Arc<T>) -> bool {
+    fn lt(&self, other: &Arc<T, A>) -> bool {
         *(*self) < *(*other)
     }
 
@@ -2223,7 +2970,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five <= Arc::new(5));
     /// ```
-    fn le(&self, other: &Arc<T>) -> bool {
+    fn le(&self, other: &Arc<T, A>) -> bool {
         *(*self) <= *(*other)
     }
 
@@ -2240,7 +2987,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five > Arc::new(4));
     /// ```
-    fn gt(&self, other: &Arc<T>) -> bool {
+    fn gt(&self, other: &Arc<T, A>) -> bool {
         *(*self) > *(*other)
     }
 
@@ -2257,12 +3004,12 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five >= Arc::new(5));
     /// ```
-    fn ge(&self, other: &Arc<T>) -> bool {
+    fn ge(&self, other: &Arc<T, A>) -> bool {
         *(*self) >= *(*other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Arc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Arc<T, A> {
     /// Comparison for two `Arc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -2277,29 +3024,29 @@ impl<T: ?Sized + Ord> Ord for Arc<T> {
     ///
     /// assert_eq!(Ordering::Less, five.cmp(&Arc::new(6)));
     /// ```
-    fn cmp(&self, other: &Arc<T>) -> Ordering {
+    fn cmp(&self, other: &Arc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Arc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Arc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Arc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Arc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Arc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -2324,7 +3071,7 @@ impl<T: Default> Default for Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Arc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Arc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state)
     }
@@ -2333,6 +3080,20 @@ impl<T: ?Sized + Hash> Hash for Arc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Arc<T> {
+    /// Converts a `T` into an `Arc<T>`
+    ///
+    /// The conversion moves the value into a
+    /// newly allocated `Arc`. It is equivalent to
+    /// calling `Arc::new(t)`.
+    ///
+    /// # Example
+    /// ```rust
+    /// # use std::sync::Arc;
+    /// let x = 5;
+    /// let arc = Arc::new(5);
+    ///
+    /// assert_eq!(Arc::from(x), arc);
+    /// ```
     fn from(t: T) -> Self {
         Arc::new(t)
     }
@@ -2397,7 +3158,7 @@ impl From<String> for Arc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Arc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Arc<T, A> {
     /// Move a boxed object to a new, reference-counted allocation.
     ///
     /// # Example
@@ -2409,14 +3170,14 @@ impl<T: ?Sized> From<Box<T>> for Arc<T> {
     /// assert_eq!("eggplant", &shared[..]);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Arc<T> {
-        Arc::from_box(v)
+    fn from(v: Box<T, A>) -> Arc<T, A> {
+        Arc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Arc<[T]> {
+impl<T, A: Allocator + Clone> From<Vec<T, A>> for Arc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -2428,9 +3189,9 @@ impl<T> From<Vec<T>> for Arc<[T]> {
     /// assert_eq!(&[1, 2, 3], &shared[..]);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Arc<[T]> {
+    fn from(mut v: Vec<T, A>) -> Arc<[T], A> {
         unsafe {
-            let arc = Arc::copy_from_slice(&v);
+            let arc = Arc::copy_from_slice_in(&v, v.allocator().clone());
 
             // Allow the Vec to free its memory, but not destroy its contents
             v.set_len(0);
@@ -2493,13 +3254,33 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Arc<str>> for Arc<[u8]> {
+    /// Converts an atomically reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::sync::Arc;
+    /// let string: Arc<str> = Arc::from("eggplant");
+    /// let bytes: Arc<[u8]> = Arc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Arc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Arc::from_raw(Arc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
-impl<T, const N: usize> TryFrom<Arc<[T]>> for Arc<[T; N]> {
-    type Error = Arc<[T]>;
+impl<T, A: Allocator + Clone, const N: usize> TryFrom<Arc<[T], A>> for Arc<[T; N], A> {
+    type Error = Arc<[T], A>;
 
-    fn try_from(boxed_slice: Arc<[T]>) -> Result<Self, Self::Error> {
+    fn try_from(boxed_slice: Arc<[T], A>) -> Result<Self, Self::Error> {
         if boxed_slice.len() == N {
-            Ok(unsafe { Arc::from_raw(Arc::into_raw(boxed_slice) as *mut [T; N]) })
+            let alloc = boxed_slice.alloc.clone();
+            Ok(unsafe { Arc::from_raw_in(Arc::into_raw(boxed_slice) as *mut [T; N], alloc) })
         } else {
             Err(boxed_slice)
         }
@@ -2592,21 +3373,21 @@ impl<T, I: iter::TrustedLen<Item = T>> ToArcSlice<T> for I {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Arc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Arc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Arc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Arc<T, A> {}
 
 /// Get the offset within an `ArcInner` for the payload behind a pointer.
 ///
@@ -2620,7 +3401,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
diff --git a/rust/helpers.c b/rust/helpers.c
index 3416351e0..649c8c2a8 100644
--- a/rust/helpers.c
+++ b/rust/helpers.c
@@ -44,6 +44,7 @@
 #include <linux/irqstage.h>
 #include <linux/dovetail.h>
 #include <linux/spinlock_pipeline.h>
+#include <linux/log2.h>
 
 void rust_helper_BUG(void)
 {
@@ -270,6 +271,12 @@ int rust_helper_page_aligned(unsigned long size)
 }
 EXPORT_SYMBOL_GPL(rust_helper_page_aligned); 
 
+size_t rust_helper_align(size_t x, unsigned long a)
+{
+	return ALIGN(x,a);
+}
+EXPORT_SYMBOL_GPL(rust_helper_align); 
+
 bool rust_helper_running_inband(void)
 {
 	return running_inband();
@@ -632,6 +639,7 @@ void rust_helper_dovetail_leave_oob(void) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_dovetail_leave_oob);
 
+
 void rust_helper_hard_spin_lock(struct raw_spinlock *rlock) {
 	hard_spin_lock(rlock);
 }
@@ -642,6 +650,17 @@ void rust_helper_hard_spin_unlock(struct raw_spinlock *rlock) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_hard_spin_unlock);
 
+inline int rust_helper_ilog2(size_t size) {
+	return ilog2(size);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ilog2);
+
+int rust_helper_ffs(unsigned long x) {
+	return ffs(x);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ffs);
+
+
 /* We use bindgen's --size_t-is-usize option to bind the C size_t type
  * as the Rust usize type, so we can use it in contexts where Rust
  * expects a usize like slice (array) indices. usize is defined to be
diff --git a/rust/kernel/allocator.rs b/rust/kernel/allocator.rs
index 759cec47d..2047a1c6b 100644
--- a/rust/kernel/allocator.rs
+++ b/rust/kernel/allocator.rs
@@ -4,10 +4,10 @@
 
 use core::alloc::{GlobalAlloc, Layout};
 use core::ptr;
-
+use crate::timekeeping::*;
 use crate::bindings;
 use crate::c_types;
-
+use crate::pr_info;
 pub struct KernelAllocator;
 
 unsafe impl GlobalAlloc for KernelAllocator {
@@ -32,7 +32,8 @@ static ALLOCATOR: KernelAllocator = KernelAllocator;
 // let's generate them ourselves instead.
 #[no_mangle]
 pub fn __rust_alloc(size: usize, _align: usize) -> *mut u8 {
-    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 }
+    let x = unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 };
+    return x;
 }
 
 #[no_mangle]
diff --git a/rust/kernel/double_linked_list3.rs b/rust/kernel/double_linked_list3.rs
new file mode 100644
index 000000000..0d106223e
--- /dev/null
+++ b/rust/kernel/double_linked_list3.rs
@@ -0,0 +1,366 @@
+use core::cmp::Ordering;
+use core::hash::{Hash, Hasher};
+use core::marker::PhantomData;
+use core::ptr::NonNull;
+use alloc::boxed::{Box};
+use alloc::alloc::{Global};
+
+use crate::pr_info;
+
+/// åé¾è¡¨
+pub struct LinkedList<T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<T>,
+}
+
+/// é¾è¡¨èç¹
+struct Node<T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    elem: T,
+}
+
+/// é¾è¡¨è¿­ä»£å¨
+pub struct Iter<'a, T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<&'a T>,
+}
+
+/// é¾è¡¨å¯åè¿­ä»£å¨
+pub struct IterMut<'a, T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<&'a mut T>,
+}
+
+impl<T> LinkedList<T> {
+    /// åå»ºä¸ä¸ªç©ºé¾è¡¨
+    pub fn new() -> Self {
+        Self {
+            front: None,
+            back: None,
+            len: 0,
+            marker: PhantomData,
+        }
+    }
+
+    /// å°åç´ æå¥å°é¾è¡¨å¤´é¨
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_front(1);
+    /// assert_eq!(list.front(), Some(&1));
+    /// ```
+    pub fn push_front(&mut self, elem: T) {
+            // let new = NonNull::new_unchecked(Box::into_raw(Box::try_new_in(Node {
+            //     front: None,
+            //     back: None,
+            //     elem,
+            // }, Global).unwrap()));
+            //TODO:
+            unimplemented!();
+    }
+
+    /// å°åç´ æå¥å°é¾è¡¨å°¾é¨
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.back(), Some(&1));
+    /// ```
+    pub fn push_back(&mut self, elem: T) {
+        // let new = NonNull::new_unchecked(Box::into_raw(Box::try_new_in(Node {
+        //     back: None,
+        //     front: None,
+        //     elem,
+        // }, Global).unwrap()));
+        // TODO:
+        unimplemented!();
+    }
+
+    /// å°ç¬¬ä¸ä¸ªåç´ è¿å
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_front(1);
+    /// assert_eq!(list.pop_front(), Some(1));
+    /// ```
+    pub fn pop_front(&mut self) -> Option<T> {
+        //TODO:
+        None
+    }
+
+    /// å°æåä¸ä¸ªåç´ è¿å
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.pop_back(), Some(1));
+    /// ```
+    pub fn pop_back(&mut self) -> Option<T> {
+        // TODO:
+        None
+    }
+
+    /// è¿åé¾è¡¨ç¬¬ä¸ä¸ªåç´ çå¼ç¨  
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// assert_eq!(list.front(), None);
+    /// list.push_front(1);
+    /// assert_eq!(list.front(), Some(&1));
+    /// ```
+    pub fn front(&self) -> Option<&T> {
+        unsafe { self.front.map(|node| &(*node.as_ptr()).elem) }
+    }
+
+    /// è¿åé¾è¡¨ç¬¬ä¸ä¸ªåç´ çå¯åå¼ç¨   
+    pub fn front_mut(&mut self) -> Option<&mut T> {
+        unsafe { self.front.map(|node| &mut (*node.as_ptr()).elem) }
+    }
+
+    /// è¿åé¾è¡¨æåä¸ä¸ªåç´ çå¼ç¨
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// assert_eq!(list.back(), None);
+    /// list.push_back(1);
+    /// assert_eq!(list.back(), Some(&1));
+    /// ```
+    pub fn back(&self) -> Option<&T> {
+        unsafe { self.back.map(|node| &(*node.as_ptr()).elem) }
+    }
+
+    /// è¿åé¾è¡¨æåä¸ä¸ªåç´ çå¯åå¼ç¨
+    pub fn back_mut(&mut self) -> Option<&mut T> {
+        unsafe { self.back.map(|node| &mut (*node.as_ptr()).elem) }
+    }
+
+    /// è¿åé¾è¡¨é¿åº¦
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.len(), 1);
+    /// ```
+    pub fn len(&self) -> usize {
+        self.len
+    }
+
+    pub fn is_empty(&self) -> bool{
+        self.len == 0
+    }
+
+    /// æ¸ç©ºé¾è¡¨
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// list.push_back(2);
+    /// assert_eq!(list.len(), 2);
+    /// list.clear();
+    /// assert_eq!(list.len(), 0);
+    /// ```
+    pub fn clear(&mut self) {
+        // Oh look it's drop again
+        while self.pop_front().is_some() {}
+    }
+
+    /// è¿åä¸ä¸ªè¿­ä»£å¨
+    pub fn iter(&self) -> Iter<'_,T> {
+        Iter {
+            front: self.front,
+            back: self.back,
+            len: self.len,
+            marker: PhantomData,
+        }
+    }
+
+    /// è¿åä¸ä¸ªå¯åè¿­ä»£å¨
+    pub fn iter_mut(&mut self) -> IterMut<'_,T> {
+        IterMut {
+            front: self.front,
+            back: self.back,
+            len: self.len,
+            marker: PhantomData,
+        }
+    }
+
+
+    /// ç§»é¤é¾è¡¨ä¸­ä¸æ ä¸ºiçåç´ 
+    /// å¦æè¶åºèå´ï¼ä½¿ç¨panic!å®æåºå¼å¸¸
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::from_iter(vec![1,2,3]);
+    /// assert_eq!(list.remove(1), 2);
+    pub fn remove(&mut self, index: usize) -> T {
+        // TODO:
+        unimplemented!()
+    }
+
+    
+}
+
+
+impl<'a, T> IntoIterator for &'a LinkedList<T> {
+    type IntoIter = Iter<'a, T>;
+    type Item = &'a T;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.iter()
+    }
+}
+
+impl<'a, T> IntoIterator for &'a mut LinkedList<T> {
+    type IntoIter = IterMut<'a, T>;
+    type Item = &'a mut T;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.iter_mut()
+    }
+}
+
+impl<'a, T> Iterator for Iter<'a, T> {
+    type Item = &'a T;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.len > 0 {
+            // We could unwrap front, but this is safer and easier
+            self.front.map(|node| unsafe {
+                self.len -= 1;
+                self.front = (*node.as_ptr()).back;
+                &(*node.as_ptr()).elem
+            })
+        } else {
+            None
+        }
+    }
+
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        (self.len, Some(self.len))
+    }
+}
+impl<'a, T> Iterator for IterMut<'a, T> {
+    type Item = &'a mut T;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        // While self.front == self.back is a tempting condition to check here,
+        // it won't do the right for yielding the last element! That sort of
+        // thing only works for arrays because of "one-past-the-end" pointers.
+        if self.len > 0 {
+            // We could unwrap front, but this is safer and easier
+            self.front.map(|node| unsafe {
+                self.len -= 1;
+                self.front = (*node.as_ptr()).back;
+                &mut (*node.as_ptr()).elem
+            })
+        } else {
+            None
+        }
+    }
+
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        (self.len, Some(self.len))
+    }
+}
+
+
+impl<T> Drop for LinkedList<T> {
+    fn drop(&mut self) {
+        // Pop until we have to stop
+        while self.pop_front().is_some() {}
+    }
+}
+
+impl<T> Default for LinkedList<T> {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+impl<T: Clone> Clone for LinkedList<T> {
+    fn clone(&self) -> Self {
+        let mut new_list = Self::new();
+        for item in self {
+            new_list.push_back(item.clone());
+        }
+        new_list
+    }
+}
+impl<T> Extend<T> for LinkedList<T> {
+    fn extend<I: IntoIterator<Item = T>>(&mut self, iter: I) {
+        for item in iter {
+            self.push_back(item);
+        }
+    }
+}
+// impl<T> FromIterator<T> for LinkedList<T> {
+//     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {
+//         let mut list = Self::new();
+//         list.extend(iter);
+//         list
+//     }
+// }
+
+// impl<T: Debug> Debug for LinkedList<T> {
+//     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+//         f.debug_list().entries(self).finish()
+//     }
+// }
+
+impl<T: PartialEq> PartialEq for LinkedList<T> {
+    fn eq(&self, other: &Self) -> bool {
+        self.len() == other.len() && self.iter().eq(other)
+    }
+
+    fn ne(&self, other: &Self) -> bool {
+        self.len() != other.len() || self.iter().ne(other)
+    }
+}
+
+impl<T: Eq> Eq for LinkedList<T> {}
+
+impl<T: PartialOrd> PartialOrd for LinkedList<T> {
+    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
+        self.iter().partial_cmp(other)
+    }
+}
+
+impl<T: Ord> Ord for LinkedList<T> {
+    fn cmp(&self, other: &Self) -> Ordering {
+        self.iter().cmp(other)
+    }
+}
+
+
+unsafe impl<T: Send> Send for LinkedList<T> {}
+unsafe impl<T: Sync> Sync for LinkedList<T> {}
+
+unsafe impl<'a, T: Send> Send for Iter<'a, T> {}
+unsafe impl<'a, T: Sync> Sync for Iter<'a, T> {}
+
+unsafe impl<'a, T: Send> Send for IterMut<'a, T> {}
+unsafe impl<'a, T: Sync> Sync for IterMut<'a, T> {}
diff --git a/rust/kernel/lib.rs b/rust/kernel/lib.rs
index e7b8c318a..66d90cca5 100644
--- a/rust/kernel/lib.rs
+++ b/rust/kernel/lib.rs
@@ -25,7 +25,9 @@
     try_reserve,
     unsafe_cell_raw_get
 )]
-
+#![feature(destructuring_assignment)]
+#![feature(array_map)]
+#![feature(new_uninit)]
 // Ensure conditional compilation based on the kernel configuration works;
 // otherwise we may silently break things like initcall handling.
 #[cfg(not(CONFIG_RUST))]
@@ -100,9 +102,12 @@ pub mod kthread;
 pub mod ktime;
 pub mod dovetail;
 pub mod completion;
+pub mod memory_rros;
 #[cfg(CONFIG_NET)]
 pub mod net;
 
+pub mod double_linked_list3;
+
 #[doc(hidden)]
 pub use build_error::build_error;
 
diff --git a/rust/kernel/memory_rros.rs b/rust/kernel/memory_rros.rs
new file mode 100644
index 000000000..ccaef6ad2
--- /dev/null
+++ b/rust/kernel/memory_rros.rs
@@ -0,0 +1,644 @@
+use crate::{
+    rbtree::{RBTree, RBTreeNode},
+    prelude::*,sync::SpinLock,
+    vmalloc, mm, premmpt, spinlock_init, c_types,
+};
+use crate::{bindings, Result};
+use core::{borrow::BorrowMut, mem::size_of, mem::zeroed, ptr::addr_of_mut, ptr::addr_of};
+use crate::timekeeping::*;
+
+const PAGE_SIZE: u32 = 4096 as u32;
+const EVL_HEAP_PAGE_SHIFT: u32 = 9; /* 2^9 => 512 bytes */
+const EVL_HEAP_PAGE_SIZE: u32 =	(1 << EVL_HEAP_PAGE_SHIFT);
+const EVL_HEAP_PAGE_MASK: u32 =	(!(EVL_HEAP_PAGE_SIZE - 1));
+const EVL_HEAP_MIN_LOG2: u32 = 	4; /* 16 bytes */
+/*
+ * Use bucketed memory for sizes between 2^EVL_HEAP_MIN_LOG2 and
+ * 2^(EVL_HEAP_PAGE_SHIFT-1).
+ */
+const EVL_HEAP_MAX_BUCKETS: u32 = (EVL_HEAP_PAGE_SHIFT - EVL_HEAP_MIN_LOG2);
+const EVL_HEAP_MIN_ALIGN: u32 = (1 << EVL_HEAP_MIN_LOG2);//16
+/* Maximum size of a heap (4Gb - PAGE_SIZE). */
+const EVL_HEAP_MAX_HEAPSZ: u32 = (4294967295 - PAGE_SIZE + 1);
+/* Bits we need for encoding a page # */
+const EVL_HEAP_PGENT_BITS: u32 = (32 - EVL_HEAP_PAGE_SHIFT);
+/* Each page is represented by a page map entry. */
+// const EVL_HEAP_PGMAP_BYTES	sizeof(struct evl_heap_pgentry)
+const CONFIG_EVL_NR_THREADS: usize = 256;
+const CONFIG_EVL_NR_MONITORS: usize = 512;
+pub type size_t = usize;
+
+extern "C" {
+    fn rust_helper_rb_link_node(
+        node: *mut bindings::rb_node,
+        parent: *const bindings::rb_node,
+        rb_link: *mut *mut bindings::rb_node,
+    );
+    fn rust_helper_ilog2(
+        size: size_t
+    ) -> c_types::c_int;
+    fn rust_helper_align(
+        x: size_t,
+        a: u32,
+    ) -> c_types::c_ulong;
+    fn rust_helper_ffs(
+        x: u32,
+    ) -> c_types::c_int;
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_alloc(size: usize, align: usize) -> *mut u8 {
+    // pr_info!("__rros_sys_heap_alloc: begin");
+    unsafe {
+        evl_system_heap.evl_alloc_chunk(size).unwrap()
+    }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_dealloc(ptr: *mut u8, size: usize, align: usize) {
+    unsafe { evl_system_heap.evl_free_chunk(ptr); }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8 {
+    unsafe {
+        evl_system_heap.evl_realloc_chunk(ptr, old_size, new_size).unwrap()
+    }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_alloc_zerod(size: usize, align: usize) -> *mut u8 {
+    unsafe {
+        evl_system_heap.evl_alloc_chunk_zeroed(size).unwrap()
+    }
+}
+
+struct evl_user_window {
+    state: u32,
+    info: u32,
+    pp_pending: u32,
+}
+
+#[repr(C)]
+union pginfo {
+    map: u32,
+    bsize: u32,
+}
+
+pub struct evl_heap_pgentry {
+    pub prev: u32,
+    pub next: u32,
+    pub page_type: u32,
+    pginfo: pginfo,
+}
+
+pub struct evl_heap_range {
+    pub addr_node: bindings::rb_node,
+    pub size_node: bindings::rb_node,
+    pub size: size_t,
+}
+
+pub fn new_evl_heap_range(addr: *mut u8, size: size_t) -> *mut evl_heap_range{
+    let addr = addr as *mut evl_heap_range;
+    unsafe{
+        (*addr).addr_node = bindings::rb_node::default();
+        (*addr).size_node = bindings::rb_node::default();
+        (*addr).size = size;
+    }
+    return addr;
+}
+
+#[inline]
+pub fn addr_add_size(addr: *mut u8, size: size_t) -> *mut u8 {
+    (addr as u64 + size as u64) as *mut u8
+}
+
+
+pub struct evl_heap {
+    pub membase: *mut u8,
+    pub addr_tree: Option<bindings::rb_root>, //æ ¹æ®é¦å°åæ¾å¤§å°
+    pub size_tree: Option<bindings::rb_root>, //æ ¹æ®å¤§å°æ¾é¦å°å
+    pub pagemap: Option<*mut evl_heap_pgentry>,
+    pub usable_size: size_t,
+    pub used_size: size_t,
+    pub buckets: [u32; EVL_HEAP_MAX_BUCKETS as usize],
+    pub lock: Option<SpinLock<i32>>,
+}
+
+impl evl_heap {
+    pub fn init(&mut self, membase: *mut u8, size: size_t) -> Result<usize>{
+        premmpt::running_inband()?;
+        mm::page_aligned(size)?;
+        if (size as u32) > EVL_HEAP_MAX_HEAPSZ {
+            return Err(crate::Error::EINVAL);
+        }
+
+        let mut spinlock = unsafe{ SpinLock::new(1) };
+        let pinned = unsafe { Pin::new_unchecked(&mut spinlock) };
+        spinlock_init!(pinned, "spinlock");
+        self.lock = Some(spinlock);
+        
+        for i in self.buckets.iter_mut() {
+            *i = u32::MAX;
+        }
+
+        let nrpages = size >> EVL_HEAP_PAGE_SHIFT;
+        let a: u64 = size_of::<evl_heap_pgentry>() as u64;
+        let kzalloc_res = vmalloc::c_kzalloc(a * nrpages as u64);
+        match kzalloc_res {
+            Some(x) => self.pagemap = Some(x as *mut evl_heap_pgentry),
+            None => {
+                return Err(crate::Error::ENOMEM);
+            }
+        }
+
+        self.membase = membase;
+        self.usable_size = size;
+        self.used_size = 0;
+        
+        self.size_tree = Some(bindings::rb_root::default());
+        self.addr_tree = Some(bindings::rb_root::default());
+        self.release_page_range(membase, size);
+        
+        Ok(0)
+    }
+
+    pub fn release_page_range(&mut self, page: *mut u8, size: size_t) {
+        let mut freed = page as *mut evl_heap_range;
+        let mut addr_linked = false;
+        
+        unsafe{ (*freed).size = size; }
+        let left_op = self.search_left_mergeable(freed);
+        match left_op {
+            Some(left) => {
+                let node_links = unsafe {addr_of_mut!((*left).size_node)};
+                let mut root = self.size_tree.as_mut().unwrap();
+                unsafe{bindings::rb_erase(node_links, root);}
+                unsafe{ (*left).size += (*freed).size; }
+                freed = left;
+                addr_linked = true;
+            },
+            None => (),
+        }
+        let right_op = self.search_right_mergeable(freed); // FIXME: å¥½ååéäº
+        match right_op {
+            Some(right) => {
+                let mut node_links = unsafe{addr_of_mut!((*right).size_node)};
+                let mut root = self.size_tree.as_mut().unwrap();
+                unsafe{bindings::rb_erase(node_links, root);}
+                unsafe{ (*freed).size += (*right).size; }
+                node_links = unsafe{addr_of_mut!((*right).addr_node)};
+                root = self.addr_tree.as_mut().unwrap();
+                if addr_linked {
+                    unsafe{bindings::rb_erase(node_links, root)};
+                } else {
+                    let freed_node_links = unsafe{addr_of_mut!((*freed).addr_node)};
+                    unsafe{bindings::rb_replace_node(node_links, freed_node_links, root)};
+                }
+            },
+            None => {
+                self.insert_range_byaddr(freed);
+            },
+        }
+        // pr_info!("release_page_range: 4");
+        self.insert_range_bysize(freed);
+        // pr_info!("release_page_range: 5");
+    }
+
+    pub fn search_left_mergeable(&self, r: *mut evl_heap_range) -> Option<*mut evl_heap_range> {
+        let mut node: *mut bindings::rb_node = self.addr_tree.clone().unwrap().rb_node;
+        while !node.is_null() {
+            let p = crate::container_of!(node, evl_heap_range, addr_node);
+            unsafe {
+                if addr_add_size(p as *mut u8, (*p).size) as u64 == r as u64 {
+                    return Some(p as *mut evl_heap_range);
+                }
+                let addr_node_addr = addr_of_mut!((*r).addr_node);
+                if (addr_node_addr as u64) < (node as u64) {
+                    node = (*node).rb_left;
+                }else {
+                    node = (*node).rb_right;
+                }
+            }
+        }
+        None
+    }
+
+    pub fn search_right_mergeable(&self, r: *mut evl_heap_range) -> Option<*mut evl_heap_range> {
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+
+    pub fn insert_range_byaddr(&mut self, r: *mut evl_heap_range) {
+        unsafe {
+            let node_links = addr_of_mut!((*r).addr_node);
+            let mut root = self.addr_tree.as_mut().unwrap();
+            let mut new_link: &mut *mut bindings::rb_node = &mut root.rb_node;
+            let mut parent = core::ptr::null_mut();
+            while !new_link.is_null() {
+                let p = crate::container_of!(*new_link, evl_heap_range, addr_node);
+                parent = *new_link;
+                if (r as u64) < (p as u64) {
+                    new_link = &mut (*parent).rb_left;
+                } else {            
+                    new_link = &mut (*parent).rb_right;
+                }
+            }
+            rust_helper_rb_link_node(node_links, parent, new_link);
+            bindings::rb_insert_color(node_links, root);
+        }
+    }
+
+    pub fn insert_range_bysize(&mut self, r: *mut evl_heap_range) {
+        unsafe {
+            let node_links = addr_of_mut!((*r).size_node);
+            let mut root = self.size_tree.as_mut().unwrap();
+            let mut new_link: &mut *mut bindings::rb_node = &mut root.rb_node;
+            let mut parent = core::ptr::null_mut();
+            // TODO: YOUR CODE HERE
+
+            // END OF YOUR CODE
+            rust_helper_rb_link_node(node_links, parent, new_link);
+            bindings::rb_insert_color(node_links, root);
+        }
+    }
+
+    #[no_mangle]
+    pub fn evl_alloc_chunk(&mut self, size: size_t) -> Option<*mut u8> {
+        let mut log2size:i32 =0;
+        let mut ilog: i32 = 0;
+        let mut pg: i32 = 0;
+        let mut b: i32 = -1;
+        let mut flags: u32 = 0;
+        let mut bsize: size_t = 0;
+        
+        let mut block: Option<*mut u8>;
+        if size == 0 {
+            return None;
+        }
+        // è®¡ç®bsizeålog2sizeçå¼
+        // TODO: YOUR CODE HERE
+
+        // END OF YOUR CODE
+
+        //ä¸é
+        if bsize >= (EVL_HEAP_PAGE_SIZE as usize) {
+            block = self.add_free_range(bsize, 0);
+        } else {
+            ilog = log2size - EVL_HEAP_MIN_LOG2 as i32;
+            pg = self.buckets[ilog as usize] as i32;
+            unsafe{
+                if pg < 0 {
+                    block = self.add_free_range(bsize, log2size);
+                } else {
+                    let pagemap = self.get_pagemap(pg);
+                    if (*pagemap).pginfo.map == u32::MAX {
+                        block = self.add_free_range(bsize, log2size);
+                    } else {
+                        let x = !(*pagemap).pginfo.map; // FIXME
+                        b = rust_helper_ffs(x) - 1;
+                        (*pagemap).pginfo.map |= (1<< b);
+                        self.used_size += bsize; // FIXME: used_size
+                        block = Some(addr_add_size(self.membase, ((pg << EVL_HEAP_PAGE_SHIFT) + (b << log2size)) as size_t));
+                        if (*pagemap).pginfo.map == u32::MAX {
+                            self.move_page_back(pg, log2size);
+                        }
+                    }
+                }
+            }
+        }
+        //è§£é
+        return block;
+    }
+
+    //å°ç³è¯·çåå­ç©ºé´åå§åä¸º0
+    pub fn evl_alloc_chunk_zeroed(&mut self, size: size_t) -> Option<*mut u8> {
+        let block = self.evl_alloc_chunk(size);
+        match block {
+            Some(x) => {
+                unsafe{bindings::memset(x as *mut c_types::c_void, 0, size as c_types::c_ulong)};
+                return Some(x);
+            },
+            None => return None,
+        }
+        None
+    }
+
+    //éæ°åéç©ºé´
+    pub fn evl_realloc_chunk(&mut self, raw: *mut u8, old_size: size_t, new_size: size_t) -> Option<*mut u8> {
+        //å¼è¾æ°ç©ºé´
+        let ptr_op = self.evl_alloc_chunk(new_size);
+        match ptr_op {
+            Some(ptr) => {
+                unsafe{ bindings::memcpy(ptr as *mut c_types::c_void, raw as *mut c_types::c_void, old_size as c_types::c_ulong)  };
+                self.evl_free_chunk(raw);
+                return Some(ptr);
+            },
+            None => return None,
+        }
+        None
+    }
+    
+    #[inline]
+    fn addr_to_pagenr(&mut self, p: *mut u8) -> i32 {
+        ( (p as u32 - self.membase as u32) >> EVL_HEAP_PAGE_SHIFT ) as i32
+    }
+
+    fn add_free_range(&mut self, bsize:size_t, log2size: i32) -> Option<*mut u8> {
+        let pg_op = self.reserve_page_range(unsafe{rust_helper_align(bsize, EVL_HEAP_PAGE_SIZE)} as size_t);
+        let pg: i32;
+        match pg_op {
+            Some(x) => {
+                if x < 0 {
+                    return None;
+                }
+                pg = x;
+            },
+            None => return None,
+        }
+        
+        let pagemap = self.get_pagemap(pg);
+        if log2size != 0 {
+            unsafe {
+                (*pagemap).page_type = log2size as u32;
+                (*pagemap).pginfo.map = !gen_block_mask(log2size) | 1;
+                self.add_page_front(pg, log2size);
+            }
+        } else {
+            unsafe {
+                (*pagemap).page_type = 2;
+                (*pagemap).pginfo.bsize = bsize as u32;
+            }
+        }
+
+        self.used_size += bsize;
+        return Some(self.pagenr_to_addr(pg));
+    }
+
+    #[inline]
+    fn pagenr_to_addr(&mut self, pg: i32) -> *mut u8 {
+        addr_add_size(self.membase, (pg as size_t) << EVL_HEAP_PAGE_SHIFT) as *mut u8
+    }
+
+    pub fn search_size_ge(&mut self, size: size_t) -> Option<*mut evl_heap_range> {
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+
+    pub fn reserve_page_range(&mut self, size: size_t) -> Option<i32> {
+        let new_op = self.search_size_ge(size);
+        let mut new;
+        match new_op {
+            Some(x) => new = x,
+            None => return None,
+        }
+        let mut node_links = unsafe{addr_of_mut!((*new).size_node)};
+        let mut root = self.size_tree.as_mut().unwrap();
+        unsafe{bindings::rb_erase(node_links, root)};
+
+        // å¦æå¤§å°ç¸å
+        if (unsafe{(*new).size == size}) { 
+            // TODO: YOUR CODE HERE
+            return None;
+            // END OF YOUR CODE
+
+        }
+        // å¦åéè¦è£åª
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+    
+    pub fn move_page_back(&mut self, pg: i32, log2size: i32) {
+        let old = self.get_pagemap(pg); // è·åå¯¹åºçé¡µ
+        // TODO: YOUR CODE HERE
+        // END OF YOUR CODE
+    }
+    
+    fn move_page_front(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+
+        if self.buckets[ilog as usize] == (pg as u32) {
+            return;
+        }
+        
+        self.remove_page(pg, log2size);
+        self.add_page_front(pg, log2size);
+    }
+    
+    fn remove_page(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+        let old = self.get_pagemap(pg);
+        if pg == unsafe{(*old).next as i32} {
+            self.buckets[ilog as usize] = u32::MAX;
+        } else {
+            if pg == (self.buckets[ilog as usize] as i32) {
+                self.buckets[ilog as usize] = unsafe{(*old).next};
+            }
+            unsafe {
+                let prev = self.get_pagemap((*old).prev as i32);
+                (*prev).next = (*old).next;
+                let next = self.get_pagemap((*old).next as i32);
+                (*next).prev = (*old).prev;
+            }
+        }
+    }
+
+    pub fn add_page_front(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+        // pr_info!("add_page_front: ilog is {}",ilog);
+        let new = self.get_pagemap(pg);
+        if self.buckets[ilog as usize] == u32::MAX {
+            self.buckets[ilog as usize] = pg as u32;
+            unsafe {
+                (*new).prev = pg as u32;
+                (*new).next = pg as u32;
+            }
+            // pr_info!("add_page_front: pg is {}",pg);
+        } else {
+            let head = self.get_pagemap(self.buckets[ilog as usize] as i32);
+            unsafe {
+                // (*new).prev = self.buckets[ilog as usize];
+                // (*new).next = (*head).next;
+                // let next = self.get_pagemap((*new).next as i32);
+                // (*next).prev = pg as u32;
+                // (*next).next = pg as u32;
+
+                (*new).prev = (*head).prev;
+                (*new).next = self.buckets[ilog as usize];
+                let next = self.get_pagemap((*new).next as i32);
+                (*next).prev = pg as u32;
+                if (*next).next == self.buckets[ilog as usize]{
+                    (*next).next = pg as u32;
+                }
+                self.buckets[ilog as usize] = pg as u32;
+            }
+        }
+    }
+
+    #[inline]
+    pub fn get_pagemap(&self, pg: i32) -> *mut evl_heap_pgentry {
+        addr_add_size(self.pagemap.clone().unwrap() as *mut u8, 
+                        ((pg as u32) * (size_of::<evl_heap_pgentry>() as u32)) as size_t) 
+                            as *mut evl_heap_pgentry
+    }
+
+    pub fn evl_free_chunk(&mut self, block: *mut u8) {
+        let pgoff = ((block as usize) - (self.membase as usize)) as u32;
+	    let pg = (pgoff >> EVL_HEAP_PAGE_SHIFT) as i32;
+        let pagemap = self.get_pagemap(pg);
+        let page_type = unsafe { (*pagemap).page_type };
+        let bsize: size_t;
+        if page_type == 0x2 {
+            bsize = unsafe{ (*pagemap).pginfo.bsize as usize };
+            let addr = self.pagenr_to_addr(pg);
+            self.release_page_range(addr, bsize);
+        } else {
+            let log2size = page_type as i32;
+            bsize = (1 << log2size);
+            let boff = pgoff & !EVL_HEAP_PAGE_MASK;
+            if (boff & ((bsize - 1) as u32)) != 0 {
+                //è§£é
+                //raw_spin_unlock_irqrestore(&heap->lock, flags);
+                panic!()
+            }
+            // ä¸é¢çä»£ç æ¯å¤æ­åç§»éæ¯å¦æ­£ç¡®
+            // ç§»é¤ä¸ä¸ªfast block
+            // TODO: YOUR CODE HERE
+            let n = boff >> log2size;
+            let oldmap = unsafe{ (*pagemap).pginfo.map };
+            unsafe{ (*pagemap).pginfo.map &= !((1 as u32) << n) };
+            if unsafe{ (*pagemap).pginfo.map == !gen_block_mask(log2size) } {
+                self.remove_page(pg, log2size);
+                let addr = self.pagenr_to_addr(pg);
+                self.release_page_range(addr, EVL_HEAP_PAGE_SIZE as size_t);
+            } else if oldmap == u32::MAX {
+                self.move_page_front(pg, log2size);
+            }
+            // END OF YOUR CODE
+        }
+        self.used_size -= bsize;
+	    //raw_spin_unlock_irqrestore(&heap->lock, flags);
+    }
+
+    pub fn evl_destroy_heap(&mut self) {
+        let res = premmpt::running_inband();
+        match res {
+            Err(_) => {
+                pr_info!("warning: evl_destroy_heap not inband");
+            },
+            Ok(_) => (),
+        }
+        vmalloc::c_kzfree(self.pagemap.clone().unwrap() as *const c_types::c_void);
+    }
+
+}
+
+pub fn cleanup_shared_heap() {
+    unsafe {
+        evl_shared_heap.evl_destroy_heap();
+        vmalloc::c_vfree(evl_shared_heap.membase as *const c_types::c_void);
+    }
+}
+
+pub fn cleanup_system_heap() {
+    unsafe {
+        evl_system_heap.evl_destroy_heap();
+        vmalloc::c_vfree(evl_system_heap.membase as *const c_types::c_void);
+    }
+}
+
+pub fn gen_block_mask(log2size: i32) -> u32 {
+    return u32::MAX >> (32 - (EVL_HEAP_PAGE_SIZE >> log2size));
+} 
+
+pub static mut evl_system_heap: evl_heap = evl_heap {
+    membase: 0 as *mut u8,
+    addr_tree: None,
+    size_tree: None,
+    pagemap: None,
+    usable_size: 0,
+    used_size: 0,
+    buckets: [0; EVL_HEAP_MAX_BUCKETS as usize],
+    lock: None,
+};
+
+pub static mut evl_shared_heap: evl_heap = evl_heap {
+    membase: 0 as *mut u8,
+    addr_tree: None,
+    size_tree: None,
+    pagemap: None,
+    usable_size: 0,
+    used_size: 0,
+    buckets: [0; EVL_HEAP_MAX_BUCKETS as usize],
+    lock: None,
+};
+
+static mut evl_shm_size: usize = 0;
+
+pub fn init_system_heap() -> Result<usize> {
+    let size = 2048 * 1024;
+    let system = vmalloc::c_vmalloc(size as c_types::c_ulong);
+    match system {
+        Some(x) => {
+            let ret = unsafe{evl_system_heap.init(x as *mut u8, size as usize)};
+            match ret {
+                Err(_) => {
+                    vmalloc::c_vfree(x);
+                    return Err(crate::Error::ENOMEM);
+                },
+                Ok(_) => (),
+            }
+        },
+        None => return Err(crate::Error::ENOMEM),
+    }
+    pr_info!("rros_mem: init_system_heap success");
+    Ok(0)
+}
+
+pub fn init_shared_heap() -> Result<usize> {
+    let mut size: usize = CONFIG_EVL_NR_THREADS * size_of::<evl_user_window>()
+        + CONFIG_EVL_NR_MONITORS * 40;
+    size = mm::page_align(size)?;
+    mm::page_aligned(size)?;
+    let shared = vmalloc::c_kzalloc(size as u64);
+    match shared {
+        Some(x) => {
+            let ret = unsafe{evl_shared_heap.init(x as *mut u8, size as usize)};
+            match ret {
+                Err(_e) => {
+                    vmalloc::c_kzfree(x);
+                    return Err(_e);
+                },
+                Ok(_) => (),
+            }
+        },
+        None => return Err(crate::Error::ENOMEM),
+    }
+    unsafe{ evl_shm_size = size };
+    Ok(0)
+}
+
+pub fn evl_init_memory() -> Result<usize> {
+    let mut ret = init_system_heap();
+    match ret {
+        Err(_) => return ret,
+        Ok(_) => (),
+    }
+    ret = init_shared_heap();
+    match ret {
+        Err(_) => {
+            cleanup_system_heap();
+            return ret;
+        },
+        Ok(_) => (),
+    }
+    Ok(0)
+}
+
+pub fn evl_cleanup_memory() {
+    cleanup_shared_heap();
+	cleanup_system_heap();
+}
\ No newline at end of file
diff --git a/scripts/Makefile.build b/scripts/Makefile.build
index bb22acf84..b31db2a9d 100644
--- a/scripts/Makefile.build
+++ b/scripts/Makefile.build
@@ -300,7 +300,7 @@ quiet_cmd_rustc_o_rs = $(RUSTC_OR_CLIPPY_QUIET) $(quiet_modtag) $@
       cmd_rustc_o_rs = \
 	RUST_MODFILE=$(modfile) \
 	$(RUSTC_OR_CLIPPY) $(rust_flags) $(rust_cross_flags) \
-		-Zallow-features=allocator_api,bench_black_box,concat_idents,global_asm,try_reserve \
+		-Zallow-features=allocator_api,bench_black_box,concat_idents,global_asm,try_reserve,destructuring_assignment,array_map,new_uninit,array_map \
 		--extern alloc --extern kernel \
 		--crate-type rlib --out-dir $(obj) -L $(objtree)/rust/ \
 		--crate-name $(patsubst %.o,%,$(notdir $@)) $<; \
diff --git a/test1.py b/test1.py
index cd738511a..d5144adc6 100644
--- a/test1.py
+++ b/test1.py
@@ -2,8 +2,8 @@ import sys
 import re
 import os
 import threading
-times = 3000
-thread_nums = 10
+times = 10000
+thread_nums = 20
 error_nums = 0
 mutex = threading.Lock()
 def handler():
-- 
2.34.1

