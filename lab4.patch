From 07102682da8eac4382f45d124aa629a9cffc6da6 Mon Sep 17 00:00:00 2001
From: Qiu Qichen <cheemsFries@gmail.com>
Date: Wed, 14 Dec 2022 07:16:16 +0800
Subject: [PATCH] lab4

---
 .gitignore                                |    1 +
 kernel/dovetail.c                         |    1 -
 kernel/rros/init.rs                       |   35 +-
 kernel/rros/lab_mem_test/mod.rs           |  132 +++
 kernel/rros/lab_mem_test/rros_mem_test.rs |  559 +++++++++
 kernel/rros/lab_mem_test/tlsf_test.rs     |  657 +++++++++++
 kernel/rros/list_head.rs                  |    0
 kernel/rros/memory_test.rs                |  273 +++++
 kernel/rros/thread.rs                     |    3 +-
 kernel/rros/thread_test.rs                |   47 +-
 kernel/rros/tlsf.rs                       |  752 ++++++++++++
 kernel/rros/tlsf_raw_list.rs              |  220 ++++
 lab4.md                                   |  455 ++++++++
 rust/alloc/alloc_rros.rs                  |  210 ++++
 rust/alloc/lib.rs                         |    2 +-
 rust/alloc/rc.rs                          | 1226 ++++++++++++++++----
 rust/alloc/string.rs                      |  438 ++++---
 rust/alloc/sync.rs                        | 1257 +++++++++++++++++----
 rust/helpers.c                            |   19 +
 rust/kernel/allocator.rs                  |    7 +-
 rust/kernel/double_linked_list3.rs        |  366 ++++++
 rust/kernel/lib.rs                        |    7 +-
 rust/kernel/memory_rros.rs                |  644 +++++++++++
 scripts/Makefile.build                    |    2 +-
 test1.py                                  |    4 +-
 25 files changed, 6646 insertions(+), 671 deletions(-)
 create mode 100644 kernel/rros/lab_mem_test/mod.rs
 create mode 100644 kernel/rros/lab_mem_test/rros_mem_test.rs
 create mode 100644 kernel/rros/lab_mem_test/tlsf_test.rs
 create mode 100644 kernel/rros/list_head.rs
 create mode 100644 kernel/rros/memory_test.rs
 create mode 100644 kernel/rros/tlsf.rs
 create mode 100644 kernel/rros/tlsf_raw_list.rs
 create mode 100644 lab4.md
 create mode 100644 rust/alloc/alloc_rros.rs
 create mode 100644 rust/kernel/double_linked_list3.rs
 create mode 100644 rust/kernel/memory_rros.rs

diff --git a/.gitignore b/.gitignore
index aab78d7c3..279cf70f2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -50,6 +50,7 @@
 *.zst
 *.exp
 *.text
+*.py
 Module.symvers
 modules.order
 
diff --git a/kernel/dovetail.c b/kernel/dovetail.c
index 0bcb0f705..09aaacd78 100644
--- a/kernel/dovetail.c
+++ b/kernel/dovetail.c
@@ -278,7 +278,6 @@ extern void rust_handle_inband_event(enum inband_event_type event, void *data);
 
 void __weak handle_inband_event(enum inband_event_type event, void *data)
 {
-	pr_info("rust_handle_inband_event in");
 	rust_handle_inband_event(event,data);
 }
 
diff --git a/kernel/rros/init.rs b/kernel/rros/init.rs
index baeeec936..e9daa5f3f 100644
--- a/kernel/rros/init.rs
+++ b/kernel/rros/init.rs
@@ -1,5 +1,8 @@
 ﻿#![no_std]
 #![feature(allocator_api, global_asm)]
+#![feature(new_uninit)]
+#![feature(array_map)]
+#![feature(destructuring_assignment)]
 use alloc::vec;
 use kernel::{cpumask, irqstage, prelude::*, str::CStr,
      ThisModule, c_str, chrdev, file_operations::FileOperations,
@@ -30,6 +33,7 @@ mod list;
 mod list_test;
 mod lock;
 mod memory;
+mod memory_test;
 mod monitor;
 mod timer;
 mod timer_test;
@@ -43,9 +47,14 @@ mod factory;
 use factory::rros_early_init_factories;
 
 use crate::sched::this_rros_rq;
+use kernel::memory_rros::evl_init_memory;
 mod file;
 mod crossing;
 mod wait;
+mod tlsf;
+// mod lab_mem_test;
+mod lab_mem_test;
+mod tlsf_raw_list;
 #[cfg(CONFIG_NET)]
 mod net;
 module! {
@@ -135,7 +144,6 @@ fn set_rros_state(state: RrosRunStates) {
 fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>> {
     let res =
         irqstage::enable_oob_stage(CStr::from_bytes_with_nul("rros\0".as_bytes())?.as_char_ptr());
-    pr_info!("hello");
     match res {
         Ok(_o) => (),
         Err(_e) => {
@@ -143,15 +151,14 @@ fn init_core() -> Result<Pin<Box<chrdev::Registration<{factory::NR_FACTORIES}>>>
             return Err(kernel::Error::EINVAL);
         }
     }
-    pr_info!("hella");
-    // let res = init_memory(*sysheap_size_arg.read());
-    // match res {
-    //     Ok(_o) => (),
-    //     Err(_e) => {
-    //         pr_warn!("memory init wrong");
-    //         return Err(_e);
-    //     }
-    // }
+    let res = evl_init_memory();
+    match res {
+        Ok(_o) => (),
+        Err(_e) => {
+            pr_warn!("memory init wrong");
+            return Err(_e);
+        }
+    }
     let res = rros_early_init_factories(&THIS_MODULE);
     let fac_reg;
     match res {
@@ -246,6 +253,11 @@ fn test_sched() {
 fn test_fifo() {
     fifo_test::test___rros_enqueue_fifo_thread();
 }
+fn test_mem() {
+    lab_mem_test::tlsf_test::test_tlsf();
+    lab_mem_test::rros_mem_test::test_evl_heap();
+}
+
 
 impl KernelModule for Rros {
     fn init() -> Result<Self> {
@@ -296,7 +308,8 @@ impl KernelModule for Rros {
         // test_clock();
         // test_sched();
         // test_fifo();
-        test_thread();
+        // test_thread();
+        test_mem();
         //test_double_linked_list();
         match res {
             Ok(_o) => {
diff --git a/kernel/rros/lab_mem_test/mod.rs b/kernel/rros/lab_mem_test/mod.rs
new file mode 100644
index 000000000..6489fa650
--- /dev/null
+++ b/kernel/rros/lab_mem_test/mod.rs
@@ -0,0 +1,132 @@
+use core::fmt::{Display,Formatter};
+use kernel::{bindings};
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use core::fmt;
+
+
+#[derive(Debug, Clone)]
+pub struct TestFailed{
+    file: &'static str,
+    line: u32,
+    col: u32,
+    msg: &'static str,
+}
+
+impl TestFailed {
+    fn new_without_msg(file: &'static str, line: u32, col: u32) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg: "",
+        }
+    }
+    fn new(file: &'static str, line: u32, col: u32, msg: &'static str) -> Self {
+        TestFailed {
+            file,
+            line,
+            col,
+            msg,
+        }
+    }
+}
+
+impl fmt::Display for TestFailed {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        if self.msg.is_empty(){
+            write!(f, "Test failed on {}:{}:{}",self.file, self.line, self.col)
+        }else{
+            write!(f, "Test failed on {}:{}:{}  :  {}",self.file, self.line, self.col, self.msg)
+        }
+    }
+}
+macro_rules! test_failed_here {
+    () => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""});
+    };
+    ($msg:expr) => {
+        Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg});
+    };
+}
+
+macro_rules! test {
+    ($left:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $msg:expr) => {
+        match &$left {
+            (left_val) => {
+                if !(*left_val == true) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+
+
+macro_rules! test_eq {
+    ($left:expr, $right:expr $(,)?) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(),msg:""})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+    ($left:expr, $right:expr, $msg:expr) => {
+        match (&$left, &$right) {
+            (left_val, right_val) => {
+                if !(*left_val == *right_val) {
+                    Err(TestFailed{file: file!(), line: line!(), col: column!(), msg: $msg})
+                }else{
+                    Ok(())
+                }
+            }
+        }
+    };
+}
+type Result<T> = core::result::Result<T, TestFailed>;
+#[inline]
+fn handle_and_print_result(test_name:&'static str, r: Result<()>){
+    match r{
+        Ok(())=>{pr_crit!("[RAND]Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+}
+// macro_rules! pr_color_crit {
+//     ($color:expr, $($arg:tt)*) => {{
+//         pr_crit!("\x1b[{}m",$color);
+//         pr_crit!($($arg)*);
+//         pr_crit!("\x1b[0m");
+//     }};
+// }
+// macro_rules! pr_green_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(32,$($arg)*);
+//     }};
+// }
+// macro_rules! pr_red_crit {
+//     ($($arg:tt)*) => {{
+//         pr_color_crit!(31,$($arg)*);
+//     }};
+// }
+
+
+
+
+pub mod rros_mem_test;
+pub mod tlsf_test;
\ No newline at end of file
diff --git a/kernel/rros/lab_mem_test/rros_mem_test.rs b/kernel/rros/lab_mem_test/rros_mem_test.rs
new file mode 100644
index 000000000..e98f95148
--- /dev/null
+++ b/kernel/rros/lab_mem_test/rros_mem_test.rs
@@ -0,0 +1,559 @@
+
+use alloc::alloc_rros::RrosMem;
+use alloc::format;
+use kernel::memory_rros::{evl_heap, evl_heap_range, addr_add_size};
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use alloc::boxed::Box;
+use alloc::alloc::Global;
+use core::ptr::{null, null_mut};
+use core::{ptr, slice};
+use alloc::alloc::Layout;
+use alloc::alloc::alloc;
+use core::fmt::{Display,Formatter};
+use crate::{bindings};
+use super::{TestFailed,handle_and_print_result};
+type Result<T> = core::result::Result<T, TestFailed>;
+
+#[inline]
+fn handle_alloc_ptr_result_default(test_name:&'static str, test_fn:fn(*mut c_types::c_void,usize) -> Result<()>){
+    let size = mm::page_align(1024).unwrap();
+    let ptr = vmalloc::c_vmalloc(size as c_types::c_ulong).unwrap();
+    let r = test_fn(ptr,size);
+    match r{
+        Ok(())=>{pr_crit!("[RAND]Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+    vmalloc::c_vfree(ptr as *const c_types::c_void);
+}
+
+pub fn test_evl_heap(){
+    pr_crit!("--------------test rros dynamic memory allocator start------------------");
+
+    handle_alloc_ptr_result_default("insert_by_size",test_insert_bysize);
+    handle_alloc_ptr_result_default("insert_by_addr",test_insert_byaddr);
+    handle_alloc_ptr_result_default("search_right_mergeable",test_search_right_mergeable);
+    handle_alloc_ptr_result_default("release_page_range",test_release_page_range);
+    handle_alloc_ptr_result_default("search_size_ge",test_search_size_ge);
+    handle_alloc_ptr_result_default("reserve_page_range",test_reserve_page_range);
+    handle_alloc_ptr_result_default("move_page_back",test_move_page_back);
+    handle_alloc_ptr_result_default("heap_alloc",test_heap_alloc);
+    handle_alloc_ptr_result_default("heap_alloc_small",test_heap_alloc_small);
+    handle_alloc_ptr_result_default("heap_write_then_free", test_heap_alloc_write_then_free);
+    handle_alloc_ptr_result_default("heap_free", test_heap_free);
+
+    handle_and_print_result("heap_box",test_rrosmem_box());
+
+    
+    // handle_alloc_ptr_result_default("test_init",test_search_right_mergeable);
+    // 后面可以使用init
+    pr_crit!("--------------test rros dynamic memory allocator end------------------");
+
+}
+
+pub fn get_uninit_heap(ptr:*mut c_types::c_void,size:usize)->evl_heap{
+    // unsafe
+    let mut evl_heap: evl_heap = evl_heap {
+        membase: 0 as *mut u8,
+        addr_tree: None,
+        size_tree: None,
+        pagemap: None,
+        usable_size: 0,
+        used_size: 0,
+        buckets: [0; 5 as usize],
+        lock: None,
+    };
+    
+    evl_heap.membase = unsafe{ ptr as *mut u8};
+    evl_heap.usable_size = size;
+    evl_heap.used_size = 0;
+    evl_heap.size_tree = Some(bindings::rb_root::default());
+    evl_heap.addr_tree = Some(bindings::rb_root::default());
+    evl_heap
+}
+pub fn clear_heap(heap:&mut evl_heap){
+    heap.used_size = 0;
+    heap.size_tree = Some(bindings::rb_root::default());
+    heap.addr_tree = Some(bindings::rb_root::default());
+}
+pub fn get_evl_heap(ptr:*mut c_types::c_void,size:usize)->Result<evl_heap>{
+    let mut evl_heap: evl_heap = evl_heap {
+        membase: 0 as *mut u8,
+        addr_tree: None,
+        size_tree: None,
+        pagemap: None,
+        usable_size: 0,
+        used_size: 0,
+        buckets: [0; 5 as usize],
+        lock: None,
+    };
+    let ret = unsafe{evl_heap.init(ptr as *mut u8, size as usize)};
+    match ret {
+        Err(_) => {
+            test_failed_here!()
+        },
+        Ok(_) => Ok(evl_heap)
+    }
+    
+}
+const EVL_HEAP_PAGE_SIZE:usize = 512;
+
+
+pub fn check_if_size_in_tree(root:&*mut bindings::rb_node,size:u64) -> bool {
+    unsafe {
+        let mut node = root;
+        let mut parent = core::ptr::null_mut();
+        if ((*container_of!(*node, evl_heap_range, size_node)).size as u64==size){ return true};
+        while !node.is_null() {
+            let p = container_of!(*node, evl_heap_range, size_node);
+            parent = *node;
+            if size == ((*p).size as u64) {
+                return true;
+            } else if size < ((*p).size as u64) {
+                node = &mut (*parent).rb_left;
+            } else {            
+                node = &mut (*parent).rb_right;
+            }
+        }
+        return false;
+
+    }
+}
+
+pub fn check_if_addr_in_tree(root:&*mut bindings::rb_node,addr:u64) -> bool {
+    unsafe {
+        let mut node = root;
+        let mut parent = core::ptr::null_mut();
+        if (container_of!(*node, evl_heap_range, addr_node) as u64==addr){ return true};
+        while !node.is_null() {
+            let p = container_of!(*node, evl_heap_range, addr_node);
+            parent = *node;
+            if addr == (p as u64) {
+                return true;
+            } else if addr < (p as u64) {
+                node = &mut (*parent).rb_left;
+            } else {            
+                node = &mut (*parent).rb_right;
+            }
+        }
+        return false;
+
+    }
+}
+pub fn next_addr(node: *const bindings::rb_node) -> Option<*const evl_heap_range>{
+    unsafe{
+        let next:*const bindings::rb_node = bindings::rb_next(node) as *const bindings::rb_node;
+        if next.is_null(){
+            None
+        }else{
+            Some(container_of!(next, evl_heap_range, addr_node))
+        }
+    }
+}
+
+pub fn next_size(node: *const bindings::rb_node) -> Option<*const evl_heap_range>{
+    unsafe{
+        let next:*const bindings::rb_node = bindings::rb_next(node) as *const bindings::rb_node;
+        if next.is_null(){
+            None
+        }else{
+            Some(container_of!(next, evl_heap_range, size_node))
+        }
+    }
+}
+
+// pub fn walk_size_tree(root: *const bindings::rb_root,func:fn(*const evl_heap_range)){
+//     let node = unsafe{&((*root).rb_node) as *const bindings::rb_node};
+//     loop{
+//         bindings::rb_next(node) as *const bindings::rb_node
+//     }
+// }
+pub fn print_size_tree(heap:&evl_heap){
+    let root = heap.size_tree.as_ref().unwrap();
+    let mut node = unsafe{(*root).rb_node as *const bindings::rb_node};
+    loop{
+        let p = unsafe{container_of!(&*node, evl_heap_range, size_node)};
+        pr_info!("size:{}",(*p).size);
+        node = unsafe{bindings::rb_next(&*node) as *const bindings::rb_node};
+        if node.is_null(){
+            break;
+        }
+    }
+}
+pub fn test_insert_bysize(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,3,4,5].map(|s|s*EVL_HEAP_PAGE_SIZE); // SAFETY: 需要保证size>偏移
+    if size_array[0] != 1024{
+        panic!();
+    }
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let mut ptr = ptr as *mut evl_heap_range;
+        for i in size_array{
+            (*ptr).size = i;
+            heap.insert_range_bysize(ptr);
+            let size_root = heap.size_tree.as_ref().unwrap();
+            test!(check_if_size_in_tree(&size_root.rb_node, i as u64),"insert failed")?;
+            ptr = ((ptr as *mut u8).offset(i as isize) as * mut evl_heap_range);
+        }
+        for i in [512,4096,8192]{
+            let size_root = heap.size_tree.clone().unwrap();
+            test!(!check_if_size_in_tree(&size_root.rb_node, i as u64),"insert failed")?;
+        }
+    }
+    Ok(())
+}
+
+pub fn test_insert_byaddr(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,3,4,5].map(|s|s*EVL_HEAP_PAGE_SIZE); // SAFETY: 需要保证size>偏移
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let mut ptr = ptr as *mut evl_heap_range;
+        for i in size_array{
+            (*ptr).size = i;
+            heap.insert_range_byaddr(ptr);
+            let addr_root = heap.addr_tree.as_ref().unwrap();
+            test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64),"addr insert failed")?;
+            ptr = ((ptr as *mut u8).offset(i as isize) as * mut evl_heap_range);
+        }
+    }
+    Ok(())
+}
+
+pub fn test_search_right_mergeable(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let ptr = ptr as *mut evl_heap_range;
+        (*ptr).size += size;
+        test!(heap.search_right_mergeable(ptr).is_none(),"test_search_right_mergeable failed 1");
+        
+        // 第一片是4Page，第二片是2Page
+        let size1 = EVL_HEAP_PAGE_SIZE * 4;
+        let size2 = EVL_HEAP_PAGE_SIZE * 2;
+        let block1 = ptr;
+        (*block1).size = size1;
+        let block2 = unsafe{ addr_add_size(ptr as *mut u8, size1) as *mut evl_heap_range };
+        (*block2).size = size2;
+        heap.addr_tree.as_mut().unwrap();
+        heap.insert_range_bysize(block1);
+        heap.insert_range_byaddr(block1);
+        heap.insert_range_bysize(block2);
+        heap.insert_range_byaddr(block2);
+        let result = heap.search_right_mergeable(block1);
+        test!(result.is_some(),"merge 1");
+        test_eq!(result.unwrap(),block2,"merge 1 failed");
+
+        // 现在切成两片，但是不是邻近的。
+        clear_heap(&mut heap);
+        let size1 = EVL_HEAP_PAGE_SIZE * 4;
+        let size2 = EVL_HEAP_PAGE_SIZE * 2;
+
+        let block1 = ptr;
+        (*block1).size = size1;
+        let block2 = unsafe{ addr_add_size(ptr as *mut u8, size1+EVL_HEAP_PAGE_SIZE) as *mut evl_heap_range };
+        (*block2).size = size2;
+        heap.addr_tree.as_mut().unwrap();
+        heap.insert_range_bysize(block1);
+        heap.insert_range_byaddr(block1);
+        heap.insert_range_bysize(block2);
+        heap.insert_range_byaddr(block2);
+        test!(result.is_none());
+    }
+    Ok(())
+}
+
+pub fn test_release_page_range(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let size_array = [2,2,2].map(|s|s*EVL_HEAP_PAGE_SIZE);
+    unsafe{
+        let mut heap = get_uninit_heap(ptr,size);
+        let ptr = ptr as *mut evl_heap_range;
+        // 这里只测试3种情况
+        // 没有可合并的
+        (*ptr).size = size;
+        heap.release_page_range(ptr as *mut u8,size); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64));
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, size as u64));
+
+        // left可合并
+        clear_heap(&mut heap);
+        (*ptr).size = size_array[0];
+        heap.insert_range_bysize(ptr);// 预先插入左边
+        heap.insert_range_byaddr(ptr);// 预先插入左边
+        let block2 = ((ptr as *mut u8).offset(size_array[0] as isize) as * mut evl_heap_range);
+        (*block2).size = size_array[1];
+        heap.release_page_range(block2 as *mut u8, (*block2).size); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, ptr as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block2 as u64));
+
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size_array[0] + size_array[1]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[0] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[1] as u64));
+
+
+        // left，right都可以合并
+        clear_heap(&mut heap);
+        let block1 = ptr;
+        (*block1).size = size_array[0];
+        heap.insert_range_bysize(block1);// 预先插入左边
+        heap.insert_range_byaddr(block1);// 预先插入左边
+        let block2 = ((ptr as *mut u8).offset(size_array[0] as isize) as * mut evl_heap_range);
+        (*block2).size = size_array[1]; 
+        let block3 = ((block2 as *mut u8).offset(size_array[1] as isize) as * mut evl_heap_range);
+        (*block3).size = size_array[2]; 
+        heap.insert_range_bysize(block3);// 预先插入右边
+        heap.insert_range_byaddr(block3);// 预先插入右边
+
+        heap.release_page_range(block2 as *mut u8,size_array[1]); // call here
+        let addr_root = heap.addr_tree.as_ref().unwrap();
+        test!(check_if_addr_in_tree(&addr_root.rb_node, block1 as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block2 as u64));
+        test!(!check_if_addr_in_tree(&addr_root.rb_node, block3 as u64));
+
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size_array[0]+size_array[1]+size_array[2]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, (size_array[0]+size_array[1]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, (size_array[1]+size_array[2]) as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[0] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[1] as u64));
+        test!(!check_if_size_in_tree(&size_root.rb_node, size_array[2] as u64));
+    }
+    Ok(())
+}
+
+pub fn test_search_size_ge(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    let split_area = EVL_HEAP_PAGE_SIZE;
+    unsafe{
+        let mut heap = get_uninit_heap(ptr, size);
+        let block_array = [1,3,5,7].map(|s|s*EVL_HEAP_PAGE_SIZE);
+        let mut block = ptr as *mut evl_heap_range;
+        for each_size in block_array{
+            (*block).size = each_size;
+            heap.insert_range_byaddr(block);
+            heap.insert_range_bysize(block);
+            block = (block as *mut u8).offset(each_size as isize) as * mut evl_heap_range;
+        };
+        // should not use block[7]
+
+        // test equal
+        let result = heap.search_size_ge(block_array[0]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[0]);
+
+        let result = heap.search_size_ge(block_array[1]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[1]);
+        
+        let result = heap.search_size_ge(block_array[2]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[2]);
+
+        let result = heap.search_size_ge(block_array[3]);
+        test!(result.is_some());
+        test!((*result.unwrap()).size >= block_array[3]);
+
+        let result = heap.search_size_ge(block_array[3]+10);
+        test!(result.is_none());
+
+        // let result = heap.search_size_ge(4*EVL_HEAP_PAGE_SIZE);
+        // test!(result.is_some());
+        // test_but_allow_failing!((*result.unwrap()).size != 5*EVL_HEAP_PAGE_SIZE,"not best fit");
+
+        // let result = heap.search_size_ge(5*EVL_HEAP_PAGE_SIZE);
+        // test!(result.is_some());
+        // test_but_allow_failing!((*result.unwrap()).size != 5*EVL_HEAP_PAGE_SIZE,"not best fit");
+    }
+    Ok(())
+}
+pub fn test_reserve_page_range(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size);
+        test!(heap.is_ok());
+        let mut heap = heap.unwrap();
+        let alloc1 = EVL_HEAP_PAGE_SIZE * 2;
+        let a = heap.reserve_page_range(alloc1);
+        test!(a.is_some());
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size - alloc1) as u64));
+        
+        let alloc2 = EVL_HEAP_PAGE_SIZE * 4;
+        let a = heap.reserve_page_range(alloc2);
+        test!(a.is_some());
+        let size_root = heap.size_tree.as_ref().unwrap();
+        test!(check_if_size_in_tree(&size_root.rb_node, (size - alloc1 -alloc2) as u64));
+
+    }
+    Ok(())
+}
+pub fn test_move_page_back(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size);
+        test!(heap.is_ok());
+        let mut heap =heap.unwrap();
+        heap.add_page_front(16,4);
+        heap.add_page_front(17,4);
+        heap.add_page_front(18,4);
+        heap.add_page_front(19,4);
+        heap.add_page_front(20,4);
+        // let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        // 20 -> 19 -> 18 -> 17 -> 16
+
+        heap.move_page_back(20, 4);// 19 -> 18 -> 17 -> 16 -> 20
+        let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        let start = 19;
+        let mut counter = 0;
+        let mut large_array = [0;20];
+        large_array[counter] = start;
+        let mut node = heap.get_pagemap(start as i32);
+        while  counter != 5{
+            counter+=1;
+            large_array[counter] = (*node).next;
+            node = heap.get_pagemap((*node).next as i32);
+        }
+        test_eq!(large_array[counter],20);
+
+        // 19  -> 17 -> 16 -> 20 -> 18
+        heap.move_page_back(18, 4);
+        let p = [16,17,18,19,20].map(|a|heap.get_pagemap(a));
+        let start = 19;
+        let mut counter = 0;
+        let mut large_array = [0;20];
+        large_array[counter] = start;
+        let mut node = heap.get_pagemap(start as i32);
+        while  counter != 5{
+            counter+=1;
+            large_array[counter] = (*node).next;
+            node = heap.get_pagemap((*node).next as i32);
+        }
+        test_eq!(large_array[counter],18);
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        for i in 0..5{
+            let r = heap.evl_alloc_chunk(1000); // alloc 1024
+            test!(r.is_some());
+        }
+        for i in 0..5{
+            let r = heap.evl_alloc_chunk(256); // alloc 1024
+            test!(r.is_some());
+        }
+    }
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        // test alloc big
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(1000); // alloc 1024
+            test!(r.is_some());
+        }
+        let r = heap.evl_alloc_chunk(1000); // alloc 1024
+        test!(r.is_none());
+    }
+    unsafe{
+        let mut heap = get_evl_heap(ptr, size).unwrap();
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(256); 
+            test!(r.is_some());
+        }
+        for i in 0..64{
+            let r = heap.evl_alloc_chunk(128); 
+            test!(r.is_some());
+        }
+        for i in 0..1024{
+            let r = heap.evl_alloc_chunk(64); 
+            test!(r.is_some());
+        }
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc_small(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    // 测试份分配小块
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        for small_block_size in [16,32,64,128,256]{
+            let num = EVL_HEAP_PAGE_SIZE / small_block_size;
+            let mut page = 0;
+            for i in 0..num{
+                let r = heap.evl_alloc_chunk(small_block_size);
+                test!(r.is_some());
+                let r = unsafe{(r.unwrap() as usize - heap.membase as usize) >> 9};
+                if i == 0{
+                    page = r;
+                }else{
+                    test_eq!(r,page);
+                }
+            }
+        }
+    }
+    Ok(())
+}
+
+pub fn test_heap_alloc_write_then_free(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    // 测试分配后写入，然后释放
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        let block_sizes = [2,3,4,5,6,7,8,9,10,11,12,13].map(|s|s*EVL_HEAP_PAGE_SIZE);
+        let mut ptrs = [null_mut();20];
+        let magic = 0x2A;
+        for i in 0..block_sizes.len(){
+            let block_size = block_sizes[i];
+            let mut a = heap.evl_alloc_chunk(block_size);
+            test!(a.is_some());
+            let mut a = a.unwrap();
+            ptrs[i] = a;
+            unsafe{
+                let buf: &mut [u8] = core::slice::from_raw_parts_mut(a, block_size as usize);
+                buf.fill(magic);
+            }
+        }
+        for i in 0..block_sizes.len(){
+            heap.evl_free_chunk(ptrs[i]);
+        }
+    }
+    Ok(())
+}
+
+
+pub fn test_rrosmem_box()->Result<()>{
+    for i in 0..10{
+        let mut a = Box::try_new_in([0;256],RrosMem).unwrap();
+        a[0] = 2;
+        a[1] = 3;
+    }
+    Ok(())
+}
+
+pub fn test_heap_free(ptr:*mut c_types::c_void ,size:usize)->Result<()>{
+    unsafe{
+        let mut heap = get_evl_heap(ptr,size).unwrap();
+        let alloc_big = [1230,2340,4560,7890,1098,2356,2330,1140,5140,1270];
+        let mut alloc_big_ptrs = [null_mut()  ;20];
+        let alloc_small = [16,32,64,114,51,4,16,32,64,114];
+        let mut page =[0;10];
+        let mut alloc_small_ptrs= [null_mut() ;20];
+        for i in 0..10{
+            for j in 0..10{
+                let p = heap.evl_alloc_chunk(alloc_small[j]);
+                test!(p.is_some());
+                alloc_small_ptrs[j] = p.unwrap();
+                page[j] = (p.unwrap() as usize - heap.membase as usize) >> 9;
+            }
+            let p = heap.evl_alloc_chunk(alloc_big[i]);
+            test!(p.is_some());
+            alloc_big_ptrs[i] = p.unwrap();
+
+            for j in 0..10{
+                heap.evl_free_chunk(alloc_small_ptrs[j] );
+            }
+        }
+        for i in 0..alloc_big.len(){
+            heap.evl_free_chunk(alloc_big_ptrs[i]);
+        }
+    }
+    Ok(())
+}
diff --git a/kernel/rros/lab_mem_test/tlsf_test.rs b/kernel/rros/lab_mem_test/tlsf_test.rs
new file mode 100644
index 000000000..66425d3f4
--- /dev/null
+++ b/kernel/rros/lab_mem_test/tlsf_test.rs
@@ -0,0 +1,657 @@
+
+use alloc::format;
+use kernel::{vmalloc, pr_info, c_types, pr_crit, mm, container_of};
+use crate::{tlsf::*, tlsf_raw_list};
+use alloc::boxed::Box;
+use alloc::alloc::{Global, dealloc};
+use core::convert::AsMut;
+use core::mem::size_of;
+use core::ptr::{null, null_mut};
+use core::{ptr, slice};
+use alloc::alloc::Layout;
+use alloc::alloc::alloc;
+use kernel::bindings;
+use super::{TestFailed,handle_and_print_result};
+
+type Result<T> = core::result::Result<T, TestFailed>;
+#[repr(C)]
+#[repr(C)]
+pub struct MockBlockHeader {
+    pub prev_phys_block: *mut MockBlockHeader,
+    magic : u32,
+    size: u32, 
+}
+
+impl MockBlockHeader {
+    pub fn get_size(&self) -> usize {
+        self.size as usize & !(BLOCK_HEADER_FREE_BIT | BLOCK_HEADER_PREV_FREE_BIT)
+    }
+    pub fn set_size(&mut self, size: usize) {
+        let old_size = self.size;
+        self.size = size as u32 | (old_size & (BLOCK_HEADER_FREE_BIT as u32 | BLOCK_HEADER_PREV_FREE_BIT as u32));
+        self.magic = BLOCK_HEADER_MAGIC as u32;
+    }
+    pub fn set_prev_free(&mut self) {
+        self.size |= BLOCK_HEADER_PREV_FREE_BIT as u32;
+    }
+    pub fn set_prev_used(&mut self) {
+        self.size  &= !(BLOCK_HEADER_PREV_FREE_BIT) as u32;
+    }
+    pub fn is_prev_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_PREV_FREE_BIT) != 0
+    }
+    pub fn set_free(&mut self) {
+        self.size |= BLOCK_HEADER_FREE_BIT as u32; 
+    }
+    pub fn set_used(&mut self) {
+        self.size &= !(BLOCK_HEADER_FREE_BIT) as u32;
+    }
+    pub fn is_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_FREE_BIT) != 0
+    }
+    pub fn data_ptr<T>(&self) -> *mut T {
+        unsafe { (self as *const _ as *mut u8).offset(size_of::<Self>() as isize) as *mut T }
+    }
+    pub fn link_next(&mut self) -> *mut MockBlockHeader {
+        unsafe {
+            let next = ptr::NonNull::new(
+                (self.data_ptr() as *mut u8)
+                    .offset((self.get_size() - BLOCK_HEADER_OVERHEAD) as isize)
+                    as *mut MockBlockHeader)
+            .unwrap()
+            .as_mut();
+            next.prev_phys_block = self as *const _ as *mut MockBlockHeader;
+            return (next as *mut MockBlockHeader)
+        }
+    }
+}
+
+macro_rules! as_blockheader_mut {
+    ($mock:expr) => {
+        unsafe{
+            &mut *($mock as *const MockBlockHeader as *mut FreeBlockHeader)
+        }
+    };
+}
+/// 遍历一个内存池
+fn walk_pool(mut ptr: *mut MockBlockHeader, mut walk: impl FnMut(MockBlockHeader)) {
+    unsafe {
+        loop {
+            let block = ptr::read(ptr as *mut MockBlockHeader);
+            let size = block.get_size();
+            walk(block);
+            if size == 0 {
+                return;
+            }
+            ptr = (ptr as *mut u8)
+                .offset((size + (size_of::<MockBlockHeader>() - BLOCK_HEADER_OVERHEAD)) as isize)
+                as *mut MockBlockHeader;
+        }
+    }
+}
+
+/// 调试使用。打印出每个block的信息
+fn walk_debug(mut ptr: *mut MockBlockHeader) {
+    let mut counter = 0;
+    walk_pool(ptr, |block| {
+        counter += 1;
+        let size = block.get_size();
+        let is_prev_free = block.is_prev_free();
+        let is_free = block.is_free();
+        if (size != 0) {
+            pr_info!(
+                "block#{},size={},is_prev_free={},is_free={}",
+                counter, size, is_prev_free, is_free
+            );
+        } else {
+            pr_info!("sentinel,is_prev_free={},is_free={}", is_prev_free, is_free);
+        }
+    })
+}
+
+pub fn test_tlsf(){
+    pr_info!("--------------test tlsf begin-----------------");
+    handle_and_print_result("tlsf c-style list example",test_c_style_list()); // 5
+    handle_and_print_result("tlsf blockHeader(split)",test_blockHeader_split()); // 10
+    handle_and_print_result("tlsf blockHeader(absorb)",test_blockHeader_absorb());// 5
+    handle_and_print_result("tlsf mapping",test_mapping_insert()); // 5
+    handle_and_print_result("tlsf heap init(add pool)",test_init()); // 5
+
+    handle_and_print_result("tlsf malloc",test_malloc());// 15
+    handle_and_print_result("tlsf free",test_free()); // 15
+    handle_and_print_result("tlsf multiple allocation",test_multiple_alloc()); // 10
+    handle_and_print_result("tlsf torture", test_torture()); // 10
+    pr_info!("tlsf test ok!");
+    init_tlsfheap();
+    pr_info!("initialized tlsf heap done");
+    handle_and_print_result("tlsf allocator",tlsf_allocator()); // 5
+    // a report 15
+    // pr_crit!("pass all tests of tlsf");
+    pr_info!("--------------test tlsf end------------------");
+}
+fn get_block_vector(ptr: *mut MockBlockHeader) -> (u32,Box<[Option<MockBlockHeader>;20]>){
+    let mut result : Box<[Option<MockBlockHeader>;20]>= unsafe{Box::try_new_uninit_in(Global).unwrap().assume_init()};
+    let mut counter = 0;
+    walk_pool(ptr,|block|{
+        result[counter] = Some(block);
+        counter+=1;
+    });
+    (counter as u32,result)
+}
+
+pub fn write_buffer(ptr: *mut u8, size:usize){
+    /// generate random number
+    /// we don't use rand generator because we have to wait it to be initialized
+    // kernel::random::getrandom(dst); // not check
+    let dst = unsafe{core::slice::from_raw_parts_mut(ptr,size)};
+    let b = &[b'B', b'U', b'P', b'T'];
+    let mut i = 0;
+    while i < dst.len() {
+        for &byte in b {
+            if i >= dst.len() {
+                break;
+            }
+            dst[i] = byte;
+            i += 1;
+        }
+    }
+}
+
+pub fn test_c_style_list() -> Result<()>{
+    extern "C"{
+        fn rust_helper_INIT_LIST_HEAD(list : *mut bindings::list_head);
+    }
+    unsafe{
+        let mut l1 = bindings::list_head::default();
+        let mut l2 = bindings::list_head::default();
+        rust_helper_INIT_LIST_HEAD(&mut l1 as *mut bindings::list_head);
+        rust_helper_INIT_LIST_HEAD(&mut l2 as *mut bindings::list_head);
+
+        let mut ex1 = Example{
+            val1 : 1,
+            val2 : 2,
+            list1 : bindings::list_head::default(),
+            list2 : bindings::list_head::default(),
+        };
+        let mut ex2 = Example{
+            val1 : 0,
+            val2 : 4,
+            list1 : bindings::list_head::default(),
+            list2 : bindings::list_head::default(),
+        };
+        list_example1_sort_decending_respectively(&mut l1, &mut l2, &mut ex1, &mut ex2);
+        let mut next = list_entry!(&l1,Example,list1);
+        test!(core::ptr::eq(&ex1 as *const Example,next));
+        next = list_entry!(next,Example,list1);
+        test!(core::ptr::eq(&ex2 as *const Example,next));
+
+        let mut next = list_entry!(&l2,Example,list2);
+        test!(core::ptr::eq(&ex2 as *const Example,next));
+        next = list_entry!(next,Example,list2);
+        test!(core::ptr::eq(&ex1 as *const Example,next));
+    }
+
+    Ok(())
+}
+
+pub fn test_blockHeader_split() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap()) as *mut MockBlockHeader;
+        let block = ptr::NonNull::new(ptr)
+        .unwrap()
+        .as_mut();
+        block.set_size(32);
+        block.set_free();
+        block.set_prev_used();
+       
+        let mut next = (&mut *block.link_next());
+        next.set_size(0);
+        next.set_used();
+        next.set_prev_free();
+
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),32)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+        as_blockheader_mut!(block).split(16);
+
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,3)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),16)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),8)?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0)?;
+
+        as_blockheader_mut!(block).split(8);// split failed
+
+        let (len,_result) = get_block_vector(ptr);
+        test_eq!(len,3)?;
+
+        dealloc(ptr as *mut u8, Layout::from_size_align(size, 8).unwrap());
+    }
+
+    // pr_info!("end of test MockBlockHeader");
+    Ok(())
+}
+
+pub fn test_blockHeader_absorb() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap()) as *mut MockBlockHeader;
+        let mut block = ptr::NonNull::new(ptr)
+        .unwrap()
+        .as_mut();
+
+        let mut blocks : [*mut MockBlockHeader;5] = [core::ptr::null_mut();5];
+        for i in 0..5{
+            block.set_size(64);
+            block.set_free();
+            if i==0{
+                block.set_prev_used();
+            }else{
+                block.set_prev_free();
+            }
+            blocks[i] = block as *mut MockBlockHeader;
+            block = (&mut *block.link_next());
+        }
+        block.set_size(0);
+        block.set_used();
+        block.set_prev_free();
+
+        // init done;
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,6)?;
+        (*(blocks[0] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,5)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),128+BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true)?;
+        test!(core::ptr::eq(result[1].as_ref().unwrap().prev_phys_block, blocks[0]))?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),64)?;
+
+        // 0 - 128+8
+        // 1(不存在)
+        // 2 - 64
+        // 3 - 64
+        // 4 - 64
+        // 5 - 0
+
+        (*(blocks[2] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,4)?;
+        // 0 - 128+8
+        // 1
+        // 2 - 128+8
+        // 3 
+        // 4 - 64
+        // 5 - 0
+        test_eq!(result[1].as_ref().unwrap().get_size(),128+BLOCK_HEADER_OVERHEAD)?;
+        test!(core::ptr::eq(result[2].as_ref().unwrap().prev_phys_block, blocks[2]))?;
+
+        (*(blocks[4] as *mut FreeBlockHeader)).absorb();
+        let (len,result) = get_block_vector(ptr);
+        test_eq!(len,4)?;
+        // 0 - 128+8
+        // 1
+        // 2 - 128+8
+        // 3 
+        // 4 - 64
+        // 5 - 0
+        test_eq!(result[2].as_ref().unwrap().get_size(),64)?;
+
+        dealloc(ptr as *mut u8, Layout::from_size_align(size, 8).unwrap());
+    }
+    // pr_info!("end of test MockBlockHeader");
+    Ok(())
+}
+
+pub fn test_init() -> Result<()>{
+    unsafe{
+        let size1 = 1024;
+        let layout = Layout::from_size_align(size1, 8);
+        let ptr1 = alloc(layout.unwrap()) as *mut u8;
+
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr1 as  *mut u8, size1 as usize);
+        let ptr1 = ptr1.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+        let (len,result) = get_block_vector(ptr1 as *mut MockBlockHeader);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size1 as usize-2*BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free (),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+
+        let size2 = 1024*4;
+        let layout = Layout::from_size_align(size2, 8);
+        let ptr2 = alloc(layout.unwrap()) as *mut u8;
+
+        control.add_pool(ptr2 as  *mut u8, size2 as usize);
+        let ptr2 = ptr2.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+        let (len,result) = get_block_vector(ptr2 as *mut MockBlockHeader);
+        test_eq!(len,2)?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size2 as usize-2*BLOCK_HEADER_OVERHEAD)?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true)?;
+        test_eq!(result[0].as_ref().unwrap().is_prev_free(),false)?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0)?;
+        
+    }
+    // pr_info!("test tlsf init ok!");
+    Ok(())
+}
+
+pub fn test_mapping_insert() -> Result<()>{
+    let (a,b) = mapping_insert(234);
+    test_eq!(a,0,"mapping_insert(234) failed")?;
+    test_eq!(b,29,"mapping_insert(234) failed")?;
+    let (a,b) = mapping_insert(1234);
+    test_eq!(a,3,"mapping_insert(1234) failed")?;
+    test_eq!(b,6,"mapping_insert(1234) failed")?;
+    let (a,b) = mapping_insert(560);
+    test_eq!(a,2,"mapping_insert(560) failed")?;
+    test_eq!(b,3,"mapping_insert(560) failed")?;
+    let (a,b) = mapping_insert(1024);
+    test_eq!(a,3,"mapping_insert(1024) failed")?;
+    test_eq!(b,0,"mapping_insert(1024) failed")?;
+    let (a,b) = mapping_insert(2345);
+    test_eq!(a,4,"mapping_insert(2345) failed")?;
+    test_eq!(b,4,"mapping_insert(2345) failed")?;
+    let (a,b) = mapping_insert(12345);
+    test_eq!(a,6,"mapping_insert(12345) failed")?;
+    test_eq!(b,16,"mapping_insert(12345) failed")?;
+    // pr_info!("test mapping_insert ok!");
+    Ok(())
+}
+
+pub fn test_malloc()->Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        // test begin
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"init failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size-2*BLOCK_HEADER_OVERHEAD,"init failed")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"init failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"init failed:missing sentinel")?;
+
+        // test malloc 1
+        let user_ptr = control.malloc::<u8>(256);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,3,"malloc 1 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 1 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),744,"malloc 1 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true,"malloc 1 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 1 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0,"malloc 1 failed:missing sentinel")?;
+        write_buffer(user_ptr,256);
+
+
+        // test malloc 2
+        let user_ptr = control.malloc::<u8>(256);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 2 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 2 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),256,"malloc 2 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),480,"malloc 2 failed:remain size error")?;
+        test_eq!(result[2].as_ref().unwrap().is_free(),true,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().is_prev_free(),false,"malloc 2 failed:remain block prev free error")?;
+        test_eq!(result[3].as_ref().unwrap().get_size(),0,"malloc 2 failed:missing sentinel")?;
+        write_buffer(user_ptr,256);
+
+        // test malloc 3
+        let user_ptr = control.malloc::<u8>(512);
+        test!(user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 3 failed: wrong length")?;
+
+        // test malloc 4
+        let user_ptr = control.malloc::<u8>(472);
+        test!(!user_ptr.is_null())?;
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,4,"malloc 4 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),256,"malloc 4 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"malloc failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),256,"malloc 4 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),480,"malloc 4 failed:remain size error")?;
+        test_eq!(result[2].as_ref().unwrap().is_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().is_prev_free(),false,"malloc 4 failed:remain block prev free error")?;
+        test_eq!(result[3].as_ref().unwrap().get_size(),0,"malloc 2 failed:missing sentinel")?;
+        write_buffer(user_ptr,472);
+
+    }
+    // pr_info!("test_malloc ok");
+    Ok(())
+}
+
+pub fn test_free() -> Result<()>{
+    unsafe{
+        let size = 1024;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"init failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),size-2*BLOCK_HEADER_OVERHEAD,"init failed")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"init failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"init failed:missing sentinel")?;
+
+        // test free 1
+        let user_ptr1 = control.malloc::<u8>(512);
+        test!(!user_ptr1.is_null(),"malloc 1 failed")?;
+        write_buffer(user_ptr1,512);
+
+        let user_ptr2 = control.malloc::<u8>(240);
+        test!(!user_ptr2.is_null(),"malloc 2 failed")?;
+        write_buffer(user_ptr2,240);
+
+        let user_ptr3 = control.malloc::<u8>(248); //  should failed
+        test!(user_ptr3.is_null(),"malloc 3 failed")?;
+
+        control.free(user_ptr2 as *mut u8);
+        drop(user_ptr2);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,3,"free 1 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),512,"free 1 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),false,"free 1 failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),488,"free 1 failed:remain size error")?;
+        test_eq!(result[1].as_ref().unwrap().is_free(),true,"free 1 failed:remain block prev free error")?;
+        test_eq!(result[1].as_ref().unwrap().is_prev_free(),false,"free 1 failed:remain block prev free error")?;
+        test_eq!(result[2].as_ref().unwrap().get_size(),0,"free 1 failed:missing sentinel")?;
+
+        control.free(user_ptr1 as *mut u8);
+        drop(user_ptr1);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+        test_eq!(len,2,"free 2 failed: wrong len length")?;
+        test_eq!(result[0].as_ref().unwrap().get_size(),1008,"free 2 failed:size error")?;
+        test_eq!(result[0].as_ref().unwrap().is_free(),true,"free 2 failed")?;
+        test_eq!(result[1].as_ref().unwrap().get_size(),0,"free 2 failed:missing sentinel")?;
+    }
+    // pr_info!("test_free ok");
+    Ok(())
+}
+
+pub fn test_multiple_alloc() -> Result<()>{
+    unsafe{
+        let size = 2048;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+
+
+        let ptr = ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize));
+
+        let user_ptr1 = control.malloc::<u8>(512);
+        test!(!user_ptr1.is_null(),"malloc 1 failed")?;
+        write_buffer(user_ptr1,512);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+
+        let user_ptr2 = control.malloc::<u8>(512);
+        test!(!user_ptr2.is_null(),"malloc 2 failed")?;
+        write_buffer(user_ptr2,512);
+
+        let user_ptr3 = control.malloc::<u8>(512);
+        test!(!user_ptr3.is_null(),"malloc 3 failed")?;
+        write_buffer(user_ptr3,512);
+
+        let null = control.malloc::<u8>(600);
+        test!(null.is_null(),"malloc 4 failed")?;
+        
+        control.free(user_ptr2 as *mut u8);
+        // walk_debug(ptr.offset(-(BLOCK_HEADER_OVERHEAD as isize)) as *mut MockBlockHeader);
+        // 目前应该为：
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=512,is_prev_free=false,is_free=true
+        // block#3,size=512,is_prev_free=true,is_free=false
+        // block#4,size=472,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+
+        drop(user_ptr2);
+        let (len,result) = get_block_vector(ptr as *mut MockBlockHeader);
+
+        let null = control.malloc::<u8>(600); 
+        test!(null.is_null(),"malloc 5 failed"); // should faile?d
+
+        control.free(user_ptr3 as *mut u8);
+        drop(user_ptr3);
+        // 此时应该为：
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1512,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+
+        let user_ptr4 = control.malloc::<u8>(1024);
+        test!(!user_ptr4.is_null(),"malloc 6 failed"); // should successs
+        // write_buffer(user_ptr4,1024);
+
+        // 此时应该为
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1024,is_prev_free=false,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=true
+        // sentinel,is_prev_free=true,is_free=false
+
+        control.free(user_ptr1 as *mut u8);
+        let null = control.malloc::<u8>(600);
+        test!(null.is_null(),"malloc 7 failed")?;
+
+
+        let user_ptr5= control.malloc::<u8>(472);
+        test!(!user_ptr5.is_null(),"malloc 8 failed")?;
+        // write_buffer(user_ptr5,472);
+
+        // 此时应该为
+        // block#1,size=512,is_prev_free=false,is_free=true
+        // block#2,size=1024,is_prev_free=true,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=false
+        // sentinel,is_prev_free=true,is_free=false
+
+        let user_ptr6 = control.malloc::<u8>(510);
+        test!(!user_ptr6.is_null(),"malloc 9 failed")?;
+        // 此时应该为
+        // block#1,size=512,is_prev_free=false,is_free=false
+        // block#2,size=1024,is_prev_free=false,is_free=false
+        // block#3,size=480,is_prev_free=false,is_free=false
+        // sentinel,is_prev_free=true,is_free=false
+        
+        let null = control.malloc::<u8>(8);
+        test!(null.is_null(),"malloc 10 failed")?;
+    }
+    Ok(())
+    // pr_info!("test_multiple_alloc ok");
+}
+pub fn tlsf_allocator() -> Result<()>{
+    unsafe{
+        for i in 0..100{
+            let a = Box::try_new_in("hello world from our tlsf allocator", TLSFMem).unwrap();
+            let b = Box::try_new_in(123456789, TLSFMem).unwrap();
+            let c = Box::try_new_in(1.23456789, TLSFMem).unwrap();
+            let d = Box::try_new_in([1,2,3,4,5,6,7,8,9,0], TLSFMem).unwrap();
+            let e = Box::try_new_in((1,2,3,4,5), TLSFMem).unwrap();
+        }
+        let a = Box::try_new_in("hello world from our tlsf allocator", TLSFMem).unwrap();
+        pr_info!("{}",a);
+        // pr_info!("tlsf_allocator ok");
+    }
+    Ok(())
+}
+
+#[inline]
+fn handle_alloc_ptr_result_default(test_name:&'static str, test_fn:fn(*mut c_types::c_void,usize) -> Result<()>){
+    let size = mm::page_align(1024).unwrap();
+    let ptr = vmalloc::c_vmalloc(size as c_types::c_ulong).unwrap();
+    let r = test_fn(ptr,size);
+    match r{
+        Ok(())=>{pr_crit!("Pass test {}.\n",test_name)},
+        Err(e)=>{pr_crit!("Failed to pass {}\ncaused by:  {}",test_name,e)}
+    }
+    vmalloc::c_vfree(ptr as *const c_types::c_void);
+}
+
+fn test_torture()->Result<()>{
+    unsafe{
+        let size = 2048;
+        let layout = Layout::from_size_align(size, 8);
+        let ptr = alloc(layout.unwrap());
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        let mut control = TLSFControl::init_on_heap(tmp);
+        control.add_pool(ptr as  *mut u8, size);
+        let mut array = Box::<[*mut u8;64]>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(array.as_mut() as *const _ as *mut u8, 0,size_of::<[*mut u8;64]>());
+        let mut array = array.assume_init();
+        for block_size in [32,64,128,256,512,42,33,123,114,514]{
+            let mut counter = 0;
+            loop{
+                let ptr = control.malloc::<u8>(block_size as usize);
+                if ptr.is_null(){
+                    break;
+                }
+                array[counter] = ptr;
+                counter+=1;
+            }
+
+            for i in 0..counter{
+                control.free(array[i]);
+            }
+        }
+    }
+    // 不测连续分配，随机释放
+    Ok(())
+}
\ No newline at end of file
diff --git a/kernel/rros/list_head.rs b/kernel/rros/list_head.rs
new file mode 100644
index 000000000..e69de29bb
diff --git a/kernel/rros/memory_test.rs b/kernel/rros/memory_test.rs
new file mode 100644
index 000000000..29eabfac9
--- /dev/null
+++ b/kernel/rros/memory_test.rs
@@ -0,0 +1,273 @@
+use core::sync::atomic::AtomicUsize;
+use core::{borrow::BorrowMut, mem::size_of, mem::zeroed};
+
+use kernel::{
+    init_static_sync, mm,
+    prelude::*,
+    premmpt, spinlock_init,
+    sync::{self, Mutex, SpinLock},
+    vmalloc, memory_rros::*,
+    rbtree::{RBTree, RBTreeNode},
+    memory_rros::*,container_of,
+};
+
+use alloc::alloc::*;
+use alloc::alloc_rros::*;
+use alloc::boxed::*;
+
+use crate::{list, monitor};
+
+pub fn mem_test() {
+    // mem_test1();
+    // mem_test2();
+    // test_rbtree();
+    // test_init_system_heap();
+    // test_insert_system_heap();
+    // test_alloc_chunk();
+    test_box_allocator();
+    test_chunk();
+    // test_arc();
+    // test_buckets();
+}
+
+fn test_buckets() {
+    test_213();
+    unsafe {
+        pr_info!("test_buckets: xxx is {}",evl_system_heap.buckets[1]);
+    }
+}
+
+fn test_213() {
+    unsafe {
+        evl_system_heap.buckets[1] = 22;
+    }
+}
+
+fn test_arc() {
+    pr_info!("test_arc: begin");
+    let b = 5;
+    let x = Arc::try_new_in(8881, RrosMem);
+    match x {
+        Ok(a) => {
+            test_fn(a.clone());
+            pr_info!("x is {}",a);
+        },
+        Err(_) => {
+            pr_info!("test_arc: arc alloc err");
+        },
+    }
+    pr_info!("test_arc: end");
+}
+
+fn test_fn(x: Arc<i32,RrosMem>) {
+    pr_info!("test_fn x is {}",x);
+}
+
+fn mem_test1() {
+    let b = 5;
+    let x = Box::try_new_in(123, Global);
+    match x {
+        Err(_) => {
+            pr_info!("alloc error");
+        },
+        Ok(y) => {
+            let z =y;
+            pr_info!("z is {}",z);
+        },
+    }
+    pr_info!("alloc success");
+}
+struct mem_testxy {
+    x: i32,
+    y: i32,
+    z: i32,
+}
+//测试申请到的内存直接转换为结构体指针：结论是可以直接使用
+pub fn mem_test2() -> Result<usize>{
+    let vmalloc_res = vmalloc::c_vmalloc(1024 as u64);
+    let memptr;
+    match vmalloc_res {
+        Some(ptr) => memptr = ptr,
+        None => return Err(kernel::Error::ENOMEM),
+    }
+    let xxx = memptr as *mut mem_testxy;
+    unsafe {
+        (*xxx).x = 11;
+        (*xxx).y = 22;
+        (*xxx).z = 33;
+        pr_info!("mem_test2: z is {}",(*xxx).z);
+        pr_info!("mem_test2: x addr is {:p}",&mut (*xxx).x as *mut i32);
+        pr_info!("mem_test2: y addr is {:p}",&mut (*xxx).y as *mut i32);
+        pr_info!("mem_test2: z addr is {:p}",&mut (*xxx).z as *mut i32);
+    }
+    Ok(0)
+}
+
+struct pageinfo {
+    membase:u32,
+    size:u32,
+}
+
+//测试完成：
+fn test_rbtree() -> Result<usize>{
+    pr_info!("~~~test_rbtree begin~~~");
+    let mut root: RBTree<u32, pageinfo> = RBTree::new();
+
+    let mut x1 = pageinfo{
+        membase:100,
+        size:200,
+    };
+    let mut x2 = pageinfo{
+        membase:101,
+        size:200,
+    };
+    let mut x3 = pageinfo{
+        membase:102,
+        size:200,
+    };
+    
+    let mut node1 = RBTree::try_allocate_node(100,x1)?;
+    // let mut node1: = RBTree::try_allocate_node(300,x2)?;
+    let mut node2 = RBTree::try_allocate_node(101,x2)?;
+    let mut node3 = RBTree::try_allocate_node(102,x3)?;
+    root.insert(node1);
+    root.insert(node2);
+    root.insert(node3);
+    //遍历红黑树方式：
+    for item in root.iter() {
+        pr_info!("item.0 is {}",item.0);
+        pr_info!("item.1.size is {}",item.1.size);
+    }
+    pr_info!("~~~test_rbtree end~~~");
+    Ok(0)
+}
+
+//测试初始化系统堆
+fn test_init_system_heap() {
+    init_system_heap();
+}
+
+//测试系统堆插入节点——测试通过
+fn test_insert_system_heap() -> Result<usize> {
+    pr_info!("~~~test_insert_system_heap begin~~~");
+    init_system_heap();
+    
+    unsafe{
+        let membase = evl_system_heap.membase;
+        pr_info!("test_insert_system_heap: membase is {:p}",membase);
+        let mut x1 = new_evl_heap_range(membase, 1024);
+        let mut x2 = new_evl_heap_range(addr_add_size(membase,1024), 2048);
+        let mut x3 = new_evl_heap_range(addr_add_size(membase,2048), 4096);
+
+        pr_info!("test_insert_system_heap: 1");
+        evl_system_heap.insert_range_byaddr(x1);
+        evl_system_heap.insert_range_byaddr(x2);
+        evl_system_heap.insert_range_byaddr(x3);
+        pr_info!("test_insert_system_heap: 2");
+        let mut rb_node = evl_system_heap.addr_tree.clone().unwrap().rb_node;
+        if rb_node.is_null() {
+            pr_info!("test_insert_system_heap: root is null");
+        } else {
+            let p = container_of!(rb_node, evl_heap_range, addr_node);
+            pr_info!("test_insert_system_heap root size is {}",(*p).size);
+        }
+        pr_info!("test_insert_system_heap: 3");
+    }
+    Ok(0)
+}
+
+//测试小内存的分配与回收
+fn test_small_chunk() {
+
+}
+use kernel::timekeeping::ktime_get_real_fast_ns;
+//多次分配回收
+fn test_chunk() {
+    pr_info!("~~~test_chunk: begin~~~");
+    let t1 = ktime_get_real_fast_ns();
+    for i in 0..100{
+        let mut a = Box::try_new_in([0;128],RrosMem).unwrap();
+    }
+    let t2 = ktime_get_real_fast_ns();
+    pr_info!("rros alloc time is {}",t2-t1);
+    let t3 = ktime_get_real_fast_ns();
+    for i in 0..100
+    {
+        let mut a = Box::try_new_in([0;64],Global).unwrap();
+    }
+    let t4 = ktime_get_real_fast_ns();
+    pr_info!("rust alloc time is {}",t4-t3);
+    pr_info!("~~~test_chunk: 1~~~");
+    let y = __rros_sys_heap_alloc(4,0);
+    pr_info!("~~~test_chunk: end~~~");
+}
+
+//测试分配chunk
+fn test_alloc_chunk() {
+    pr_info!("~~~test_alloc_chunk begin~~~");
+    unsafe {
+        //查看当前evl_system_heap size树的根
+        let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+        let mut p = container_of!(rb_node, evl_heap_range, size_node);
+        let raw_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", raw_size);
+        let membase = evl_system_heap.membase;
+        pr_info!("test_alloc_chunk: membase is {}",membase as u32);
+        let res = evl_system_heap.evl_alloc_chunk(1024);
+        let mut x:u32 = 0;
+        let mut addr = 0 as *mut u8;
+        match res {
+            Some(a) => {
+                addr = a;
+                x = a as u32;
+                pr_info!("test_alloc_chunk: alloc addr is {}",x as u32);
+            },
+            None => {
+                pr_info!("test_alloc_chunk: alloc err");
+            }
+        }
+        pr_info!("test_alloc_chunk: membase - alloc = {}",x as u32 - membase as u32);
+        p = container_of!(rb_node, evl_heap_range, size_node);
+        let mut new_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", new_size);
+        pr_info!("test_alloc_chunk: raw_size - new_size = {}", raw_size - new_size);
+        //测试回收
+        pr_info!("~~~test_alloc_chunk: test free begin~~~");
+        evl_system_heap.evl_free_chunk(addr);
+        p = container_of!(rb_node, evl_heap_range, size_node);
+        new_size = (*p).size;
+        pr_info!("test_insert_system_heap root size is {}", new_size);
+        pr_info!("~~~test_alloc_chunk: test free end~~~");
+    }
+    pr_info!("~~~test_alloc_chunk end~~~");
+
+}
+
+//测试box的自定义分配器
+fn test_box_allocator() {
+    pr_info!("test_box_allocator: begin");
+    let b = 5;
+    let x = Box::try_new_in(123, RrosMem);
+    match x {
+        Err(_) => {
+            pr_info!("test_box_allocator: alloc error");
+            return ;
+        },
+        Ok(_x) => {
+            unsafe {
+                let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+                let mut p = container_of!(rb_node, evl_heap_range, size_node);
+                let raw_size = (*p).size;
+                pr_info!("test_box_allocator: root size is {}", raw_size);
+            }
+            pr_info!("test_box_allocator: x is {}",_x);
+        },
+    }
+    unsafe {
+        let mut rb_node = evl_system_heap.size_tree.clone().unwrap().rb_node;
+        let mut p = container_of!(rb_node, evl_heap_range, size_node);
+        let raw_size = (*p).size;
+        pr_info!("test_box_allocator: root size is {}", raw_size);
+    }
+    pr_info!("test_box_allocator: alloc success");
+}
\ No newline at end of file
diff --git a/kernel/rros/thread.rs b/kernel/rros/thread.rs
index 43c956c47..0baf851b8 100644
--- a/kernel/rros/thread.rs
+++ b/kernel/rros/thread.rs
@@ -273,7 +273,8 @@ fn evl_wakeup_thread_locked(thread: Arc<SpinLock<rros_thread>>, mut mask: u32, i
         // let mut oldstate = thread.lock();
         thread.lock().state &= !mask;
 
-		if (mask & (T_DELAY|T_PEND)) != 0x0 {
+
+		if (mask & (sched::T_DELAY|sched::T_PEND)) != 0x0 {
             let rtimer = thread.lock().rtimer.clone();
             timer::rros_stop_timer(rtimer.unwrap());
         }
diff --git a/kernel/rros/thread_test.rs b/kernel/rros/thread_test.rs
index 7036692cb..e3bcfaafc 100644
--- a/kernel/rros/thread_test.rs
+++ b/kernel/rros/thread_test.rs
@@ -3,7 +3,8 @@ use crate::{
     sched::{self, this_rros_rq}
 };
 use kernel::{bindings, prelude::*, c_str, spinlock_init, sync::{SpinLock, Lock, Guard}, c_types, };
-use alloc::rc::Rc;
+use alloc::alloc_rros::*;
+use alloc::alloc::*;
 use core::cell::RefCell;
 
 struct KthreadRunner {
@@ -57,7 +58,7 @@ pub fn test_thread_context_switch() {
         kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
         kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
         
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
+        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
         // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
         // kthread_runner_1.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
 
@@ -90,7 +91,7 @@ pub fn test_thread_context_switch() {
         kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer = Some(Arc::try_new(r).unwrap());
         kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer = Some(Arc::try_new(p).unwrap());
 
-        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_MONO_CLOCK, 0);
+        // let mut tmb = timer::rros_percpu_timers(&clock::RROS_REALTIME_CLOCK, 0);
         // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().rtimer.as_mut().unwrap().lock().set_base(tmb);
         // kthread_runner_2.as_mut().unwrap().kthread.thread.as_mut().unwrap().lock().ptimer.as_mut().unwrap().lock().set_base(tmb);
         
@@ -164,32 +165,16 @@ pub fn kfn_1() {
 
         thread::rros_sleep(1000000000);//sleep有大问题 暂时不用了
         
+        unsafe { pr_info!("kfn1: time begin is {}", clock::RROS_REALTIME_CLOCK.read())};
         
-        // unsafe{
-        //     let mut tmb = timer::rros_this_cpu_timers(&clock::RROS_MONO_CLOCK);
-        //     if (*tmb).q.is_empty() == true {
-        //         // tick
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        //         pr_info!("empty/n");
-        // //         // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
-        //     }
-        // //     // tick::proxy_set_next_ktime(1000000, 0 as *mut bindings::clock_event_device);
+        // for i in 1..3 {
+        let a = Arc::try_new_in(1000, RrosMem);
+        unsafe { pr_info!("kfn1: time1 is {}", clock::RROS_REALTIME_CLOCK.read())};
+        let b = Arc::try_new_in(1000, RrosMem);
+        unsafe { pr_info!("kfn1: time2 is {}", clock::RROS_REALTIME_CLOCK.read())};
         // }
-        // // unsafe{
-        //     let this_rq = this_rros_rq();
-        //     tick::rros_notify_proxy_tick(this_rq);
-        // }
-        // pr_info!("hello! from rros~~~~~~~~~~~~");
+        unsafe { pr_info!("kfn1: time end is {}", clock::RROS_REALTIME_CLOCK.read())};
+        // pr_info!("kfn1: waste time is {}",y-x);
         pr_emerg!("hello! from rros~~~~~~~~~~~~");
         
     // }
@@ -198,6 +183,14 @@ pub fn kfn_1() {
 pub fn kfn_2() {
     // while 1==1 {
         thread::rros_sleep(1000000000);
+        let x = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        pr_info!("kfn2: x is {}",x);
+        // for i in 1..100 {
+            let a = Arc::try_new(1000);
+        // }
+        let y = unsafe{clock::RROS_REALTIME_CLOCK.read()};
+        pr_info!("kfn2: y is {}",y);
+        pr_info!("kfn_2: waste time is {}",y-x);
         pr_info!("world! from rros~~~~~~~~~~~~");
         
     // }
diff --git a/kernel/rros/tlsf.rs b/kernel/rros/tlsf.rs
new file mode 100644
index 000000000..075bdd579
--- /dev/null
+++ b/kernel/rros/tlsf.rs
@@ -0,0 +1,752 @@
+use core::alloc::{Layout};
+use core::{ptr, slice};
+use core::ptr::NonNull;
+use core::mem::{size_of,MaybeUninit};
+use core::ops::{Deref, DerefMut};
+use alloc::alloc::Global;
+use alloc::boxed::Box;
+use alloc::vec::Vec;
+use crate::tlsf_raw_list::{Links,RawList,GetLinks};
+
+
+// FIXME: You can still try to use your own list in lab1 instead!
+// use kernel::double_linked_list3::LinkedList;
+
+/// the last `ALIGN_SIZE_LOG2` bit of the size field is used to store some flags
+pub const ALIGN_SIZE_LOG2: usize = 3; 
+
+/// memory alignment size
+pub const ALIGN_SIZE: usize = 1 << ALIGN_SIZE_LOG2; 
+
+ /// the second level index table has 2^SL_INDEX_COUNT_LOG2 entries
+pub const SL_INDEX_COUNT_LOG2: usize = 5;
+
+/// the maximum size of a block is 2^FL_INDEX_MAX
+pub const FL_INDEX_MAX: usize = 32; 
+
+/// The number of shift bits of the first level index
+pub const FL_INDEX_SHIFT: usize = (SL_INDEX_COUNT_LOG2 + ALIGN_SIZE_LOG2); 
+
+/// Size of first level index table
+pub const FL_INDEX_COUNT: usize = (FL_INDEX_MAX - FL_INDEX_SHIFT + 1);
+
+/// Size of second level index table
+pub const SL_INDEX_COUNT: usize = 1 << SL_INDEX_COUNT_LOG2;
+
+/// The maximum size of a block stores in blocks[0]
+pub const SMALL_BLOCK_SIZE: usize = 1 << FL_INDEX_SHIFT; // 第0层可能存放的最大块大小
+
+/// is_free flag mask
+pub const BLOCK_HEADER_FREE_BIT: usize = 1 << 0; 
+
+/// is_prev_free flag mask
+pub const BLOCK_HEADER_PREV_FREE_BIT: usize = 1 << 1; 
+
+/// overhead of a block. The overhead is the size of the BlockHeader struct.
+pub const BLOCK_HEADER_OVERHEAD: usize = core::mem::size_of::<usize>();
+
+/// overhead of a pool. Both the first block and sentinel block need a size field.
+pub const ALLOC_POOL_OVERHEAD: usize = 2*BLOCK_HEADER_OVERHEAD; 
+
+pub const BLOCK_HEADER_MAGIC : usize = 0xdeadbeef;
+
+
+trait ValidityCheck{
+    fn check_validity(&self)->bool;
+}
+
+/// A wrapper for a non-null pointer to a BlockHeader
+/// We have implemented Deref and DerefMut for this type,
+/// So you can visit the fields and methods of BlockHeader directly
+/// Just use it as if it is a BlockHeader.
+///
+/// For example:
+/// ```
+/// let hdr = AutoDerefPointer::from_raw_pointer(ptr);
+/// hdr.set_size(100);
+/// ```
+#[derive(Clone, Copy)]
+pub struct AutoDerefPointer<T>(NonNull<T>);
+
+impl<T> Deref for AutoDerefPointer<T>{
+    type Target = T;
+    fn deref(&self) -> &Self::Target{
+        unsafe{
+            self.0.as_ref()
+        }
+    }
+}
+
+impl<T> DerefMut for AutoDerefPointer<T>{
+    fn deref_mut(&mut self) -> &mut Self::Target{
+        unsafe{
+            self.0.as_mut()
+        }
+    }
+}
+impl<T:ValidityCheck> AutoDerefPointer<T>{
+    // convert a raw pointer to AutoDerefPointer
+    fn from_raw_pointer(ptr: *mut T) -> Self {
+        let p =AutoDerefPointer{
+            0 : NonNull::new(ptr as *mut T)
+                .unwrap(),
+        };
+        if p.check_validity(){
+            p
+        }else{
+            panic!("invalid block header");
+        }
+    }
+    unsafe fn from_raw_pointer_unchecked(ptr: *mut T) -> Self {
+        AutoDerefPointer{
+            0 : NonNull::new(ptr as *mut T).unwrap(),
+        }
+    }
+}
+
+
+pub type BlockHeaderPointer = AutoDerefPointer<BlockHeader>;
+pub type FreeBlockHeaderPointer = AutoDerefPointer<FreeBlockHeader>;
+
+
+
+/// The header of a memory block. <br>
+/// The header is stored before the data part of the block, see the documentation for detail.<br>
+/// You should **not** allocate or create Block Header manually.
+/// Because the new block is either on stack or other heap memory, not our memory pool.<br>
+/// Typically, you should use BlockHeaderPointer to access the header of a block.
+/// There are mainly two ways we get the header of a block:
+/// * Through the pointer stored in the TLSFControl.blocks
+/// * Use the pointer passing by user in `free`function 
+#[repr(C)]
+pub struct BlockHeader {
+    /// we can get the next header easily through `size` field.
+    /// but unless we have a pointer to previous header,
+    /// we don't know the exact location of last block start.
+    prev_phys_block: *mut BlockHeader,
+
+    /// magic number to check the validity of the block
+    magic : u32,
+
+    /// the last 3 **bit** is used to store some flags
+    size: u32, 
+}
+
+
+/// The header of a free memory block. <br>
+/// * The `links` is used to link the free blocks in the same size class.
+/// * When the block is allocated, the links will be covered by the user data.
+/// * In this lab, you are **not** required to use `links`. You can use your own list instead 
+/// as long as you can pass the tests.
+pub struct FreeBlockHeader{
+    header : BlockHeader,
+    links : Links<FreeBlockHeader>,
+}
+impl GetLinks for FreeBlockHeader{
+    type EntryType = FreeBlockHeader;
+    fn get_links(data:&Self::EntryType) -> &Links<Self::EntryType> {
+        &data.links
+    }
+}
+
+
+/// The control block of the TLSF memory allocator.<br>
+/// If you are not familiar with bit operation, replace the bitmap with a bool array.
+pub struct TLSFControl {
+    /// first level bitmap
+    fl_bitmap: usize, 
+
+    /// second level bitmap
+    sl_bitmap: [usize; FL_INDEX_COUNT],
+
+    /// a linked list of free blocks for each size class
+    blocks: [[RawList<FreeBlockHeader>; SL_INDEX_COUNT]; FL_INDEX_COUNT],
+}
+
+
+
+impl BlockHeader {
+    /// 将指向数据部分的裸指针转换成指向块头部的引用  
+    /// 返回值：         --->| prev_phys_block |   
+    ///                     | size            |   
+    /// 传入的参数： ptr --> | data            |  
+    /// 
+    /// convert a pointer passed by user to BlockHeaderPointer <br>
+    /// Some evil users would pass a pointer not allocated by our allocator.
+    /// You can check the safety of pointer but it is not compulsory in our tests.
+    /// # Argument
+    /// * `ptr` - the pointer passed by user
+    /// # Return
+    /// block header 
+    /// 
+    /// # Safety
+    /// The caller should guarantee that the pointer is valid.
+    /// 
+    pub fn from_user_pointer<T>(ptr: *mut T) -> BlockHeaderPointer {
+        unsafe{
+            BlockHeaderPointer::from_raw_pointer(ptr.offset(-(size_of::<Self>() as isize)) as *mut BlockHeader)
+        }
+    }
+
+    /// 返回数据块管理的内存大小 <br>
+    /// Get the size of the memory block managed by this header.   <br>
+    /// Note that the size is aligned to ALIGN_SIZE, 
+    /// and the last few bit of `size` is used to store flags.
+    pub fn get_size(&self) -> usize {
+        self.size as usize & !(BLOCK_HEADER_FREE_BIT | BLOCK_HEADER_PREV_FREE_BIT)
+    }
+
+    /// Check if the magic number is correct.
+    pub fn check_magic(&self) -> bool {
+        self.magic == BLOCK_HEADER_MAGIC as u32
+    }
+
+    /// 设置标志位is_prev_free为false <br>
+    /// Set the flag is_prev_free to false.
+    pub fn set_prev_used(&mut self) {
+        self.size  &= !(BLOCK_HEADER_PREV_FREE_BIT) as u32;
+    }
+
+    /// 返回标志位is_prev_free的值：查看前一个块是否空闲 <br>
+    /// Return the value of the flag is_prev_free: whether the previous block is free.
+    pub fn is_prev_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_PREV_FREE_BIT) != 0
+    }
+
+    /// 返回当前块是否空闲 <br>
+    /// Return whether the current block is free.
+    pub fn is_free(&self) -> bool {
+        (self.size as usize & BLOCK_HEADER_FREE_BIT) != 0
+    }
+
+    /// 获取当前块的数据指针 <br>
+    /// Return the pointer to the data of the current block.
+    /// # Safety
+    /// If the `self` is valid, the returned pointer is valid and it can write `size` bytes at most.
+    pub fn data_ptr<T>(&self) -> *mut T {
+        unsafe { (self as *const _ as *mut u8).offset(size_of::<Self>() as isize) as *mut T }
+    }
+
+    /// 设置当前块空闲，并把下一个块的is_prev_free标志位置为true <br>
+    /// Set the current block to free, and set the is_prev_free flag of the next block to true.
+    pub fn mark_as_free(&mut self) -> FreeBlockHeaderPointer {
+        self.set_free();
+        self.link_next();
+        FreeBlockHeaderPointer::from_raw_pointer(self as *const _ as *mut FreeBlockHeader)
+    }
+
+    /// 获取下一个块。如果下一个块为哨兵，返回None <br>
+    /// Return the pointer to the next block. 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel
+    pub fn next(&self) -> Option<BlockHeaderPointer> {
+        let next_block = unsafe{&mut *self.next_uncheked()};
+        if (next_block.get_size() == 0) {
+            None
+        } else {
+            Some(
+                BlockHeaderPointer::from_raw_pointer(next_block as *mut BlockHeader)
+            )
+        }
+    }
+
+    /// Return the pointer to the next block.
+    /// # Return 
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel or is not free block.
+    pub fn next_free(&self) -> Option<FreeBlockHeaderPointer>{
+        match self.next(){
+            Some(next) => {
+                if next.is_free(){
+                    Some(FreeBlockHeaderPointer::from_raw_pointer(next.0.as_ptr() as *const _ as *mut FreeBlockHeader))
+                }else{
+                    None
+                }
+            },
+            None => None
+        }
+    }
+
+    /// 获取前一个块。如果前一个块已被分配，返回None <br>
+    /// Return the pointer to the previous block.
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the previous block
+    /// * `None` - the previous block is allocated or just not exists.
+    pub fn prev(&self) -> Option<BlockHeaderPointer> {
+        if self.prev_phys_block.is_null() {
+            None
+        } else {
+            Some(BlockHeaderPointer::from_raw_pointer(self.prev_phys_block))
+        }
+    }
+
+    /// Return the pointer to the previous block.
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the previous block
+    /// * `None` - the previous block is allocated or just not exists.
+    /// 
+    /// # Safety
+    /// You may encouter some memory page fault here. Make sure the `prev_phys_block` is valid
+    /// otherwise it will panic here.
+    pub fn prev_free(&self) -> Option<FreeBlockHeaderPointer>{
+        match self.prev(){
+            Some(prev) => {
+                if prev.is_free(){
+                    Some(FreeBlockHeaderPointer::from_raw_pointer(prev.0.as_ptr() as *const _ as *mut FreeBlockHeader))
+                }else{
+                    None
+                }
+            },
+            None => None
+        }
+    }
+
+
+    /// 设置数据块管理的内存大小 <br>
+    /// Set the size of the memory block header. 
+    fn set_size(&mut self, size: usize) {
+        let old_size = self.size;
+        self.size = size as u32 | (old_size & (BLOCK_HEADER_FREE_BIT as u32 | BLOCK_HEADER_PREV_FREE_BIT as u32));
+        self.magic = BLOCK_HEADER_MAGIC as u32;
+    }
+
+    /// 设置标志位is_prev_free为true <br>
+    /// Set the flag is_prev_free to true.
+    fn set_prev_free(&mut self) {
+        self.size |= BLOCK_HEADER_PREV_FREE_BIT as u32;
+    }
+
+    /// 设置当前块空闲 <br>
+    /// Set the current block to free.
+    fn set_free(&mut self) {
+        self.size |= BLOCK_HEADER_FREE_BIT as u32; 
+    }
+
+    /// 设置当前块已使用 <br>
+    /// Set the current block to used.
+    fn set_used(&mut self) {
+        self.size &= !(BLOCK_HEADER_FREE_BIT) as u32;
+    }
+
+    /// 获取下一个块的指针 <br>
+    /// Return the pointer to the next block. 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the next block
+    /// * `None` - the next block is the sentinel
+    #[inline]
+    unsafe fn next_uncheked(&self) -> *mut BlockHeader {
+        unsafe { &mut *((self.data_ptr() as *mut u8).offset((self.get_size() - BLOCK_HEADER_OVERHEAD) as isize) as *mut BlockHeader)}
+    }
+   
+    fn link_next(&mut self){
+        let next = self.next(); // we don't cate if the next block is free or not
+        if let Some(mut next) = next {
+            next.prev_phys_block = self as *const _ as *mut BlockHeader;
+            next.set_prev_free();
+        }
+    }
+}
+
+impl ValidityCheck for BlockHeader {
+    fn check_validity(&self) -> bool {
+        self.magic == BLOCK_HEADER_MAGIC as u32
+    }
+}
+
+impl ValidityCheck for FreeBlockHeader {
+    fn check_validity(&self) -> bool {
+        self.header.magic == BLOCK_HEADER_MAGIC as u32
+    }
+}
+
+impl FreeBlockHeader{
+    /// 初始化一个内存池 <br>
+    /// Initialize a new memory pool as a block.
+    /// # Input
+    /// * `ptr` - the pointer to the memory pool
+    /// * `size` - the real size of the memory pool(which means that you should minus the overhead)
+    /// # Safety
+    /// user must ensure that the pointer point to a memory that owned by us.
+    /// 
+    /// # Hints
+    /// * Don't forget to add a sentinel block at the end of the memory pool. The sentinel block's size is 0
+    /// and it's `set_used`.
+    /// * The first block's `previous_used` can be set to true.
+    /// * use `link_next_init` to generate the next block's pointer.**Don't** use `link_next` here because it
+    /// will do some checkings.
+    pub unsafe fn init_block(ptr : *mut FreeBlockHeader,size:usize) -> FreeBlockHeaderPointer{
+        let block = unsafe{&mut (*ptr)};
+        // TODO: YOUR CODE HERE
+
+        FreeBlockHeaderPointer::from_raw_pointer(ptr)
+    }
+
+    /// 设置当前块已使用，并把下一个块的is_prev_free标志位置为false <br>
+    /// Set the current block to used, and set the is_prev_free flag of the next block to false.
+    pub fn mark_as_used(&mut self) -> BlockHeaderPointer {
+        self.header.set_used();
+        let next = self.header.next();
+        if let Some(mut next) = next {
+            next.set_prev_used();
+        }
+        BlockHeaderPointer::from_raw_pointer(self as *mut FreeBlockHeader as *mut BlockHeader)
+    }
+
+     /// 判断当前块是否可以分割
+    /// Return whether the current block can be split.
+    pub fn can_split(&self, size: usize) -> bool {
+        self.header.get_size() >= size_of::<BlockHeader>() + size
+    }
+
+    /// 把当前块分为两个块，前一个块大小为size，后一个块大小为self.size - size - BLOCK_HEADER_OVERHEAD <br>
+    /// Split the current block into two blocks, the size of the first block is `size`,
+    /// and the size of the second block is `self.size - size - BLOCK_HEADER_OVERHEAD`.
+    /// 
+    /// # Arguments
+    /// * `size` - the size of the first block(`self` block) after `split`.
+    /// 
+    /// # Return
+    /// * `Some(BlockHeaderPointer)` - the pointer to the second block, convert it to `BlockHeaderPointer`
+    /// * `None` - the block cannot be split
+    /// 
+    /// # Tips:
+    /// * When compute the pointer to the next block, cast it to `*mut u8` first.
+    /// * What is the position of the second block? Think carefully before you starting to write the code.
+    /// * The second block should be marked as free.
+    /// * The size of the second block should consider the overhead of the block header.
+    /// * Some useful functions: `self.header.set_size`,`can_split`,`self.header.get_size,`mark_as_free`
+    pub fn split(&mut self, size: usize) -> Option<FreeBlockHeaderPointer> {
+        // TODO: YOUR CODE HERE
+        None
+    }
+
+    /// 合并两个块 <br>
+    /// Merge the next block into the current block.
+    /// The next block should exist and free
+    /// # Safety
+    /// Don't use the pointer to the next block after calling this function.
+    /// 
+    /// # Hint
+    /// * Update the size of the current block.
+    /// * Remember to update the prev_phys_block of the next block.(You may use `link_next`)
+    pub fn absorb(&mut self) {
+        if self.header.get_size() == 0 {
+            return;
+        }
+        // TODO: YOUR CODE HERE
+    }
+    
+    /// * Let next block point to self <br>
+    /// * You may only use it in `FreeBlockHeader::init_block`
+    fn link_next_init(&mut self) -> BlockHeaderPointer {
+        let next = unsafe{&mut *self.header.next_uncheked()};
+        next.prev_phys_block = self as *const _ as *mut BlockHeader;
+        unsafe{BlockHeaderPointer::from_raw_pointer_unchecked(next as *mut BlockHeader)}
+    }
+}
+
+impl TLSFControl {
+    /// 在堆上初始化控制块 <br>
+    /// Initialize the control block on the heap.(Kernel global heap)
+    /// # Arguments
+    /// * `tmp` - the temporary control block
+    /// 
+    /// # Return
+    /// * `Box<Self,Global>` - initialize the value of `tmp` and return it.
+    pub fn init_on_heap(mut tmp : Box<TLSFControl,Global>) -> Box<Self,Global>{
+        // TODO: YOUR CODE HERE
+        tmp.fl_bitmap = 0;
+        tmp.sl_bitmap = [0; FL_INDEX_COUNT];
+        for i in 0..FL_INDEX_COUNT {
+            for j in 0..SL_INDEX_COUNT {
+                tmp.blocks[i][j] = RawList::new();
+            }
+        }
+        tmp
+        // END OF YOUR CODE
+    }
+
+    /// 将一个内存地址加入管理 <br>
+    /// Add a contiguous memory block to the control block. 
+    /// 
+    /// # Arguments
+    /// * `mem_base` - the base address of the memory block
+    /// * `size` - the size of the memory block
+    /// 
+    /// # Hints
+    /// * Fill in the blank in `init_block` first
+    /// * Use `self.insert_block` to insert the block into the free list
+    pub fn add_pool(&mut self,mut mem_base: *mut u8, size: usize){
+        let real_size = align_down(size - ALLOC_POOL_OVERHEAD, ALIGN_SIZE);
+        assert_eq!(
+            mem_base.align_offset(ALIGN_SIZE),
+            0,
+            "mem_base is not align"
+        );
+        // TODO: YOUR CODE HERE
+        let mut block = unsafe{FreeBlockHeader::init_block(mem_base.offset(-(BLOCK_HEADER_OVERHEAD as isize)) as *mut FreeBlockHeader, real_size)};
+        // END OF YOUR CODE
+    }
+
+    /// 分配一个大小至少为size的内存块。如果分配失败，返回空指针。 <br>
+    /// Allocate a memory block whose size is at least `size`.
+    /// # Arguments
+    /// * `size` - the size of the memory block user needs
+    /// 
+    /// # Return
+    /// * `*mut T` - the pointer to the allocated memory block
+    /// * `nullptr` - if the allocation fails
+    /// 
+    /// # Hints
+    /// * Align the size first
+    /// * You may need a helper method to find a free block
+    /// * Split the free block to perfectly fit the size we need, and put the rest into the free pool
+    /// * Use `self.insert_block` to insert the rest block into the free list
+    /// * Don't forget to mark the block as used
+    /// * You may use the following functions: `split`,`mark_as_used`,`set_prev_used`,`insert_block`
+    pub fn malloc<T>(&mut self, size: usize) -> *mut T {
+        let adjust = if size > 0 {
+            align_up(size, ALIGN_SIZE)
+        } else {
+            panic!("size must be greater than 0");
+        };
+        // TODO: YOUR CODE HERE
+        ptr::null_mut()
+        // END OF YOUR CODE
+    }
+
+    /// 释放一个指针指向的内存 <br>
+    /// Free a memory block.
+    /// 
+    /// # Arguments
+    /// * `ptr` - the pointer to the memory block
+    /// 
+    /// # Hints
+    /// * You don't have to check the validity of the pointer
+    /// * Use `BlockHeader::from_user_pointer` to get the block header
+    /// * After free the block, try to merge the neighbor free blocks.
+    ///   You may need a helper method to deal with the merging. As a reference,
+    ///   We use `merge_prev_block` and `merge_next_block` to merge the previous 
+    ///   and next blocks respectively.
+    /// * Don't forget to put the merged block into the free list
+    /// 
+    pub fn free<T>(&mut self, ptr: *mut T) {
+        if ptr.is_null() {
+            panic!("try to free null pointer");
+        }
+        // TODO: YOUR CODE HERE
+    }
+
+    /// 把一个内存块插入空闲链表 <br>
+    /// Insert a free block into the free list.
+    /// 
+    /// # Hints
+    /// * Use `mapping_insert` to get the index of the free list
+    /// * Fill in the blank in `mapping_insert` first
+    /// * Use raw_list or your own implementation to insert the block into the free list
+    /// * Use `self.set_bitmap` to set the bitmap
+    fn insert_block(&mut self, mut block: FreeBlockHeaderPointer) {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+    }
+
+
+    /// 查找一个不小于请求size的空闲内存块  <br>
+    /// find a free block whose size is not less than `size`
+    /// # Return 
+    /// * `Option<BlockHeaderPointer>` - the pointer to the free block
+    /// * `None` - if no such block exists
+    /// 
+    /// # Hints
+    /// * You need to know the exact `fl`, `sl` index of the free list. Use `mapping_search` to get them.
+    /// * If there's no such block, you need to find a larger block and split it. If you are using bit operations
+    ///   with `fl_bitmap`  and `sl_bitmap`, you may find `ffs` and `fls` useful.
+    /// * pop a block from the free list and return it
+    /// * Remember to unset the bitmap after you find a block
+    fn block_locate_free(&mut self, size: usize) -> Option<FreeBlockHeaderPointer> {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        None
+    }
+
+    /// set the bitmap of control block
+    fn set_bitmap(&mut self, fl: u32, sl: u32) {
+        self.fl_bitmap |= 1 << fl;
+        self.sl_bitmap[fl as usize] |= 1 << sl;
+    }
+
+    /// unset the bitmap of control block
+    fn unset_bitmap(&mut self, fl: u32, sl: u32) {
+        if self.blocks[fl as usize][sl as usize].is_empty() {
+            self.sl_bitmap[fl as usize] &= !(1 << sl);
+            if self.sl_bitmap[fl as usize] == 0 {
+                self.fl_bitmap &= !(1 << fl);
+            }
+        }
+    }
+
+    /// remove a block from the free list  
+    /// * The function search the free list to find the block and remove it.
+    /// 
+    /// # Arguments
+    /// * `block` - the block to be removed
+    fn remove_block(&mut self, block: &mut FreeBlockHeaderPointer) {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+    }
+
+    /// 尝试和前一个块合并 <br>
+    /// Try to merge the previous block
+    /// 
+    /// # Return 
+    /// * `BlockHeaderPointer` - the pointer to the merged block
+    /// * If merge is successful, the pointer to the merged block is returned.
+    /// * Otherwise the pointer to the original block.
+    /// 
+    /// # Hints
+    /// * If the previous block is not free, just return current block.
+    /// * Try to use `absorb` to merge the block
+    /// * Use `prev_free` instead of `prev`
+    /// * Don't forget to remove the previous block from the free list
+    fn merge_prev_block(&mut self, block: FreeBlockHeaderPointer) -> FreeBlockHeaderPointer {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        unimplemented!()
+
+    }
+
+    /// 尝试合并下一个块 <br>
+    /// Try to merge the next block
+    /// 
+    /// # Return 
+    /// * `BlockHeaderPointer` - the pointer to the merged block
+    /// * If merge is successful, the pointer to the merged block is returned.
+    /// * Otherwise the pointer to the original block.
+    fn merge_next_block(&mut self, mut block: FreeBlockHeaderPointer) -> FreeBlockHeaderPointer {
+        // TODO: YOUR CAN ADD CODE HERE(NO TEST HERE)
+        unimplemented!()
+    }
+}
+
+
+/// 将x向上对齐到align的倍数 <br>
+/// Align x up to a multiple of align
+#[inline]
+fn align_up(x: usize, align: usize) -> usize {
+    (x + align - 1) & !(align - 1)
+}
+
+/// 将x向下对齐到align的倍数 <br>
+/// Align x down to a multiple of align
+#[inline]
+fn align_down(x: usize, align: usize) -> usize {
+    x - (x & (align - 1))
+}
+
+/// 计算x的最高位1的位置 <br>
+/// get the position of the highest bit 1 of x, also known as find last set
+#[inline]
+fn fls(word: usize) -> u32 {
+    (32 - (word as u32).leading_zeros() - 1) as u32
+}
+
+/// 计算x的最低位1的位置 <br>
+/// get the position of the lowest bit 1 of x,also known as find first set
+#[inline]
+fn ffs(word: usize) -> u32 {
+    (word as u32).trailing_zeros() as u32
+}
+
+/// 按TLSF所说的方法将size映射到fl和sl
+/// Mapping the size to fl and sl according to the method in TLSF
+/// 
+/// # Arguments
+/// * `size` - the size of the block
+/// 
+/// # Return
+/// - (fl, sl) - the fl and sl of the block
+/// 
+/// # Hints
+/// * You can handle the situation when size is smaller than SMALL_BLOCK_SIZE specially.
+/// * The methods above like ffs,fls may be helpful.
+#[inline]
+pub fn mapping_insert(size: usize) -> (u32, u32) {
+    // TODO: YOUR CODE HERE
+    (0,0)
+    // END OF YOUR CODE
+}
+
+/// 分配时，根据size找到合适的fl和sl。这里会把size向上加一个round，防止分的内存比实际少 <br>
+/// round指的是一个second_level range的大小，例如，当SLI=5, f=10，每个round=2^10/2^5=32字节
+#[inline]
+fn mapping_search(mut size: usize) -> (u32, u32) {
+    if size >= SMALL_BLOCK_SIZE {
+        // 为防止分的内存比实际少，向上取整
+        let round = (1 << (fls(size) as usize - SL_INDEX_COUNT_LOG2)) - 1;
+        size += round as usize;
+    }
+    mapping_insert(size)
+}
+
+pub static mut TLSFHeap : Option<Box<TLSFControl>> = None;
+
+use core::alloc::{Allocator,AllocError};
+pub struct TLSFMem;
+unsafe impl Allocator for TLSFMem{
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError>{
+        let size = layout.size();
+        unsafe{
+            let ptr = TLSFHeap.as_mut().unwrap().malloc::<u8>(size);
+            if ptr.is_null(){
+                Err(AllocError)
+            }else{
+                let r= slice::from_raw_parts(ptr, size);
+                Ok(NonNull::from(r))
+            }
+        }
+    }
+
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, _layout: Layout){
+        unsafe{
+            TLSFHeap.as_mut().unwrap().free(ptr.as_ptr() as *mut u8);
+        }
+    }
+}
+use kernel::{vmalloc, pr_info};
+pub fn init_tlsfheap(){
+    unsafe{
+        let mut tmp = Box::<TLSFControl>::try_new_uninit_in(Global).unwrap();
+        ptr::write_bytes(tmp.as_mut(), 0, 1);
+        let tmp = tmp.assume_init();
+        TLSFHeap = Some(
+            TLSFControl::init_on_heap(tmp)
+        );
+        let size = 4096*4;
+        let ptr = vmalloc::c_vmalloc(size).unwrap();
+        TLSFHeap.as_mut().unwrap().add_pool(ptr as *mut u8,size as usize);
+    }
+}
+
+
+use kernel::bindings;
+pub struct Example{
+    pub val1 : u32,
+    pub list1 : bindings::list_head,
+
+    pub val2 : u32,
+    pub list2 : bindings::list_head
+}
+
+
+/// 从大到小排序
+/// sort the ex1 and ex2 in descending order in two lists respectively
+/// # Hints
+/// * You can use the rust_helper_list_add_tail function.
+/// * Just compare the val1 of ex1 and ex2 and the add the ex1 and ex2 to the list respectively.
+pub fn list_example1_sort_decending_respectively(val_list1:&mut bindings::list_head,
+                    val_list2:&mut bindings::list_head,
+                    ex1: &mut Example,
+                    ex2: &mut Example){
+    extern "C"{
+        fn rust_helper_list_add_tail(new : *mut bindings::list_head, list : *mut bindings::list_head);
+    }
+    // TODO: YOUR CODE HERE
+}
+
diff --git a/kernel/rros/tlsf_raw_list.rs b/kernel/rros/tlsf_raw_list.rs
new file mode 100644
index 000000000..48b486045
--- /dev/null
+++ b/kernel/rros/tlsf_raw_list.rs
@@ -0,0 +1,220 @@
+// SPDX-License-Identifier: GPL-2.0
+
+//! A fork of raw lists.
+//!
+
+use core::{
+    cell::UnsafeCell,
+    ptr,
+    ptr::NonNull,
+    sync::atomic::{AtomicBool, Ordering},
+};
+
+/// A descriptor of list elements.
+///
+/// It describes the type of list elements and provides a function to determine how to get the
+/// links to be used on a list.
+///
+/// A type that may be in multiple lists simultaneously needs to implement one of these for each
+/// simultaneous list.
+pub trait GetLinks {
+    /// The type of the entries in the list.
+    type EntryType: ?Sized;
+
+    /// Returns the links to be used when linking an entry within a list.
+    fn get_links(data: &Self::EntryType) -> &Links<Self::EntryType>;
+}
+
+/// The links used to link an object on a linked list.
+///
+/// Instances of this type are usually embedded in structures and returned in calls to
+/// [`GetLinks::get_links`].
+pub struct Links<T: ?Sized> {
+    entry: UnsafeCell<ListEntry<T>>,
+}
+
+impl<T: ?Sized> Links<T> {
+    /// Constructs a new [`Links`] instance that isn't inserted on any lists yet.
+    pub fn new() -> Self {
+        Self {
+            entry: UnsafeCell::new(ListEntry::new()),
+        }
+    }
+}
+
+impl<T: ?Sized> Default for Links<T> {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+struct ListEntry<T: ?Sized> {
+    next: Option<NonNull<T>>,
+    prev: Option<NonNull<T>>,
+}
+
+impl<T: ?Sized> ListEntry<T> {
+    fn new() -> Self {
+        Self {
+            next: None,
+            prev: None,
+        }
+    }
+}
+
+/// A linked list.
+///
+/// # Invariants
+///
+/// The links of objects added to a list are owned by the list.
+pub struct RawList<G: GetLinks> {
+    head: Option<NonNull<G::EntryType>>,
+}
+
+impl<G: GetLinks> RawList<G> {
+    pub fn new() -> Self {
+        Self { head: None }
+    }
+
+    pub fn is_empty(&self) -> bool {
+        self.head.is_none()
+    }
+
+    fn insert_after_priv(
+        &mut self,
+        existing: &G::EntryType,
+        new_entry: &mut ListEntry<G::EntryType>,
+        new_ptr: Option<NonNull<G::EntryType>>,
+    ) {
+        {
+            // SAFETY: It's safe to get the previous entry of `existing` because the list cannot
+            // change.
+            let existing_links = unsafe { &mut *G::get_links(existing).entry.get() };
+            new_entry.next = existing_links.next;
+            existing_links.next = new_ptr;
+        }
+
+        new_entry.prev = Some(NonNull::from(existing));
+
+        // SAFETY: It's safe to get the next entry of `existing` because the list cannot change.
+        let next_links =
+            unsafe { &mut *G::get_links(new_entry.next.unwrap().as_ref()).entry.get() };
+        next_links.prev = new_ptr;
+    }
+
+    /// Inserts the given object after `existing`.
+    ///
+    /// # Safety
+    ///
+    /// Callers must ensure that `existing` points to a valid entry that is on the list.
+    pub unsafe fn insert_after(
+        &mut self,
+        existing: &G::EntryType,
+        new: &G::EntryType,
+    ) -> bool {
+        let links = G::get_links(new);
+        // if !links.acquire_for_insertion() {
+        //     // Nothing to do if already inserted.
+        //     return false;
+        // }
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let new_entry = unsafe { &mut *links.entry.get() };
+        self.insert_after_priv(existing, new_entry, Some(NonNull::from(new)));
+        true
+    }
+
+    fn push_back_internal(&mut self, new: &G::EntryType) -> bool {
+        let links = G::get_links(new);
+        // if !links.acquire_for_insertion() {
+        //     // Nothing to do if already inserted.
+        //     return false;
+        // }
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let new_entry = unsafe { &mut *links.entry.get() };
+        let new_ptr = Some(NonNull::from(new));
+        match self.back() {
+            // SAFETY: `back` is valid as the list cannot change.
+            Some(back) => self.insert_after_priv(unsafe { back.as_ref() }, new_entry, new_ptr),
+            None => {
+                self.head = new_ptr;
+                new_entry.next = new_ptr;
+                new_entry.prev = new_ptr;
+            }
+        }
+        true
+    }
+
+    pub unsafe fn push_back(&mut self, new: &G::EntryType) -> bool {
+        self.push_back_internal(new)
+    }
+
+    fn remove_internal(&mut self, data: &G::EntryType) -> bool {
+        let links = G::get_links(data);
+
+        // SAFETY: The links are now owned by the list, so it is safe to get a mutable reference.
+        let entry = unsafe { &mut *links.entry.get() };
+        let next = if let Some(next) = entry.next {
+            next
+        } else {
+            // Nothing to do if the entry is not on the list.
+            return false;
+        };
+
+        if ptr::eq(data, next.as_ptr()) {
+            // We're removing the only element.
+            self.head = None
+        } else {
+            // Update the head if we're removing it.
+            if let Some(raw_head) = self.head {
+                if ptr::eq(data, raw_head.as_ptr()) {
+                    self.head = Some(next);
+                }
+            }
+
+            // SAFETY: It's safe to get the previous entry because the list cannot change.
+            unsafe { &mut *G::get_links(entry.prev.unwrap().as_ref()).entry.get() }.next =
+                entry.next;
+
+            // SAFETY: It's safe to get the next entry because the list cannot change.
+            unsafe { &mut *G::get_links(next.as_ref()).entry.get() }.prev = entry.prev;
+        }
+
+        // Reset the links of the element we're removing so that we know it's not on any list.
+        entry.next = None;
+        entry.prev = None;
+        // links.release_after_removal();
+        true
+    }
+
+    /// Removes the given entry.
+    ///
+    /// # Safety
+    ///
+    /// Callers must ensure that `data` is either on this list or in no list. It being on another
+    /// list leads to memory unsafety.
+    pub unsafe fn remove(&mut self, data: &G::EntryType) -> bool {
+        self.remove_internal(data)
+    }
+
+    fn pop_front_internal(&mut self) -> Option<NonNull<G::EntryType>> {
+        let head = self.head?;
+        // SAFETY: The head is on the list as we just got it from there and it cannot change.
+        unsafe { self.remove(head.as_ref()) };
+        Some(head)
+    }
+
+    pub fn pop_front(&mut self) -> Option<NonNull<G::EntryType>> {
+        self.pop_front_internal()
+    }
+
+    pub fn front(&self) -> Option<NonNull<G::EntryType>> {
+        self.head
+    }
+
+    pub fn back(&self) -> Option<NonNull<G::EntryType>> {
+        // SAFETY: The links of head are owned by the list, so it is safe to get a reference.
+        unsafe { &*G::get_links(self.head?.as_ref()).entry.get() }.prev
+    }
+}
\ No newline at end of file
diff --git a/lab4.md b/lab4.md
new file mode 100644
index 000000000..ec7ffb162
--- /dev/null
+++ b/lab4.md
@@ -0,0 +1,455 @@
+# OS Lab 4  mem
+
+ 在rros的基础上填空实现一个内存分配算法
+
+实现一个相对实时的内存分配算法（主要是rfl的内存分配算法如何和linux集成部分），对代码挖空，把内存分配算法部分挖掉
+
+写一个实验手册Reference
+
+* 实时内存分配算法论文1
+
+* 实时内存分配算法论文2
+
+## Intro
+
+
+
+## 提交格式
+
+## Quick Start
+
+### 环境搭建
+
+（这里应该和上一个实验一致，因此看上一个实验怎么搭建了.）
+
+主要要修改的文件；
+
+* `rust/kernel/memory_rros.rs`
+
+
+
+### Rust for linux环境介绍
+
+
+
+## 内存分配器的相关知识
+
+### 常见内存分配算法
+
+
+
+### 常用机制
+
+大多数动态内存分配(DSA)算法都使用了下面的一个或者多个机制的组合。这里将它们列出，我们主要参考了[Dynamic Storage Allocation: A Survey and Critical Review](https://users.cs.northwestern.edu/~pdinda/ics-s05/doc/dsa.pdf) ，读者感兴趣可以深入了解。
+
+* **header**
+
+  大多数分配器分配的块上面都会带着一个特殊的header，其中包括了块的重要信息，例如块的长度。一般来说，header字段的长度都是一个字长。由于分配的块长度一般是对齐的，尾部会有几个冗余的bit。这几个bit一般作为状态位。
+
+* **boundary tags**
+
+  除了header，有的块还会有footer字段。footer同样保存块长度和块是否被使用。在块需要合并的时候，可以方便地检查上面的块是否可以被合并（只需要看它的footer字段即可）。
+
+  同时使用header和footer浪费了很多空间。但是实际上，因为我们只在合并块的时候用到footer，因此footer字段在块被使用的时候是无效的，可以只在块不被用的时候使用footer，这样footer块就没有（使用时）空间开销了。
+
+  TODO: 加个图
+
+* **link field**
+
+  链表和树结构经常被用于管理块内存。同样，一般只有空闲块需要被管理，因此链表/树节点直接被放在空闲块内（当然，需要限制最小块的大小，以便能放下节点）；当块被分配的时候，块的所有权转移给用户，并被从链表/树上面删除。
+
+* **lookup table**
+
+  一些分配器不会按给定大小分配块，而是向上取整到一个大小后分配给用户。而分配器内部会按一定规则预先放好一些固定大小的块，分配时直接查找对应大小的块链表/树即可。通常会按2的幂去分配，也有使用斐波那契函数等的。但是，lookup table如果分得太细可能占用很大空间。
+
+* **针对小对象的优化**
+
+  对于大多数系统，分配小对象的次数会远多于大对象。因此一些分配器会特殊处理小对象的分配。一些常见的组合有：对小对象使用快速分配算法，对大对象使用节省空间的技巧；对小对象使用lookup table，对大对象使用比较复杂的计算（时间换空间）。
+
+## TLSF(Two-Level Segregated Fit) 实时内存分配算法
+
+在这一部分，我们将介绍一个比较简单的实时内存分配算法——TLSF。它在[TLSF: A new dynamic memory allocator for real-time systems(Masmano,et al.)](http://www.gii.upv.es/tlsf/files/ecrts04_tlsf.pdf)这篇论文中提出。
+
+TLSF是针对实时操作系统的动态内存分配算法，能够在`O(1)`的时间内返回分配的内存。TLSF的分配速度快，但是相对内存利用率低，容易出现内部碎片，比较常用于嵌入式场景。
+
+### 算法介绍
+
+这一部分，我们将介绍TLSF算法。我们采用的实现是参考[mattconte/tlsf: Two-Level Segregated Fit memory allocator implementation. (github.com)](https://github.com/mattconte/tlsf)，这与原版的TLSF算法略有不同。
+
+#### 空闲内存管理
+
+TLSF将管理的空闲内存分为放在一个两维链表数组里，数组的每个元素表示该内存区间的free list（空闲内存链表）。
+
+
+
+![tlsf1](assets\tlsf1.png)
+
+具体来说，对于一个size的空闲块，它的映射公式如下：
+
+![tlsf1](assets\tlsf2.png)
+
+其中f表示第一层，s表示第二层，SLI(second level index)表示第二层管理的比特数。第一层是按2的幂进行划分，也就是说，最高bit是第几位，第一层f就是多少。这样内存就被划分为了[2^4,2^5-1], [2^5,2^6-1]....。第二层进一步把每个区间分为2^{SLI}份（分成几份都可以，但是分为2^{SLI}份按位运算更好算）。一般SLI=4或者5。
+
+例如，当SLI=4, size=460时
+$$
+f=log2(460) = 8，s = (16/256)*(460-256)=12
+$$
+![tlsf1](assets\tlsf3.png)
+
+再比如，当SLI=5, size=1234时
+$$
+f=log2(1234) = 10，s = (32/1024)*(1234-1024)=6
+$$
+上面f的最小值一般不为0。一方面，分配块一般会有最小块的限制，例如16字节或者32字节。另一方面，内存块大小通常会规定为4字节或者8字节的整数倍，这样块大小的低2,3个bit永远为0，可以作为标志位管理内存块。
+
+我们后面的默认实现采用的是SLI=5, 分配长度按8字节的整数倍。
+
+#### 内存块结构
+
+空闲块和非空闲块的内存元数据结构如下：
+
+![tlsf_block](assets\tlsf_block.jpg)
+
+可以看到，空闲块比非空闲块多了两个字段：`next_free`,`prev_free`。这两个字段是上一个小节空闲块管理中的链表用到的。`size`表示的是分配内存的大小（不包括元数据开销），上图的size是8的整数倍，因此最后3bit是空闲的，这里只用了两个bit，分别表示当前块是否空闲，以及前一个块是否空闲。`prev_phys_block` 是一个指向前一个块的指针。这里只有指向前一个块的指针，而没有指向后一个的。这是因为每个块都是紧挨着的，我们只需要将指针向后移动`sizeof(prev_phys_block) + sizeof(size) + size` 就可以跳转到下一个块。而前一个块，则无法通过这种方式得到位置。
+
+这里可以做一个优化，让`prev_phys_block`在非空闲块中不占用位置。在算法中，`prev_phys_block`是用来做空闲块间合并的。`prev_phys_block`（指向前一个块的指针）只有在`is_prev_free`（前一个块是空闲）时才会被使用， 因此可以让`prev_phys_block`“侵占”前一个块的位置。如果前一个块有效，那么我们的`prev_phys_block`可能会被用户数据覆盖了，但是它本身也用不到，覆盖了也没事；如果前一个块是空闲的，`prev_phys_block`则不会被覆盖掉。可以参见下一小节中的示意图，其中虚线的`prev_phys_block`表示不能使用的指针，实线表示可以使用。
+
+块的完整视图如下，其中block指针表示操作块时用的指针，而ptr是返回给用户的指针。当要释放内存时，取到的指针也是这个指针。将它向上移动两个字长就能得到操作块的指针
+
+![used_block](assets\used_block.jpg)
+
+#### 控制块的结构体
+
+控制块除了上面提到了二维数组链表，还有两个字段`fl_bitmap`,`sl_bitmap[]` 使用bit位表示对应区间的内存是否还有空闲。在管理内存的最后面有一个哨兵块，它的长度为0。同时，第一个块的`is_prev_free`会置为0，防止访问到分配内存外的地址。
+
+![tlsf1](assets\overview.jpg)
+
+#### 初始化
+
+初始化时，将fl_bitmap和sl_bitmap置为0，表示没有空闲块。然后初始化blocks数组里面的链表头。
+
+当向控制块添加一块管理的内存时，把连续的内存当做一个块插入到对应二维数组的链表中，设置bitmap。注意这个块的`size`需要预留位置给哨兵块，同时还有扣除自身元数据`size`的大小（8字节），一共是16字节。设置这个块`is_prev_free`为false，跳转到这个块最后面，插入一个哨兵块。
+
+例如，在SLI=5的时候，初始化的时候插入的内存块是2048字节，其中16字节被用于做元数据，可用长度为2032字节。那么初始化后有两个块，一个长度为2032字节，一个长度为0字节。2032字节的块会被插入到f=10,s=31的链表里，相应的标志位也会被置位。
+
+在后面的实现中，因为我们按8字节对齐，并且SLI=5，因此最小的大块应为256字节而不是0字节。我们可以把f减去(5+3-1)=7以节省数组空间。**后面使用fl 指代偏移后的f**，当fl=0时，区间表示[0,256)字节；当fl=1时，区间为[256,512)字节，对应原来的f=8，以此类推。
+
+因此，插入2048字节（实际可用2032字节）时，fl=3,sl=31。`fl_bitmap |= 1 << 3`，`sl_bitmap[3] |= 1<<31`
+
+#### 获取一块内存
+
+当请求一块内存时，首先要把大小按8字节向上对齐。然后查找对应大小区间中的空闲链表，看是否有可用的内存。之前维护的bitmap用来寻找符合要求的最小内存块。
+
+例如，SLI=5时，分配一块大小为460的内存，对齐后是464，那么：
+$$
+f=log2(464)=8,fl = 8-7=1，s = (32/256)*(464-256)=26
+$$
+查询bitmap，发现`sl_bitmap[1]`等于0，没有对应的块；再查询`fl_bitmap`，发现最低置位(`lowbit`)为第4个bit，`sl_bitmap[3]`的第32个bit为1，因此从`blocks[3][31]`的链表队头或者队尾取出一个空闲块。如果这时链表为空，需要对bitmap清空。
+
+找到一个块后，还要对块进行切割，切出需要分配的字节数，剩下的内存如果可以建块，那么新建一个块，然后将它加入到空闲链表中。以上面的情况为例，得到的是2032字节的块，切出464字节后还剩1568字节，可以新建一个1560长度的块（还有8字节用于存放长度）。
+
+最后，将得到的块转换成一个指针，返回给用户。
+
+![used_block](assets\used_block.jpg)
+
+#### 释放一块内存
+
+释放内存的时候，首先要把用户的指针转换成块的指针。将这个块B标记为空闲块，下一个块C的`is_prev_free`标记置为true。
+
+![used_block](assets\release1.jpg)
+
+然后，尝试将这个块与它的邻居（上下的块）合并。对于上面的块A，如果当前块B`is_prev_free=true`,那么合并A，B两个块，得到新的块B。同理，如果下面的块C也是空闲的，那么将它合并。最后，将当前块B插入到空闲链表中。
+
+![used_block](assets\release2.jpg)
+
+#### 参考
+
+* [esp-idf的内存管理——tlsf算法](https://blog.csdn.net/gzxb1995/article/details/124504705)
+* [TLSF 内存分配算法详解](https://blog.csdn.net/pwl999/article/details/118253758)
+
+### 任务
+
+在这一部分，你需要实现一个TLSF分配器，并通过相应测试。
+
+你不需要独立完成全部代码，我们已经提供了BlockHeader的部分代码和一个简单的框架。你需要完成的代码主要在`rust/kernel/tlsf.rs`，测试的代码在`test/rros/test_tlsf.rs`。
+
+为了减小实现的难度，你可以使用你在Lab1里面实现的链表来管理空闲内存，而不是直接使用双向指针。下面给出了一个示例的结构体定义：
+
+```rust
+const FL_INDEX_COUNT:usize = 25;
+const SL_INDEX_COUNT:usize = 32;
+pub struct TLSFControl<'a> {
+    fl_bitmap: usize,
+    sl_bitmap: [usize; FL_INDEX_COUNT],
+    blocks: [[LinkedList<&'a mut BlockHeader>; SL_INDEX_COUNT]; FL_INDEX_COUNT],
+}
+```
+
+如果你不熟悉位运算，你也可以使用bool数组来替代bitmap。
+
+虽然上面的控制块只占用几KB的内存，但是linux内核栈一般也只有4KB或者8KB，如果把上面的控制块放在内核栈上面会导致内核栈溢出。因此我们要把控制块整体放到堆上。在框架代码中，我们使用了Rust的MaybeUninit。调用Box::try_new_uninit_in没有马上进行初始化，而是返回一个`Box<MaybeUninit<T>>`.然后我们直接对堆上的内存初始化。这样可以防止在初始化阶段就发生栈溢出。这里提供一个初始化的示例代码：
+
+```rust
+pub fn init_on_heap(tmp : Box<TLSFControl<'a>,Global>) -> Box<Self,Global>{
+    // TODO: YOUR CODE HERE
+    for i in 0..FL_INDEX_COUNT {
+        for j in 0..SL_INDEX_COUNT {
+            tmp.blocks[i][j] = LinkedList::new();
+        }
+    }
+    tmp.fl_bitmap = 0;
+    tmp.sl_bitmap = [0; FL_INDEX_COUNT];
+    tmp
+    // END OF YOUR CODE
+}
+```
+
+
+
+#### 实现blockHeader
+
+我们已经实现了一个简单的BlockHeader，即管理内存块的结构体，并提供了一些可能会用到的方法；你也可以重写一个BlockHeader，但是它内存中的保存方式要和下图相同（size可以不用是8字节的，4字节的u32也可以。剩下4字节放标志位）
+
+![used_block](assets\block.jpg)
+
+如果你直接使用我们所给的BlockHeader，你还需要自己实现`split`和`link_next`两个函数。
+
+* `BlockHeader::split`： 将一个块分为两个块，前一个块的大小为size，后一个块大小为`self.size-size-BLOCK_HEADER_OVERHEAD`。如果无法分割，返回None
+* `BlockHeader::link_next`： 找到下一个块，然后把下一个块的`prev_phys_block`指针指向当前块的地址。
+  * 获取当前块的地址可以这样写：`unsafe{next.prev_phys_block = self as *const _ as *mut BlockHeader;}`
+  * 下一个块的地址为`当前块地址+size-BLOCK_HEADER_OVERHEAD`
+
+当你的代码正确的时候，你应该能通过`test/rros/test_tlsf.rs`中的测试`test_blockHeader`。
+
+#### 堆的初始化
+
+接下来实现堆的初始化。完善
+
+* `TLSFControl::init_on_heap`： 初始化控制块
+* `TLSFControl::add_pool` : 将一个内存地址加入管理
+
+两个函数。这一部分只需要按照上面所述实现即可.
+
+当你的代码正确的时候，你应该能通过`test/rros/test_tlsf.rs`中的测试`test_init`。
+
+#### mapping
+
+实现一个mapping函数，将size映射为对应的f和s。完善
+
+* `mapping_insert` 将size映射到fl和sl
+
+函数。
+
+后面你会用到mapping函数。一共有两个mapping函数，`mapping_insert`和`mapping_search`（已实现）。`mapping_insert`在插入空闲块时使用，`mapping_search`用于分配内存时获取比给定size稍大的块。
+
+#### malloc
+
+下面实现一个malloc的接口。这是malloc的伪代码：
+
+```
+void* malloc(self,size){
+	var size = adjust_size(size); // 将size向上对齐，可以使用align_up函数
+	var block = self.block_locate_free(size); //找到一个合适的块
+	if !block{
+		return null;// 返回空指针
+	}
+	var remain_block = block.split(size); // 把原来的块切成两部分
+	设置remain_block的标记位，并把remain_block的上一个块设置为block的地址
+	self.insert(remain_block); // 加入空闲链表
+	return block.get_ptr(); // 返回指针
+}
+```
+
+完善`TLSFControl::malloc`函数。你可能需要花较多时间在`block_locate_free`函数。
+
+当你的代码正确的时候，你应该能通过`test/rros/test_tlsf.rs`中的测试`test_malloc`。
+
+#### 内存释放
+
+最后实现一个free的接口。这是free的伪代码：
+
+```
+void free(self,ptr){
+	var block = BlockHeader::from_raw_pointer(ptr);
+	将block设置为free，同时block的下一个块的is_prev_free要设置为true.
+	尝试合并上一块和当前块
+	尝试合并当前块和下一块
+	self.insert(block); // 将当前块插入空闲链表
+}
+```
+
+完善`TLSFControl::free`函数。
+
+当你的代码正确的时候，你应该能通过`test/rros/test_tlsf.rs`中的测试`test_free`和`test_multiple_alloc`。
+
+#### rust的alloc_api
+
+rust提供了一个`Allocator` trait。当一个结构体实现了`allocate`和`deallocate`两个属性时，它就能成为一个`Allocator` 。我们已经为`TLSFMem`实现了`Allocator` 。`tlsf.rs`声明了一个全局静态变量`TLSFHeap`。构造器对象`TLSFMem`会调用`TLSFHeap`进行内存分配和销毁。`TLSFMem`初始时是一个None。当调用init_tlsfheap时，会分配给堆若干个页，然后初始化内存池。
+
+```rust
+pub fn init_tlsfheap(){
+    unsafe{
+        let tmp = Box::<TLSFControl<'static>>::try_new_uninit_in(Global).unwrap().assume_init();
+        TLSFHeap = Some(
+            TLSFControl::init_on_heap(tmp)
+        );
+        let size = 4096*4;
+        let ptr = vmalloc::c_vmalloc(size).unwrap();
+        TLSFHeap.as_mut().unwrap().add_pool(ptr as *mut u8,size as usize);
+    }
+}
+```
+
+我们编写了一个测试`tlsf_allocator`。这里会使用你写的堆进行内存的分配和销毁，
+
+```rust
+pub fn tlsf_allocator(){
+    unsafe{
+        for i in 0..100{
+            let a = Box::try_new_in("hello world from our allocator", TLSFMem).unwrap();
+            let b = Box::try_new_in(123456789, TLSFMem).unwrap();
+            let c = Box::try_new_in(1.23456789, TLSFMem).unwrap();
+            let d = Box::try_new_in([1,2,3,4,5,6,7,8,9,0], TLSFMem).unwrap();
+            let e = Box::try_new_in((1,2,3,4,5), TLSFMem).unwrap();
+        }
+        let a = Box::try_new_in("hello world from our allocator", TLSFMem).unwrap();
+        pr_info!("{}",a);
+        pr_info!("tlsf_allocator ok");
+    }
+}
+
+```
+
+如果之前测试全部顺利通过，你应该能在运行时看到“hello world from our allocator”
+
+#### 内存释放
+
+#### 测试截图
+
+当全部测试成功时，会在输出中看到：
+
+![test succ](assets\test_succ.png)
+
+而如果测试中出现了错误，会直接导致panic，init进程被杀死：
+
+![test fail](assets\failed.png)
+
+TIPS：
+
+> * 如果在内核环境下面开发和调试比较困难，也可以新建cargo项目并拷贝TLSF和测试的代码进行测试和调试。但是，你的实现应该没有第三方的依赖，因为rust for linux不支持cargo。（当然，如果你可以把依赖的代码复制到项目里进来，并且能通过内核的编译，也是可以的）
+> * 你可以使用你自己实现的链表来完成这个实验，也可以用kernel内实现好的一个链表(`rust/kernel/double_linked_list`)
+> * 在使用堆分配内存时，和std环境下不同，你需要使用`use alloc::boxed::Box;`引入`Box`。由于rust for linux传入了`no_global_oom_handling`标志，因此不能使用`Box::new`,`Vec`等接口。你可以使用`Box::try_new_in<T,Global>.unwrap()`来替代(`use alloc::alloc::Global;`)。
+
+## EVL中的实时内存分配算法
+
+
+
+## 测试和调试
+
+### 调试
+
+#### gdb
+
+使用gdb的remote debug可以调试内核。首先在启动内核的命令后面加上`-s -S`（-s 表示启动gdb server，-S表示不要立刻执行指令，按`c`可以开始执行）。例如：
+
+```bash
+qemu-system-aarch64 -nographic  -kernel arch/arm64/boot/Image -initrd /data/rootfs.cpio.gz  -machine type=virt -cpu cortex-a57 -append "rdinit=/linuxrc console=ttyAMA0" -device virtio-scsi-device -smp 1 -m 4096 -s -S
+```
+
+其中`arch/arm64/boot/Image`是内核的路径，`/data/rootfs.cpio.gz`是文件系统的路径
+
+然后新建一个窗口，启动gdb(例如`gdb-multiarch`）。在gdb里，输入下面的命令
+
+```
+file vmlinux
+target remote localhost:1234
+set lang rust
+```
+
+然后，在文件上面断点，例如：
+
+```
+b rust/kernel/memory_rros.rs:131
+```
+
+表示在131行断点。
+
+输入`c`(continue)开始调试。
+
+下面给出了gdb的一些常见命令
+
+TODO:加图片
+
+#### vscode
+
+也可以给vscode添加配置文件。
+
+首先，同样还是在命令行启动调试的qemu：
+
+```
+qemu-system-aarch64 -nographic  -kernel arch/arm64/boot/Image -initrd /data/rootfs.cpio.gz  -machine type=virt -cpu cortex-a57 -append "rdinit=/linuxrc console=ttyAMA0" -device virtio-scsi-device -smp 1 -m 4096 -s -S
+```
+
+其中`arch/arm64/boot/Image`是内核的路径，`/data/rootfs.cpio.gz`是文件系统的路径
+
+
+
+然后，在项目根目录的.vscode文件夹中，打开launch.json(没有的话新建一个)，把下面的配置粘贴进去：
+
+```json
+{
+    // Use IntelliSense to learn about possible attributes.
+    // Hover to view descriptions of existing attributes.
+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
+    "version": "0.2.0",
+    "configurations": [
+        {
+            "name": "linux kernel debug",
+            "type": "cppdbg",
+            "request": "launch",
+            "miDebuggerServerAddress": "127.0.0.1:1234",
+            "program": "${workspaceFolder}/vmlinux",
+            "args": [],
+            "stopAtEntry": false,
+            "cwd": "${workspaceFolder}",
+            "environment": [],
+            "externalConsole": false,
+            "logging": {
+                "engineLogging": false
+            },
+            "MIMode": "gdb",
+        }
+    ]
+}
+```
+
+
+
+## TODO
+
+设计思路：
+
+* 稍微提一下rust for linux
+* 讲解linux红黑树.
+
+要讲解的模块
+
+* `rust/kernel/memory_rros.rs`
+* linux红黑树部分
+* 和linux的接口
+
+`string.rs`
+
+### 要写的模块
+
+* `insert_range_bysize`
+
+* `evl_alloc_chunk`
+
+* `reserve_page_range` ,`search_size_ge`
+* `remove_page`
+* `add_page_front`
+
+## Q&A
+
+### 如果需要使用`#feature[]`
\ No newline at end of file
diff --git a/rust/alloc/alloc_rros.rs b/rust/alloc/alloc_rros.rs
new file mode 100644
index 000000000..a461a3662
--- /dev/null
+++ b/rust/alloc/alloc_rros.rs
@@ -0,0 +1,210 @@
+// SPDX-License-Identifier: Apache-2.0 OR MIT
+
+//! Memory allocation APIs
+
+#![stable(feature = "alloc_module", since = "1.28.0")]
+
+#[cfg(not(test))]
+use core::intrinsics;
+use core::intrinsics::{min_align_of_val, size_of_val};
+
+use core::ptr::Unique;
+#[cfg(not(test))]
+use core::ptr::{self, NonNull};
+
+#[stable(feature = "alloc_module", since = "1.28.0")]
+#[doc(inline)]
+pub use core::alloc::*;
+
+#[cfg(test)]
+mod tests;
+
+extern "Rust" {
+    #[rustc_allocator]
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc(size: usize, align: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_dealloc(ptr: *mut u8, size: usize, align: usize);
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;
+    #[rustc_allocator_nounwind]
+    fn __rros_sys_heap_alloc_zerod(size: usize, align: usize) -> *mut u8;
+}
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[derive(Copy, Clone, Default, Debug)]
+#[cfg(not(test))]
+pub struct RrosMem;
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc(layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_dealloc(ptr: *mut u8, layout: Layout) {
+    unsafe { __rros_sys_heap_dealloc(ptr, layout.size(), layout.align()) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+    unsafe { __rros_sys_heap_realloc(ptr, layout.size(), layout.align(), new_size) }
+}
+
+#[stable(feature = "rros_alloc", since = "1.28.0")]
+#[inline]
+pub unsafe fn rros_alloc_zeroed(layout: Layout) -> *mut u8 {
+    unsafe { __rros_sys_heap_alloc_zerod(layout.size(), layout.align()) }
+}
+
+#[cfg(not(test))]
+impl RrosMem {
+    #[inline]
+    fn alloc_impl(&self, layout: Layout, zeroed: bool) -> Result<NonNull<[u8]>, AllocError> {
+        match layout.size() {
+            0 => Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0)),
+            size => unsafe {
+                let raw_ptr = if zeroed { rros_alloc_zeroed(layout) } else { rros_alloc(layout) };
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, size))
+            },
+        }
+    }
+
+    // SAFETY: Same as `Allocator::grow`
+    #[inline]
+    unsafe fn grow_impl(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+        zeroed: bool,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() >= old_layout.size(),
+            "`new_layout.size()` must be greater than or equal to `old_layout.size()`"
+        );
+
+        match old_layout.size() {
+            0 => self.alloc_impl(new_layout, zeroed),
+
+            // SAFETY: `new_size` is non-zero as `old_size` is greater than or equal to `new_size`
+            // as required by safety conditions. Other conditions must be upheld by the caller
+            old_size if old_layout.align() == new_layout.align() => unsafe {
+                let new_size = new_layout.size();
+
+                // `realloc` probably checks for `new_size >= old_layout.size()` or something similar.
+                intrinsics::assume(new_size >= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                if zeroed {
+                    raw_ptr.add(old_size).write_bytes(0, new_size - old_size);
+                }
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_layout.size()` must be greater than or equal to `old_size`,
+            // both the old and new memory allocation are valid for reads and writes for `old_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            old_size => unsafe {
+                let new_ptr = self.alloc_impl(new_layout, zeroed)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), old_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
+
+#[unstable(feature = "allocator_api", issue = "32838")]
+#[cfg(not(test))]
+unsafe impl Allocator for RrosMem {
+    #[inline]
+    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, false)
+    }
+
+    #[inline]
+    fn allocate_zeroed(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
+        self.alloc_impl(layout, true)
+    }
+
+    #[inline]
+    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
+        if layout.size() != 0 {
+            // SAFETY: `layout` is non-zero in size,
+            // other conditions must be upheld by the caller
+            unsafe { rros_dealloc(ptr.as_ptr(), layout) }
+        }
+    }
+
+    #[inline]
+    unsafe fn grow(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, false) }
+    }
+
+    #[inline]
+    unsafe fn grow_zeroed(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        // SAFETY: all conditions must be upheld by the caller
+        unsafe { self.grow_impl(ptr, old_layout, new_layout, true) }
+    }
+
+    #[inline]
+    unsafe fn shrink(
+        &self,
+        ptr: NonNull<u8>,
+        old_layout: Layout,
+        new_layout: Layout,
+    ) -> Result<NonNull<[u8]>, AllocError> {
+        debug_assert!(
+            new_layout.size() <= old_layout.size(),
+            "`new_layout.size()` must be smaller than or equal to `old_layout.size()`"
+        );
+
+        match new_layout.size() {
+            // SAFETY: conditions must be upheld by the caller
+            0 => unsafe {
+                self.deallocate(ptr, old_layout);
+                Ok(NonNull::slice_from_raw_parts(new_layout.dangling(), 0))
+            },
+
+            // SAFETY: `new_size` is non-zero. Other conditions must be upheld by the caller
+            new_size if old_layout.align() == new_layout.align() => unsafe {
+                // `realloc` probably checks for `new_size <= old_layout.size()` or something similar.
+                intrinsics::assume(new_size <= old_layout.size());
+
+                let raw_ptr = rros_realloc(ptr.as_ptr(), old_layout, new_size);
+                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;
+                Ok(NonNull::slice_from_raw_parts(ptr, new_size))
+            },
+
+            // SAFETY: because `new_size` must be smaller than or equal to `old_layout.size()`,
+            // both the old and new memory allocation are valid for reads and writes for `new_size`
+            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap
+            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract
+            // for `dealloc` must be upheld by the caller.
+            new_size => unsafe {
+                let new_ptr = self.allocate(new_layout)?;
+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), new_size);
+                self.deallocate(ptr, old_layout);
+                Ok(new_ptr)
+            },
+        }
+    }
+}
\ No newline at end of file
diff --git a/rust/alloc/lib.rs b/rust/alloc/lib.rs
index f109e7902..84a37e361 100644
--- a/rust/alloc/lib.rs
+++ b/rust/alloc/lib.rs
@@ -161,7 +161,7 @@ mod macros;
 // Heaps provided for low-level allocation strategies
 
 pub mod alloc;
-
+pub mod alloc_rros;
 // Primitive types using the heaps above
 
 // Need to conditionally define the mod from `boxed.rs` to avoid
diff --git a/rust/alloc/rc.rs b/rust/alloc/rc.rs
index 7344cd9a4..6ac5ecb70 100644
--- a/rust/alloc/rc.rs
+++ b/rust/alloc/rc.rs
@@ -1,5 +1,3 @@
-// SPDX-License-Identifier: Apache-2.0 OR MIT
-
 //! Single-threaded reference-counting pointers. 'Rc' stands for 'Reference
 //! Counted'.
 //!
@@ -43,7 +41,7 @@
 //! use std::rc::Rc;
 //!
 //! let my_rc = Rc::new(());
-//! Rc::downgrade(&my_rc);
+//! let my_weak = Rc::downgrade(&my_rc);
 //! ```
 //!
 //! `Rc<T>`'s implementations of traits like `Clone` may also be called using
@@ -143,7 +141,7 @@
 //! ```
 //!
 //! If our requirements change, and we also need to be able to traverse from
-//! `Owner` to `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
+//! `Owner` to `Gadget`, we will run into problems. An [`Rc`] pointer from `Owner`
 //! to `Gadget` introduces a cycle. This means that their
 //! reference counts can never reach 0, and the allocation will never be destroyed:
 //! a memory leak. In order to get around this, we can use [`Weak`]
@@ -264,6 +262,7 @@ use core::marker::{self, PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw, forget};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 #[cfg(not(no_global_oom_handling))]
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
@@ -306,23 +305,51 @@ struct RcBox<T: ?Sized> {
 /// [get_mut]: Rc::get_mut
 #[cfg_attr(not(test), rustc_diagnostic_item = "Rc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Rc<T: ?Sized> {
+#[rustc_insignificant_dtor]
+pub struct Rc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<RcBox<T>>,
     phantom: PhantomData<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Send for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Rc<T, A> {}
+
+// Note that this negative impl isn't strictly necessary for correctness,
+// as `Rc` transitively contains a `Cell`, which is itself `!Sync`.
+// However, given how important `Rc`'s `!Sync`-ness is,
+// having an explicit negative impl is nice for documentation purposes
+// and results in nicer error messages.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> !marker::Sync for Rc<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Rc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Rc<T, A> {}
+// #[stable(feature = "rc_ref_unwind_safe", since = "1.58.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> RefUnwindSafe for Rc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Rc<U>> for Rc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Rc<U, A>> for Rc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Rc<U>> for Rc<T> {}
 
 impl<T: ?Sized> Rc<T> {
+    #[inline]
+    unsafe fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
+    }
+
+    #[inline]
+    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
+        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     #[inline(always)]
     fn inner(&self) -> &RcBox<T> {
         // This unsafety is ok because while this Rc is alive we're guaranteed
@@ -330,12 +357,12 @@ impl<T: ?Sized> Rc<T> {
         unsafe { self.ptr.as_ref() }
     }
 
-    fn from_inner(ptr: NonNull<RcBox<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner_in(ptr: NonNull<RcBox<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
     }
 
-    unsafe fn from_ptr(ptr: *mut RcBox<T>) -> Self {
-        Self::from_inner(unsafe { NonNull::new_unchecked(ptr) })
+    unsafe fn from_ptr_in(ptr: *mut RcBox<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
@@ -356,50 +383,83 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Self::from_inner(
-            Box::leak(box RcBox { strong: Cell::new(1), weak: Cell::new(1), value }).into(),
-        )
+        unsafe {
+            Self::from_inner(
+                Box::leak(Box::new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value }))
+                    .into(),
+            )
+        }
     }
 
-    /// Constructs a new `Rc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Rc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
+    ///
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Rc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Rc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Rc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Rc<T>` is not fully-constructed until `Rc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
+    ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// # #![allow(dead_code)]
     /// use std::rc::{Rc, Weak};
     ///
     /// struct Gadget {
-    ///     self_weak: Weak<Self>,
-    ///     // ... more fields
+    ///     me: Weak<Gadget>,
     /// }
+    ///
     /// impl Gadget {
-    ///     pub fn new() -> Rc<Self> {
-    ///         Rc::new_cyclic(|self_weak| {
-    ///             Gadget { self_weak: self_weak.clone(), /* ... */ }
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Rc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Rc` we're constructing.
+    ///         Rc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
     ///         })
     ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Rc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
     /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Rc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Rc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box RcBox {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(RcBox {
             strong: Cell::new(0),
             weak: Cell::new(1),
             value: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
 
         let init_ptr: NonNull<RcBox<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -409,16 +469,16 @@ impl<T> Rc<T> {
         // otherwise.
         let data = data_fn(&weak);
 
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).value), data);
 
             let prev_value = (*inner).strong.get();
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
             (*inner).strong.set(1);
-        }
 
-        let strong = Rc::from_inner(init_ptr);
+            Rc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -438,17 +498,16 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -481,6 +540,7 @@ impl<T> Rc<T> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Rc<mem::MaybeUninit<T>> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -508,10 +568,12 @@ impl<T> Rc<T> {
         // pointers, which ensures that the weak destructor never frees
         // the allocation while the strong destructor is running, even
         // if the weak pointer is stored inside the strong one.
-        Ok(Self::from_inner(
-            Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
-                .into(),
-        ))
+        unsafe {
+            Ok(Self::from_inner(
+                Box::leak(Box::try_new(RcBox { strong: Cell::new(1), weak: Cell::new(1), value })?)
+                    .into(),
+            ))
+        }
     }
 
     /// Constructs a new `Rc` with uninitialized contents, returning an error if the allocation fails
@@ -526,12 +588,10 @@ impl<T> Rc<T> {
     ///
     /// let mut five = Rc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -584,9 +644,229 @@ impl<T> Rc<T> {
     /// `value` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(value: T) -> Pin<Rc<T>> {
         unsafe { Pin::new_unchecked(Rc::new(value)) }
     }
+}
+
+impl<T, A: Allocator> Rc<T, A> {
+    /// Constructs a new `Rc` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(value: T, alloc: A) -> Rc<T, A> {
+        // NOTE: Prefer match over unwrap_or_else since closure sometimes not inlineable.
+        // That would make code size bigger.
+        match Self::try_new_in(value, alloc) {
+            Ok(m) => m,
+            Err(_) => handle_alloc_error(Layout::new::<RcBox<T>>()),
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Rc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Rc<T>` in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Rc::try_new_in(5, System);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(value: T, alloc: A) -> Result<Self, AllocError> {
+        // There is an implicit weak pointer owned by all the strong
+        // pointers, which ensures that the weak destructor never frees
+        // the allocation while the strong destructor is running, even
+        // if the weak pointer is stored inside the strong one.
+        let (ptr, alloc) = Box::into_unique(Box::try_new_in(
+            RcBox { strong: Cell::new(1), weak: Cell::new(1), value },
+            alloc,
+        )?);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, in the provided allocator, returning an
+    /// error if the allocation fails
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Rc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Rc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if the allocation
+    /// fails
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api, new_uninit)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Rc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    //#[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Rc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Rc::from_ptr_in(
+                Rc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut RcBox<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Pin<Rc<T>>` in the provided allocator. If `T` does not implement `Unpin`, then
+    /// `value` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(value: T, alloc: A) -> Pin<Self> {
+        unsafe { Pin::new_unchecked(Rc::new_in(value, alloc)) }
+    }
 
     /// Returns the inner value, if the `Rc` has exactly one strong reference.
     ///
@@ -613,13 +893,14 @@ impl<T> Rc<T> {
         if Rc::strong_count(&this) == 1 {
             unsafe {
                 let val = ptr::read(&*this); // copy the contained object
+                let alloc = ptr::read(&this.alloc); // copy the allocator
 
                 // Indicate to Weaks that they can't be promoted by decrementing
                 // the strong count, and then remove the implicit "strong weak"
                 // pointer while also handling drop logic by just crafting a
                 // fake Weak.
                 this.inner().dec_strong();
-                let _weak = Weak { ptr: this.ptr };
+                let _weak = Weak { ptr: this.ptr, alloc };
                 forget(this);
                 Ok(val)
             }
@@ -642,19 +923,19 @@ impl<T> Rc<[T]> {
     ///
     /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe { Rc::from_ptr(Rc::allocate_for_slice(len)) }
     }
@@ -681,6 +962,7 @@ impl<T> Rc<[T]> {
     /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Rc<[mem::MaybeUninit<T>]> {
         unsafe {
             Rc::from_ptr(Rc::allocate_for_layout(
@@ -695,7 +977,84 @@ impl<T> Rc<[T]> {
     }
 }
 
-impl<T> Rc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Constructs a new reference-counted slice with uninitialized contents.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Rc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe { Rc::from_ptr_in(Rc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::rc::Rc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Rc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Rc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Rc::from_ptr_in(
+                Rc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut RcBox<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Rc<mem::MaybeUninit<T>, A> {
     /// Converts to `Rc<T>`.
     ///
     /// # Safety
@@ -718,70 +1077,185 @@ impl<T> Rc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Rc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Rc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<T> {
-        Rc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Rc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
+    }
+}
+
+impl<T, A: Allocator> Rc<[mem::MaybeUninit<T>], A> {
+    /// Converts to `Rc<[T]>`.
+    ///
+    /// # Safety
+    ///
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::rc::Rc;
+    ///
+    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Rc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Rc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Rc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Rc<T> {
+    /// Constructs an `Rc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// and alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The raw pointer must point to a block of memory allocated by the global allocator
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Rc<T>` is never accessed.
+    ///
+    /// [into_raw]: Rc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let x = Rc::new("hello".to_owned());
+    /// let x_ptr = Rc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Rc` to prevent leak.
+    ///     let x = Rc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Rc;
+    ///
+    /// let five = Rc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
+    ///
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Self::increment_strong_count_in(ptr, Global) }
     }
-}
 
-impl<T> Rc<[mem::MaybeUninit<T>]> {
-    /// Converts to `Rc<[T]>`.
+    /// Decrements the strong reference count on the `Rc<T>` associated with the
+    /// provided pointer by one.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    /// The pointer must have been obtained through `Rc::into_raw`, the
+    /// associated `Rc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by the global allocator. This method can be used to release the final `Rc` and
+    /// backing storage, but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::rc::Rc;
     ///
-    /// let mut values = Rc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Rc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Rc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Rc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Rc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Rc::into_raw(five);
+    ///     Rc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     let five = Rc::from_raw(ptr);
+    ///     assert_eq!(2, Rc::strong_count(&five));
+    ///     Rc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Rc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Rc<[T]> {
-        unsafe { Rc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Self::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Rc<T> {
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Consumes the `Rc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Rc` using
-    /// [`Rc::from_raw`][from_raw].
-    ///
-    /// [from_raw]: Rc::from_raw
+    /// [`Rc::from_raw`].
     ///
     /// # Examples
     ///
@@ -825,36 +1299,41 @@ impl<T: ?Sized> Rc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).value) }
     }
 
-    /// Constructs an `Rc<T>` from a raw pointer.
+    /// Constructs an `Rc<T, A>` from a raw pointer in the provided allocator.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Rc<U>::into_raw`][into_raw] where `U` must have the same size
+    /// [`Rc<U, A>::into_raw`][into_raw] where `U` must have the same size
     /// and alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
-    /// [`mem::transmute`][transmute] for more information on what
+    /// [`mem::transmute`] for more information on what
     /// restrictions apply in this case.
     ///
+    /// The raw pointer must point to a block of memory allocated by `alloc`
+    ///
     /// The user of `from_raw` has to make sure a specific value of `T` is only
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Rc<T>` is never accessed.
+    /// even if the returned `Rc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Rc::into_raw
-    /// [transmute]: core::mem::transmute
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let x = Rc::new("hello".to_owned());
+    /// let x = Rc::new_in("hello".to_owned(), System);
     /// let x_ptr = Rc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Rc` to prevent leak.
-    ///     let x = Rc::from_raw(x_ptr);
+    ///     let x = Rc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Rc::from_raw(x_ptr)` would be memory-unsafe.
@@ -862,15 +1341,15 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         let offset = unsafe { data_offset(ptr) };
 
         // Reverse the offset to find the original RcBox.
         let rc_ptr =
             unsafe { (ptr as *mut RcBox<T>).set_ptr_value((ptr as *mut u8).offset(-offset)) };
 
-        unsafe { Self::from_ptr(rc_ptr) }
+        unsafe { Self::from_ptr_in(rc_ptr, alloc) }
     }
 
     /// Creates a new [`Weak`] pointer to this allocation.
@@ -884,12 +1363,17 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// let weak_five = Rc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Rc`"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         this.inner().inc_weak();
         // Make sure we do not create a dangling Weak
         debug_assert!(!is_dangling(this.ptr.as_ptr()));
-        Weak { ptr: this.ptr }
+        Weak { ptr: this.ptr, alloc: this.alloc.clone() }
     }
 
     /// Gets the number of [`Weak`] pointers to this allocation.
@@ -933,30 +1417,37 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Rc, but don't touch refcount by wrapping in ManuallyDrop
-        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T>::from_raw(ptr)) };
+        let rc = unsafe { mem::ManuallyDrop::new(Rc::<T, A>::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _rc_clone: mem::ManuallyDrop<_> = rc.clone();
     }
@@ -966,33 +1457,36 @@ impl<T: ?Sized> Rc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Rc::into_raw`, and the
+    /// The pointer must have been obtained through `Rc::into_raw`, the
     /// associated `Rc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release
-    /// the final `Rc` and backing storage, but **should not** be called after
-    /// the final `Rc` has been released.
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final `Rc` and backing storage,
+    /// but **should not** be called after the final `Rc` has been released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::rc::Rc;
+    /// use std::alloc::System;
     ///
-    /// let five = Rc::new(5);
+    /// let five = Rc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Rc::into_raw(five);
-    ///     Rc::increment_strong_count(ptr);
+    ///     Rc::increment_strong_count_in(ptr, System);
     ///
-    ///     let five = Rc::from_raw(ptr);
+    ///     let five = Rc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Rc::strong_count(&five));
-    ///     Rc::decrement_strong_count(ptr);
+    ///     Rc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Rc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "rc_mutate_strong_count", since = "1.53.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Rc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Rc::from_raw_in(ptr, alloc)) };
     }
 
     /// Returns `true` if there are no other `Rc` or [`Weak`] pointers to
@@ -1009,7 +1503,7 @@ impl<T: ?Sized> Rc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Rc` pointers.
     ///
     /// [make_mut]: Rc::make_mut
     /// [clone]: Clone::clone
@@ -1084,24 +1578,24 @@ impl<T: ?Sized> Rc<T> {
     /// assert!(Rc::ptr_eq(&five, &same_five));
     /// assert!(!Rc::ptr_eq(&five, &other_five));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
 }
 
-impl<T: Clone> Rc<T> {
+impl<T: Clone, A: Allocator + Clone> Rc<T, A> {
     /// Makes a mutable reference into the given `Rc`.
     ///
     /// If there are other `Rc` pointers to the same allocation, then `make_mut` will
     /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
     /// referred to as clone-on-write.
     ///
-    /// If there are no other `Rc` pointers to this allocation, then [`Weak`]
-    /// pointers to this allocation will be disassociated.
+    /// However, if there are no other `Rc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be disassociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or diassociating [`Weak`] pointers.
     ///
     /// [`clone`]: Clone::clone
     /// [`get_mut`]: Rc::get_mut
@@ -1113,11 +1607,11 @@ impl<T: Clone> Rc<T> {
     ///
     /// let mut data = Rc::new(5);
     ///
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// let mut other_data = Rc::clone(&data);    // Won't clone inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Clones inner data
-    /// *Rc::make_mut(&mut data) += 1;        // Won't clone anything
-    /// *Rc::make_mut(&mut other_data) *= 2;  // Won't clone anything
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// let mut other_data = Rc::clone(&data); // Won't clone inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Clones inner data
+    /// *Rc::make_mut(&mut data) += 1;         // Won't clone anything
+    /// *Rc::make_mut(&mut other_data) *= 2;   // Won't clone anything
     ///
     /// // Now `data` and `other_data` point to different allocations.
     /// assert_eq!(*data, 8);
@@ -1147,7 +1641,7 @@ impl<T: Clone> Rc<T> {
         if Rc::strong_count(this) != 1 {
             // Gotta clone the data, there are other Rcs.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1155,7 +1649,7 @@ impl<T: Clone> Rc<T> {
             }
         } else if Rc::weak_count(this) != 0 {
             // Can just steal the data, all that's left is Weaks
-            let mut rc = Self::new_uninit();
+            let mut rc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Rc::get_mut_unchecked(&mut rc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1174,11 +1668,44 @@ impl<T: Clone> Rc<T> {
         // reference to the allocation.
         unsafe { &mut this.ptr.as_mut().value }
     }
-}
 
-impl Rc<dyn Any> {
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `rc_t` is of type `Rc<T>`, this function is functionally equivalent to
+    /// `(*rc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, rc::Rc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let rc = Rc::new(inner);
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let rc = Rc::new(inner);
+    /// let rc2 = rc.clone();
+    /// let inner = Rc::unwrap_or_clone(rc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `rc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Rc::unwrap_or_clone(rc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
     #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Rc::try_unwrap(this).unwrap_or_else(|rc| (*rc).clone())
+    }
+}
+
+impl<A: Allocator + Clone> Rc<dyn Any, A> {
     /// Attempt to downcast the `Rc<dyn Any>` to a concrete type.
     ///
     /// # Examples
@@ -1197,15 +1724,57 @@ impl Rc<dyn Any> {
     /// print_if_string(Rc::new(my_string));
     /// print_if_string(Rc::new(0i8));
     /// ```
-    pub fn downcast<T: Any>(self) -> Result<Rc<T>, Rc<dyn Any>> {
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T: Any>(self) -> Result<Rc<T, A>, Self> {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<RcBox<T>>();
-            forget(self);
-            Ok(Rc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<RcBox<T>>();
+                let alloc = self.alloc.clone();
+                forget(self);
+                Ok(Rc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Rc<dyn Any>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::rc::Rc;
+    ///
+    /// let x: Rc<dyn Any> = Rc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T: Any>(self) -> Rc<T, A> {
+        unsafe {
+            let ptr = self.ptr.cast::<RcBox<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Rc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T: ?Sized> Rc<T> {
@@ -1263,28 +1832,30 @@ impl<T: ?Sized> Rc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Rc<T, A> {
     /// Allocates an `RcBox<T>` with sufficient space for an unsized inner value
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut RcBox<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut RcBox<T> {
         // Allocate for the `RcBox<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Rc::<T>::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut RcBox<T>).set_ptr_value(mem),
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut RcBox<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Rc<T> {
+    fn from_box_in(v: Box<T, A>) -> Rc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1294,9 +1865,9 @@ impl<T: ?Sized> Rc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1376,6 +1947,20 @@ impl<T> Rc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Rc<[T], A> {
+    /// Allocates an `RcBox<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut RcBox<[T]> {
+        unsafe {
+            Rc::<[T]>::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut RcBox<[T]>,
+            )
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 trait RcFromSlice<T> {
     fn from_slice(slice: &[T]) -> Self;
@@ -1398,7 +1983,7 @@ impl<T: Copy> RcFromSlice<T> for Rc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Rc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Rc<T, A> {
     type Target = T;
 
     #[inline(always)]
@@ -1411,7 +1996,7 @@ impl<T: ?Sized> Deref for Rc<T> {
 impl<T: ?Sized> Receiver for Rc<T> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Rc<T, A> {
     /// Drops the `Rc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1449,7 +2034,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
                 self.inner().dec_weak();
 
                 if self.inner().weak() == 0 {
-                    Global.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
+                    self.alloc.deallocate(self.ptr.cast(), Layout::for_value(self.ptr.as_ref()));
                 }
             }
         }
@@ -1457,7 +2042,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Rc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Rc<T, A> {
     /// Makes a clone of the `Rc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1473,9 +2058,11 @@ impl<T: ?Sized> Clone for Rc<T> {
     /// let _ = Rc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Rc<T> {
-        self.inner().inc_strong();
-        Self::from_inner(self.ptr)
+    fn clone(&self) -> Self {
+        unsafe {
+            self.inner().inc_strong();
+            Self::from_inner_in(self.ptr, self.alloc.clone())
+        }
     }
 }
 
@@ -1499,20 +2086,20 @@ impl<T: Default> Default for Rc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait RcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Rc<T>) -> bool;
-    fn ne(&self, other: &Rc<T>) -> bool;
+trait RcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Rc<T, A>) -> bool;
+    fn ne(&self, other: &Rc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Rc<T>) -> bool {
+    default fn eq(&self, other: &Rc<T, A>) -> bool {
         **self == **other
     }
 
     #[inline]
-    default fn ne(&self, other: &Rc<T>) -> bool {
+    default fn ne(&self, other: &Rc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -1531,20 +2118,20 @@ impl<T: Eq> MarkerEq for T {}
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + MarkerEq> RcEqIdent<T> for Rc<T> {
+impl<T: ?Sized + MarkerEq, A: Allocator> RcEqIdent<T, A> for Rc<T, A> {
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         Rc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         !Rc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Rc<T, A> {
     /// Equality for two `Rc`s.
     ///
     /// Two `Rc`s are equal if their inner values are equal, even if they are
@@ -1564,7 +2151,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five == Rc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Rc<T>) -> bool {
+    fn eq(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::eq(self, other)
     }
 
@@ -1586,16 +2173,16 @@ impl<T: ?Sized + PartialEq> PartialEq for Rc<T> {
     /// assert!(five != Rc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Rc<T>) -> bool {
+    fn ne(&self, other: &Rc<T, A>) -> bool {
         RcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Rc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Rc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Rc<T, A> {
     /// Partial comparison for two `Rc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -1611,7 +2198,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Rc::new(6)));
     /// ```
     #[inline(always)]
-    fn partial_cmp(&self, other: &Rc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Rc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -1629,7 +2216,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five < Rc::new(6));
     /// ```
     #[inline(always)]
-    fn lt(&self, other: &Rc<T>) -> bool {
+    fn lt(&self, other: &Rc<T, A>) -> bool {
         **self < **other
     }
 
@@ -1647,7 +2234,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five <= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn le(&self, other: &Rc<T>) -> bool {
+    fn le(&self, other: &Rc<T, A>) -> bool {
         **self <= **other
     }
 
@@ -1665,7 +2252,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five > Rc::new(4));
     /// ```
     #[inline(always)]
-    fn gt(&self, other: &Rc<T>) -> bool {
+    fn gt(&self, other: &Rc<T, A>) -> bool {
         **self > **other
     }
 
@@ -1683,13 +2270,13 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Rc<T> {
     /// assert!(five >= Rc::new(5));
     /// ```
     #[inline(always)]
-    fn ge(&self, other: &Rc<T>) -> bool {
+    fn ge(&self, other: &Rc<T, A>) -> bool {
         **self >= **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Rc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Rc<T, A> {
     /// Comparison for two `Rc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -1705,34 +2292,34 @@ impl<T: ?Sized + Ord> Ord for Rc<T> {
     /// assert_eq!(Ordering::Less, five.cmp(&Rc::new(6)));
     /// ```
     #[inline]
-    fn cmp(&self, other: &Rc<T>) -> Ordering {
+    fn cmp(&self, other: &Rc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Rc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Rc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state);
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Rc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Rc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Rc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Rc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -1741,7 +2328,7 @@ impl<T: ?Sized> fmt::Pointer for Rc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Rc<T> {
-    /// Converts a generic type `T` into a `Rc<T>`
+    /// Converts a generic type `T` into an `Rc<T>`
     ///
     /// The conversion allocates on the heap and moves `t`
     /// from the stack into it.
@@ -1818,7 +2405,7 @@ impl From<String> for Rc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Rc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Rc<T, A> {
     /// Move a boxed object to a new, reference counted, allocation.
     ///
     /// # Example
@@ -1830,14 +2417,14 @@ impl<T: ?Sized> From<Box<T>> for Rc<T> {
     /// assert_eq!(1, *shared);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Rc<T> {
-        Rc::from_box(v)
+    fn from(v: Box<T, A>) -> Rc<T, A> {
+        Rc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Rc<[T]> {
+impl<T, A: Allocator> From<Vec<T, A>> for Rc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -1849,15 +2436,9 @@ impl<T> From<Vec<T>> for Rc<[T]> {
     /// assert_eq!(vec![1, 2, 3], *shared);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Rc<[T]> {
-        unsafe {
-            let rc = Rc::copy_from_slice(&v);
-
-            // Allow the Vec to free its memory, but not destroy its contents
-            v.set_len(0);
-
-            rc
-        }
+    fn from(v: Vec<T, A>) -> Rc<[T], A> {
+        let boxed_slice = v.into_boxed_slice();
+        Self::from(boxed_slice)
     }
 }
 
@@ -1888,6 +2469,25 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Rc<str>> for Rc<[u8]> {
+    /// Converts a reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::rc::Rc;
+    /// let string: Rc<str> = Rc::from("eggplant");
+    /// let bytes: Rc<[u8]> = Rc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Rc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Rc::from_raw(Rc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
 impl<T, const N: usize> TryFrom<Rc<[T]>> for Rc<[T; N]> {
     type Error = Rc<[T]>;
@@ -1989,7 +2589,7 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 
 /// `Weak` is a version of [`Rc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Rc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Rc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -2008,7 +2608,10 @@ impl<T, I: iter::TrustedLen<Item = T>> ToRcSlice<T> for I {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "rc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesn’t need
@@ -2016,15 +2619,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<RcBox<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Send for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Send for Weak<T, A> {}
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> !marker::Sync for Weak<T> {}
+impl<T: ?Sized, A: Allocator> !marker::Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
@@ -2043,9 +2647,37 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") }
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::Weak;
+    ///
+    /// let empty: Weak<i64> = Weak::new();
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr:  { NonNull::new(usize::MAX as *mut RcBox<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -2062,6 +2694,56 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference, and `ptr` must point to a block of memory allocated by the global allocator.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Rc::downgrade(&strong).into_raw();
+    /// let raw_2 = Rc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Rc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    /// [`new`]: Weak::new
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Self::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -2086,7 +2768,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: ptr::null
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut RcBox<T> = NonNull::as_ptr(self.ptr);
@@ -2096,7 +2779,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as RcBox (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).value) }
@@ -2130,6 +2813,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -2137,6 +2821,44 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
+    /// Consumes the `Weak<T>` and turns it into a raw pointer.
+    ///
+    /// This converts the weak pointer into a raw pointer, while still preserving the ownership of
+    /// one weak reference (the weak count is not modified by this operation). It can be turned
+    /// back into the `Weak<T>` with [`from_raw`].
+    ///
+    /// The same restrictions of accessing the target of the pointer as with
+    /// [`as_ptr`] apply.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::rc::{Rc, Weak};
+    ///
+    /// let strong = Rc::new("hello".to_owned());
+    /// let weak = Rc::downgrade(&strong);
+    /// let raw = weak.into_raw();
+    ///
+    /// assert_eq!(1, Rc::weak_count(&strong));
+    /// assert_eq!("hello", unsafe { &*raw });
+    ///
+    /// drop(unsafe { Weak::from_raw(raw) });
+    /// assert_eq!(0, Rc::weak_count(&strong));
+    /// ```
+    ///
+    /// [`from_raw`]: Weak::from_raw
+    /// [`as_ptr`]: Weak::as_ptr
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn into_raw_and_alloc(self) -> (*const T, A)
+    where
+        A: Clone,
+    {
+        let result = self.as_ptr();
+        let alloc = self.alloc.clone();
+        mem::forget(self);
+        (result, alloc)
+    }
+
     /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
@@ -2148,7 +2870,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and `ptr` must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -2179,8 +2901,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
     /// [`new`]: Weak::new
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -2196,7 +2918,7 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 
     /// Attempts to upgrade the `Weak` pointer to an [`Rc`], delaying
@@ -2222,20 +2944,29 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Rc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "rc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Rc<T>> {
+    pub fn upgrade(&self) -> Option<Rc<T, A>>
+    where
+        A: Clone,
+    {
         let inner = self.inner()?;
+
         if inner.strong() == 0 {
             None
         } else {
-            inner.inc_strong();
-            Some(Rc::from_inner(self.ptr))
+            unsafe {
+                inner.inc_strong();
+                Some(Rc::from_inner_in(self.ptr, self.alloc.clone()))
+            }
         }
     }
 
     /// Gets the number of strong (`Rc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
         if let Some(inner) = self.inner() { inner.strong() } else { 0 }
@@ -2244,6 +2975,7 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of `Weak` pointers pointing to this allocation.
     ///
     /// If no strong pointers remain, this will return zero.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
@@ -2313,9 +3045,8 @@ impl<T: ?Sized> Weak<T> {
     /// let third = Rc::downgrade(&third_rc);
     /// assert!(!first.ptr_eq(&third));
     /// ```
-    ///
-    /// [`ptr::eq`]: core::ptr::eq
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -2323,7 +3054,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2356,14 +3087,14 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
         // the strong pointers have disappeared.
         if inner.weak() == 0 {
             unsafe {
-                Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()));
             }
         }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2376,16 +3107,16 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         if let Some(inner) = self.inner() {
             inner.inc_weak()
         }
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
 #[stable(feature = "rc_weak", since = "1.4.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Weak<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Weak<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         write!(f, "(Weak)")
     }
@@ -2396,7 +3127,6 @@ impl<T> Default for Weak<T> {
     /// Constructs a new `Weak<T>`, without allocating any memory.
     /// Calling [`upgrade`] on the return value always gives [`None`].
     ///
-    /// [`None`]: Option
     /// [`upgrade`]: Weak::upgrade
     ///
     /// # Examples
@@ -2435,14 +3165,23 @@ trait RcInnerPtr {
     fn inc_strong(&self) {
         let strong = self.strong();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(strong != 0);
+        }
+
+        let strong = strong.wrapping_add(1);
+        self.strong_ref().set(strong);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if strong == 0 || strong == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(strong == 0) {
             abort();
         }
-        self.strong_ref().set(strong + 1);
     }
 
     #[inline]
@@ -2459,14 +3198,23 @@ trait RcInnerPtr {
     fn inc_weak(&self) {
         let weak = self.weak();
 
+        // We insert an `assume` here to hint LLVM at an otherwise
+        // missed optimization.
+        // SAFETY: The reference count will never be zero when this is
+        // called.
+        unsafe {
+            core::intrinsics::assume(weak != 0);
+        }
+
+        let weak = weak.wrapping_add(1);
+        self.weak_ref().set(weak);
+
         // We want to abort on overflow instead of dropping the value.
-        // The reference count will never be zero when this is called;
-        // nevertheless, we insert an abort here to hint LLVM at
-        // an otherwise missed optimization.
-        if weak == 0 || weak == usize::MAX {
+        // Checking for overflow after the store instead of before
+        // allows for slightly better code generation.
+        if core::intrinsics::unlikely(weak == 0) {
             abort();
         }
-        self.weak_ref().set(weak + 1);
     }
 
     #[inline]
@@ -2500,21 +3248,21 @@ impl<'a> RcInnerPtr for WeakInner<'a> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Rc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Rc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Rc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Rc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Rc<T, A> {}
 
 /// Get the offset within an `RcBox` for the payload behind a pointer.
 ///
@@ -2528,7 +3276,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
@@ -2536,4 +3284,4 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
 fn data_offset_align(align: usize) -> isize {
     let layout = Layout::new::<RcBox<()>>();
     (layout.size() + layout.padding_needed_for(align)) as isize
-}
+}
\ No newline at end of file
diff --git a/rust/alloc/string.rs b/rust/alloc/string.rs
index 55293c304..c4b72c009 100644
--- a/rust/alloc/string.rs
+++ b/rust/alloc/string.rs
@@ -46,11 +46,13 @@
 
 #[cfg(not(no_global_oom_handling))]
 use core::char::{decode_utf16, REPLACEMENT_CHARACTER};
+use core::cmp::Ordering;
+// use core::error::Error;
 use core::fmt;
 use core::hash;
+use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::iter::{from_fn, FromIterator};
-use core::iter::FusedIterator;
 #[cfg(not(no_global_oom_handling))]
 use core::ops::Add;
 #[cfg(not(no_global_oom_handling))]
@@ -60,10 +62,11 @@ use core::ops::Bound::{Excluded, Included, Unbounded};
 use core::ops::{self, Index, IndexMut, Range, RangeBounds};
 use core::ptr;
 use core::slice;
-#[cfg(not(no_global_oom_handling))]
-use core::str::lossy;
 use core::str::pattern::Pattern;
+#[cfg(not(no_global_oom_handling))]
+use core::str::Utf8Chunks;
 
+use crate::alloc::{Allocator, Global};
 #[cfg(not(no_global_oom_handling))]
 use crate::borrow::{Cow, ToOwned};
 use crate::boxed::Box;
@@ -81,7 +84,7 @@ use crate::vec::Vec;
 ///
 /// # Examples
 ///
-/// You can create a `String` from [a literal string][`str`] with [`String::from`]:
+/// You can create a `String` from [a literal string][`&str`] with [`String::from`]:
 ///
 /// [`String::from`]: From::from
 ///
@@ -119,31 +122,103 @@ use crate::vec::Vec;
 ///
 /// # UTF-8
 ///
-/// `String`s are always valid UTF-8. This has a few implications, the first of
-/// which is that if you need a non-UTF-8 string, consider [`OsString`]. It is
-/// similar, but without the UTF-8 constraint. The second implication is that
-/// you cannot index into a `String`:
+/// `String`s are always valid UTF-8. If you need a non-UTF-8 string, consider
+/// [`OsString`]. It is similar, but without the UTF-8 constraint. Because UTF-8
+/// is a variable width encoding, `String`s are typically smaller than an array of
+/// the same `chars`:
+///
+/// ```
+/// use std::mem;
+///
+/// // `s` is ASCII which represents each `char` as one byte
+/// let s = "hello";
+/// assert_eq!(s.len(), 5);
+///
+/// // A `char` array with the same contents would be longer because
+/// // every `char` is four bytes
+/// let s = ['h', 'e', 'l', 'l', 'o'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+///
+/// // However, for non-ASCII strings, the difference will be smaller
+/// // and sometimes they are the same
+/// let s = "💖💖💖💖💖";
+/// assert_eq!(s.len(), 20);
+///
+/// let s = ['💖', '💖', '💖', '💖', '💖'];
+/// let size: usize = s.into_iter().map(|c| mem::size_of_val(&c)).sum();
+/// assert_eq!(size, 20);
+/// ```
+///
+/// This raises interesting questions as to how `s[i]` should work.
+/// What should `i` be here? Several options include byte indices and
+/// `char` indices but, because of UTF-8 encoding, only byte indices
+/// would provide constant time indexing. Getting the `i`th `char`, for
+/// example, is available using [`chars`]:
+///
+/// ```
+/// let s = "hello";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('l'));
+///
+/// let s = "💖💖💖💖💖";
+/// let third_character = s.chars().nth(2);
+/// assert_eq!(third_character, Some('💖'));
+/// ```
+///
+/// Next, what should `s[i]` return? Because indexing returns a reference
+/// to underlying data it could be `&u8`, `&[u8]`, or something else similar.
+/// Since we're only providing one index, `&u8` makes the most sense but that
+/// might not be what the user expects and can be explicitly achieved with
+/// [`as_bytes()`]:
+///
+/// ```
+/// // The first byte is 104 - the byte value of `'h'`
+/// let s = "hello";
+/// assert_eq!(s.as_bytes()[0], 104);
+/// // or
+/// assert_eq!(s.as_bytes()[0], b'h');
+///
+/// // The first byte is 240 which isn't obviously useful
+/// let s = "💖💖💖💖💖";
+/// assert_eq!(s.as_bytes()[0], 240);
+/// ```
+///
+/// Due to these ambiguities/restrictions, indexing with a `usize` is simply
+/// forbidden:
 ///
 /// ```compile_fail,E0277
 /// let s = "hello";
 ///
-/// println!("The first letter of s is {}", s[0]); // ERROR!!!
+/// // The following will not compile!
+/// println!("The first letter of s is {}", s[0]);
 /// ```
 ///
-/// [`OsString`]: ../../std/ffi/struct.OsString.html
+/// It is more clear, however, how `&s[i..j]` should work (that is,
+/// indexing with a range). It should accept byte indices (to be constant-time)
+/// and return a `&str` which is UTF-8 encoded. This is also called "string slicing".
+/// Note this will panic if the byte indices provided are not character
+/// boundaries - see [`is_char_boundary`] for more details. See the implementations
+/// for [`SliceIndex<str>`] for more details on string slicing. For a non-panicking
+/// version of string slicing, see [`get`].
+///
+/// [`OsString`]: ../../std/ffi/struct.OsString.html "ffi::OsString"
+/// [`SliceIndex<str>`]: core::slice::SliceIndex
+/// [`as_bytes()`]: str::as_bytes
+/// [`get`]: str::get
+/// [`is_char_boundary`]: str::is_char_boundary
 ///
-/// Indexing is intended to be a constant-time operation, but UTF-8 encoding
-/// does not allow us to do this. Furthermore, it's not clear what sort of
-/// thing the index should return: a byte, a codepoint, or a grapheme cluster.
-/// The [`bytes`] and [`chars`] methods return iterators over the first
-/// two, respectively.
+/// The [`bytes`] and [`chars`] methods return iterators over the bytes and
+/// codepoints of the string, respectively. To iterate over codepoints along
+/// with byte indices, use [`char_indices`].
 ///
 /// [`bytes`]: str::bytes
 /// [`chars`]: str::chars
+/// [`char_indices`]: str::char_indices
 ///
 /// # Deref
 ///
-/// `String`s implement [`Deref`]`<Target=str>`, and so inherit all of [`str`]'s
+/// `String` implements <code>[Deref]<Target = [str]></code>, and so inherits all of [`str`]'s
 /// methods. In addition, this means that you can pass a `String` to a
 /// function which takes a [`&str`] by using an ampersand (`&`):
 ///
@@ -184,7 +259,7 @@ use crate::vec::Vec;
 /// to explicitly extract the string slice containing the string. The second
 /// way changes `example_func(&example_string);` to
 /// `example_func(&*example_string);`. In this case we are dereferencing a
-/// `String` to a [`str`][`&str`], then referencing the [`str`][`&str`] back to
+/// `String` to a [`str`], then referencing the [`str`] back to
 /// [`&str`]. The second way is more idiomatic, however both work to do the
 /// conversion explicitly rather than relying on the implicit conversion.
 ///
@@ -247,11 +322,11 @@ use crate::vec::Vec;
 ///
 /// ```text
 /// 0
-/// 5
-/// 10
-/// 20
-/// 20
-/// 40
+/// 8
+/// 16
+/// 16
+/// 32
+/// 32
 /// ```
 ///
 /// At first, we have no memory allocated at all, but as we append to the
@@ -284,15 +359,16 @@ use crate::vec::Vec;
 ///
 /// Here, there's no need to allocate more memory inside the loop.
 ///
-/// [`str`]: prim@str
-/// [`&str`]: prim@str
-/// [`Deref`]: core::ops::Deref
+/// [str]: prim@str "str"
+/// [`str`]: prim@str "str"
+/// [`&str`]: prim@str "&str"
+/// [Deref]: core::ops::Deref "ops::Deref"
+/// [`Deref`]: core::ops::Deref "ops::Deref"
 /// [`as_str()`]: String::as_str
-#[derive(PartialOrd, Eq, Ord)]
-#[cfg_attr(not(test), rustc_diagnostic_item = "string_type")]
+#[cfg_attr(not(test), rustc_diagnostic_item = "String")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct String {
-    vec: Vec<u8>,
+pub struct String<#[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global> {
+    vec: Vec<u8, A>,
 }
 
 /// A possible error value when converting a `String` from a UTF-8 byte vector.
@@ -310,10 +386,10 @@ pub struct String {
 /// an analogue to `FromUtf8Error`, and you can get one from a `FromUtf8Error`
 /// through the [`utf8_error`] method.
 ///
-/// [`Utf8Error`]: core::str::Utf8Error
-/// [`std::str`]: core::str
-/// [`&str`]: prim@str
-/// [`utf8_error`]: Self::utf8_error
+/// [`Utf8Error`]: str::Utf8Error "std::str::Utf8Error"
+/// [`std::str`]: core::str "std::str"
+/// [`&str`]: prim@str "&str"
+/// [`utf8_error`]: FromUtf8Error::utf8_error
 ///
 /// # Examples
 ///
@@ -378,17 +454,18 @@ impl String {
     #[inline]
     #[rustc_const_stable(feature = "const_string_new", since = "1.39.0")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub const fn new() -> String {
         String { vec: Vec::new() }
     }
 
-    /// Creates a new empty `String` with a particular capacity.
+    /// Creates a new empty `String` with at least the specified capacity.
     ///
     /// `String`s have an internal buffer to hold their data. The capacity is
     /// the length of that buffer, and can be queried with the [`capacity`]
     /// method. This method creates an empty `String`, but one with an initial
-    /// buffer that can hold `capacity` bytes. This is useful when you may be
-    /// appending a bunch of data to the `String`, reducing the number of
+    /// buffer that can hold at least `capacity` bytes. This is useful when you
+    /// may be appending a bunch of data to the `String`, reducing the number of
     /// reallocations it needs to do.
     ///
     /// [`capacity`]: String::capacity
@@ -421,9 +498,8 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[doc(alias = "alloc")]
-    #[doc(alias = "malloc")]
     #[stable(feature = "rust1", since = "1.0.0")]
+    #[must_use]
     pub fn with_capacity(capacity: usize) -> String {
         String { vec: Vec::with_capacity(capacity) }
     }
@@ -491,8 +567,8 @@ impl String {
     /// with this error.
     ///
     /// [`from_utf8_unchecked`]: String::from_utf8_unchecked
-    /// [`Vec<u8>`]: crate::vec::Vec
-    /// [`&str`]: prim@str
+    /// [`Vec<u8>`]: crate::vec::Vec "Vec"
+    /// [`&str`]: prim@str "&str"
     /// [`into_bytes`]: String::into_bytes
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
@@ -528,7 +604,7 @@ impl String {
     /// it's already valid UTF-8, we don't need a new allocation. This return
     /// type allows us to handle both cases.
     ///
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     ///
     /// # Examples
     ///
@@ -552,18 +628,19 @@ impl String {
     ///
     /// assert_eq!("Hello �World", output);
     /// ```
+    #[must_use]
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf8_lossy(v: &[u8]) -> Cow<'_, str> {
-        let mut iter = lossy::Utf8Lossy::from_bytes(v).chunks();
+        let mut iter = Utf8Chunks::new(v);
 
-        let (first_valid, first_broken) = if let Some(chunk) = iter.next() {
-            let lossy::Utf8LossyChunk { valid, broken } = chunk;
-            if valid.len() == v.len() {
-                debug_assert!(broken.is_empty());
+        let first_valid = if let Some(chunk) = iter.next() {
+            let valid = chunk.valid();
+            if chunk.invalid().is_empty() {
+                debug_assert_eq!(valid.len(), v.len());
                 return Cow::Borrowed(valid);
             }
-            (valid, broken)
+            valid
         } else {
             return Cow::Borrowed("");
         };
@@ -572,13 +649,11 @@ impl String {
 
         let mut res = String::with_capacity(v.len());
         res.push_str(first_valid);
-        if !first_broken.is_empty() {
-            res.push_str(REPLACEMENT);
-        }
+        res.push_str(REPLACEMENT);
 
-        for lossy::Utf8LossyChunk { valid, broken } in iter {
-            res.push_str(valid);
-            if !broken.is_empty() {
+        for chunk in iter {
+            res.push_str(chunk.valid());
+            if !chunk.invalid().is_empty() {
                 res.push_str(REPLACEMENT);
             }
         }
@@ -629,7 +704,7 @@ impl String {
     /// conversion requires a memory allocation.
     ///
     /// [`from_utf8_lossy`]: String::from_utf8_lossy
-    /// [`Cow<'a, str>`]: crate::borrow::Cow
+    /// [`Cow<'a, str>`]: crate::borrow::Cow "borrow::Cow"
     /// [U+FFFD]: core::char::REPLACEMENT_CHARACTER
     ///
     /// # Examples
@@ -646,6 +721,7 @@ impl String {
     ///            String::from_utf16_lossy(v));
     /// ```
     #[cfg(not(no_global_oom_handling))]
+    #[must_use]
     #[inline]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn from_utf16_lossy(v: &[u16]) -> String {
@@ -678,6 +754,7 @@ impl String {
     /// let rebuilt = unsafe { String::from_raw_parts(ptr, len, cap) };
     /// assert_eq!(rebuilt, "hello");
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[unstable(feature = "vec_into_raw_parts", reason = "new API", issue = "65816")]
     pub fn into_raw_parts(self) -> (*mut u8, usize, usize) {
         self.vec.into_raw_parts()
@@ -697,7 +774,10 @@ impl String {
     /// * The first `length` bytes at `buf` need to be valid UTF-8.
     ///
     /// Violating these may cause problems like corrupting the allocator's
-    /// internal data structures.
+    /// internal data structures. For example, it is normally **not** safe to
+    /// build a `String` from a pointer to a C `char` array containing UTF-8
+    /// _unless_ you are certain that array was originally allocated by the
+    /// Rust standard library's allocator.
     ///
     /// The ownership of `buf` is effectively transferred to the
     /// `String` which may then deallocate, reallocate or change the
@@ -763,6 +843,7 @@ impl String {
     /// assert_eq!("💖", sparkle_heart);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub unsafe fn from_utf8_unchecked(bytes: Vec<u8>) -> String {
         String { vec: bytes }
@@ -783,6 +864,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111][..], &bytes[..]);
     /// ```
     #[inline]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.vec
@@ -800,6 +882,7 @@ impl String {
     /// assert_eq!("foo", s.as_str());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_str(&self) -> &str {
         self
@@ -820,6 +903,7 @@ impl String {
     /// assert_eq!("FOOBAR", s_mut_str);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "string_as_str", since = "1.7.0")]
     pub fn as_mut_str(&mut self) -> &mut str {
         self
@@ -893,26 +977,22 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn capacity(&self) -> usize {
         self.vec.capacity()
     }
 
-    /// Ensures that this `String`'s capacity is at least `additional` bytes
-    /// larger than its length.
-    ///
-    /// The capacity may be increased by more than `additional` bytes if it
-    /// chooses, to prevent frequent reallocations.
-    ///
-    /// If you do not want this "at least" behavior, see the [`reserve_exact`]
-    /// method.
+    /// Reserves capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `reserve`,
+    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Does nothing if capacity is already sufficient.
     ///
     /// # Panics
     ///
     /// Panics if the new capacity overflows [`usize`].
     ///
-    /// [`reserve_exact`]: String::reserve_exact
-    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -925,22 +1005,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -949,17 +1030,18 @@ impl String {
         self.vec.reserve(additional)
     }
 
-    /// Ensures that this `String`'s capacity is `additional` bytes
-    /// larger than its length.
-    ///
-    /// Consider using the [`reserve`] method unless you absolutely know
-    /// better than the allocator.
+    /// Reserves the minimum capacity for at least `additional` bytes more than
+    /// the current length. Unlike [`reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `reserve_exact`, capacity will be greater than or equal to
+    /// `self.len() + additional`. Does nothing if the capacity is already
+    /// sufficient.
     ///
     /// [`reserve`]: String::reserve
     ///
     /// # Panics
     ///
-    /// Panics if the new capacity overflows `usize`.
+    /// Panics if the new capacity overflows [`usize`].
     ///
     /// # Examples
     ///
@@ -973,22 +1055,23 @@ impl String {
     /// assert!(s.capacity() >= 10);
     /// ```
     ///
-    /// This may not actually increase the capacity:
+    /// This might not actually increase the capacity:
     ///
     /// ```
     /// let mut s = String::with_capacity(10);
     /// s.push('a');
     /// s.push('b');
     ///
-    /// // s now has a length of 2 and a capacity of 10
+    /// // s now has a length of 2 and a capacity of at least 10
+    /// let capacity = s.capacity();
     /// assert_eq!(2, s.len());
-    /// assert_eq!(10, s.capacity());
+    /// assert!(capacity >= 10);
     ///
-    /// // Since we already have an extra 8 capacity, calling this...
+    /// // Since we already have at least an extra 8 capacity, calling this...
     /// s.reserve_exact(8);
     ///
     /// // ... doesn't actually increase.
-    /// assert_eq!(10, s.capacity());
+    /// assert_eq!(capacity, s.capacity());
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
@@ -997,11 +1080,12 @@ impl String {
         self.vec.reserve_exact(additional)
     }
 
-    /// Tries to reserve capacity for at least `additional` more elements to be inserted
-    /// in the given `String`. The collection may reserve more space to avoid
-    /// frequent reallocations. After calling `reserve`, capacity will be
-    /// greater than or equal to `self.len() + additional`. Does nothing if
-    /// capacity is already sufficient.
+    /// Tries to reserve capacity for at least `additional` bytes more than the
+    /// current length. The allocator may reserve more space to speculatively
+    /// avoid frequent allocations. After calling `try_reserve`, capacity will be
+    /// greater than or equal to `self.len() + additional` if it returns
+    /// `Ok(())`. Does nothing if capacity is already sufficient. This method
+    /// preserves the contents even if an error occurs.
     ///
     /// # Errors
     ///
@@ -1011,7 +1095,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
@@ -1032,14 +1115,18 @@ impl String {
         self.vec.try_reserve(additional)
     }
 
-    /// Tries to reserve the minimum capacity for exactly `additional` more elements to
-    /// be inserted in the given `String`. After calling `reserve_exact`,
-    /// capacity will be greater than or equal to `self.len() + additional`.
+    /// Tries to reserve the minimum capacity for at least `additional` bytes
+    /// more than the current length. Unlike [`try_reserve`], this will not
+    /// deliberately over-allocate to speculatively avoid frequent allocations.
+    /// After calling `try_reserve_exact`, capacity will be greater than or
+    /// equal to `self.len() + additional` if it returns `Ok(())`.
     /// Does nothing if the capacity is already sufficient.
     ///
     /// Note that the allocator may give the collection more space than it
     /// requests. Therefore, capacity can not be relied upon to be precisely
-    /// minimal. Prefer `reserve` if future insertions are expected.
+    /// minimal. Prefer [`try_reserve`] if future insertions are expected.
+    ///
+    /// [`try_reserve`]: String::try_reserve
     ///
     /// # Errors
     ///
@@ -1049,14 +1136,13 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(try_reserve)]
     /// use std::collections::TryReserveError;
     ///
     /// fn process_data(data: &str) -> Result<String, TryReserveError> {
     ///     let mut output = String::new();
     ///
     ///     // Pre-reserve the memory, exiting if we can't
-    ///     output.try_reserve(data.len())?;
+    ///     output.try_reserve_exact(data.len())?;
     ///
     ///     // Now we know this can't OOM in the middle of our complex work
     ///     output.push_str(data);
@@ -1102,7 +1188,6 @@ impl String {
     /// # Examples
     ///
     /// ```
-    /// #![feature(shrink_to)]
     /// let mut s = String::from("foo");
     ///
     /// s.reserve(100);
@@ -1115,7 +1200,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "shrink_to", reason = "new API", issue = "56431")]
+    #[stable(feature = "shrink_to", since = "1.56.0")]
     pub fn shrink_to(&mut self, min_capacity: usize) {
         self.vec.shrink_to(min_capacity)
     }
@@ -1161,6 +1246,7 @@ impl String {
     /// assert_eq!(&[104, 101, 108, 108, 111], s.as_bytes());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.vec
@@ -1354,13 +1440,14 @@ impl String {
     /// assert_eq!(s, "foobar");
     /// ```
     ///
-    /// The exact order may be useful for tracking external state, like an index.
+    /// Because the elements are visited exactly once in the original order,
+    /// external state may be used to decide which elements to keep.
     ///
     /// ```
     /// let mut s = String::from("abcde");
     /// let keep = [false, true, true, false, true];
-    /// let mut i = 0;
-    /// s.retain(|_| (keep[i], i += 1).0);
+    /// let mut iter = keep.iter();
+    /// s.retain(|_| *iter.next().unwrap());
     /// assert_eq!(s, "bce");
     /// ```
     #[inline]
@@ -1387,19 +1474,28 @@ impl String {
         let mut guard = SetLenOnDrop { s: self, idx: 0, del_bytes: 0 };
 
         while guard.idx < len {
-            let ch = unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap() };
+            let ch =
+                // SAFETY: `guard.idx` is positive-or-zero and less that len so the `get_unchecked`
+                // is in bound. `self` is valid UTF-8 like string and the returned slice starts at
+                // a unicode code point so the `Chars` always return one character.
+                unsafe { guard.s.get_unchecked(guard.idx..len).chars().next().unwrap_unchecked() };
             let ch_len = ch.len_utf8();
 
             if !f(ch) {
                 guard.del_bytes += ch_len;
             } else if guard.del_bytes > 0 {
-                unsafe {
-                    ptr::copy(
-                        guard.s.vec.as_ptr().add(guard.idx),
-                        guard.s.vec.as_mut_ptr().add(guard.idx - guard.del_bytes),
-                        ch_len,
-                    );
-                }
+                // SAFETY: `guard.idx` is in bound and `guard.del_bytes` represent the number of
+                // bytes that are erased from the string so the resulting `guard.idx -
+                // guard.del_bytes` always represent a valid unicode code point.
+                //
+                // `guard.del_bytes` >= `ch.len_utf8()`, so taking a slice with `ch.len_utf8()` len
+                // is safe.
+                ch.encode_utf8(unsafe {
+                    crate::slice::from_raw_parts_mut(
+                        guard.s.as_mut_ptr().add(guard.idx - guard.del_bytes),
+                        ch.len_utf8(),
+                    )
+                });
             }
 
             // Point idx to the next char
@@ -1453,7 +1549,7 @@ impl String {
 
         unsafe {
             ptr::copy(self.vec.as_ptr().add(idx), self.vec.as_mut_ptr().add(idx + amt), len - idx);
-            ptr::copy(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
+            ptr::copy_nonoverlapping(bytes.as_ptr(), self.vec.as_mut_ptr().add(idx), amt);
             self.vec.set_len(len + amt);
         }
     }
@@ -1494,10 +1590,11 @@ impl String {
     ///
     /// # Safety
     ///
-    /// This function is unsafe because it does not check that the bytes passed
-    /// to it are valid UTF-8. If this constraint is violated, it may cause
-    /// memory unsafety issues with future users of the `String`, as the rest of
-    /// the standard library assumes that `String`s are valid UTF-8.
+    /// This function is unsafe because the returned `&mut Vec` allows writing
+    /// bytes which are not valid UTF-8. If this constraint is violated, using
+    /// the original `String` after dropping the `&mut Vec` may violate memory
+    /// safety, as the rest of the standard library assumes that `String`s are
+    /// valid UTF-8.
     ///
     /// # Examples
     ///
@@ -1521,7 +1618,7 @@ impl String {
     }
 
     /// Returns the length of this `String`, in bytes, not [`char`]s or
-    /// graphemes. In other words, it may not be what a human considers the
+    /// graphemes. In other words, it might not be what a human considers the
     /// length of the string.
     ///
     /// # Examples
@@ -1536,8 +1633,8 @@ impl String {
     /// assert_eq!(fancy_f.len(), 4);
     /// assert_eq!(fancy_f.chars().count(), 3);
     /// ```
-    #[doc(alias = "length")]
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn len(&self) -> usize {
         self.vec.len()
@@ -1557,6 +1654,7 @@ impl String {
     /// assert!(!v.is_empty());
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn is_empty(&self) -> bool {
         self.len() == 0
@@ -1619,17 +1717,24 @@ impl String {
         self.vec.clear()
     }
 
-    /// Creates a draining iterator that removes the specified range in the `String`
-    /// and yields the removed `chars`.
+    /// Removes the specified range from the string in bulk, returning all
+    /// removed characters as an iterator.
     ///
-    /// Note: The element range is removed even if the iterator is not
-    /// consumed until the end.
+    /// The returned iterator keeps a mutable borrow on the string to optimize
+    /// its implementation.
     ///
     /// # Panics
     ///
     /// Panics if the starting point or end point do not lie on a [`char`]
     /// boundary, or if they're out of bounds.
     ///
+    /// # Leaking
+    ///
+    /// If the returned iterator goes out of scope without being dropped (due to
+    /// [`core::mem::forget`], for example), the string may still contain a copy
+    /// of any drained characters, or may have lost characters arbitrarily,
+    /// including characters outside the range.
+    ///
     /// # Examples
     ///
     /// Basic usage:
@@ -1643,7 +1748,7 @@ impl String {
     /// assert_eq!(t, "α is alpha, ");
     /// assert_eq!(s, "β is beta");
     ///
-    /// // A full range clears the string
+    /// // A full range clears the string, like `clear()` does
     /// s.drain(..);
     /// assert_eq!(s, "");
     /// ```
@@ -1724,11 +1829,11 @@ impl String {
         unsafe { self.as_mut_vec() }.splice((start, end), replace_with.bytes());
     }
 
-    /// Converts this `String` into a [`Box`]`<`[`str`]`>`.
+    /// Converts this `String` into a <code>[Box]<[str]></code>.
     ///
     /// This will drop any excess capacity.
     ///
-    /// [`str`]: prim@str
+    /// [str]: prim@str "str"
     ///
     /// # Examples
     ///
@@ -1741,6 +1846,7 @@ impl String {
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "box_str", since = "1.4.0")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
     pub fn into_boxed_str(self) -> Box<str> {
         let slice = self.vec.into_boxed_slice();
@@ -1763,6 +1869,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(&[0, 159], value.unwrap_err().as_bytes());
     /// ```
+    #[must_use]
     #[stable(feature = "from_utf8_error_as_bytes", since = "1.26.0")]
     pub fn as_bytes(&self) -> &[u8] {
         &self.bytes[..]
@@ -1786,6 +1893,7 @@ impl FromUtf8Error {
     ///
     /// assert_eq!(vec![0, 159], value.unwrap_err().into_bytes());
     /// ```
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn into_bytes(self) -> Vec<u8> {
         self.bytes
@@ -1798,8 +1906,8 @@ impl FromUtf8Error {
     /// an analogue to `FromUtf8Error`. See its documentation for more details
     /// on using it.
     ///
-    /// [`std::str`]: core::str
-    /// [`&str`]: prim@str
+    /// [`std::str`]: core::str "std::str"
+    /// [`&str`]: prim@str "&str"
     ///
     /// # Examples
     ///
@@ -1814,6 +1922,7 @@ impl FromUtf8Error {
     /// // the first byte is invalid here
     /// assert_eq!(1, error.valid_up_to());
     /// ```
+    #[must_use]
     #[stable(feature = "rust1", since = "1.0.0")]
     pub fn utf8_error(&self) -> Utf8Error {
         self.error
@@ -1834,6 +1943,22 @@ impl fmt::Display for FromUtf16Error {
     }
 }
 
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf8Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-8"
+//     }
+// }
+
+// #[stable(feature = "rust1", since = "1.0.0")]
+// impl Error for FromUtf16Error {
+//     #[allow(deprecated)]
+//     fn description(&self) -> &str {
+//         "invalid utf-16"
+//     }
+// }
+
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "rust1", since = "1.0.0")]
 impl Clone for String {
@@ -2057,17 +2182,38 @@ impl<'a, 'b> Pattern<'a> for &'b String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl PartialEq for String {
+impl<A: Allocator, B: Allocator> PartialEq<String<B>> for String<A> {
     #[inline]
-    fn eq(&self, other: &String) -> bool {
+    fn eq(&self, other: &String<B>) -> bool {
         PartialEq::eq(&self[..], &other[..])
     }
     #[inline]
-    fn ne(&self, other: &String) -> bool {
+    fn ne(&self, other: &String<B>) -> bool {
         PartialEq::ne(&self[..], &other[..])
     }
 }
 
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Eq for String<A> {}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator, B: Allocator>  PartialOrd<String<B>> for String<A> {
+    #[inline]
+    fn partial_cmp(&self, other: &String<B>) -> Option<Ordering> {
+        PartialOrd::partial_cmp(&self[..], &other[..])
+    }
+}
+
+#[stable(feature = "rust1", since = "1.0.0")]
+impl<A: Allocator> Ord for String<A> {
+    #[inline]
+    fn cmp(&self, other: &String<A>) -> Ordering {
+        Ord::cmp(&self[..], &other[..])
+    }
+}
+
+
+
 macro_rules! impl_eq {
     ($lhs:ty, $rhs: ty) => {
         #[stable(feature = "rust1", since = "1.0.0")]
@@ -2202,7 +2348,7 @@ impl AddAssign<&str> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::Range<usize>> for String {
+impl<A: Allocator> ops::Index<ops::Range<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2211,7 +2357,7 @@ impl ops::Index<ops::Range<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeTo<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2220,7 +2366,7 @@ impl ops::Index<ops::RangeTo<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeFrom<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2229,7 +2375,7 @@ impl ops::Index<ops::RangeFrom<usize>> for String {
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Index<ops::RangeFull> for String {
+impl<A: Allocator> ops::Index<ops::RangeFull> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2238,7 +2384,7 @@ impl ops::Index<ops::RangeFull> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2247,7 +2393,7 @@ impl ops::Index<ops::RangeInclusive<usize>> for String {
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::Index<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::Index<ops::RangeToInclusive<usize>> for String<A> {
     type Output = str;
 
     #[inline]
@@ -2257,42 +2403,42 @@ impl ops::Index<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::Range<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::Range<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::Range<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeTo<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeTo<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeTo<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFrom<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFrom<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeFrom<usize>) -> &mut str {
         &mut self[..][index]
     }
 }
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::IndexMut<ops::RangeFull> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeFull> for String<A> {
     #[inline]
     fn index_mut(&mut self, _index: ops::RangeFull) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
     }
 }
 #[stable(feature = "inclusive_range", since = "1.26.0")]
-impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
+impl<A: Allocator> ops::IndexMut<ops::RangeToInclusive<usize>> for String<A> {
     #[inline]
     fn index_mut(&mut self, index: ops::RangeToInclusive<usize>) -> &mut str {
         IndexMut::index_mut(&mut **self, index)
@@ -2300,7 +2446,7 @@ impl ops::IndexMut<ops::RangeToInclusive<usize>> for String {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl ops::Deref for String {
+impl<A: Allocator> ops::Deref for String<A> {
     type Target = str;
 
     #[inline]
@@ -2310,7 +2456,7 @@ impl ops::Deref for String {
 }
 
 #[stable(feature = "derefmut_for_string", since = "1.3.0")]
-impl ops::DerefMut for String {
+impl<A: Allocator> ops::DerefMut for String<A> {
     #[inline]
     fn deref_mut(&mut self) -> &mut str {
         unsafe { str::from_utf8_unchecked_mut(&mut *self.vec) }
@@ -2321,7 +2467,7 @@ impl ops::DerefMut for String {
 ///
 /// This alias exists for backwards compatibility, and may be eventually deprecated.
 ///
-/// [`Infallible`]: core::convert::Infallible
+/// [`Infallible`]: core::convert::Infallible "convert::Infallible"
 #[stable(feature = "str_parse_error", since = "1.5.0")]
 pub type ParseError = core::convert::Infallible;
 
@@ -2608,7 +2754,7 @@ impl<'a> From<&'a str> for Cow<'a, str> {
     /// assert_eq!(Cow::from("eggplant"), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a str) -> Cow<'a, str> {
         Cow::Borrowed(s)
@@ -2631,7 +2777,7 @@ impl<'a> From<String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(s), Cow::<'static, str>::Owned(s2));
     /// ```
     ///
-    /// [`Owned`]: crate::borrow::Cow::Owned
+    /// [`Owned`]: crate::borrow::Cow::Owned "borrow::Cow::Owned"
     #[inline]
     fn from(s: String) -> Cow<'a, str> {
         Cow::Owned(s)
@@ -2653,7 +2799,7 @@ impl<'a> From<&'a String> for Cow<'a, str> {
     /// assert_eq!(Cow::from(&s), Cow::Borrowed("eggplant"));
     /// ```
     ///
-    /// [`Borrowed`]: crate::borrow::Cow::Borrowed
+    /// [`Borrowed`]: crate::borrow::Cow::Borrowed "borrow::Cow::Borrowed"
     #[inline]
     fn from(s: &'a String) -> Cow<'a, str> {
         Cow::Borrowed(s.as_str())
@@ -2697,7 +2843,7 @@ impl From<String> for Vec<u8> {
     /// let v1 = Vec::from(s1);
     ///
     /// for b in v1 {
-    ///     println!("{}", b);
+    ///     println!("{b}");
     /// }
     /// ```
     fn from(string: String) -> Vec<u8> {
@@ -2771,14 +2917,14 @@ impl<'a> Drain<'a> {
     /// # Examples
     ///
     /// ```
-    /// #![feature(string_drain_as_str)]
     /// let mut s = String::from("abc");
     /// let mut drain = s.drain(..);
     /// assert_eq!(drain.as_str(), "abc");
     /// let _ = drain.next().unwrap();
     /// assert_eq!(drain.as_str(), "bc");
     /// ```
-    #[unstable(feature = "string_drain_as_str", issue = "76905")] // Note: uncomment AsRef impls below when stabilizing.
+    #[must_use]
+    #[stable(feature = "string_drain_as_str", since = "1.55.0")]
     pub fn as_str(&self) -> &str {
         self.iter.as_str()
     }
diff --git a/rust/alloc/sync.rs b/rust/alloc/sync.rs
index 1f4e44680..382f849d7 100644
--- a/rust/alloc/sync.rs
+++ b/rust/alloc/sync.rs
@@ -21,13 +21,13 @@ use core::marker::{PhantomData, Unpin, Unsize};
 use core::mem::size_of_val;
 use core::mem::{self, align_of_val_raw};
 use core::ops::{CoerceUnsized, Deref, DispatchFromDyn, Receiver};
-#[cfg(not(no_global_oom_handling))]
+// use core::panic::{RefUnwindSafe, UnwindSafe};
 use core::pin::Pin;
 use core::ptr::{self, NonNull};
 #[cfg(not(no_global_oom_handling))]
 use core::slice::from_raw_parts_mut;
 use core::sync::atomic;
-use core::sync::atomic::Ordering::{Acquire, Relaxed, Release, SeqCst};
+use core::sync::atomic::Ordering::{Acquire, Relaxed, Release};
 
 #[cfg(not(no_global_oom_handling))]
 use crate::alloc::handle_alloc_error;
@@ -101,8 +101,8 @@ macro_rules! acquire {
 /// first: after all, isn't the point of `Arc<T>` thread safety? The key is
 /// this: `Arc<T>` makes it thread safe to have multiple ownership of the same
 /// data, but it  doesn't add thread safety to its data. Consider
-/// `Arc<`[`RefCell<T>`]`>`. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
-/// [`Send`], `Arc<`[`RefCell<T>`]`>` would be as well. But then we'd have a problem:
+/// <code>Arc<[RefCell\<T>]></code>. [`RefCell<T>`] isn't [`Sync`], and if `Arc<T>` was always
+/// [`Send`], <code>Arc<[RefCell\<T>]></code> would be as well. But then we'd have a problem:
 /// [`RefCell<T>`] is not thread safe; it keeps track of the borrowing count using
 /// non-atomic operations.
 ///
@@ -148,7 +148,7 @@ macro_rules! acquire {
 /// use std::sync::Arc;
 ///
 /// let my_arc = Arc::new(());
-/// Arc::downgrade(&my_arc);
+/// let my_weak = Arc::downgrade(&my_arc);
 /// ```
 ///
 /// `Arc<T>`'s implementations of traits like `Clone` may also be called using
@@ -178,6 +178,7 @@ macro_rules! acquire {
 /// [deref]: core::ops::Deref
 /// [downgrade]: Arc::downgrade
 /// [upgrade]: Weak::upgrade
+/// [RefCell\<T>]: core::cell::RefCell
 /// [`RefCell<T>`]: core::cell::RefCell
 /// [`std::sync`]: ../../std/sync/index.html
 /// [`Arc::clone(&from)`]: Arc::clone
@@ -201,14 +202,14 @@ macro_rules! acquire {
 ///     let five = Arc::clone(&five);
 ///
 ///     thread::spawn(move || {
-///         println!("{:?}", five);
+///         println!("{five:?}");
 ///     });
 /// }
 /// ```
 ///
 /// Sharing a mutable [`AtomicUsize`]:
 ///
-/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize
+/// [`AtomicUsize`]: core::sync::atomic::AtomicUsize "sync::atomic::AtomicUsize"
 ///
 /// ```no_run
 /// use std::sync::Arc;
@@ -222,7 +223,7 @@ macro_rules! acquire {
 ///
 ///     thread::spawn(move || {
 ///         let v = val.fetch_add(1, Ordering::SeqCst);
-///         println!("{:?}", v);
+///         println!("{v:?}");
 ///     });
 /// }
 /// ```
@@ -233,35 +234,52 @@ macro_rules! acquire {
 /// [rc_examples]: crate::rc#examples
 #[cfg_attr(not(test), rustc_diagnostic_item = "Arc")]
 #[stable(feature = "rust1", since = "1.0.0")]
-pub struct Arc<T: ?Sized> {
+pub struct Arc<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     ptr: NonNull<ArcInner<T>>,
     phantom: PhantomData<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Arc<T, A> {}
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Arc<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Arc<T, A> {}
+
+// #[stable(feature = "catch_unwind", since = "1.9.0")]
+// impl<T: RefUnwindSafe + ?Sized, A: Allocator + UnwindSafe> UnwindSafe for Arc<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Arc<U>> for Arc<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Arc<U, A>> for Arc<T, A> {}
 
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Arc<U>> for Arc<T> {}
 
 impl<T: ?Sized> Arc<T> {
-    fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
-        Self { ptr, phantom: PhantomData }
+    unsafe fn from_inner(ptr: NonNull<ArcInner<T>>) -> Self {
+        unsafe { Self::from_inner_in(ptr, Global) }
     }
 
     unsafe fn from_ptr(ptr: *mut ArcInner<T>) -> Self {
-        unsafe { Self::from_inner(NonNull::new_unchecked(ptr)) }
+        unsafe { Self::from_ptr_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
+    unsafe fn from_inner_in(ptr: NonNull<ArcInner<T>>, alloc: A) -> Self {
+        Self { ptr, phantom: PhantomData, alloc }
+    }
+
+    unsafe fn from_ptr_in(ptr: *mut ArcInner<T>, alloc: A) -> Self {
+        unsafe { Self::from_inner_in(NonNull::new_unchecked(ptr), alloc) }
     }
 }
 
 /// `Weak` is a version of [`Arc`] that holds a non-owning reference to the
 /// managed allocation. The allocation is accessed by calling [`upgrade`] on the `Weak`
-/// pointer, which returns an [`Option`]`<`[`Arc`]`<T>>`.
+/// pointer, which returns an <code>[Option]<[Arc]\<T>></code>.
 ///
 /// Since a `Weak` reference does not count towards ownership, it will not
 /// prevent the value stored in the allocation from being dropped, and `Weak` itself makes no
@@ -280,7 +298,10 @@ impl<T: ?Sized> Arc<T> {
 ///
 /// [`upgrade`]: Weak::upgrade
 #[stable(feature = "arc_weak", since = "1.4.0")]
-pub struct Weak<T: ?Sized> {
+pub struct Weak<
+    T: ?Sized,
+    #[unstable(feature = "allocator_api", issue = "32838")] A: Allocator = Global,
+> {
     // This is a `NonNull` to allow optimizing the size of this type in enums,
     // but it is not necessarily a valid pointer.
     // `Weak::new` sets this to `usize::MAX` so that it doesn’t need
@@ -288,15 +309,16 @@ pub struct Weak<T: ?Sized> {
     // will ever have because RcBox has alignment at least 2.
     // This is only possible when `T: Sized`; unsized `T` never dangle.
     ptr: NonNull<ArcInner<T>>,
+    alloc: A,
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Send for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Send> Send for Weak<T, A> {}
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<T: ?Sized + Sync + Send> Sync for Weak<T> {}
+unsafe impl<T: ?Sized + Sync + Send, A: Allocator + Sync> Sync for Weak<T, A> {}
 
 #[unstable(feature = "coerce_unsized", issue = "27732")]
-impl<T: ?Sized + Unsize<U>, U: ?Sized> CoerceUnsized<Weak<U>> for Weak<T> {}
+impl<T: ?Sized + Unsize<U>, U: ?Sized, A: Allocator> CoerceUnsized<Weak<U, A>> for Weak<T, A> {}
 #[unstable(feature = "dispatch_from_dyn", issue = "none")]
 impl<T: ?Sized + Unsize<U>, U: ?Sized> DispatchFromDyn<Weak<U>> for Weak<T> {}
 
@@ -341,49 +363,83 @@ impl<T> Arc<T> {
     pub fn new(data: T) -> Arc<T> {
         // Start the weak pointer count as 1 which is the weak pointer that's
         // held by all the strong pointers (kinda), see std/rc.rs for more info
-        let x: Box<_> = box ArcInner {
+        let x: Box<_> = Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(1),
             weak: atomic::AtomicUsize::new(1),
             data,
-        };
-        Self::from_inner(Box::leak(x).into())
+        });
+        unsafe { Self::from_inner(Box::leak(x).into()) }
     }
 
-    /// Constructs a new `Arc<T>` using a weak reference to itself. Attempting
-    /// to upgrade the weak reference before this function returns will result
-    /// in a `None` value. However, the weak reference may be cloned freely and
-    /// stored for use at a later time.
+    /// Constructs a new `Arc<T>` while giving you a `Weak<T>` to the allocation,
+    /// to allow you to construct a `T` which holds a weak pointer to itself.
     ///
-    /// # Examples
-    /// ```
-    /// #![feature(arc_new_cyclic)]
-    /// #![allow(dead_code)]
+    /// Generally, a structure circularly referencing itself, either directly or
+    /// indirectly, should not hold a strong reference to itself to prevent a memory leak.
+    /// Using this function, you get access to the weak pointer during the
+    /// initialization of `T`, before the `Arc<T>` is created, such that you can
+    /// clone and store it inside the `T`.
+    ///
+    /// `new_cyclic` first allocates the managed allocation for the `Arc<T>`,
+    /// then calls your closure, giving it a `Weak<T>` to this allocation,
+    /// and only afterwards completes the construction of the `Arc<T>` by placing
+    /// the `T` returned from your closure into the allocation.
+    ///
+    /// Since the new `Arc<T>` is not fully-constructed until `Arc<T>::new_cyclic`
+    /// returns, calling [`upgrade`] on the weak reference inside your closure will
+    /// fail and result in a `None` value.
     ///
+    /// # Panics
+    ///
+    /// If `data_fn` panics, the panic is propagated to the caller, and the
+    /// temporary [`Weak<T>`] is dropped normally.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # #![allow(dead_code)]
     /// use std::sync::{Arc, Weak};
     ///
-    /// struct Foo {
-    ///     me: Weak<Foo>,
+    /// struct Gadget {
+    ///     me: Weak<Gadget>,
     /// }
     ///
-    /// let foo = Arc::new_cyclic(|me| Foo {
-    ///     me: me.clone(),
-    /// });
+    /// impl Gadget {
+    ///     /// Construct a reference counted Gadget.
+    ///     fn new() -> Arc<Self> {
+    ///         // `me` is a `Weak<Gadget>` pointing at the new allocation of the
+    ///         // `Arc` we're constructing.
+    ///         Arc::new_cyclic(|me| {
+    ///             // Create the actual struct here.
+    ///             Gadget { me: me.clone() }
+    ///         })
+    ///     }
+    ///
+    ///     /// Return a reference counted pointer to Self.
+    ///     fn me(&self) -> Arc<Self> {
+    ///         self.me.upgrade().unwrap()
+    ///     }
+    /// }
     /// ```
+    /// [`upgrade`]: Weak::upgrade
     #[cfg(not(no_global_oom_handling))]
     #[inline]
-    #[unstable(feature = "arc_new_cyclic", issue = "75861")]
-    pub fn new_cyclic(data_fn: impl FnOnce(&Weak<T>) -> T) -> Arc<T> {
+    #[stable(feature = "arc_new_cyclic", since = "1.60.0")]
+    pub fn new_cyclic<F>(data_fn: F) -> Arc<T>
+    where
+        F: FnOnce(&Weak<T>) -> T,
+    {
         // Construct the inner in the "uninitialized" state with a single
         // weak reference.
-        let uninit_ptr: NonNull<_> = Box::leak(box ArcInner {
+        let uninit_ptr: NonNull<_> = Box::leak(Box::new(ArcInner {
             strong: atomic::AtomicUsize::new(0),
             weak: atomic::AtomicUsize::new(1),
             data: mem::MaybeUninit::<T>::uninit(),
-        })
+        }))
         .into();
         let init_ptr: NonNull<ArcInner<T>> = uninit_ptr.cast();
 
-        let weak = Weak { ptr: init_ptr };
+        let weak = Weak { ptr: init_ptr, alloc: Global };
 
         // It's important we don't give up ownership of the weak pointer, or
         // else the memory might be freed by the time `data_fn` returns. If
@@ -395,7 +451,7 @@ impl<T> Arc<T> {
 
         // Now we can properly initialize the inner value and turn our weak
         // reference into a strong reference.
-        unsafe {
+        let strong = unsafe {
             let inner = init_ptr.as_ptr();
             ptr::write(ptr::addr_of_mut!((*inner).data), data);
 
@@ -413,9 +469,9 @@ impl<T> Arc<T> {
             // possible with safe code alone.
             let prev_value = (*inner).strong.fetch_add(1, Release);
             debug_assert_eq!(prev_value, 0, "No prior strong references should exist");
-        }
 
-        let strong = Arc::from_inner(init_ptr);
+            Arc::from_inner(init_ptr)
+        };
 
         // Strong references should collectively own a shared weak reference,
         // so don't run the destructor for our old weak reference.
@@ -435,17 +491,16 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -475,9 +530,10 @@ impl<T> Arc<T> {
     /// assert_eq!(*zero, 0)
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed() -> Arc<mem::MaybeUninit<T>> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -492,10 +548,18 @@ impl<T> Arc<T> {
     /// `data` will be pinned in memory and unable to be moved.
     #[cfg(not(no_global_oom_handling))]
     #[stable(feature = "pin", since = "1.33.0")]
+    #[must_use]
     pub fn pin(data: T) -> Pin<Arc<T>> {
         unsafe { Pin::new_unchecked(Arc::new(data)) }
     }
 
+    /// Constructs a new `Pin<Arc<T>>`, return an error if allocation fails.
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin(data: T) -> Result<Pin<Arc<T>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new(data)?)) }
+    }
+
     /// Constructs a new `Arc<T>`, returning an error if allocation fails.
     ///
     /// # Examples
@@ -517,7 +581,7 @@ impl<T> Arc<T> {
             weak: atomic::AtomicUsize::new(1),
             data,
         })?;
-        Ok(Self::from_inner(Box::leak(x).into()))
+        unsafe { Ok(Self::from_inner(Box::leak(x).into())) }
     }
 
     /// Constructs a new `Arc` with uninitialized contents, returning an error
@@ -533,12 +597,10 @@ impl<T> Arc<T> {
     ///
     /// let mut five = Arc::<u32>::try_new_uninit()?;
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5);
     /// # Ok::<(), std::alloc::AllocError>(())
@@ -587,6 +649,246 @@ impl<T> Arc<T> {
             )?))
         }
     }
+}
+
+impl<T, A: Allocator> Arc<T, A> {
+    /// Constructs a new `Arc<T>` in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::new_in(5, System);
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn new_in(data: T, alloc: A) -> Arc<T, A> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        );
+        let (ptr, alloc) = Box::into_unique(x);
+        unsafe { Self::from_inner_in(ptr.into(), alloc) }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents in the provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::new_uninit_in(System);
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5)
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::new_zeroed_in(System);
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0)
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_in(alloc: A) -> Arc<mem::MaybeUninit<T>, A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                ),
+                alloc,
+            )
+        }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator. If `T` does not implement `Unpin`,
+    /// then `data` will be pinned in memory and unable to be moved.
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn pin_in(data: T, alloc: A) -> Pin<Arc<T, A>> {
+        unsafe { Pin::new_unchecked(Arc::new_in(data, alloc)) }
+    }
+
+    /// Constructs a new `Pin<Arc<T, A>>` in the provided allocator, return an error if allocation
+    /// fails.
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_pin_in(data: T, alloc: A) -> Result<Pin<Arc<T, A>>, AllocError> {
+        unsafe { Ok(Pin::new_unchecked(Arc::try_new_in(data, alloc)?)) }
+    }
+
+    /// Constructs a new `Arc<T, A>` in the provided allocator, returning an error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let five = Arc::try_new_in(5, System)?;
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[inline]
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    #[inline]
+    pub fn try_new_in(data: T, alloc: A) -> Result<Arc<T, A>, AllocError> {
+        // Start the weak pointer count as 1 which is the weak pointer that's
+        // held by all the strong pointers (kinda), see std/rc.rs for more info
+        let x = Box::try_new_in(
+            ArcInner {
+                strong: atomic::AtomicUsize::new(1),
+                weak: atomic::AtomicUsize::new(1),
+                data,
+            },
+            alloc,
+        )?;
+        let (ptr, alloc) = Box::into_unique(x);
+        Ok(unsafe { Self::from_inner_in(ptr.into(), alloc) })
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, in the provided allocator, returning an
+    /// error if allocation fails.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut five = Arc::<u32, _>::try_new_uninit_in(System)?;
+    ///
+    /// let five = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    ///
+    ///     five.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*five, 5);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_uninit_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
+
+    /// Constructs a new `Arc` with uninitialized contents, with the memory
+    /// being filled with `0` bytes, in the provided allocator, returning an error if allocation
+    /// fails.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and incorrect usage
+    /// of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit, allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let zero = Arc::<u32, _>::try_new_zeroed_in(System)?;
+    /// let zero = unsafe { zero.assume_init() };
+    ///
+    /// assert_eq!(*zero, 0);
+    /// # Ok::<(), std::alloc::AllocError>(())
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    // #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn try_new_zeroed_in(alloc: A) -> Result<Arc<mem::MaybeUninit<T>, A>, AllocError> {
+        unsafe {
+            Ok(Arc::from_ptr_in(
+                Arc::try_allocate_for_layout(
+                    Layout::new::<T>(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| mem as *mut ArcInner<mem::MaybeUninit<T>>,
+                )?,
+                alloc,
+            ))
+        }
+    }
     /// Returns the inner value, if the `Arc` has exactly one strong reference.
     ///
     /// Otherwise, an [`Err`] is returned with the same `Arc` that was
@@ -617,9 +919,10 @@ impl<T> Arc<T> {
 
         unsafe {
             let elem = ptr::read(&this.ptr.as_ref().data);
+            let alloc = ptr::read(&this.alloc); // copy the allocator
 
             // Make a weak pointer to clean up the implicit strong-weak reference
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc };
             mem::forget(this);
 
             Ok(elem)
@@ -640,19 +943,19 @@ impl<T> Arc<[T]> {
     ///
     /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
     ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
     ///
-    ///     values.assume_init()
-    /// };
+    /// let values = unsafe { values.assume_init() };
     ///
     /// assert_eq!(*values, [1, 2, 3])
     /// ```
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_uninit_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe { Arc::from_ptr(Arc::allocate_for_slice(len)) }
     }
@@ -676,9 +979,10 @@ impl<T> Arc<[T]> {
     /// assert_eq!(*values, [0, 0, 0])
     /// ```
     ///
-    /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed
+    /// [zeroed]: mem::MaybeUninit::zeroed
     #[cfg(not(no_global_oom_handling))]
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use]
     pub fn new_zeroed_slice(len: usize) -> Arc<[mem::MaybeUninit<T>]> {
         unsafe {
             Arc::from_ptr(Arc::allocate_for_layout(
@@ -693,7 +997,83 @@ impl<T> Arc<[T]> {
     }
 }
 
-impl<T> Arc<mem::MaybeUninit<T>> {
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Constructs a new atomically reference-counted slice with uninitialized contents in the
+    /// provided allocator.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let mut values = Arc::<[u32], _>::new_uninit_slice_in(3, System);
+    ///
+    /// let values = unsafe {
+    ///     // Deferred initialization:
+    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
+    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
+    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    ///
+    ///     values.assume_init()
+    /// };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_uninit_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe { Arc::from_ptr_in(Arc::allocate_for_slice_in(len, &alloc), alloc) }
+    }
+
+    /// Constructs a new atomically reference-counted slice with uninitialized contents, with the memory being
+    /// filled with `0` bytes, in the provided allocator.
+    ///
+    /// See [`MaybeUninit::zeroed`][zeroed] for examples of correct and
+    /// incorrect usage of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Arc;
+    /// use std::alloc::System;
+    ///
+    /// let values = Arc::<[u32], _>::new_zeroed_slice_in(3, System);
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [0, 0, 0])
+    /// ```
+    ///
+    /// [zeroed]: mem::MaybeUninit::zeroed
+    #[cfg(not(no_global_oom_handling))]
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[inline]
+    pub fn new_zeroed_slice_in(len: usize, alloc: A) -> Arc<[mem::MaybeUninit<T>], A> {
+        unsafe {
+            Arc::from_ptr_in(
+                Arc::allocate_for_layout(
+                    Layout::array::<T>(len).unwrap(),
+                    |layout| alloc.allocate_zeroed(layout),
+                    |mem| {
+                        ptr::slice_from_raw_parts_mut(mem as *mut T, len)
+                            as *mut ArcInner<[mem::MaybeUninit<T>]>
+                    },
+                ),
+                alloc,
+            )
+        }
+    }
+}
+
+impl<T, A: Allocator> Arc<mem::MaybeUninit<T>, A> {
     /// Converts to `Arc<T>`.
     ///
     /// # Safety
@@ -704,7 +1084,7 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     /// Calling this when the content is not yet fully initialized
     /// causes immediate undefined behavior.
     ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
     ///
     /// # Examples
     ///
@@ -716,64 +1096,184 @@ impl<T> Arc<mem::MaybeUninit<T>> {
     ///
     /// let mut five = Arc::<u32>::new_uninit();
     ///
-    /// let five = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut five).as_mut_ptr().write(5);
+    /// // Deferred initialization:
+    /// Arc::get_mut(&mut five).unwrap().write(5);
     ///
-    ///     five.assume_init()
-    /// };
+    /// let five = unsafe { five.assume_init() };
     ///
     /// assert_eq!(*five, 5)
     /// ```
     #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<T> {
-        Arc::from_inner(mem::ManuallyDrop::new(self).ptr.cast())
+    pub unsafe fn assume_init(self) -> Arc<T, A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_inner_in(md_self.ptr.cast(), md_self.alloc.clone()) }
     }
 }
 
-impl<T> Arc<[mem::MaybeUninit<T>]> {
+impl<T, A: Allocator> Arc<[mem::MaybeUninit<T>], A> {
     /// Converts to `Arc<[T]>`.
     ///
     /// # Safety
     ///
-    /// As with [`MaybeUninit::assume_init`],
-    /// it is up to the caller to guarantee that the inner value
-    /// really is in an initialized state.
-    /// Calling this when the content is not yet fully initialized
-    /// causes immediate undefined behavior.
-    ///
-    /// [`MaybeUninit::assume_init`]: ../../std/mem/union.MaybeUninit.html#method.assume_init
+    /// As with [`MaybeUninit::assume_init`],
+    /// it is up to the caller to guarantee that the inner value
+    /// really is in an initialized state.
+    /// Calling this when the content is not yet fully initialized
+    /// causes immediate undefined behavior.
+    ///
+    /// [`MaybeUninit::assume_init`]: mem::MaybeUninit::assume_init
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(new_uninit)]
+    /// #![feature(get_mut_unchecked)]
+    ///
+    /// use std::sync::Arc;
+    ///
+    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
+    ///
+    /// // Deferred initialization:
+    /// let data = Arc::get_mut(&mut values).unwrap();
+    /// data[0].write(1);
+    /// data[1].write(2);
+    /// data[2].write(3);
+    ///
+    /// let values = unsafe { values.assume_init() };
+    ///
+    /// assert_eq!(*values, [1, 2, 3])
+    /// ```
+    #[unstable(feature = "new_uninit", issue = "63291")]
+    #[must_use = "`self` will be dropped if the result is not used"]
+    #[inline]
+    pub unsafe fn assume_init(self) -> Arc<[T], A>
+    where
+        A: Clone,
+    {
+        let md_self = mem::ManuallyDrop::new(self);
+        unsafe { Arc::from_ptr_in(md_self.ptr.as_ptr() as _, md_self.alloc.clone()) }
+    }
+}
+
+impl<T: ?Sized> Arc<T> {
+    /// Constructs an `Arc<T>` from a raw pointer.
+    ///
+    /// The raw pointer must have been previously returned by a call to
+    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// alignment as `T`. This is trivially true if `U` is `T`.
+    /// Note that if `U` is not `T` but has the same size and alignment, this is
+    /// basically like transmuting references of different types. See
+    /// [`mem::transmute`][transmute] for more information on what
+    /// restrictions apply in this case.
+    ///
+    /// The user of `from_raw` has to make sure a specific value of `T` is only
+    /// dropped once.
+    ///
+    /// This function is unsafe because improper use may lead to memory unsafety,
+    /// even if the returned `Arc<T>` is never accessed.
+    ///
+    /// [into_raw]: Arc::into_raw
+    /// [transmute]: core::mem::transmute
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let x = Arc::new("hello".to_owned());
+    /// let x_ptr = Arc::into_raw(x);
+    ///
+    /// unsafe {
+    ///     // Convert back to an `Arc` to prevent leak.
+    ///     let x = Arc::from_raw(x_ptr);
+    ///     assert_eq!(&*x, "hello");
+    ///
+    ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
+    /// }
+    ///
+    /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
+    /// ```
+    #[inline]
+    #[stable(feature = "rc_raw", since = "1.17.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Arc::from_raw_in(ptr, Global) }
+    }
+
+    /// Increments the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) for the duration of this method.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let five = Arc::new(5);
+    ///
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
+    ///
+    ///     // This assertion is deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    /// }
+    /// ```
+    #[inline]
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn increment_strong_count(ptr: *const T) {
+        unsafe { Arc::increment_strong_count_in(ptr, Global) }
+    }
+
+    /// Decrements the strong reference count on the `Arc<T>` associated with the
+    /// provided pointer by one.
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// associated `Arc` instance must be valid (i.e. the strong count must be at
+    /// least 1) when invoking this method. This method can be used to release the final
+    /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
+    /// released.
     ///
     /// # Examples
     ///
     /// ```
-    /// #![feature(new_uninit)]
-    /// #![feature(get_mut_unchecked)]
-    ///
     /// use std::sync::Arc;
     ///
-    /// let mut values = Arc::<[u32]>::new_uninit_slice(3);
-    ///
-    /// let values = unsafe {
-    ///     // Deferred initialization:
-    ///     Arc::get_mut_unchecked(&mut values)[0].as_mut_ptr().write(1);
-    ///     Arc::get_mut_unchecked(&mut values)[1].as_mut_ptr().write(2);
-    ///     Arc::get_mut_unchecked(&mut values)[2].as_mut_ptr().write(3);
+    /// let five = Arc::new(5);
     ///
-    ///     values.assume_init()
-    /// };
+    /// unsafe {
+    ///     let ptr = Arc::into_raw(five);
+    ///     Arc::increment_strong_count(ptr);
     ///
-    /// assert_eq!(*values, [1, 2, 3])
+    ///     // Those assertions are deterministic because we haven't shared
+    ///     // the `Arc` between threads.
+    ///     let five = Arc::from_raw(ptr);
+    ///     assert_eq!(2, Arc::strong_count(&five));
+    ///     Arc::decrement_strong_count(ptr);
+    ///     assert_eq!(1, Arc::strong_count(&five));
+    /// }
     /// ```
-    #[unstable(feature = "new_uninit", issue = "63291")]
     #[inline]
-    pub unsafe fn assume_init(self) -> Arc<[T]> {
-        unsafe { Arc::from_ptr(mem::ManuallyDrop::new(self).ptr.as_ptr() as _) }
+    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
+    pub unsafe fn decrement_strong_count(ptr: *const T) {
+        unsafe { Arc::decrement_strong_count_in(ptr, Global) }
     }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Consumes the `Arc`, returning the wrapped pointer.
     ///
     /// To avoid a memory leak the pointer must be converted back to an `Arc` using
@@ -788,6 +1288,7 @@ impl<T: ?Sized> Arc<T> {
     /// let x_ptr = Arc::into_raw(x);
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use = "losing the pointer will leak memory"]
     #[stable(feature = "rc_raw", since = "1.17.0")]
     pub fn into_raw(this: Self) -> *const T {
         let ptr = Self::as_ptr(&this);
@@ -811,6 +1312,7 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(x_ptr, Arc::as_ptr(&y));
     /// assert_eq!(unsafe { &*x_ptr }, "hello");
     /// ```
+    #[must_use]
     #[stable(feature = "rc_as_ptr", since = "1.45.0")]
     pub fn as_ptr(this: &Self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(this.ptr);
@@ -821,10 +1323,10 @@ impl<T: ?Sized> Arc<T> {
         unsafe { ptr::addr_of_mut!((*ptr).data) }
     }
 
-    /// Constructs an `Arc<T>` from a raw pointer.
+    /// Constructs an `Arc<T, A>` from a raw pointer.
     ///
     /// The raw pointer must have been previously returned by a call to
-    /// [`Arc<U>::into_raw`][into_raw] where `U` must have the same size and
+    /// [`Arc<U, A>::into_raw`][into_raw] where `U` must have the same size and
     /// alignment as `T`. This is trivially true if `U` is `T`.
     /// Note that if `U` is not `T` but has the same size and alignment, this is
     /// basically like transmuting references of different types. See
@@ -835,7 +1337,8 @@ impl<T: ?Sized> Arc<T> {
     /// dropped once.
     ///
     /// This function is unsafe because improper use may lead to memory unsafety,
-    /// even if the returned `Arc<T>` is never accessed.
+    /// even if the returned `Arc<T>` is never accessed. The pointer must point to
+    /// a region of memory allocated by `alloc`.
     ///
     /// [into_raw]: Arc::into_raw
     /// [transmute]: core::mem::transmute
@@ -843,14 +1346,17 @@ impl<T: ?Sized> Arc<T> {
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let x = Arc::new("hello".to_owned());
+    /// let x = Arc::new_in("hello".to_owned(), System);
     /// let x_ptr = Arc::into_raw(x);
     ///
     /// unsafe {
     ///     // Convert back to an `Arc` to prevent leak.
-    ///     let x = Arc::from_raw(x_ptr);
+    ///     let x = Arc::from_raw_in(x_ptr, System);
     ///     assert_eq!(&*x, "hello");
     ///
     ///     // Further calls to `Arc::from_raw(x_ptr)` would be memory-unsafe.
@@ -858,15 +1364,15 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// // The memory was freed when `x` went out of scope above, so `x_ptr` is now dangling!
     /// ```
-    #[stable(feature = "rc_raw", since = "1.17.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         unsafe {
             let offset = data_offset(ptr);
 
             // Reverse the offset to find the original ArcInner.
             let arc_ptr = (ptr as *mut ArcInner<T>).set_ptr_value((ptr as *mut u8).offset(-offset));
 
-            Self::from_ptr(arc_ptr)
+            Self::from_ptr_in(arc_ptr, alloc)
         }
     }
 
@@ -881,8 +1387,13 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// let weak_five = Arc::downgrade(&five);
     /// ```
+    #[must_use = "this returns a new `Weak` pointer, \
+                  without modifying the original `Arc`"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn downgrade(this: &Self) -> Weak<T> {
+    pub fn downgrade(this: &Self) -> Weak<T, A>
+    where
+        A: Clone,
+    {
         // This Relaxed is OK because we're checking the value in the CAS
         // below.
         let mut cur = this.inner().weak.load(Relaxed);
@@ -906,7 +1417,7 @@ impl<T: ?Sized> Arc<T> {
                 Ok(_) => {
                     // Make sure we do not create a dangling Weak
                     debug_assert!(!is_dangling(this.ptr.as_ptr()));
-                    return Weak { ptr: this.ptr };
+                    return Weak { ptr: this.ptr, alloc: this.alloc.clone() };
                 }
                 Err(old) => cur = old,
             }
@@ -934,9 +1445,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(1, Arc::weak_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn weak_count(this: &Self) -> usize {
-        let cnt = this.inner().weak.load(SeqCst);
+        let cnt = this.inner().weak.load(Acquire);
         // If the weak count is currently locked, the value of the
         // count was 0 just before taking the lock.
         if cnt == usize::MAX { 0 } else { cnt - 1 }
@@ -963,9 +1475,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert_eq!(2, Arc::strong_count(&five));
     /// ```
     #[inline]
+    #[must_use]
     #[stable(feature = "arc_counts", since = "1.15.0")]
     pub fn strong_count(this: &Self) -> usize {
-        this.inner().strong.load(SeqCst)
+        this.inner().strong.load(Acquire)
     }
 
     /// Increments the strong reference count on the `Arc<T>` associated with the
@@ -975,30 +1488,37 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// The pointer must have been obtained through `Arc::into_raw`, and the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) for the duration of this method.
+    /// least 1) for the duration of this method,, and `ptr` must point to a block of memory
+    /// allocated by `alloc`.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // This assertion is deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn increment_strong_count(ptr: *const T) {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn increment_strong_count_in(ptr: *const T, alloc: A)
+    where
+        A: Clone,
+    {
         // Retain Arc, but don't touch refcount by wrapping in ManuallyDrop
-        let arc = unsafe { mem::ManuallyDrop::new(Arc::<T>::from_raw(ptr)) };
+        let arc = unsafe { mem::ManuallyDrop::new(Arc::from_raw_in(ptr, alloc)) };
         // Now increase refcount, but don't drop new refcount either
         let _arc_clone: mem::ManuallyDrop<_> = arc.clone();
     }
@@ -1008,35 +1528,39 @@ impl<T: ?Sized> Arc<T> {
     ///
     /// # Safety
     ///
-    /// The pointer must have been obtained through `Arc::into_raw`, and the
+    /// The pointer must have been obtained through `Arc::into_raw`,  the
     /// associated `Arc` instance must be valid (i.e. the strong count must be at
-    /// least 1) when invoking this method. This method can be used to release the final
+    /// least 1) when invoking this method, and `ptr` must point to a block of memory
+    /// allocated by `alloc`. This method can be used to release the final
     /// `Arc` and backing storage, but **should not** be called after the final `Arc` has been
     /// released.
     ///
     /// # Examples
     ///
     /// ```
+    /// #![feature(allocator_api)]
+    ///
     /// use std::sync::Arc;
+    /// use std::alloc::System;
     ///
-    /// let five = Arc::new(5);
+    /// let five = Arc::new_in(5, System);
     ///
     /// unsafe {
     ///     let ptr = Arc::into_raw(five);
-    ///     Arc::increment_strong_count(ptr);
+    ///     Arc::increment_strong_count_in(ptr, System);
     ///
     ///     // Those assertions are deterministic because we haven't shared
     ///     // the `Arc` between threads.
-    ///     let five = Arc::from_raw(ptr);
+    ///     let five = Arc::from_raw_in(ptr, System);
     ///     assert_eq!(2, Arc::strong_count(&five));
-    ///     Arc::decrement_strong_count(ptr);
+    ///     Arc::decrement_strong_count_in(ptr, System);
     ///     assert_eq!(1, Arc::strong_count(&five));
     /// }
     /// ```
     #[inline]
-    #[stable(feature = "arc_mutate_strong_count", since = "1.51.0")]
-    pub unsafe fn decrement_strong_count(ptr: *const T) {
-        unsafe { mem::drop(Arc::from_raw(ptr)) };
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn decrement_strong_count_in(ptr: *const T, alloc: A) {
+        unsafe { mem::drop(Arc::from_raw_in(ptr, alloc)) };
     }
 
     #[inline]
@@ -1052,16 +1576,17 @@ impl<T: ?Sized> Arc<T> {
     // Non-inlined part of `drop`.
     #[inline(never)]
     unsafe fn drop_slow(&mut self) {
-        // Destroy the data at this time, even though we may not free the box
-        // allocation itself (there may still be weak pointers lying around).
+        // Destroy the data at this time, even though we must not free the box
+        // allocation itself (there might still be weak pointers lying around).
         unsafe { ptr::drop_in_place(Self::get_mut_unchecked(self)) };
 
         // Drop the weak ref collectively held by all strong references
-        drop(Weak { ptr: self.ptr });
+        // Take a reference to `self.alloc` instead of cloning because 1. it'll
+        // last long enough, and 2. you should be able to drop `Arc`s with
+        // unclonable allocators
+        drop(Weak { ptr: self.ptr, alloc: &self.alloc });
     }
 
-    #[inline]
-    #[stable(feature = "ptr_eq", since = "1.17.0")]
     /// Returns `true` if the two `Arc`s point to the same allocation
     /// (in a vein similar to [`ptr::eq`]).
     ///
@@ -1078,7 +1603,10 @@ impl<T: ?Sized> Arc<T> {
     /// assert!(!Arc::ptr_eq(&five, &other_five));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
+    #[inline]
+    #[must_use]
+    #[stable(feature = "ptr_eq", since = "1.17.0")]
     pub fn ptr_eq(this: &Self, other: &Self) -> bool {
         this.ptr.as_ptr() == other.ptr.as_ptr()
     }
@@ -1137,28 +1665,30 @@ impl<T: ?Sized> Arc<T> {
 
         Ok(inner)
     }
+}
 
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Allocates an `ArcInner<T>` with sufficient space for an unsized inner value.
     #[cfg(not(no_global_oom_handling))]
-    unsafe fn allocate_for_ptr(ptr: *const T) -> *mut ArcInner<T> {
+    unsafe fn allocate_for_ptr_in(ptr: *const T, alloc: &A) -> *mut ArcInner<T> {
         // Allocate for the `ArcInner<T>` using the given value.
         unsafe {
-            Self::allocate_for_layout(
+            Arc::allocate_for_layout(
                 Layout::for_value(&*ptr),
-                |layout| Global.allocate(layout),
-                |mem| (ptr as *mut ArcInner<T>).set_ptr_value(mem) as *mut ArcInner<T>,
+                |layout| alloc.allocate(layout),
+                |mem| mem.with_metadata_of(ptr as *mut ArcInner<T>),
             )
         }
     }
 
     #[cfg(not(no_global_oom_handling))]
-    fn from_box(v: Box<T>) -> Arc<T> {
+    fn from_box_in(v: Box<T, A>) -> Arc<T, A> {
         unsafe {
             let (box_unique, alloc) = Box::into_unique(v);
             let bptr = box_unique.as_ptr();
 
             let value_size = size_of_val(&*bptr);
-            let ptr = Self::allocate_for_ptr(bptr);
+            let ptr = Self::allocate_for_ptr_in(bptr, &alloc);
 
             // Copy value as bytes
             ptr::copy_nonoverlapping(
@@ -1168,9 +1698,9 @@ impl<T: ?Sized> Arc<T> {
             );
 
             // Free the allocation without dropping its contents
-            box_free(box_unique, alloc);
+            box_free(box_unique, &alloc);
 
-            Self::from_ptr(ptr)
+            Self::from_ptr_in(ptr, alloc)
         }
     }
 }
@@ -1277,6 +1807,34 @@ impl<T> Arc<[T]> {
     }
 }
 
+impl<T, A: Allocator> Arc<[T], A> {
+    /// Allocates an `ArcInner<[T]>` with the given length.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn allocate_for_slice_in(len: usize, alloc: &A) -> *mut ArcInner<[T]> {
+        unsafe {
+            Arc::allocate_for_layout(
+                Layout::array::<T>(len).unwrap(),
+                |layout| alloc.allocate(layout),
+                |mem| ptr::slice_from_raw_parts_mut(mem as *mut T, len) as *mut ArcInner<[T]>,
+            )
+        }
+    }
+
+    /// Copy elements from slice into newly allocated Arc<\[T\]>
+    ///
+    /// Unsafe because the caller must either take ownership or bind `T: Copy`.
+    #[cfg(not(no_global_oom_handling))]
+    unsafe fn copy_from_slice_in(v: &[T], alloc: A) -> Arc<[T], A> {
+        unsafe {
+            let ptr = Self::allocate_for_slice_in(v.len(), &alloc);
+
+            ptr::copy_nonoverlapping(v.as_ptr(), &mut (*ptr).data as *mut [T] as *mut T, v.len());
+
+            Self::from_ptr_in(ptr, alloc)
+        }
+    }
+}
+
 /// Specialization trait used for `From<&[T]>`.
 #[cfg(not(no_global_oom_handling))]
 trait ArcFromSlice<T> {
@@ -1300,7 +1858,7 @@ impl<T: Copy> ArcFromSlice<T> for Arc<[T]> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Clone for Arc<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Arc<T, A> {
     /// Makes a clone of the `Arc` pointer.
     ///
     /// This creates another pointer to the same allocation, increasing the
@@ -1316,7 +1874,7 @@ impl<T: ?Sized> Clone for Arc<T> {
     /// let _ = Arc::clone(&five);
     /// ```
     #[inline]
-    fn clone(&self) -> Arc<T> {
+    fn clone(&self) -> Arc<T, A> {
         // Using a relaxed ordering is alright here, as knowledge of the
         // original reference prevents other threads from erroneously deleting
         // the object.
@@ -1330,25 +1888,26 @@ impl<T: ?Sized> Clone for Arc<T> {
         // [1]: (www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html)
         let old_size = self.inner().strong.fetch_add(1, Relaxed);
 
-        // However we need to guard against massive refcounts in case someone
-        // is `mem::forget`ing Arcs. If we don't do this the count can overflow
-        // and users will use-after free. We racily saturate to `isize::MAX` on
-        // the assumption that there aren't ~2 billion threads incrementing
-        // the reference count at once. This branch will never be taken in
-        // any realistic program.
+        // However we need to guard against massive refcounts in case someone is `mem::forget`ing
+        // Arcs. If we don't do this the count can overflow and users will use-after free. This
+        // branch will never be taken in any realistic program. We abort because such a program is
+        // incredibly degenerate, and we don't care to support it.
         //
-        // We abort because such a program is incredibly degenerate, and we
-        // don't care to support it.
+        // This check is not 100% water-proof: we error when the refcount grows beyond `isize::MAX`.
+        // But we do that check *after* having done the increment, so there is a chance here that
+        // the worst already happened and we actually do overflow the `usize` counter. However, that
+        // requires the counter to grow from `isize::MAX` to `usize::MAX` between the increment
+        // above and the `abort` below, which seems exceedingly unlikely.
         if old_size > MAX_REFCOUNT {
             abort();
         }
 
-        Self::from_inner(self.ptr)
+        unsafe { Self::from_inner_in(self.ptr, self.alloc.clone()) }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> Deref for Arc<T> {
+impl<T: ?Sized, A: Allocator> Deref for Arc<T, A> {
     type Target = T;
 
     #[inline]
@@ -1360,21 +1919,22 @@ impl<T: ?Sized> Deref for Arc<T> {
 #[unstable(feature = "receiver_trait", issue = "none")]
 impl<T: ?Sized> Receiver for Arc<T> {}
 
-impl<T: Clone> Arc<T> {
+impl<T: Clone, A: Allocator + Clone> Arc<T, A> {
     /// Makes a mutable reference into the given `Arc`.
     ///
-    /// If there are other `Arc` or [`Weak`] pointers to the same allocation,
-    /// then `make_mut` will create a new allocation and invoke [`clone`][clone] on the inner value
-    /// to ensure unique ownership. This is also referred to as clone-on-write.
+    /// If there are other `Arc` pointers to the same allocation, then `make_mut` will
+    /// [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also
+    /// referred to as clone-on-write.
     ///
-    /// Note that this differs from the behavior of [`Rc::make_mut`] which disassociates
-    /// any remaining `Weak` pointers.
+    /// However, if there are no other `Arc` pointers to this allocation, but some [`Weak`]
+    /// pointers, then the [`Weak`] pointers will be dissociated and the inner value will not
+    /// be cloned.
     ///
-    /// See also [`get_mut`][get_mut], which will fail rather than cloning.
+    /// See also [`get_mut`], which will fail rather than cloning the inner value
+    /// or dissociating [`Weak`] pointers.
     ///
-    /// [clone]: Clone::clone
-    /// [get_mut]: Arc::get_mut
-    /// [`Rc::make_mut`]: super::rc::Rc::make_mut
+    /// [`clone`]: Clone::clone
+    /// [`get_mut`]: Arc::get_mut
     ///
     /// # Examples
     ///
@@ -1393,6 +1953,23 @@ impl<T: Clone> Arc<T> {
     /// assert_eq!(*data, 8);
     /// assert_eq!(*other_data, 12);
     /// ```
+    ///
+    /// [`Weak`] pointers will be dissociated:
+    ///
+    /// ```
+    /// use std::sync::Arc;
+    ///
+    /// let mut data = Arc::new(75);
+    /// let weak = Arc::downgrade(&data);
+    ///
+    /// assert!(75 == *data);
+    /// assert!(75 == *weak.upgrade().unwrap());
+    ///
+    /// *Arc::make_mut(&mut data) += 1;
+    ///
+    /// assert!(76 == *data);
+    /// assert!(weak.upgrade().is_none());
+    /// ```
     #[cfg(not(no_global_oom_handling))]
     #[inline]
     #[stable(feature = "arc_unique", since = "1.4.0")]
@@ -1408,7 +1985,7 @@ impl<T: Clone> Arc<T> {
         if this.inner().strong.compare_exchange(1, 0, Acquire, Relaxed).is_err() {
             // Another strong pointer exists, so we must clone.
             // Pre-allocate memory to allow writing the cloned value directly.
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 (**this).write_clone_into_raw(data.as_mut_ptr());
@@ -1429,10 +2006,10 @@ impl<T: Clone> Arc<T> {
 
             // Materialize our own implicit weak pointer, so that it can clean
             // up the ArcInner as needed.
-            let _weak = Weak { ptr: this.ptr };
+            let _weak = Weak { ptr: this.ptr, alloc: this.alloc.clone() };
 
             // Can just steal the data, all that's left is Weaks
-            let mut arc = Self::new_uninit();
+            let mut arc = Self::new_uninit_in(this.alloc.clone());
             unsafe {
                 let data = Arc::get_mut_unchecked(&mut arc);
                 data.as_mut_ptr().copy_from_nonoverlapping(&**this, 1);
@@ -1448,9 +2025,44 @@ impl<T: Clone> Arc<T> {
         // either unique to begin with, or became one upon cloning the contents.
         unsafe { Self::get_mut_unchecked(this) }
     }
+
+    /// If we have the only reference to `T` then unwrap it. Otherwise, clone `T` and return the
+    /// clone.
+    ///
+    /// Assuming `arc_t` is of type `Arc<T>`, this function is functionally equivalent to
+    /// `(*arc_t).clone()`, but will avoid cloning the inner value where possible.
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(arc_unwrap_or_clone)]
+    /// # use std::{ptr, sync::Arc};
+    /// let inner = String::from("test");
+    /// let ptr = inner.as_ptr();
+    ///
+    /// let arc = Arc::new(inner);
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // The inner value was not cloned
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    ///
+    /// let arc = Arc::new(inner);
+    /// let arc2 = arc.clone();
+    /// let inner = Arc::unwrap_or_clone(arc);
+    /// // Because there were 2 references, we had to clone the inner value.
+    /// assert!(!ptr::eq(ptr, inner.as_ptr()));
+    /// // `arc2` is the last reference, so when we unwrap it we get back
+    /// // the original `String`.
+    /// let inner = Arc::unwrap_or_clone(arc2);
+    /// assert!(ptr::eq(ptr, inner.as_ptr()));
+    /// ```
+    #[inline]
+    #[unstable(feature = "arc_unwrap_or_clone", issue = "93610")]
+    pub fn unwrap_or_clone(this: Self) -> T {
+        Arc::try_unwrap(this).unwrap_or_else(|arc| (*arc).clone())
+    }
 }
 
-impl<T: ?Sized> Arc<T> {
+impl<T: ?Sized, A: Allocator> Arc<T, A> {
     /// Returns a mutable reference into the given `Arc`, if there are
     /// no other `Arc` or [`Weak`] pointers to the same allocation.
     ///
@@ -1458,7 +2070,7 @@ impl<T: ?Sized> Arc<T> {
     /// mutate a shared value.
     ///
     /// See also [`make_mut`][make_mut], which will [`clone`][clone]
-    /// the inner value when there are other pointers.
+    /// the inner value when there are other `Arc` pointers.
     ///
     /// [make_mut]: Arc::make_mut
     /// [clone]: Clone::clone
@@ -1555,7 +2167,7 @@ impl<T: ?Sized> Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Arc<T, A> {
     /// Drops the `Arc`.
     ///
     /// This will decrement the strong reference count. If the strong reference
@@ -1626,9 +2238,7 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Arc<T> {
     }
 }
 
-impl Arc<dyn Any + Send + Sync> {
-    #[inline]
-    #[stable(feature = "rc_downcast", since = "1.29.0")]
+impl<A: Allocator + Clone> Arc<dyn Any + Send + Sync, A> {
     /// Attempt to downcast the `Arc<dyn Any + Send + Sync>` to a concrete type.
     ///
     /// # Examples
@@ -1647,18 +2257,63 @@ impl Arc<dyn Any + Send + Sync> {
     /// print_if_string(Arc::new(my_string));
     /// print_if_string(Arc::new(0i8));
     /// ```
-    pub fn downcast<T>(self) -> Result<Arc<T>, Self>
+    #[inline]
+    #[stable(feature = "rc_downcast", since = "1.29.0")]
+    pub fn downcast<T>(self) -> Result<Arc<T, A>, Self>
     where
-        T: Any + Send + Sync + 'static,
+        T: Any + Send + Sync,
     {
         if (*self).is::<T>() {
-            let ptr = self.ptr.cast::<ArcInner<T>>();
-            mem::forget(self);
-            Ok(Arc::from_inner(ptr))
+            unsafe {
+                let ptr = self.ptr.cast::<ArcInner<T>>();
+                let alloc = self.alloc.clone();
+                mem::forget(self);
+                Ok(Arc::from_inner_in(ptr, alloc))
+            }
         } else {
             Err(self)
         }
     }
+
+    /// Downcasts the `Arc<dyn Any + Send + Sync>` to a concrete type.
+    ///
+    /// For a safe alternative see [`downcast`].
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(downcast_unchecked)]
+    ///
+    /// use std::any::Any;
+    /// use std::sync::Arc;
+    ///
+    /// let x: Arc<dyn Any + Send + Sync> = Arc::new(1_usize);
+    ///
+    /// unsafe {
+    ///     assert_eq!(*x.downcast_unchecked::<usize>(), 1);
+    /// }
+    /// ```
+    ///
+    /// # Safety
+    ///
+    /// The contained value must be of type `T`. Calling this method
+    /// with the incorrect type is *undefined behavior*.
+    ///
+    ///
+    /// [`downcast`]: Self::downcast
+    #[inline]
+    #[unstable(feature = "downcast_unchecked", issue = "90850")]
+    pub unsafe fn downcast_unchecked<T>(self) -> Arc<T, A>
+    where
+        T: Any + Send + Sync,
+    {
+        unsafe {
+            let ptr = self.ptr.cast::<ArcInner<T>>();
+            let alloc = self.alloc.clone();
+            mem::forget(self);
+            Arc::from_inner_in(ptr, alloc)
+        }
+    }
 }
 
 impl<T> Weak<T> {
@@ -1675,9 +2330,40 @@ impl<T> Weak<T> {
     /// let empty: Weak<i64> = Weak::new();
     /// assert!(empty.upgrade().is_none());
     /// ```
+    #[inline]
     #[stable(feature = "downgraded_weak", since = "1.10.0")]
     pub fn new() -> Weak<T> {
-        Weak { ptr: NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") }
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc: Global,
+        }
+    }
+}
+
+impl<T, A: Allocator> Weak<T, A> {
+    /// Constructs a new `Weak<T, A>`, without allocating any memory, technically in the provided
+    /// allocator.
+    /// Calling [`upgrade`] on the return value always gives [`None`].
+    ///
+    /// [`upgrade`]: Weak::upgrade
+    ///
+    /// # Examples
+    ///
+    /// ```
+    /// #![feature(allocator_api)]
+    ///
+    /// use std::sync::Weak;
+    /// use std::alloc::System;
+    ///
+    /// let empty: Weak<i64, _> = Weak::new_in(System);
+    /// assert!(empty.upgrade().is_none());
+    /// ```
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub fn new_in(alloc: A) -> Weak<T, A> {
+        Weak {
+            ptr: unsafe { NonNull::new(usize::MAX as *mut ArcInner<T>).expect("MAX is not 0") },
+            alloc,
+        }
     }
 }
 
@@ -1689,6 +2375,55 @@ struct WeakInner<'a> {
 }
 
 impl<T: ?Sized> Weak<T> {
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    ///
+    /// This can be used to safely get a strong reference (by calling [`upgrade`]
+    /// later) or to deallocate the weak count by dropping the `Weak<T>`.
+    ///
+    /// It takes ownership of one weak reference (with the exception of pointers created by [`new`],
+    /// as these don't own anything; the method still works on them).
+    ///
+    /// # Safety
+    ///
+    /// The pointer must have originated from the [`into_raw`] and must still own its potential
+    /// weak reference.
+    ///
+    /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
+    /// takes ownership of one weak reference currently represented as a raw pointer (the weak
+    /// count is not modified by this operation) and therefore it must be paired with a previous
+    /// call to [`into_raw`].
+    /// # Examples
+    ///
+    /// ```
+    /// use std::sync::{Arc, Weak};
+    ///
+    /// let strong = Arc::new("hello".to_owned());
+    ///
+    /// let raw_1 = Arc::downgrade(&strong).into_raw();
+    /// let raw_2 = Arc::downgrade(&strong).into_raw();
+    ///
+    /// assert_eq!(2, Arc::weak_count(&strong));
+    ///
+    /// assert_eq!("hello", &*unsafe { Weak::from_raw(raw_1) }.upgrade().unwrap());
+    /// assert_eq!(1, Arc::weak_count(&strong));
+    ///
+    /// drop(strong);
+    ///
+    /// // Decrement the last weak count.
+    /// assert!(unsafe { Weak::from_raw(raw_2) }.upgrade().is_none());
+    /// ```
+    ///
+    /// [`new`]: Weak::new
+    /// [`into_raw`]: Weak::into_raw
+    /// [`upgrade`]: Weak::upgrade
+    #[inline]
+    #[stable(feature = "weak_into_raw", since = "1.45.0")]
+    pub unsafe fn from_raw(ptr: *const T) -> Self {
+        unsafe { Weak::from_raw_in(ptr, Global) }
+    }
+}
+
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Returns a raw pointer to the object `T` pointed to by this `Weak<T>`.
     ///
     /// The pointer is valid only if there are some strong references. The pointer may be dangling,
@@ -1713,7 +2448,8 @@ impl<T: ?Sized> Weak<T> {
     /// // assert_eq!("hello", unsafe { &*weak.as_ptr() });
     /// ```
     ///
-    /// [`null`]: core::ptr::null
+    /// [`null`]: core::ptr::null "ptr::null"
+    #[must_use]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn as_ptr(&self) -> *const T {
         let ptr: *mut ArcInner<T> = NonNull::as_ptr(self.ptr);
@@ -1723,7 +2459,7 @@ impl<T: ?Sized> Weak<T> {
             // a valid payload address, as the payload is at least as aligned as ArcInner (usize).
             ptr as *const T
         } else {
-            // SAFETY: if is_dangling returns false, then the pointer is dereferencable.
+            // SAFETY: if is_dangling returns false, then the pointer is dereferenceable.
             // The payload may be dropped at this point, and we have to maintain provenance,
             // so use raw pointer manipulation.
             unsafe { ptr::addr_of_mut!((*ptr).data) }
@@ -1757,6 +2493,7 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// [`from_raw`]: Weak::from_raw
     /// [`as_ptr`]: Weak::as_ptr
+    #[must_use = "`self` will be dropped if the result is not used"]
     #[stable(feature = "weak_into_raw", since = "1.45.0")]
     pub fn into_raw(self) -> *const T {
         let result = self.as_ptr();
@@ -1764,7 +2501,8 @@ impl<T: ?Sized> Weak<T> {
         result
     }
 
-    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>`.
+    /// Converts a raw pointer previously created by [`into_raw`] back into `Weak<T>` in the provided
+    /// allocator.
     ///
     /// This can be used to safely get a strong reference (by calling [`upgrade`]
     /// later) or to deallocate the weak count by dropping the `Weak<T>`.
@@ -1775,7 +2513,7 @@ impl<T: ?Sized> Weak<T> {
     /// # Safety
     ///
     /// The pointer must have originated from the [`into_raw`] and must still own its potential
-    /// weak reference.
+    /// weak reference, and must point to a block of memory allocated by `alloc`.
     ///
     /// It is allowed for the strong count to be 0 at the time of calling this. Nevertheless, this
     /// takes ownership of one weak reference currently represented as a raw pointer (the weak
@@ -1805,9 +2543,8 @@ impl<T: ?Sized> Weak<T> {
     /// [`new`]: Weak::new
     /// [`into_raw`]: Weak::into_raw
     /// [`upgrade`]: Weak::upgrade
-    /// [`forget`]: std::mem::forget
-    #[stable(feature = "weak_into_raw", since = "1.45.0")]
-    pub unsafe fn from_raw(ptr: *const T) -> Self {
+    #[unstable(feature = "allocator_api", issue = "32838")]
+    pub unsafe fn from_raw_in(ptr: *const T, alloc: A) -> Self {
         // See Weak::as_ptr for context on how the input pointer is derived.
 
         let ptr = if is_dangling(ptr as *mut T) {
@@ -1823,11 +2560,11 @@ impl<T: ?Sized> Weak<T> {
         };
 
         // SAFETY: we now have recovered the original Weak pointer, so can create the Weak.
-        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) } }
+        Weak { ptr: unsafe { NonNull::new_unchecked(ptr) }, alloc }
     }
 }
 
-impl<T: ?Sized> Weak<T> {
+impl<T: ?Sized, A: Allocator> Weak<T, A> {
     /// Attempts to upgrade the `Weak` pointer to an [`Arc`], delaying
     /// dropping of the inner value if successful.
     ///
@@ -1851,8 +2588,13 @@ impl<T: ?Sized> Weak<T> {
     ///
     /// assert!(weak_five.upgrade().is_none());
     /// ```
+    #[must_use = "this returns a new `Arc`, \
+                  without modifying the original weak pointer"]
     #[stable(feature = "arc_weak", since = "1.4.0")]
-    pub fn upgrade(&self) -> Option<Arc<T>> {
+    pub fn upgrade(&self) -> Option<Arc<T, A>>
+    where
+        A: Clone,
+    {
         // We use a CAS loop to increment the strong count instead of a
         // fetch_add as this function should never take the reference count
         // from zero to one.
@@ -1879,7 +2621,7 @@ impl<T: ?Sized> Weak<T> {
             // value can be initialized after `Weak` references have already been created. In that case, we
             // expect to observe the fully initialized value.
             match inner.strong.compare_exchange_weak(n, n + 1, Acquire, Relaxed) {
-                Ok(_) => return Some(Arc::from_inner(self.ptr)), // null checked above
+                Ok(_) => return Some(unsafe { Arc::from_inner_in(self.ptr, self.alloc.clone()) }), // null checked above
                 Err(old) => n = old,
             }
         }
@@ -1888,9 +2630,10 @@ impl<T: ?Sized> Weak<T> {
     /// Gets the number of strong (`Arc`) pointers pointing to this allocation.
     ///
     /// If `self` was created using [`Weak::new`], this will return 0.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn strong_count(&self) -> usize {
-        if let Some(inner) = self.inner() { inner.strong.load(SeqCst) } else { 0 }
+        if let Some(inner) = self.inner() { inner.strong.load(Acquire) } else { 0 }
     }
 
     /// Gets an approximation of the number of `Weak` pointers pointing to this
@@ -1904,12 +2647,13 @@ impl<T: ?Sized> Weak<T> {
     /// Due to implementation details, the returned value can be off by 1 in
     /// either direction when other threads are manipulating any `Arc`s or
     /// `Weak`s pointing to the same allocation.
+    #[must_use]
     #[stable(feature = "weak_counts", since = "1.41.0")]
     pub fn weak_count(&self) -> usize {
         self.inner()
             .map(|inner| {
-                let weak = inner.weak.load(SeqCst);
-                let strong = inner.strong.load(SeqCst);
+                let weak = inner.weak.load(Acquire);
+                let strong = inner.strong.load(Acquire);
                 if strong == 0 {
                     0
                 } else {
@@ -1981,8 +2725,9 @@ impl<T: ?Sized> Weak<T> {
     /// assert!(!first.ptr_eq(&third));
     /// ```
     ///
-    /// [`ptr::eq`]: core::ptr::eq
+    /// [`ptr::eq`]: core::ptr::eq "ptr::eq"
     #[inline]
+    #[must_use]
     #[stable(feature = "weak_ptr_eq", since = "1.39.0")]
     pub fn ptr_eq(&self, other: &Self) -> bool {
         self.ptr.as_ptr() == other.ptr.as_ptr()
@@ -1990,7 +2735,7 @@ impl<T: ?Sized> Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-impl<T: ?Sized> Clone for Weak<T> {
+impl<T: ?Sized, A: Allocator + Clone> Clone for Weak<T, A> {
     /// Makes a clone of the `Weak` pointer that points to the same allocation.
     ///
     /// # Examples
@@ -2003,11 +2748,11 @@ impl<T: ?Sized> Clone for Weak<T> {
     /// let _ = Weak::clone(&weak_five);
     /// ```
     #[inline]
-    fn clone(&self) -> Weak<T> {
+    fn clone(&self) -> Weak<T, A> {
         let inner = if let Some(inner) = self.inner() {
             inner
         } else {
-            return Weak { ptr: self.ptr };
+            return Weak { ptr: self.ptr, alloc: self.alloc.clone() };
         };
         // See comments in Arc::clone() for why this is relaxed.  This can use a
         // fetch_add (ignoring the lock) because the weak count is only locked
@@ -2020,7 +2765,7 @@ impl<T: ?Sized> Clone for Weak<T> {
             abort();
         }
 
-        Weak { ptr: self.ptr }
+        Weak { ptr: self.ptr, alloc: self.alloc.clone() }
     }
 }
 
@@ -2046,7 +2791,7 @@ impl<T> Default for Weak<T> {
 }
 
 #[stable(feature = "arc_weak", since = "1.4.0")]
-unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
+unsafe impl<#[may_dangle] T: ?Sized, A: Allocator> Drop for Weak<T, A> {
     /// Drops the `Weak` pointer.
     ///
     /// # Examples
@@ -2084,25 +2829,27 @@ unsafe impl<#[may_dangle] T: ?Sized> Drop for Weak<T> {
 
         if inner.weak.fetch_sub(1, Release) == 1 {
             acquire!(inner.weak);
-            unsafe { Global.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr())) }
+            unsafe {
+                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()))
+            }
         }
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-trait ArcEqIdent<T: ?Sized + PartialEq> {
-    fn eq(&self, other: &Arc<T>) -> bool;
-    fn ne(&self, other: &Arc<T>) -> bool;
+trait ArcEqIdent<T: ?Sized + PartialEq, A: Allocator> {
+    fn eq(&self, other: &Arc<T, A>) -> bool;
+    fn ne(&self, other: &Arc<T, A>) -> bool;
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    default fn eq(&self, other: &Arc<T>) -> bool {
+    default fn eq(&self, other: &Arc<T, A>) -> bool {
         **self == **other
     }
     #[inline]
-    default fn ne(&self, other: &Arc<T>) -> bool {
+    default fn ne(&self, other: &Arc<T, A>) -> bool {
         **self != **other
     }
 }
@@ -2115,20 +2862,20 @@ impl<T: ?Sized + PartialEq> ArcEqIdent<T> for Arc<T> {
 ///
 /// We can only do this when `T: Eq` as a `PartialEq` might be deliberately irreflexive.
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + crate::rc::MarkerEq> ArcEqIdent<T> for Arc<T> {
+impl<T: ?Sized + crate::rc::MarkerEq, A: Allocator> ArcEqIdent<T, A> for Arc<T, A> {
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         Arc::ptr_eq(self, other) || **self == **other
     }
 
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         !Arc::ptr_eq(self, other) && **self != **other
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
+impl<T: ?Sized + PartialEq, A: Allocator> PartialEq for Arc<T, A> {
     /// Equality for two `Arc`s.
     ///
     /// Two `Arc`s are equal if their inner values are equal, even if they are
@@ -2147,7 +2894,7 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five == Arc::new(5));
     /// ```
     #[inline]
-    fn eq(&self, other: &Arc<T>) -> bool {
+    fn eq(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::eq(self, other)
     }
 
@@ -2168,13 +2915,13 @@ impl<T: ?Sized + PartialEq> PartialEq for Arc<T> {
     /// assert!(five != Arc::new(6));
     /// ```
     #[inline]
-    fn ne(&self, other: &Arc<T>) -> bool {
+    fn ne(&self, other: &Arc<T, A>) -> bool {
         ArcEqIdent::ne(self, other)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
+impl<T: ?Sized + PartialOrd, A: Allocator> PartialOrd for Arc<T, A> {
     /// Partial comparison for two `Arc`s.
     ///
     /// The two are compared by calling `partial_cmp()` on their inner values.
@@ -2189,7 +2936,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert_eq!(Some(Ordering::Less), five.partial_cmp(&Arc::new(6)));
     /// ```
-    fn partial_cmp(&self, other: &Arc<T>) -> Option<Ordering> {
+    fn partial_cmp(&self, other: &Arc<T, A>) -> Option<Ordering> {
         (**self).partial_cmp(&**other)
     }
 
@@ -2206,7 +2953,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five < Arc::new(6));
     /// ```
-    fn lt(&self, other: &Arc<T>) -> bool {
+    fn lt(&self, other: &Arc<T, A>) -> bool {
         *(*self) < *(*other)
     }
 
@@ -2223,7 +2970,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five <= Arc::new(5));
     /// ```
-    fn le(&self, other: &Arc<T>) -> bool {
+    fn le(&self, other: &Arc<T, A>) -> bool {
         *(*self) <= *(*other)
     }
 
@@ -2240,7 +2987,7 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five > Arc::new(4));
     /// ```
-    fn gt(&self, other: &Arc<T>) -> bool {
+    fn gt(&self, other: &Arc<T, A>) -> bool {
         *(*self) > *(*other)
     }
 
@@ -2257,12 +3004,12 @@ impl<T: ?Sized + PartialOrd> PartialOrd for Arc<T> {
     ///
     /// assert!(five >= Arc::new(5));
     /// ```
-    fn ge(&self, other: &Arc<T>) -> bool {
+    fn ge(&self, other: &Arc<T, A>) -> bool {
         *(*self) >= *(*other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Ord> Ord for Arc<T> {
+impl<T: ?Sized + Ord, A: Allocator> Ord for Arc<T, A> {
     /// Comparison for two `Arc`s.
     ///
     /// The two are compared by calling `cmp()` on their inner values.
@@ -2277,29 +3024,29 @@ impl<T: ?Sized + Ord> Ord for Arc<T> {
     ///
     /// assert_eq!(Ordering::Less, five.cmp(&Arc::new(6)));
     /// ```
-    fn cmp(&self, other: &Arc<T>) -> Ordering {
+    fn cmp(&self, other: &Arc<T, A>) -> Ordering {
         (**self).cmp(&**other)
     }
 }
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Eq> Eq for Arc<T> {}
+impl<T: ?Sized + Eq, A: Allocator> Eq for Arc<T, A> {}
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Display> fmt::Display for Arc<T> {
+impl<T: ?Sized + fmt::Display, A: Allocator> fmt::Display for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Display::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + fmt::Debug> fmt::Debug for Arc<T> {
+impl<T: ?Sized + fmt::Debug, A: Allocator> fmt::Debug for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Debug::fmt(&**self, f)
     }
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> fmt::Pointer for Arc<T> {
+impl<T: ?Sized, A: Allocator> fmt::Pointer for Arc<T, A> {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         fmt::Pointer::fmt(&(&**self as *const T), f)
     }
@@ -2324,7 +3071,7 @@ impl<T: Default> Default for Arc<T> {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized + Hash> Hash for Arc<T> {
+impl<T: ?Sized + Hash, A: Allocator> Hash for Arc<T, A> {
     fn hash<H: Hasher>(&self, state: &mut H) {
         (**self).hash(state)
     }
@@ -2333,6 +3080,20 @@ impl<T: ?Sized + Hash> Hash for Arc<T> {
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "from_for_ptrs", since = "1.6.0")]
 impl<T> From<T> for Arc<T> {
+    /// Converts a `T` into an `Arc<T>`
+    ///
+    /// The conversion moves the value into a
+    /// newly allocated `Arc`. It is equivalent to
+    /// calling `Arc::new(t)`.
+    ///
+    /// # Example
+    /// ```rust
+    /// # use std::sync::Arc;
+    /// let x = 5;
+    /// let arc = Arc::new(5);
+    ///
+    /// assert_eq!(Arc::from(x), arc);
+    /// ```
     fn from(t: T) -> Self {
         Arc::new(t)
     }
@@ -2397,7 +3158,7 @@ impl From<String> for Arc<str> {
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T: ?Sized> From<Box<T>> for Arc<T> {
+impl<T: ?Sized, A: Allocator> From<Box<T, A>> for Arc<T, A> {
     /// Move a boxed object to a new, reference-counted allocation.
     ///
     /// # Example
@@ -2409,14 +3170,14 @@ impl<T: ?Sized> From<Box<T>> for Arc<T> {
     /// assert_eq!("eggplant", &shared[..]);
     /// ```
     #[inline]
-    fn from(v: Box<T>) -> Arc<T> {
-        Arc::from_box(v)
+    fn from(v: Box<T, A>) -> Arc<T, A> {
+        Arc::from_box_in(v)
     }
 }
 
 #[cfg(not(no_global_oom_handling))]
 #[stable(feature = "shared_from_slice", since = "1.21.0")]
-impl<T> From<Vec<T>> for Arc<[T]> {
+impl<T, A: Allocator + Clone> From<Vec<T, A>> for Arc<[T], A> {
     /// Allocate a reference-counted slice and move `v`'s items into it.
     ///
     /// # Example
@@ -2428,9 +3189,9 @@ impl<T> From<Vec<T>> for Arc<[T]> {
     /// assert_eq!(&[1, 2, 3], &shared[..]);
     /// ```
     #[inline]
-    fn from(mut v: Vec<T>) -> Arc<[T]> {
+    fn from(mut v: Vec<T, A>) -> Arc<[T], A> {
         unsafe {
-            let arc = Arc::copy_from_slice(&v);
+            let arc = Arc::copy_from_slice_in(&v, v.allocator().clone());
 
             // Allow the Vec to free its memory, but not destroy its contents
             v.set_len(0);
@@ -2493,13 +3254,33 @@ where
     }
 }
 
+#[stable(feature = "shared_from_str", since = "1.62.0")]
+impl From<Arc<str>> for Arc<[u8]> {
+    /// Converts an atomically reference-counted string slice into a byte slice.
+    ///
+    /// # Example
+    ///
+    /// ```
+    /// # use std::sync::Arc;
+    /// let string: Arc<str> = Arc::from("eggplant");
+    /// let bytes: Arc<[u8]> = Arc::from(string);
+    /// assert_eq!("eggplant".as_bytes(), bytes.as_ref());
+    /// ```
+    #[inline]
+    fn from(rc: Arc<str>) -> Self {
+        // SAFETY: `str` has the same layout as `[u8]`.
+        unsafe { Arc::from_raw(Arc::into_raw(rc) as *const [u8]) }
+    }
+}
+
 #[stable(feature = "boxed_slice_try_from", since = "1.43.0")]
-impl<T, const N: usize> TryFrom<Arc<[T]>> for Arc<[T; N]> {
-    type Error = Arc<[T]>;
+impl<T, A: Allocator + Clone, const N: usize> TryFrom<Arc<[T], A>> for Arc<[T; N], A> {
+    type Error = Arc<[T], A>;
 
-    fn try_from(boxed_slice: Arc<[T]>) -> Result<Self, Self::Error> {
+    fn try_from(boxed_slice: Arc<[T], A>) -> Result<Self, Self::Error> {
         if boxed_slice.len() == N {
-            Ok(unsafe { Arc::from_raw(Arc::into_raw(boxed_slice) as *mut [T; N]) })
+            let alloc = boxed_slice.alloc.clone();
+            Ok(unsafe { Arc::from_raw_in(Arc::into_raw(boxed_slice) as *mut [T; N], alloc) })
         } else {
             Err(boxed_slice)
         }
@@ -2592,21 +3373,21 @@ impl<T, I: iter::TrustedLen<Item = T>> ToArcSlice<T> for I {
 }
 
 #[stable(feature = "rust1", since = "1.0.0")]
-impl<T: ?Sized> borrow::Borrow<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> borrow::Borrow<T> for Arc<T, A> {
     fn borrow(&self) -> &T {
         &**self
     }
 }
 
 #[stable(since = "1.5.0", feature = "smart_ptr_as_ref")]
-impl<T: ?Sized> AsRef<T> for Arc<T> {
+impl<T: ?Sized, A: Allocator> AsRef<T> for Arc<T, A> {
     fn as_ref(&self) -> &T {
         &**self
     }
 }
 
 #[stable(feature = "pin", since = "1.33.0")]
-impl<T: ?Sized> Unpin for Arc<T> {}
+impl<T: ?Sized, A: Allocator> Unpin for Arc<T, A> {}
 
 /// Get the offset within an `ArcInner` for the payload behind a pointer.
 ///
@@ -2620,7 +3401,7 @@ unsafe fn data_offset<T: ?Sized>(ptr: *const T) -> isize {
     // SAFETY: since the only unsized types possible are slices, trait objects,
     // and extern types, the input safety requirement is currently enough to
     // satisfy the requirements of align_of_val_raw; this is an implementation
-    // detail of the language that may not be relied upon outside of std.
+    // detail of the language that must not be relied upon outside of std.
     unsafe { data_offset_align(align_of_val_raw(ptr)) }
 }
 
diff --git a/rust/helpers.c b/rust/helpers.c
index 3416351e0..649c8c2a8 100644
--- a/rust/helpers.c
+++ b/rust/helpers.c
@@ -44,6 +44,7 @@
 #include <linux/irqstage.h>
 #include <linux/dovetail.h>
 #include <linux/spinlock_pipeline.h>
+#include <linux/log2.h>
 
 void rust_helper_BUG(void)
 {
@@ -270,6 +271,12 @@ int rust_helper_page_aligned(unsigned long size)
 }
 EXPORT_SYMBOL_GPL(rust_helper_page_aligned); 
 
+size_t rust_helper_align(size_t x, unsigned long a)
+{
+	return ALIGN(x,a);
+}
+EXPORT_SYMBOL_GPL(rust_helper_align); 
+
 bool rust_helper_running_inband(void)
 {
 	return running_inband();
@@ -632,6 +639,7 @@ void rust_helper_dovetail_leave_oob(void) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_dovetail_leave_oob);
 
+
 void rust_helper_hard_spin_lock(struct raw_spinlock *rlock) {
 	hard_spin_lock(rlock);
 }
@@ -642,6 +650,17 @@ void rust_helper_hard_spin_unlock(struct raw_spinlock *rlock) {
 }
 EXPORT_SYMBOL_GPL(rust_helper_hard_spin_unlock);
 
+inline int rust_helper_ilog2(size_t size) {
+	return ilog2(size);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ilog2);
+
+int rust_helper_ffs(unsigned long x) {
+	return ffs(x);
+}
+EXPORT_SYMBOL_GPL(rust_helper_ffs);
+
+
 /* We use bindgen's --size_t-is-usize option to bind the C size_t type
  * as the Rust usize type, so we can use it in contexts where Rust
  * expects a usize like slice (array) indices. usize is defined to be
diff --git a/rust/kernel/allocator.rs b/rust/kernel/allocator.rs
index 759cec47d..2047a1c6b 100644
--- a/rust/kernel/allocator.rs
+++ b/rust/kernel/allocator.rs
@@ -4,10 +4,10 @@
 
 use core::alloc::{GlobalAlloc, Layout};
 use core::ptr;
-
+use crate::timekeeping::*;
 use crate::bindings;
 use crate::c_types;
-
+use crate::pr_info;
 pub struct KernelAllocator;
 
 unsafe impl GlobalAlloc for KernelAllocator {
@@ -32,7 +32,8 @@ static ALLOCATOR: KernelAllocator = KernelAllocator;
 // let's generate them ourselves instead.
 #[no_mangle]
 pub fn __rust_alloc(size: usize, _align: usize) -> *mut u8 {
-    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 }
+    let x = unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 };
+    return x;
 }
 
 #[no_mangle]
diff --git a/rust/kernel/double_linked_list3.rs b/rust/kernel/double_linked_list3.rs
new file mode 100644
index 000000000..0d106223e
--- /dev/null
+++ b/rust/kernel/double_linked_list3.rs
@@ -0,0 +1,366 @@
+use core::cmp::Ordering;
+use core::hash::{Hash, Hasher};
+use core::marker::PhantomData;
+use core::ptr::NonNull;
+use alloc::boxed::{Box};
+use alloc::alloc::{Global};
+
+use crate::pr_info;
+
+/// 双链表
+pub struct LinkedList<T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<T>,
+}
+
+/// 链表节点
+struct Node<T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    elem: T,
+}
+
+/// 链表迭代器
+pub struct Iter<'a, T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<&'a T>,
+}
+
+/// 链表可变迭代器
+pub struct IterMut<'a, T> {
+    front: Option<NonNull<Node<T>>>,
+    back: Option<NonNull<Node<T>>>,
+    len: usize,
+    marker: PhantomData<&'a mut T>,
+}
+
+impl<T> LinkedList<T> {
+    /// 创建一个空链表
+    pub fn new() -> Self {
+        Self {
+            front: None,
+            back: None,
+            len: 0,
+            marker: PhantomData,
+        }
+    }
+
+    /// 将元素插入到链表头部
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_front(1);
+    /// assert_eq!(list.front(), Some(&1));
+    /// ```
+    pub fn push_front(&mut self, elem: T) {
+            // let new = NonNull::new_unchecked(Box::into_raw(Box::try_new_in(Node {
+            //     front: None,
+            //     back: None,
+            //     elem,
+            // }, Global).unwrap()));
+            //TODO:
+            unimplemented!();
+    }
+
+    /// 将元素插入到链表尾部
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.back(), Some(&1));
+    /// ```
+    pub fn push_back(&mut self, elem: T) {
+        // let new = NonNull::new_unchecked(Box::into_raw(Box::try_new_in(Node {
+        //     back: None,
+        //     front: None,
+        //     elem,
+        // }, Global).unwrap()));
+        // TODO:
+        unimplemented!();
+    }
+
+    /// 将第一个元素返回
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_front(1);
+    /// assert_eq!(list.pop_front(), Some(1));
+    /// ```
+    pub fn pop_front(&mut self) -> Option<T> {
+        //TODO:
+        None
+    }
+
+    /// 将最后一个元素返回
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.pop_back(), Some(1));
+    /// ```
+    pub fn pop_back(&mut self) -> Option<T> {
+        // TODO:
+        None
+    }
+
+    /// 返回链表第一个元素的引用  
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// assert_eq!(list.front(), None);
+    /// list.push_front(1);
+    /// assert_eq!(list.front(), Some(&1));
+    /// ```
+    pub fn front(&self) -> Option<&T> {
+        unsafe { self.front.map(|node| &(*node.as_ptr()).elem) }
+    }
+
+    /// 返回链表第一个元素的可变引用   
+    pub fn front_mut(&mut self) -> Option<&mut T> {
+        unsafe { self.front.map(|node| &mut (*node.as_ptr()).elem) }
+    }
+
+    /// 返回链表最后一个元素的引用
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// assert_eq!(list.back(), None);
+    /// list.push_back(1);
+    /// assert_eq!(list.back(), Some(&1));
+    /// ```
+    pub fn back(&self) -> Option<&T> {
+        unsafe { self.back.map(|node| &(*node.as_ptr()).elem) }
+    }
+
+    /// 返回链表最后一个元素的可变引用
+    pub fn back_mut(&mut self) -> Option<&mut T> {
+        unsafe { self.back.map(|node| &mut (*node.as_ptr()).elem) }
+    }
+
+    /// 返回链表长度
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// assert_eq!(list.len(), 1);
+    /// ```
+    pub fn len(&self) -> usize {
+        self.len
+    }
+
+    pub fn is_empty(&self) -> bool{
+        self.len == 0
+    }
+
+    /// 清空链表
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::new();
+    /// list.push_back(1);
+    /// list.push_back(2);
+    /// assert_eq!(list.len(), 2);
+    /// list.clear();
+    /// assert_eq!(list.len(), 0);
+    /// ```
+    pub fn clear(&mut self) {
+        // Oh look it's drop again
+        while self.pop_front().is_some() {}
+    }
+
+    /// 返回一个迭代器
+    pub fn iter(&self) -> Iter<'_,T> {
+        Iter {
+            front: self.front,
+            back: self.back,
+            len: self.len,
+            marker: PhantomData,
+        }
+    }
+
+    /// 返回一个可变迭代器
+    pub fn iter_mut(&mut self) -> IterMut<'_,T> {
+        IterMut {
+            front: self.front,
+            back: self.back,
+            len: self.len,
+            marker: PhantomData,
+        }
+    }
+
+
+    /// 移除链表中下标为i的元素
+    /// 如果超出范围，使用panic!宏抛出异常
+    ///
+    /// # Examples
+    /// ```
+    /// use linked_list::double_linked_list::LinkedList;
+    /// let mut list = LinkedList::from_iter(vec![1,2,3]);
+    /// assert_eq!(list.remove(1), 2);
+    pub fn remove(&mut self, index: usize) -> T {
+        // TODO:
+        unimplemented!()
+    }
+
+    
+}
+
+
+impl<'a, T> IntoIterator for &'a LinkedList<T> {
+    type IntoIter = Iter<'a, T>;
+    type Item = &'a T;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.iter()
+    }
+}
+
+impl<'a, T> IntoIterator for &'a mut LinkedList<T> {
+    type IntoIter = IterMut<'a, T>;
+    type Item = &'a mut T;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.iter_mut()
+    }
+}
+
+impl<'a, T> Iterator for Iter<'a, T> {
+    type Item = &'a T;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.len > 0 {
+            // We could unwrap front, but this is safer and easier
+            self.front.map(|node| unsafe {
+                self.len -= 1;
+                self.front = (*node.as_ptr()).back;
+                &(*node.as_ptr()).elem
+            })
+        } else {
+            None
+        }
+    }
+
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        (self.len, Some(self.len))
+    }
+}
+impl<'a, T> Iterator for IterMut<'a, T> {
+    type Item = &'a mut T;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        // While self.front == self.back is a tempting condition to check here,
+        // it won't do the right for yielding the last element! That sort of
+        // thing only works for arrays because of "one-past-the-end" pointers.
+        if self.len > 0 {
+            // We could unwrap front, but this is safer and easier
+            self.front.map(|node| unsafe {
+                self.len -= 1;
+                self.front = (*node.as_ptr()).back;
+                &mut (*node.as_ptr()).elem
+            })
+        } else {
+            None
+        }
+    }
+
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        (self.len, Some(self.len))
+    }
+}
+
+
+impl<T> Drop for LinkedList<T> {
+    fn drop(&mut self) {
+        // Pop until we have to stop
+        while self.pop_front().is_some() {}
+    }
+}
+
+impl<T> Default for LinkedList<T> {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+impl<T: Clone> Clone for LinkedList<T> {
+    fn clone(&self) -> Self {
+        let mut new_list = Self::new();
+        for item in self {
+            new_list.push_back(item.clone());
+        }
+        new_list
+    }
+}
+impl<T> Extend<T> for LinkedList<T> {
+    fn extend<I: IntoIterator<Item = T>>(&mut self, iter: I) {
+        for item in iter {
+            self.push_back(item);
+        }
+    }
+}
+// impl<T> FromIterator<T> for LinkedList<T> {
+//     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {
+//         let mut list = Self::new();
+//         list.extend(iter);
+//         list
+//     }
+// }
+
+// impl<T: Debug> Debug for LinkedList<T> {
+//     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+//         f.debug_list().entries(self).finish()
+//     }
+// }
+
+impl<T: PartialEq> PartialEq for LinkedList<T> {
+    fn eq(&self, other: &Self) -> bool {
+        self.len() == other.len() && self.iter().eq(other)
+    }
+
+    fn ne(&self, other: &Self) -> bool {
+        self.len() != other.len() || self.iter().ne(other)
+    }
+}
+
+impl<T: Eq> Eq for LinkedList<T> {}
+
+impl<T: PartialOrd> PartialOrd for LinkedList<T> {
+    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
+        self.iter().partial_cmp(other)
+    }
+}
+
+impl<T: Ord> Ord for LinkedList<T> {
+    fn cmp(&self, other: &Self) -> Ordering {
+        self.iter().cmp(other)
+    }
+}
+
+
+unsafe impl<T: Send> Send for LinkedList<T> {}
+unsafe impl<T: Sync> Sync for LinkedList<T> {}
+
+unsafe impl<'a, T: Send> Send for Iter<'a, T> {}
+unsafe impl<'a, T: Sync> Sync for Iter<'a, T> {}
+
+unsafe impl<'a, T: Send> Send for IterMut<'a, T> {}
+unsafe impl<'a, T: Sync> Sync for IterMut<'a, T> {}
diff --git a/rust/kernel/lib.rs b/rust/kernel/lib.rs
index e7b8c318a..66d90cca5 100644
--- a/rust/kernel/lib.rs
+++ b/rust/kernel/lib.rs
@@ -25,7 +25,9 @@
     try_reserve,
     unsafe_cell_raw_get
 )]
-
+#![feature(destructuring_assignment)]
+#![feature(array_map)]
+#![feature(new_uninit)]
 // Ensure conditional compilation based on the kernel configuration works;
 // otherwise we may silently break things like initcall handling.
 #[cfg(not(CONFIG_RUST))]
@@ -100,9 +102,12 @@ pub mod kthread;
 pub mod ktime;
 pub mod dovetail;
 pub mod completion;
+pub mod memory_rros;
 #[cfg(CONFIG_NET)]
 pub mod net;
 
+pub mod double_linked_list3;
+
 #[doc(hidden)]
 pub use build_error::build_error;
 
diff --git a/rust/kernel/memory_rros.rs b/rust/kernel/memory_rros.rs
new file mode 100644
index 000000000..ccaef6ad2
--- /dev/null
+++ b/rust/kernel/memory_rros.rs
@@ -0,0 +1,644 @@
+use crate::{
+    rbtree::{RBTree, RBTreeNode},
+    prelude::*,sync::SpinLock,
+    vmalloc, mm, premmpt, spinlock_init, c_types,
+};
+use crate::{bindings, Result};
+use core::{borrow::BorrowMut, mem::size_of, mem::zeroed, ptr::addr_of_mut, ptr::addr_of};
+use crate::timekeeping::*;
+
+const PAGE_SIZE: u32 = 4096 as u32;
+const EVL_HEAP_PAGE_SHIFT: u32 = 9; /* 2^9 => 512 bytes */
+const EVL_HEAP_PAGE_SIZE: u32 =	(1 << EVL_HEAP_PAGE_SHIFT);
+const EVL_HEAP_PAGE_MASK: u32 =	(!(EVL_HEAP_PAGE_SIZE - 1));
+const EVL_HEAP_MIN_LOG2: u32 = 	4; /* 16 bytes */
+/*
+ * Use bucketed memory for sizes between 2^EVL_HEAP_MIN_LOG2 and
+ * 2^(EVL_HEAP_PAGE_SHIFT-1).
+ */
+const EVL_HEAP_MAX_BUCKETS: u32 = (EVL_HEAP_PAGE_SHIFT - EVL_HEAP_MIN_LOG2);
+const EVL_HEAP_MIN_ALIGN: u32 = (1 << EVL_HEAP_MIN_LOG2);//16
+/* Maximum size of a heap (4Gb - PAGE_SIZE). */
+const EVL_HEAP_MAX_HEAPSZ: u32 = (4294967295 - PAGE_SIZE + 1);
+/* Bits we need for encoding a page # */
+const EVL_HEAP_PGENT_BITS: u32 = (32 - EVL_HEAP_PAGE_SHIFT);
+/* Each page is represented by a page map entry. */
+// const EVL_HEAP_PGMAP_BYTES	sizeof(struct evl_heap_pgentry)
+const CONFIG_EVL_NR_THREADS: usize = 256;
+const CONFIG_EVL_NR_MONITORS: usize = 512;
+pub type size_t = usize;
+
+extern "C" {
+    fn rust_helper_rb_link_node(
+        node: *mut bindings::rb_node,
+        parent: *const bindings::rb_node,
+        rb_link: *mut *mut bindings::rb_node,
+    );
+    fn rust_helper_ilog2(
+        size: size_t
+    ) -> c_types::c_int;
+    fn rust_helper_align(
+        x: size_t,
+        a: u32,
+    ) -> c_types::c_ulong;
+    fn rust_helper_ffs(
+        x: u32,
+    ) -> c_types::c_int;
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_alloc(size: usize, align: usize) -> *mut u8 {
+    // pr_info!("__rros_sys_heap_alloc: begin");
+    unsafe {
+        evl_system_heap.evl_alloc_chunk(size).unwrap()
+    }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_dealloc(ptr: *mut u8, size: usize, align: usize) {
+    unsafe { evl_system_heap.evl_free_chunk(ptr); }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8 {
+    unsafe {
+        evl_system_heap.evl_realloc_chunk(ptr, old_size, new_size).unwrap()
+    }
+}
+
+#[no_mangle]
+pub fn __rros_sys_heap_alloc_zerod(size: usize, align: usize) -> *mut u8 {
+    unsafe {
+        evl_system_heap.evl_alloc_chunk_zeroed(size).unwrap()
+    }
+}
+
+struct evl_user_window {
+    state: u32,
+    info: u32,
+    pp_pending: u32,
+}
+
+#[repr(C)]
+union pginfo {
+    map: u32,
+    bsize: u32,
+}
+
+pub struct evl_heap_pgentry {
+    pub prev: u32,
+    pub next: u32,
+    pub page_type: u32,
+    pginfo: pginfo,
+}
+
+pub struct evl_heap_range {
+    pub addr_node: bindings::rb_node,
+    pub size_node: bindings::rb_node,
+    pub size: size_t,
+}
+
+pub fn new_evl_heap_range(addr: *mut u8, size: size_t) -> *mut evl_heap_range{
+    let addr = addr as *mut evl_heap_range;
+    unsafe{
+        (*addr).addr_node = bindings::rb_node::default();
+        (*addr).size_node = bindings::rb_node::default();
+        (*addr).size = size;
+    }
+    return addr;
+}
+
+#[inline]
+pub fn addr_add_size(addr: *mut u8, size: size_t) -> *mut u8 {
+    (addr as u64 + size as u64) as *mut u8
+}
+
+
+pub struct evl_heap {
+    pub membase: *mut u8,
+    pub addr_tree: Option<bindings::rb_root>, //根据首地址找大小
+    pub size_tree: Option<bindings::rb_root>, //根据大小找首地址
+    pub pagemap: Option<*mut evl_heap_pgentry>,
+    pub usable_size: size_t,
+    pub used_size: size_t,
+    pub buckets: [u32; EVL_HEAP_MAX_BUCKETS as usize],
+    pub lock: Option<SpinLock<i32>>,
+}
+
+impl evl_heap {
+    pub fn init(&mut self, membase: *mut u8, size: size_t) -> Result<usize>{
+        premmpt::running_inband()?;
+        mm::page_aligned(size)?;
+        if (size as u32) > EVL_HEAP_MAX_HEAPSZ {
+            return Err(crate::Error::EINVAL);
+        }
+
+        let mut spinlock = unsafe{ SpinLock::new(1) };
+        let pinned = unsafe { Pin::new_unchecked(&mut spinlock) };
+        spinlock_init!(pinned, "spinlock");
+        self.lock = Some(spinlock);
+        
+        for i in self.buckets.iter_mut() {
+            *i = u32::MAX;
+        }
+
+        let nrpages = size >> EVL_HEAP_PAGE_SHIFT;
+        let a: u64 = size_of::<evl_heap_pgentry>() as u64;
+        let kzalloc_res = vmalloc::c_kzalloc(a * nrpages as u64);
+        match kzalloc_res {
+            Some(x) => self.pagemap = Some(x as *mut evl_heap_pgentry),
+            None => {
+                return Err(crate::Error::ENOMEM);
+            }
+        }
+
+        self.membase = membase;
+        self.usable_size = size;
+        self.used_size = 0;
+        
+        self.size_tree = Some(bindings::rb_root::default());
+        self.addr_tree = Some(bindings::rb_root::default());
+        self.release_page_range(membase, size);
+        
+        Ok(0)
+    }
+
+    pub fn release_page_range(&mut self, page: *mut u8, size: size_t) {
+        let mut freed = page as *mut evl_heap_range;
+        let mut addr_linked = false;
+        
+        unsafe{ (*freed).size = size; }
+        let left_op = self.search_left_mergeable(freed);
+        match left_op {
+            Some(left) => {
+                let node_links = unsafe {addr_of_mut!((*left).size_node)};
+                let mut root = self.size_tree.as_mut().unwrap();
+                unsafe{bindings::rb_erase(node_links, root);}
+                unsafe{ (*left).size += (*freed).size; }
+                freed = left;
+                addr_linked = true;
+            },
+            None => (),
+        }
+        let right_op = self.search_right_mergeable(freed); // FIXME: 好像写错了
+        match right_op {
+            Some(right) => {
+                let mut node_links = unsafe{addr_of_mut!((*right).size_node)};
+                let mut root = self.size_tree.as_mut().unwrap();
+                unsafe{bindings::rb_erase(node_links, root);}
+                unsafe{ (*freed).size += (*right).size; }
+                node_links = unsafe{addr_of_mut!((*right).addr_node)};
+                root = self.addr_tree.as_mut().unwrap();
+                if addr_linked {
+                    unsafe{bindings::rb_erase(node_links, root)};
+                } else {
+                    let freed_node_links = unsafe{addr_of_mut!((*freed).addr_node)};
+                    unsafe{bindings::rb_replace_node(node_links, freed_node_links, root)};
+                }
+            },
+            None => {
+                self.insert_range_byaddr(freed);
+            },
+        }
+        // pr_info!("release_page_range: 4");
+        self.insert_range_bysize(freed);
+        // pr_info!("release_page_range: 5");
+    }
+
+    pub fn search_left_mergeable(&self, r: *mut evl_heap_range) -> Option<*mut evl_heap_range> {
+        let mut node: *mut bindings::rb_node = self.addr_tree.clone().unwrap().rb_node;
+        while !node.is_null() {
+            let p = crate::container_of!(node, evl_heap_range, addr_node);
+            unsafe {
+                if addr_add_size(p as *mut u8, (*p).size) as u64 == r as u64 {
+                    return Some(p as *mut evl_heap_range);
+                }
+                let addr_node_addr = addr_of_mut!((*r).addr_node);
+                if (addr_node_addr as u64) < (node as u64) {
+                    node = (*node).rb_left;
+                }else {
+                    node = (*node).rb_right;
+                }
+            }
+        }
+        None
+    }
+
+    pub fn search_right_mergeable(&self, r: *mut evl_heap_range) -> Option<*mut evl_heap_range> {
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+
+    pub fn insert_range_byaddr(&mut self, r: *mut evl_heap_range) {
+        unsafe {
+            let node_links = addr_of_mut!((*r).addr_node);
+            let mut root = self.addr_tree.as_mut().unwrap();
+            let mut new_link: &mut *mut bindings::rb_node = &mut root.rb_node;
+            let mut parent = core::ptr::null_mut();
+            while !new_link.is_null() {
+                let p = crate::container_of!(*new_link, evl_heap_range, addr_node);
+                parent = *new_link;
+                if (r as u64) < (p as u64) {
+                    new_link = &mut (*parent).rb_left;
+                } else {            
+                    new_link = &mut (*parent).rb_right;
+                }
+            }
+            rust_helper_rb_link_node(node_links, parent, new_link);
+            bindings::rb_insert_color(node_links, root);
+        }
+    }
+
+    pub fn insert_range_bysize(&mut self, r: *mut evl_heap_range) {
+        unsafe {
+            let node_links = addr_of_mut!((*r).size_node);
+            let mut root = self.size_tree.as_mut().unwrap();
+            let mut new_link: &mut *mut bindings::rb_node = &mut root.rb_node;
+            let mut parent = core::ptr::null_mut();
+            // TODO: YOUR CODE HERE
+
+            // END OF YOUR CODE
+            rust_helper_rb_link_node(node_links, parent, new_link);
+            bindings::rb_insert_color(node_links, root);
+        }
+    }
+
+    #[no_mangle]
+    pub fn evl_alloc_chunk(&mut self, size: size_t) -> Option<*mut u8> {
+        let mut log2size:i32 =0;
+        let mut ilog: i32 = 0;
+        let mut pg: i32 = 0;
+        let mut b: i32 = -1;
+        let mut flags: u32 = 0;
+        let mut bsize: size_t = 0;
+        
+        let mut block: Option<*mut u8>;
+        if size == 0 {
+            return None;
+        }
+        // 计算bsize和log2size的值
+        // TODO: YOUR CODE HERE
+
+        // END OF YOUR CODE
+
+        //上锁
+        if bsize >= (EVL_HEAP_PAGE_SIZE as usize) {
+            block = self.add_free_range(bsize, 0);
+        } else {
+            ilog = log2size - EVL_HEAP_MIN_LOG2 as i32;
+            pg = self.buckets[ilog as usize] as i32;
+            unsafe{
+                if pg < 0 {
+                    block = self.add_free_range(bsize, log2size);
+                } else {
+                    let pagemap = self.get_pagemap(pg);
+                    if (*pagemap).pginfo.map == u32::MAX {
+                        block = self.add_free_range(bsize, log2size);
+                    } else {
+                        let x = !(*pagemap).pginfo.map; // FIXME
+                        b = rust_helper_ffs(x) - 1;
+                        (*pagemap).pginfo.map |= (1<< b);
+                        self.used_size += bsize; // FIXME: used_size
+                        block = Some(addr_add_size(self.membase, ((pg << EVL_HEAP_PAGE_SHIFT) + (b << log2size)) as size_t));
+                        if (*pagemap).pginfo.map == u32::MAX {
+                            self.move_page_back(pg, log2size);
+                        }
+                    }
+                }
+            }
+        }
+        //解锁
+        return block;
+    }
+
+    //将申请的内存空间初始化为0
+    pub fn evl_alloc_chunk_zeroed(&mut self, size: size_t) -> Option<*mut u8> {
+        let block = self.evl_alloc_chunk(size);
+        match block {
+            Some(x) => {
+                unsafe{bindings::memset(x as *mut c_types::c_void, 0, size as c_types::c_ulong)};
+                return Some(x);
+            },
+            None => return None,
+        }
+        None
+    }
+
+    //重新分配空间
+    pub fn evl_realloc_chunk(&mut self, raw: *mut u8, old_size: size_t, new_size: size_t) -> Option<*mut u8> {
+        //开辟新空间
+        let ptr_op = self.evl_alloc_chunk(new_size);
+        match ptr_op {
+            Some(ptr) => {
+                unsafe{ bindings::memcpy(ptr as *mut c_types::c_void, raw as *mut c_types::c_void, old_size as c_types::c_ulong)  };
+                self.evl_free_chunk(raw);
+                return Some(ptr);
+            },
+            None => return None,
+        }
+        None
+    }
+    
+    #[inline]
+    fn addr_to_pagenr(&mut self, p: *mut u8) -> i32 {
+        ( (p as u32 - self.membase as u32) >> EVL_HEAP_PAGE_SHIFT ) as i32
+    }
+
+    fn add_free_range(&mut self, bsize:size_t, log2size: i32) -> Option<*mut u8> {
+        let pg_op = self.reserve_page_range(unsafe{rust_helper_align(bsize, EVL_HEAP_PAGE_SIZE)} as size_t);
+        let pg: i32;
+        match pg_op {
+            Some(x) => {
+                if x < 0 {
+                    return None;
+                }
+                pg = x;
+            },
+            None => return None,
+        }
+        
+        let pagemap = self.get_pagemap(pg);
+        if log2size != 0 {
+            unsafe {
+                (*pagemap).page_type = log2size as u32;
+                (*pagemap).pginfo.map = !gen_block_mask(log2size) | 1;
+                self.add_page_front(pg, log2size);
+            }
+        } else {
+            unsafe {
+                (*pagemap).page_type = 2;
+                (*pagemap).pginfo.bsize = bsize as u32;
+            }
+        }
+
+        self.used_size += bsize;
+        return Some(self.pagenr_to_addr(pg));
+    }
+
+    #[inline]
+    fn pagenr_to_addr(&mut self, pg: i32) -> *mut u8 {
+        addr_add_size(self.membase, (pg as size_t) << EVL_HEAP_PAGE_SHIFT) as *mut u8
+    }
+
+    pub fn search_size_ge(&mut self, size: size_t) -> Option<*mut evl_heap_range> {
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+
+    pub fn reserve_page_range(&mut self, size: size_t) -> Option<i32> {
+        let new_op = self.search_size_ge(size);
+        let mut new;
+        match new_op {
+            Some(x) => new = x,
+            None => return None,
+        }
+        let mut node_links = unsafe{addr_of_mut!((*new).size_node)};
+        let mut root = self.size_tree.as_mut().unwrap();
+        unsafe{bindings::rb_erase(node_links, root)};
+
+        // 如果大小相同
+        if (unsafe{(*new).size == size}) { 
+            // TODO: YOUR CODE HERE
+            return None;
+            // END OF YOUR CODE
+
+        }
+        // 否则需要裁剪
+        // TODO: YOUR CODE HERE
+        None
+        // END OF YOUR CODE
+    }
+    
+    pub fn move_page_back(&mut self, pg: i32, log2size: i32) {
+        let old = self.get_pagemap(pg); // 获取对应的页
+        // TODO: YOUR CODE HERE
+        // END OF YOUR CODE
+    }
+    
+    fn move_page_front(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+
+        if self.buckets[ilog as usize] == (pg as u32) {
+            return;
+        }
+        
+        self.remove_page(pg, log2size);
+        self.add_page_front(pg, log2size);
+    }
+    
+    fn remove_page(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+        let old = self.get_pagemap(pg);
+        if pg == unsafe{(*old).next as i32} {
+            self.buckets[ilog as usize] = u32::MAX;
+        } else {
+            if pg == (self.buckets[ilog as usize] as i32) {
+                self.buckets[ilog as usize] = unsafe{(*old).next};
+            }
+            unsafe {
+                let prev = self.get_pagemap((*old).prev as i32);
+                (*prev).next = (*old).next;
+                let next = self.get_pagemap((*old).next as i32);
+                (*next).prev = (*old).prev;
+            }
+        }
+    }
+
+    pub fn add_page_front(&mut self, pg: i32, log2size: i32) {
+        let ilog = (log2size as u32) - EVL_HEAP_MIN_LOG2;
+        // pr_info!("add_page_front: ilog is {}",ilog);
+        let new = self.get_pagemap(pg);
+        if self.buckets[ilog as usize] == u32::MAX {
+            self.buckets[ilog as usize] = pg as u32;
+            unsafe {
+                (*new).prev = pg as u32;
+                (*new).next = pg as u32;
+            }
+            // pr_info!("add_page_front: pg is {}",pg);
+        } else {
+            let head = self.get_pagemap(self.buckets[ilog as usize] as i32);
+            unsafe {
+                // (*new).prev = self.buckets[ilog as usize];
+                // (*new).next = (*head).next;
+                // let next = self.get_pagemap((*new).next as i32);
+                // (*next).prev = pg as u32;
+                // (*next).next = pg as u32;
+
+                (*new).prev = (*head).prev;
+                (*new).next = self.buckets[ilog as usize];
+                let next = self.get_pagemap((*new).next as i32);
+                (*next).prev = pg as u32;
+                if (*next).next == self.buckets[ilog as usize]{
+                    (*next).next = pg as u32;
+                }
+                self.buckets[ilog as usize] = pg as u32;
+            }
+        }
+    }
+
+    #[inline]
+    pub fn get_pagemap(&self, pg: i32) -> *mut evl_heap_pgentry {
+        addr_add_size(self.pagemap.clone().unwrap() as *mut u8, 
+                        ((pg as u32) * (size_of::<evl_heap_pgentry>() as u32)) as size_t) 
+                            as *mut evl_heap_pgentry
+    }
+
+    pub fn evl_free_chunk(&mut self, block: *mut u8) {
+        let pgoff = ((block as usize) - (self.membase as usize)) as u32;
+	    let pg = (pgoff >> EVL_HEAP_PAGE_SHIFT) as i32;
+        let pagemap = self.get_pagemap(pg);
+        let page_type = unsafe { (*pagemap).page_type };
+        let bsize: size_t;
+        if page_type == 0x2 {
+            bsize = unsafe{ (*pagemap).pginfo.bsize as usize };
+            let addr = self.pagenr_to_addr(pg);
+            self.release_page_range(addr, bsize);
+        } else {
+            let log2size = page_type as i32;
+            bsize = (1 << log2size);
+            let boff = pgoff & !EVL_HEAP_PAGE_MASK;
+            if (boff & ((bsize - 1) as u32)) != 0 {
+                //解锁
+                //raw_spin_unlock_irqrestore(&heap->lock, flags);
+                panic!()
+            }
+            // 上面的代码是判断偏移量是否正确
+            // 移除一个fast block
+            // TODO: YOUR CODE HERE
+            let n = boff >> log2size;
+            let oldmap = unsafe{ (*pagemap).pginfo.map };
+            unsafe{ (*pagemap).pginfo.map &= !((1 as u32) << n) };
+            if unsafe{ (*pagemap).pginfo.map == !gen_block_mask(log2size) } {
+                self.remove_page(pg, log2size);
+                let addr = self.pagenr_to_addr(pg);
+                self.release_page_range(addr, EVL_HEAP_PAGE_SIZE as size_t);
+            } else if oldmap == u32::MAX {
+                self.move_page_front(pg, log2size);
+            }
+            // END OF YOUR CODE
+        }
+        self.used_size -= bsize;
+	    //raw_spin_unlock_irqrestore(&heap->lock, flags);
+    }
+
+    pub fn evl_destroy_heap(&mut self) {
+        let res = premmpt::running_inband();
+        match res {
+            Err(_) => {
+                pr_info!("warning: evl_destroy_heap not inband");
+            },
+            Ok(_) => (),
+        }
+        vmalloc::c_kzfree(self.pagemap.clone().unwrap() as *const c_types::c_void);
+    }
+
+}
+
+pub fn cleanup_shared_heap() {
+    unsafe {
+        evl_shared_heap.evl_destroy_heap();
+        vmalloc::c_vfree(evl_shared_heap.membase as *const c_types::c_void);
+    }
+}
+
+pub fn cleanup_system_heap() {
+    unsafe {
+        evl_system_heap.evl_destroy_heap();
+        vmalloc::c_vfree(evl_system_heap.membase as *const c_types::c_void);
+    }
+}
+
+pub fn gen_block_mask(log2size: i32) -> u32 {
+    return u32::MAX >> (32 - (EVL_HEAP_PAGE_SIZE >> log2size));
+} 
+
+pub static mut evl_system_heap: evl_heap = evl_heap {
+    membase: 0 as *mut u8,
+    addr_tree: None,
+    size_tree: None,
+    pagemap: None,
+    usable_size: 0,
+    used_size: 0,
+    buckets: [0; EVL_HEAP_MAX_BUCKETS as usize],
+    lock: None,
+};
+
+pub static mut evl_shared_heap: evl_heap = evl_heap {
+    membase: 0 as *mut u8,
+    addr_tree: None,
+    size_tree: None,
+    pagemap: None,
+    usable_size: 0,
+    used_size: 0,
+    buckets: [0; EVL_HEAP_MAX_BUCKETS as usize],
+    lock: None,
+};
+
+static mut evl_shm_size: usize = 0;
+
+pub fn init_system_heap() -> Result<usize> {
+    let size = 2048 * 1024;
+    let system = vmalloc::c_vmalloc(size as c_types::c_ulong);
+    match system {
+        Some(x) => {
+            let ret = unsafe{evl_system_heap.init(x as *mut u8, size as usize)};
+            match ret {
+                Err(_) => {
+                    vmalloc::c_vfree(x);
+                    return Err(crate::Error::ENOMEM);
+                },
+                Ok(_) => (),
+            }
+        },
+        None => return Err(crate::Error::ENOMEM),
+    }
+    pr_info!("rros_mem: init_system_heap success");
+    Ok(0)
+}
+
+pub fn init_shared_heap() -> Result<usize> {
+    let mut size: usize = CONFIG_EVL_NR_THREADS * size_of::<evl_user_window>()
+        + CONFIG_EVL_NR_MONITORS * 40;
+    size = mm::page_align(size)?;
+    mm::page_aligned(size)?;
+    let shared = vmalloc::c_kzalloc(size as u64);
+    match shared {
+        Some(x) => {
+            let ret = unsafe{evl_shared_heap.init(x as *mut u8, size as usize)};
+            match ret {
+                Err(_e) => {
+                    vmalloc::c_kzfree(x);
+                    return Err(_e);
+                },
+                Ok(_) => (),
+            }
+        },
+        None => return Err(crate::Error::ENOMEM),
+    }
+    unsafe{ evl_shm_size = size };
+    Ok(0)
+}
+
+pub fn evl_init_memory() -> Result<usize> {
+    let mut ret = init_system_heap();
+    match ret {
+        Err(_) => return ret,
+        Ok(_) => (),
+    }
+    ret = init_shared_heap();
+    match ret {
+        Err(_) => {
+            cleanup_system_heap();
+            return ret;
+        },
+        Ok(_) => (),
+    }
+    Ok(0)
+}
+
+pub fn evl_cleanup_memory() {
+    cleanup_shared_heap();
+	cleanup_system_heap();
+}
\ No newline at end of file
diff --git a/scripts/Makefile.build b/scripts/Makefile.build
index bb22acf84..b31db2a9d 100644
--- a/scripts/Makefile.build
+++ b/scripts/Makefile.build
@@ -300,7 +300,7 @@ quiet_cmd_rustc_o_rs = $(RUSTC_OR_CLIPPY_QUIET) $(quiet_modtag) $@
       cmd_rustc_o_rs = \
 	RUST_MODFILE=$(modfile) \
 	$(RUSTC_OR_CLIPPY) $(rust_flags) $(rust_cross_flags) \
-		-Zallow-features=allocator_api,bench_black_box,concat_idents,global_asm,try_reserve \
+		-Zallow-features=allocator_api,bench_black_box,concat_idents,global_asm,try_reserve,destructuring_assignment,array_map,new_uninit,array_map \
 		--extern alloc --extern kernel \
 		--crate-type rlib --out-dir $(obj) -L $(objtree)/rust/ \
 		--crate-name $(patsubst %.o,%,$(notdir $@)) $<; \
diff --git a/test1.py b/test1.py
index cd738511a..d5144adc6 100644
--- a/test1.py
+++ b/test1.py
@@ -2,8 +2,8 @@ import sys
 import re
 import os
 import threading
-times = 3000
-thread_nums = 10
+times = 10000
+thread_nums = 20
 error_nums = 0
 mutex = threading.Lock()
 def handler():
-- 
2.34.1

